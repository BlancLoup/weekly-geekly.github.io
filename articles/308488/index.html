<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Using trigrams for correcting recognition results</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The figure shows a diagram of 8 possible trigrams taken from the book [1] 





 Natural languages ‚Äã‚Äãcan be characterized by the frequency distributio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Using trigrams for correcting recognition results</h1><div class="post__text post__text-html js-mediator-article"><p></p><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/files/805/dab/635/805dab6358894feebd58b3f432a2194b.png"></div><br>  <i>The figure shows a diagram of 8 possible trigrams taken from the book [1]</i> <p></p><br><br><p>  Natural languages ‚Äã‚Äãcan be characterized by the frequency distribution of the occurrence of their elements, such as words, single letters, or sequences of letters ( <em>N-</em> programs).  Formally, an <em>N</em> -gram is a string of <em>N</em> characters belonging to a certain alphabet consisting of a finite number of characters.  On the theoretical and applied issues of using the <em>N-</em> grams for automatic text correction can be found in [2]. <br></p><br><br><p>  In this article, we will consider only the alphabet consisting of letters of the Russian language, then we will talk about the possibilities of using trigrams (sequences of three characters) for post-processing (correction to reduce errors) recognition results of Russian-language documents.  The recognition mechanism was convolutional neural networks. <br></p><br><a name="habracut"></a><br><h3>  1. Description of the trigram model </h3><br><p>  A trigram model consists of a set (dictionary) of triples of characters { <em>g <sub>i</sub></em> <sub>1</sub> , <em>g <sub>i</sub></em> <sub>2</sub> , <em>g <sub>i</sub></em> <sub>3</sub> } and the weight <em>w <sub>i</sub></em> assigned to them.  Trigrams are ordered by weight, more weight means a higher frequency of occurrence of a trigram in a natural language, more precisely in a certain body of texts in a natural language. <br></p>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>  In the ideal case, by the weight of the trigram we mean the probability of its occurrence in natural language.  Of course, in various samples of natural language texts, we can receive differing trigram weights.  As an example, we can consider the texts of classical literature and tables containing the abbreviations of products, it is obvious that for these cases the occurrence of trigrams will vary greatly.  At the same time, extracting trigrams from the corpus of homogeneous words makes it possible to obtain stable estimates of trigram weights. <br></p><br><p>  We will tell about the extraction of trigrams from large sets of surnames, names and patronymic names of citizens of the Russian Federation and about the use of the found trigrams for correcting the recognition results of the ‚ÄúLast Name‚Äù, ‚ÄúFirst Name‚Äù and ‚ÄúPatronymic‚Äù fields in the passport of a citizen of the Russian Federation.  Earlier we talked about our software solutions for passport recognition [3, 4], in which the results of this article can be applied. <br></p><br><p>  We had three corpus words at our disposal: dictionaries of surnames, names and patronymic names of citizens of the Russian Federation.  Trigrams were extracted from each of the bodies.  For each word { <em>c</em> <sub>1</sub> , <em>c</em> <sub>2</sub> , ..., <em>c <sub>n</sub></em> } in which <em>c <sub>i is the</sub></em> symbol of the Russian language (upper and lower case letters were not distinguished), two dummy characters <em>c</em> <sub>F</sub> were added to the beginning and end of the word: { <em>c</em> <sub>F</sub> , <em>c</em> <sub>F</sub> , <em>c</em> <sub>1</sub> , <em>c</em> <sub>2</sub> , ..., <em>c <sub>n</sub></em> <em>,</em> <br>  <em>c</em> <sub>F</sub> , <em>c</em> <sub>F</sub> }.  This was done so that each of the <em>c <sub>i</sub></em> characters could be matched with trigrams &lt; <em>c <sub>i</sub></em> <sub>-2</sub> , <em>c <sub>i</sub></em> <sub>-1</sub> , <em>c <sub>i</sub></em> &gt; and &lt; <em>c <sub>i</sub></em> <em>,</em> <em>c <sub>i</sub></em> <sub>+1</sub> , <em>c <sub>i</sub></em> <sub>+ 2</sub> &gt;, for example, <em>c</em> <sub>1</sub> You can match the trigram &lt; <em>c</em> <sub>F</sub> , <em>c</em> <sub>F</sub> , <em>c</em> <sub>1</sub> &gt;.  Next, we will consider words with <em>c</em> <sub>F</sub> added to the alphabet.  For each triple of characters &lt; <em>g</em> <sub>1</sub> , <em>g</em> <sub>2</sub> , <em>g</em> <sub>3</sub> &gt; we calculated the number of copies of this triple in all words of the corpus, normalized it to the number of possible trigrams and took the normalized value as the weight (the sum of the normalized weights <em>Œ∏</em> ( <em>g</em> <sub>1</sub> , <em>g</em> <sub>2</sub> , <em>g</em> <sub>3</sub> ) is equal to one). <br></p><br><p>  The large body of names used (more than 1,500,000 names in random order, of which there are about 183,000 different names), collected earlier by us from open sources, allowed us to construct a trigram dictionary with weights that depend little on the size of the sample of words.  For example, consider the trigrams that are often found in last names. <br></p><br><p>  {O, B, A}, {K, O, B}, {H, O, B}, {I, H, A}, {E, B, A}, <br></p><br><p>  weights of which we calculate on samples of different sizes, all samples are taken from the corpus of last names.  Figure 1 shows the graphs of the weights of these trigrams versus the sample size, which characterize the stability of determining the weights of these trigrams with a sample size exceeding 50,000 words. <br></p><br><p>  We draw attention to the fact that the most common trigram for the corpus of last names are {A, <em>c</em> <sub>F</sub> <sub>,</sub> <br>  <em>c</em> <sub>F</sub> } and {B, A, <em>c</em> <sub>F</sub> } with weights of 0.044 and 0.033, respectively. <br></p><br><p><img src="https://habrastorage.org/files/cc7/a9a/dab/cc7a9adabd0e4da7aa4dbb78f3b3f71b.jpg"><br></p><br><p>  Figure 1. Graphs of examples of trigrams weights depending on sample size <br></p><br><p>  Similarly, trigram dictionaries for names and patronymic names were constructed; in them, when using samples of words of a large amount of weight, frequently occurring trigrams are also calculated stably. <br></p><br><h3>  2. Description of recognition results </h3><br><p>  The results of character recognition are represented by alternatives ( <em>c <sub>j</sub></em> , <em>p <sub>j</sub></em> ): <br></p><br><br><p>  ( <em>c</em> <sub>1</sub> , <em>p</em> <sub>1</sub> ), ( <em>c</em> <sub>2</sub> , <em>p</em> <em><sub>2</sub></em> ), ..., ( <em>c <sub>n</sub></em> , <em>p <sub>q</sub></em> ), </p><br><br><p>  where <em>c <sub>j</sub></em> is the character code, and <em>p <sub>j</sub></em> is the reliability rating of character recognition (hereinafter, recognition scores).  If <em>p <sub>j</sub></em> = 1, then the symbol <em>c <sub>j is</sub></em> recognized with the greatest reliability, and if <em>p <sub>j</sub></em> = 0, then with the least reliability.  In this case, <em>c <sub>j</sub></em> belongs to the recognition alphabet consisting of <em>q</em> characters.  All recognition scores have normalization: <br></p><br><br><p>  0‚â§ <em>p</em> <em><sub>j</sub></em> ‚â§1 <br></p><br><br><p><img src="https://habrastorage.org/files/5d3/e2b/4e8/5d3e2b4e8baf45c4b61ba49bf1d08d0e.png"><br></p><br><br><p>  Then the results of recognition of a word consisting of <em>n</em> characters (familiarity) are as follows: <br></p><br><br><p>  ( <em>c</em> <sup>1</sup> <sub>1</sub> , <em>p</em> <sup>1</sup> <sub>1</sub> ), ( <em>c</em> <sup>1</sup> <sub>2</sub> , <em>p</em> <sup>1</sup> <sub>2</sub> ), ..., ( <em>c</em> <sup>1</sup> <em><sub>n</sub> ,</em> <em>p</em> <sup>1</sup> <em><sub>n</sub></em> ) <br></p><br><p>  ( <em>c</em> <sup>2</sup> <sub>1</sub> , <em>p</em> <sup>2</sup> <sub>1</sub> ), ( <em>c</em> <sup>2</sup> <sub>2</sub> , <em>p</em> <sup>2</sup> <sub>2</sub> ), ..., ( <em>c</em> <sup>2</sup> <em><sub>n</sub> ,</em> <em>p</em> <sup>2</sup> <em><sub>n</sub></em> ) <br></p><br><p>  ... </p><br><p>  ( <em>c <sup>q</sup></em> <sub>1</sub> , <em>p <sup>q</sup></em> <sub>1</sub> ), ( <em>c <sup>q</sup></em> <sub>2</sub> , <em>p <sup>q</sup></em> <sub>2</sub> ), ..., ( <em>c <sup>q</sup> <sub>n</sub></em> <em>,</em> <em>p <sup>q</sup> <sub>n</sub></em> ). <br></p><br><br><p>  The character recognition mechanism may be wrong.  The recognized word is considered recognized with errors or without errors depending on the difference or coincidence of the well-known ‚Äúideal‚Äù word <em>c</em> <sup>*</sup> <sub>1</sub> <em>c</em> <sup>*</sup> <sub>2</sub> ... <em>c</em> <sup>*</sup> <em><sub>n</sub></em> , which was previously specified by the person, with the sequence of characters <em>c</em> <sup>1</sup> <sub>1</sub> <em>c</em> <sup>1</sup> <sub>2</sub> ... <em>c</em> <sup>1</sup> <em><sub>n</sub></em> extracted from the first alternatives {( <em>c</em> <sup>1</sup> <sub>1</sub> , <em>p</em> <sup>1</sup> <sub>1</sub> ), ( <em>c</em> <sup>1</sup> <sub>2</sub> , <em>p</em> <sup>1</sup> <sub>2</sub> ), ... ( <em>c</em> <sup>1</sup> <em><sub>n</sub></em> <em>,</em> <em>p</em> <sup>1</sup> <em><sub>n</sub></em> )}.  We define for the set of recognized words the recognition accuracy as the fraction of words recognized without errors. <br></p><br><br><p>  Below is an example of the results of the recognition of the names of "EPIFANOV": <br></p><br><br><p><img src="https://habrastorage.org/files/8c7/fed/5c9/8c7fed5c9b3d4336807b4488b4f76674.jpg"></p><br><br><p>  In the example, one familiarity (the letter ‚ÄúH‚Äù) is recognized with an error, so the entire word will be considered as recognized as a mistake. <br></p><br><br><p>  For the experiments described below, two test data sets were used: <em>T</em> <sub>1</sub> (2,722 words, consisting of 22,103 characters with low quality of digitization and, respectively, with low recognition accuracy) and <em>T</em> <sub>2</sub> (2,354 words, consisting of 19,230 characters with average quality of digitization and more reliable recognition).  Set <em>T1</em> consisted of 896 examples of surnames, 915 examples of names, 911 examples of patronymic names, and set <em>T</em> <sub>2</sub> of 776 examples of surnames, 794 examples of names and 784 examples of patronymic names, respectively. <br></p><br><br><p>  It was written above that the recognition scores <em>p</em> <em><sup>k</sup></em> <em><sub>j were</sub></em> interpreted as probabilities.  Let us look at the <em>T</em> <sub>1</sub> and <em>T</em> <sub>2</sub> sets, how the recognition scores <em>p</em> <em><sup>k</sup></em> <em><sub>j</sub></em> correlate with the number of errors that occurred.  To do this, we construct a histogram of the number of errors of our recognition mechanism that happened to characters with estimates of <em>p</em> <em><sup>k</sup></em> <em><sub>j</sub></em> from a certain range of the interval [0,1].  For clarity, we divide the interval [0,1] into 32 intervals [0, 1/32), [1/32, 2/32), ... [31/32, 1] and for the set of recognized characters with estimates of <em>p <sup>k</sup> <sub>j</sub></em> from the interval [ <em>d</em> / 32, ( <em>d</em> +1) / 32) calculate the number of mistakenly recognized characters.  This histogram is shown in Figure 2, the type of histogram argues in favor of interpreting the recognition scores <em>p</em> <em><sup>k</sup></em> <em><sub>j</sub></em> as probabilities, since there are many errors for small values ‚Äã‚Äãof estimates, and few errors for large values ‚Äã‚Äã(close to 1).  More precisely, for large values ‚Äã‚Äãof recognition scores (more than 26/32), there are no errors at all.  We did not do more accurate studies of the correlation of the recognition scores and the probabilities (frequencies) of the occurrence of errors due to the limited volume of the <em>T</em> <sub>1</sub> and <em>T</em> <sub>2</sub> sets. <br></p><br><br><p><img src="https://habrastorage.org/files/06e/1d4/4df/06e1d44dfaa1465fb1c4d886693ff929.jpg"><br></p><br><p>  Figure 2. Histogram of the number of recognition errors depending on the recognition scores <br></p><br><br><h3>  3. Algorithms for the correction of recognition results using trigrams </h3><br><br><p>  We describe a simple algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  for the correction of recognition results. <br></p><br><br><p>  For all characters <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> for 1‚â§ <em>i</em> ‚â§ <em>n</em> , 1‚â§ <em>k</em> ‚â§ <em>q</em> (i.e., for all characters that are not fictitious), consider the two preceding characters <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> <sub>-2</sub> , <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> <sub>-1</sub> , which are considered already selected (fixed) and using ready-made trigrams &lt; <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> <sub>-2</sub> , <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> <sub>-1</sub> , <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> &gt;, containing the symbol <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> , we calculate the new estimate: <br></p><br><br><p><img src="https://habrastorage.org/files/600/f15/3b7/600f153b7ca948778081ed9ce4edb58d.jpg"></p><br><br><p>  sort alternatives by new values <img src="https://habrastorage.org/files/0be/981/704/0be981704c8b4aae8e670af909186ce6.png">  <em><sup>k</sup></em> <em><sub>i</sub></em> and fix the character to correct the next character <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> <em><sub>+</sub></em> <sub>1</sub> .  That is, first we recalculate the values ‚Äã‚Äãof the estimates for the first character of the word, taking into account the two dummy characters that we consider to be chosen, and fix the first character.  After recalculating the ratings of the next character, we move on to the next character and so on. <br></p><br><br><p>  The formulas used assumptions that <em>Œ∏</em> ( <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> <sub>-2</sub> , <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> <sub>-1</sub> , <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> ) and <em>p</em> <em><sup>k</sup></em> <em><sub>i</sub></em> are probabilities.  We expected from the algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  Correction of some recognition errors due to the fact that incorrect and rarely encountered triples of recognized characters <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> <sub>-2</sub> , <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> <sub>-1</sub> , <em>with</em> <em><sup>k</sup></em> <em><sub>i</sub></em> will be replaced with those that occur more frequently. <br></p><br><br><p>  Consider the results of the algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  On sets <em>T</em> <sub>1</sub> and <em>T</em> <sub>2</sub> , summarized in Tables 1 and 2. These and subsequent similar tables contain data on words that were recognized with errors, and on unmistakably recognized words and used the following characteristics: <br></p><br><br><p>  <em>n</em> <sub>1</sub> is the number of words without errors both in the initial recognition and when using the trigram mechanism; <br></p><br><br><p>  <em>n</em> <sub>2</sub> - the number of words with errors in the original recognition, not corrected by trigrams; <br></p><br><br><p>  <em>n</em> <sub>3</sub> - the number of words with errors in the original recognition, corrected trigram <br></p><br><br><p>  <em>n</em> <sub>4</sub> - the number of words without errors in the original recognition, in which the use of trigrams introduced errors. <br></p><br><br><p>  Table 1. The results of the algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  on test set <em>T</em> <sub>1</sub> <br></p><br><br><table><tbody><tr><td></td><td><p>  Surname <br></p><br></td><td><p>  Name <br></p><br></td><td><p>  middle name <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>1</sub> <br></p><br></td><td><p>  811 (90.51%) <br></p><br></td><td><p>  840 (91.80%) <br></p><br></td><td><p>  827 (90.78%) <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>2</sub> <br></p><br></td><td><p>  44 (4.91%) <br></p><br></td><td><p>  28 (3.06%) <br></p><br></td><td><p>  31 (3.40%) <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>3</sub> <br></p><br></td><td><p>  36 (4.02%) <br></p><br></td><td><p>  38 (4.15%) <br></p><br></td><td><p>  48 (5.27%) <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>4</sub> <br></p><br></td><td><p>  5 (0.56%) <br></p><br></td><td><p>  9 (0.98%) <br></p><br></td><td><p>  5 (0.55%) <br></p><br></td></tr></tbody></table><br><br><p>  Table 2. The results of the algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  on test set <em>T</em> <sub>2</sub> <br></p><br><br><table><tbody><tr><td></td><td><p>  Surname <br></p><br></td><td><p>  Name <br></p><br></td><td><p>  middle name <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>1</sub> <br></p><br></td><td><p>  736 (94.85%) <br></p><br></td><td><p>  760 (95.72%) <br></p><br></td><td><p>  747 (95.28%) <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>2</sub> <br></p><br></td><td><p>  16 (2.06%) <br></p><br></td><td><p>  7 (0.88%) <br></p><br></td><td><p>  9 (1.15%) <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>3</sub> <br></p><br></td><td><p>  21 (2.71%) <br></p><br></td><td><p>  24 (3.02%) <br></p><br></td><td><p>  26 (3.32%) <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>4</sub> <br></p><br></td><td><p>  3 (0.39%) <br></p><br></td><td><p>  3 (0.38%) <br></p><br></td><td><p>  2 (0.26%) <br></p><br></td></tr></tbody></table><br><br><p> From the tables it follows that the adjustment algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  It gives interesting results, because trigrams correct more errors than they introduce.  At the same time, the question arises, how can we explain the dependence of the character of a word on the two previous characters?  This question remains unanswered; this dependence cannot be justified.  However, the adopted dependence contributed to the creation of a very simple algorithm. <br></p><br><br><p>  We will not expose the described algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  Further criticism, we only note that you can come up with many similar simple algorithms, for example, with the dependence of a symbol on two consecutive or on two adjacent symbols, and these algorithms will also give acceptable results. <br></p><br><br><p>  We set a goal to develop an algorithm without involving the conditions of dependence of characters on neighboring symbols, assuming that such an algorithm, being more rigorous, would work better than <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  .  Designed algorithm <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg">  adjusting recognition results using trigrams is based on calculations of marginal distributions and Bayesian networks. <br></p><br><br><p>  For a word of <i>n</i> characters, we associate a random variable with each familiarity <img src="https://habrastorage.org/files/c3e/b9c/3d6/c3eb9c3d62b2413480fb7a4ed96c3742.png">  with values ‚Äã‚Äãin finite state space <img src="https://habrastorage.org/files/44a/0fd/7a2/44a0fd7a294a4705bf7fd842ca99444a.png">  and suppose that <img src="https://habrastorage.org/files/44a/0fd/7a2/44a0fd7a294a4705bf7fd842ca99444a.png">  is the recognition alphabet <i>A</i> , that is, the set of letters of the Russian language, in which uppercase and lowercase letters do not differ.  Denote by <i>N</i> = {1, ..., <i>n</i> } the set of indices.  Name the piece <img src="https://habrastorage.org/files/3be/cd6/edd/3becd6edd24f457d883019b96c78eb64.png">  space of finite vector configurations <img src="https://habrastorage.org/files/d81/01e/f6b/d8101ef6b9624c31ae7ca58a051d1de8.png">  .  Next, build the graph <img src="https://habrastorage.org/files/211/f1e/c88/211f1ec8854a4dd5b5fe2c216ec4ef43.png">  according to the following rule.  Lots of <img src="https://habrastorage.org/files/654/ba4/ad7/654ba4ad7229434e9837d98bd703b804.png">  is a set of vertices of the graph.  We assume that with each triple of vertices &lt; <em>x <sub>j</sub></em> , <em>x <sub>j</sub></em> <sub>+1</sub> , <em>x <sub>j</sub></em> <sub>+2</sub> &gt; there is associated a normalized weighting function of trigrams <img src="https://habrastorage.org/files/566/104/475/566104475d034dc38265f5bdbcbd06bb.png">  .  Connect the edges of the top of the triple with each other, the set of obtained edges forms a set <img src="https://habrastorage.org/files/c1b/c49/a01/c1bc49a01f8a4865a52142cdbbf79c3f.png">  . <br></p><br><br><p>  We write the formula for the joint distribution of probabilities </p><br><br><p><img src="https://habrastorage.org/files/0b6/492/ba5/0b6492ba59ec4895b33190bb2d0ea628.png">  / <img src="https://habrastorage.org/files/8e5/e29/9dc/8e5e299dc58946b6a2f2fcd535eb6ccc.png"><br></p><br><br><p>  Where <img src="https://habrastorage.org/files/50f/616/e80/50f616e80ce242f3b95423613b9632e1.png">  - the function of recognition scores for the <em>i-</em> th familiarity, <img src="https://habrastorage.org/files/859/0fa/5e8/8590fa5e88e54c9e8e730ffbbed5f3df.png">  Is the normalized function of trigram weights. <br></p><br><br><p>  To recalculate the estimates, it is enough to calculate marginal distributions. <br></p><br><br><p><img src="https://habrastorage.org/files/dc3/568/871/dc356887193a443787f9a9ecccfcd40c.png">  . <br></p><br><br><p>  We implemented the calculation of marginal distributions using the HUGIN algorithm [5, 6].  Without disclosing implementation details, we point out that this algorithm is significantly more complex than the algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  . <br></p><br><br><p>  The results of the algorithm <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg">  on the same sets of <em>T</em> <sub>1</sub> and <em>T</em> <sub>2</sub> are summarized in tables 3 and 4, while the results of the algorithm are added to the tables <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  for comparison.  Also in Tables 3 and 4, the accuracy characteristics of the algorithms are added. <br></p><br><br><p>  Table 3. The results of the algorithms <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  and <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg">  on test set <em>T</em> <sub>1</sub> <br></p><br><br><table><tbody><tr><td></td><td colspan="2"><p>  Surname <br></p><br></td><td colspan="2"><p>  Name <br></p><br></td><td colspan="2"><p>  middle name <br></p><br></td></tr><tr><td></td><td><p>  algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg"><br></p><br></td><td><p>  algorithm <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg"><br></p><br></td><td><p>  algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg"><br></p><br></td><td><p>  algorithm <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg"><br></p><br></td><td><p>  algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg"><br></p><br></td><td><p>  algorithm <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg"><br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>1</sub> <br></p><br></td><td><p>  811 (90.51%) <br></p><br></td><td><p>  815 (90.96%) <br></p><br></td><td><p>  840 (91.80%) <br></p><br></td><td><p>  845 (92.35%) <br></p><br></td><td><p>  827 (90.78%) <br></p><br></td><td><p>  831 (91.22%) <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>2</sub> <br></p><br></td><td><p>  44 (4.91%) <br></p><br></td><td><p>  38 (4.24%) <br></p><br></td><td><p>  28 (3.06%) <br></p><br></td><td><p>  30 (3.28%) <br></p><br></td><td><p>  31 (3.40%) <br></p><br></td><td><p>  21 (2.31%) <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>3</sub> <br></p><br></td><td><p>  36 (4.02%) <br></p><br></td><td><p>  42 (4.69%) <br></p><br></td><td><p>  38 (4.15%) <br></p><br></td><td><p>  36 (3.93%) <br></p><br></td><td><p>  48 (5.27%) <br></p><br></td><td><p>  58 (6.37%) <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>4</sub> <br></p><br></td><td><p>  5 (0.56%) <br></p><br></td><td><p>  1 (0.11%) <br></p><br></td><td><p>  9 (0.98%) <br></p><br></td><td><p>  4 (0.44%) <br></p><br></td><td><p>  5 (0.55%) <br></p><br></td><td><p>  1 (0.11%) <br></p><br></td></tr><tr><td><p>  Source recognition accuracy <br></p><br></td><td colspan="2"><p>  91.07% <br></p><br></td><td colspan="2"><p>  92.79% <br></p><br></td><td colspan="2"><p>  91.33% <br></p><br></td></tr><tr><td><p>  Trigram Accuracy <br></p><br></td><td><p>  94.53% <br></p><br></td><td><p>  95.65% <br></p><br></td><td><p>  95.96% <br></p><br></td><td><p>  96.28% <br></p><br></td><td><p>  96.05%) <br></p><br></td><td><p>  97.59% <br></p><br></td></tr><tr><td><p>  The gain in accuracy when using trigrams <br></p><br></td><td><p>  3.46% <br></p><br></td><td><p>  4.58% <br></p><br></td><td><p>  3.17% <br></p><br></td><td><p>  3.50% <br></p><br></td><td><p>  4.72% <br></p><br></td><td><p>  6.26% <br></p><br></td></tr></tbody></table><br><br><p>  Table 4. The results of the algorithms <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  and <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg">  on test set <em>T</em> <sub>2</sub> <br></p><br><br><table><tbody><tr><td></td><td colspan="2"><p>  Surname <br></p><br></td><td colspan="2"><p>  Name <br></p><br></td><td colspan="2"><p>  middle name <br></p><br></td></tr><tr><td></td><td><p>  algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg"><br></p><br></td><td><p>  algorithm <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg"><br></p><br></td><td><p>  algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg"><br></p><br></td><td><p>  algorithm <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg"><br></p><br></td><td><p>  algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg"><br></p><br></td><td><p>  algorithm <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg"><br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>1</sub> <br></p><br></td><td><p>  736 (94.85%) <br></p><br></td><td><p>  737 (94.97%) <br></p><br></td><td><p>  760 (95.72%) <br></p><br></td><td><p>  762 (95.97%) <br></p><br></td><td><p>  747 (95.28%) <br></p><br></td><td><p>  749 (95.54%) <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>2</sub> <br></p><br></td><td><p>  16 (2.06%) <br></p><br></td><td><p>  12 (1.55%) <br></p><br></td><td><p>  7 (0.88%) <br></p><br></td><td><p>  5 (0.63%) <br></p><br></td><td><p>  9 (1.15%) <br></p><br></td><td><p>  8 (1.02%) <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>3</sub> <br></p><br></td><td><p>  21 (2.71%) <br></p><br></td><td><p>  25 (3.22%) <br></p><br></td><td><p>  24 (3.02%) <br></p><br></td><td><p>  26 (3.27%) <br></p><br></td><td><p>  26 (3.32%) <br></p><br></td><td><p>  27 (3.44%) <br></p><br></td></tr><tr><td><p>  <em>n</em> <sub>4</sub> <br></p><br></td><td><p>  3 (0.39%) <br></p><br></td><td><p>  2 (0.26%) <br></p><br></td><td><p>  3 (0.38%) <br></p><br></td><td><p>  1 (0.13%) <br></p><br></td><td><p>  2 (0.26%) <br></p><br></td><td><p>  0 (0.0%) <br></p><br></td></tr><tr><td><p>  Source recognition accuracy <br></p><br></td><td colspan="2"><p>  95.23% <br></p><br></td><td colspan="2"><p>  96.10% <br></p><br></td><td colspan="2"><p>  95.54% <br></p><br></td></tr><tr><td><p>  Trigram Accuracy <br></p><br></td><td><p>  97.55% <br></p><br></td><td><p>  98.20% <br></p><br></td><td><p>  98.74%) <br></p><br></td><td><p>  788 (99.24%) <br></p><br></td><td><p>  773 (98.60%) <br></p><br></td><td><p>  776 (98.98%) <br></p><br></td></tr><tr><td><p>  The gain in accuracy when using trigrams <br></p><br></td><td><p>  2.32% <br></p><br></td><td><p>  2.96% <br></p><br></td><td><p>  2.64% <br></p><br></td><td><p>  3.15% <br></p><br></td><td><p>  3.06% <br></p><br></td><td><p>  3.44% <br></p><br></td></tr></tbody></table><br><br><h3>  4. Discussion of the results of the algorithms </h3><br><p>  From Tables 3 and 4, it follows that on the <em>T</em> <sub>1</sub> and <em>T</em> <sub>2</sub> sets both algorithms significantly improve the initial recognition results, with low digitization quality, the recognition accuracy can be increased by more than 6%.  In all cases considered, the algorithm <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg">  gives better results than algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  . <br></p><br><br><p>  The disadvantage of the algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  is the use of an unsubstantiated hypothesis about the dependence of a symbol on the two previous symbols. <br></p><br><br><p>  The disadvantage of the algorithm <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg">  is significantly lower speed compared to the speed of the algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  - more than 100 times.  However, it should be noted that the algorithm <img src="https://habrastorage.org/files/66a/8da/992/66a8da99267d4fd8818625a56c685cd1.jpg">  being very simple, it processes one word in about 0.001 seconds. <br></p><br><br><p>  In some cases, the use of trigrams leads to a deterioration of the correct recognition results.  Basically, such cases are due to the following two reasons: <br></p><br><ul><li>  the original recognition did not give a very confident result with low marks for the first alternative, and trigrams derived from other alternatives have more weight (for example, in the word ‚ÄúDINAR‚Äù familiarity with the ‚ÄúD‚Äù symbol has a low score of 0.57, and this leads to to the ‚ÄúL‚Äù symbol due to the fact that the weight of the ‚ÄúDIN‚Äù trigram is approximately equal to 0.0001, and the weight of the ‚ÄúLIN‚Äù trigram is approximately equal to 0.0034), </li><li>  There are no trigrams in the trigram dictionary, which are necessary for confirming triples of characters in rare names, surnames or patronymic names (for example, the trigram dictionary does not contain the ‚Äú‚Äù trigram, causing the word ‚ÄúAZRWARD‚Äù to be replaced with ‚ÄúAZRAARD‚Äù). </li></ul><br><br><p>  The distribution of the number of recognition errors depending on the recognition scores after the application of trigrams also looks ‚Äúimproved‚Äù.  Figure 3 shows the histogram calculated after applying trigrams using the algorithm <img src="https://habrastorage.org/files/184/8fe/b37/1848feb37d8c4051a081895d0dbf4b16.jpg">  , in comparison with the original histogram.  It is seen that in almost all the intervals of the estimates the number of errors is reduced. <br></p><br><br><p><img src="https://habrastorage.org/files/290/752/5f4/2907525f43f8435d8aff8092974f06d4.jpg"><br></p><br><p>  Figure 3. Histogram of the number of recognition errors depending on the recognition scores before (blue) and after (red) the application of trigrams <br></p><br><br><h3>  Conclusion </h3><br><p>  We showed a method for correcting recognized documents using the trigram mechanism using the example of passport fields (full name) of a citizen of the Russian Federation.  Two algorithms of such a correction and the main problems of their work were described.  It is essential to have reasonable reliability estimates, formed by the recognition mechanism, and a representative sample of words for constructing a trigram vocabulary. <br></p><br><br><p>  The described algorithms are used to correct recognition results in running software products Smart Engines. <br></p><br><br><h3>  List of useful sources </h3><br><ol><li>  Shchutsky Yu. K. Chinese Classical "Book of Changes".  - 2nd ed.  corrected  and add.  / Ed.  A.I. Kobzev.  - M .: Science, 1993. </li><li>  Kukich, K. Techniques for Automatically Correcting Words in Text.  ACM computing survey, Computational Linguistic, V. 24, No.  4, 377-439.  1992 </li><li>  "Passport scanner with your own hands" ( <a href="https://habrahabr.ru/company/smartengines/blog/278257/">https://habrahabr.ru/company/smartengines/blog/278257/</a> ) </li><li>  "Recognition of the Passport of the Russian Federation on a mobile phone" ( <a href="https://habrahabr.ru/company/smartengines/blog/252703/">https://habrahabr.ru/company/smartengines/blog/252703/</a> ) </li><li>  R. Cowell, P. Dawid, S. Lauritzen, D. Spiegelhalter.  Probabilistic Networks and Expert Systems.  Springer, 1999. </li><li>  Michael I. Jordan.  An introduction to probabilistic graphical models.  Manuscript used for Class Notes of CS281A at UC Berkeley, Fall 2002. </li></ol></div><p>Source: <a href="https://habr.com/ru/post/308488/">https://habr.com/ru/post/308488/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../308474/index.html">Fujitsu World Tour 2016: Moving to Digital Transformation</a></li>
<li><a href="../308480/index.html">Instantly launch almost any OS under Linux using libvirt + qemu</a></li>
<li><a href="../308482/index.html">Blockchain and FINTECH-Hakaton at Innopolis University</a></li>
<li><a href="../308484/index.html">Static and dynamic typing</a></li>
<li><a href="../308486/index.html">From the experience of using SObjectizer: are the actors in the form of finite automata - is it bad or good?</a></li>
<li><a href="../308490/index.html">Unexpected behavior of the WinAPI function IsWow64Process ()</a></li>
<li><a href="../308494/index.html">How to give an adequate estimate of the time when uncertainty hits the head</a></li>
<li><a href="../308498/index.html">Air Berlin: Progressive Web App Implementation</a></li>
<li><a href="../308500/index.html">Navigator 2GIS: Extrapolation of the position of the car</a></li>
<li><a href="../308504/index.html">Over 9000: unobvious difficulties of working with counters of social buttons (+ task)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>