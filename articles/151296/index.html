<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Kinect for Windows SDK. Part 3. Functionalities</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="- Kinect for Windows SDK. Part 1. Sensor 
- Kinect for Windows SDK. Part 2. Data Flows 
- [Kinect for Windows SDK. Part 3. Functionality] 
- Playing c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Kinect for Windows SDK. Part 3. Functionalities</h1><div class="post__text post__text-html js-mediator-article"><ul><li>  <a href="http://habrahabr.ru/post/150955/">Kinect for Windows SDK.</a>  <a href="http://habrahabr.ru/post/150955/">Part 1. Sensor</a> </li><li>  <a href="http://habrahabr.ru/post/151131/">Kinect for Windows SDK.</a>  <a href="http://habrahabr.ru/post/151131/">Part 2. Data Flows</a> </li><li>  [Kinect for Windows SDK.  Part 3. Functionality] </li><li>  <a href="http://habrahabr.ru/post/142236/">Playing cubes with Kinect</a> </li><li>  <a href="http://habrahabr.ru/post/142677/">Program, aport!</a> </li></ul><br><h5>  Tracking the human figure </h5><br>  With such a great feature, Kinect is able to recognize the shape of a person and his movement.  And, in fact, not even one, but as many as six!  In the sense of determining that there are up to six people in the sensor's field of view, but only two can collect detailed information.  Take a look at the picture: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ef1/c83/9e2/ef1c839e2584235b65c0cc1c521fafcb.png" alt="image"><br><a name="habracut"></a><br>  Personally, my first question was: ‚ÄúWhy did he [Kinect] build the full 20-point skeleton for these two peppers, and the rest showed only navels?‚Äù two recognized figures build a detailed skeleton, the rest are satisfied with the fact that they were even noticed.  In MSDN, there is even <a href="http://msdn.microsoft.com/en-us/library/jj131025">an example of</a> how to change this behavior, for example, to build a detailed skeleton for people closest to the sensor. <br><br>  The points in the built skeleton are called Joint, which can be translated as a joint, joint, node.  The node seems to me a more adequate translation, and the head is called a joint, somehow not very good. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      So, the first thing that needs to be done in the application in order to receive information about the figures in the frame is to enable the necessary stream: <br><br><pre><code class="cs hljs"><span class="hljs-comment"><span class="hljs-comment">//          sensor.SkeletonFrameReady += SkeletonsReady; //   sensor.SkeletonStream.Enable();</span></span></code> </pre> <br>  The second is to handle the SkeletonFrameReady event.  All that remains to be done is to extract information about the figures of interest from the frame.  One figure - one object of the <a href="http://msdn.microsoft.com/en-us/library/microsoft.kinect.skeleton">Skeleton</a> class.  The object stores tracking status data ‚Äî the <a href="http://msdn.microsoft.com/en-us/library/microsoft.kinect.skeletontrackingstate">TrackingState</a> property (whether the complete skeleton is built or only the location of the shape is known), the data about the nodes of the shape is the <a href="http://msdn.microsoft.com/en-us/library/microsoft.kinect.jointcollection">Joints</a> property.  In essence, this is a dictionary whose keys are the values ‚Äã‚Äãof the <a href="http://msdn.microsoft.com/en-us/library/microsoft.kinect.jointtype">JointType</a> enumeration.  For example, you wanted to get the location of the left knee - nothing is easier! <br><br><pre> <code class="cs hljs">Joint kneeLeft = skeleton.Joints[JointType.KneeLeft]; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> x = kneeLeft.Position.X; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> y = kneeLeft.Position.Y; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> z = kneeLeft.Position.Z;</code> </pre><br>  The values ‚Äã‚Äãof the JointType enumeration are originally shown in the figure of the Vitruvian man. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7c5/0a6/4bc/7c50a64bcba9e7aa005b31225308ced6.png" alt="image"><br><br>  Before these lines, I wrote about a 20-node skeleton.  Build which is not always possible.  This is how a mode called seated <i><a href="http://msdn.microsoft.com/en-us/library/hh973077">skeletal tracking appeared</a></i> .  In this mode, the sensor builds an incomplete 10-node skeleton. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/196/6c9/e9e/1966c9e9ee7437d1ff9eb677383bb766.png" alt="image"><br><br>  In order for Kinect to recognize shapes in this mode, it is enough to set the <a href="http://msdn.microsoft.com/en-us/library/microsoft.kinect.skeletonstream.trackingmode">TrackingMode</a> property of the SkeletonStream object during stream initialization: <br><br><pre> <code class="cs hljs">kinect.SkeletonStream.TrackingMode = SkeletonTrackingMode.Seated;</code> </pre><br>  In the tracking mode of a sitting figure, the sensor can also recognize up to six figures and track two figures.  But there are also features.  For example, in order for the sensor to ‚Äúnotice‚Äù you need to move, wave your hands, while in the recognition mode of a complete skeleton, it is enough to stand in front of the sensor.  Tracking a seated figure is a more resource-intensive operation, so be prepared to reduce the FPS. <br><br>  Another article in the series - <a href="http://habrahabr.ru/post/142236/">Playing Kines with Kinect</a> , is entirely devoted to the subject of tracking a human figure. <br><br><h5>  Speech recognition </h5><br>  Strictly speaking, speech recognition is not a built-in Kinect feature, since it uses an additional SDK, and the sensor acts as an audio source.  Therefore, the development of speech recognition applications will require the installation of the <a href="http://www.microsoft.com/download/en/details.aspx%3Fid%3D27226">Microsoft Speech Platform</a> .  Optionally, you can install different language packs, and for client machines there is a separate package (speech platform runtime). <br><br>  In general, the use of speech platform is as follows: <br><ol><li>  select a recognition <i>engine</i> for the required language available in the system; </li><li>  create a dictionary and pass it to the selected handler.  Speaking humanly, you need to decide which words your application should be able to recognize and pass to the recognition processor as strings (there is no need to create audio files with the sound of each word); </li><li>  set the audio source for the handler.  It can be Kinect, microphone, audio file; </li><li>  give the handler a command to start recognition. </li></ol><br>  After that, it remains only to process the event that occurs whenever the handler recognizes the word.  An example of working with this platform can be found in the article <a href="http://habrahabr.ru/post/142677/">Program, Aport!</a> <br><br><h5>  Face tracking </h5><br>  In contrast to the shape <a href="http://msdn.microsoft.com/en-us/library/jj130970">tracking, face tracking is</a> fully implemented programmatically, based on data obtained from the video stream <i>(color stream)</i> and the data stream of the rangefinder <i>(depth stream)</i> .  Therefore, the resources of the client computer will depend on how quickly the tracking will work. <br><br>  It is worth noting that <i>face tracking</i> is not the same as <i>face recognition</i> .  It's funny, but in some articles I met exactly the stories that the facial recognition feature was implemented in Kinect.  So what is face tracking and where can it be useful? <br><br>  Face tracking is the tracking of a person‚Äôs face in a frame with the construction of a 87-node face pattern.  In MSDN <a href="http://msdn.microsoft.com/en-us/library/jj130970">it is said</a> that there is an opportunity to monitor several persons, but it does not say about the upper limit, it is probably equal to two (for so many people, the sensor can build an N node skeleton).  The functionality can be useful in games, so that your character <i>(avatar)</i> can convey the whole palette of emotions displayed on your face;  in applications that adapt to your mood (whiny or playful);  in face recognition applications, finally, or even emotions (Dr. Lightman?). <br><br>  So, the scheme.  Actually here it is (the scheme of my dreams): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f75/47d/209/f7547d2091a7bcd2ad8274a06bdeaf22.png" alt="image"><br><br>  In addition to these 87 nodes, you can get coordinates for another 13: the centers of the eyes, the nose, the corners of the lips, and the borders of the head.  The SDK can even build a 3D face mask, as shown in the following image: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d67/afe/b0e/d67afeb0e90ba3429dfd709eb6fec19e.jpg" alt="image"><br><br>  Now, armed with a common understanding of face tracking implemented by the Face Tracking SDK, it's time to get to know it better.  Face Tracking SDK is a COM library (FaceTrackLib.dll) that is included with the Developer Toolkit.  There is also a <a href="http://msdn.microsoft.com/en-us/library/jj131022">Microsoft.Kinect.Toolkit.FaceTracking</a> project wrapper <i>(wrapper)</i> that can be safely used in managed projects.  Unfortunately, it was not possible to find a description of the wrapper, except for the link provided (I think that while active development is underway and while the Face Tracking SDK is not included in the Kinect SDK, it remains only to expect the appearance of MSDN help). <br><br>  I will focus on only a few classes.  Central to the class is <b>FaceTracker</b> , oddly enough.  Its task is to initialize the handler <i>(engine)</i> tracking and tracking the movement of a person in the frame.  The overloaded Track method allows you to search for a person using data from a video camera and a rangefinder.  One of the method overloads takes a human figure - <a href="http://msdn.microsoft.com/en-us/library/microsoft.kinect.skeletonstream.trackingmode">Skeleton</a> , which has a positive effect on the speed and quality of the search.  The class <b>FaceModel</b> helps in building 3D models, and also deals with the transformation of models into the camera's coordinate system.  In the Microsoft.Kinect.Toolkit.FaceTracking project, in addition to the wrapper classes, you can find simpler, but equally useful types.  For example, the <b>FeaturePoint</b> enumeration describes all nodes in a face <b>map</b> (see the above figure with 87 dots). <br><br>  In general, the algorithm for using tracking may look like this: <br><ol><li>  select a sensor and turn on the video stream <i>(color stream)</i> , the rangefinder data stream <i>(depth stream)</i> and the figure tracking stream <i>(skeleton stream)</i> ; </li><li>  add the AllFramesReady sensor event handler, which occurs when the frames of all threads are ready for use; </li><li>  in the event handler, initialize FaceTracker (if this has not yet been done) to go over the shapes found in the frame and collect the constructed face schemes for them; </li><li>  process face patterns (for example, show a built 3D mask or determine the emotions of people in the frame). </li></ol><br>  I want to note that I deliberately do not provide code examples, since  I don‚Äôt want to give an example in a couple of lines, but I don‚Äôt want to overload the article with giant multi-line listings. <br><br>  Remember that the quality of finding a person in the frame depends on both the distance to the head and its position (inclinations).  Acceptable head tilts for the sensor are considered up-down ¬± 20 ¬∞, left-right ¬± 45 ¬∞, side tilt ¬± 45 ¬∞.  The optimal will be ¬± 10 ¬∞, ¬± 30 ¬∞ and ¬± 45 ¬∞ for tilt up-down, left-right and sideways, respectively (see <a href="http://msdn.microsoft.com/en-us/library/jj130970">3D Head Pose</a> ). <br><br><h5>  Kinect studio </h5><br>  When you first try to write something for Kinect, the feeling that something is missing does not leave for a minute.  And when for the hundredth time you set up breakpoints so that they work exactly with a certain gesture, and when for the hundredth time you make this gesture in front of the camera, then you understand what is really missing!  A simple emulator.  So that you can write down the necessary gestures, and then sit quietly and debug.  Oddly enough, the rays of good sent by developers around the world who have experienced development for Kinect have reached their goal.  The Developer Toolkit includes a tool called <a href="http://msdn.microsoft.com/en-us/library/hh855389">Kinect Studio</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/890/379/e5f/890379e5f10b91e2c758d37422d02e09.jpg" alt="Kinect studio"><br><br>  Kinect Studio can be viewed as a debugger or emulator.  Its role is extremely simple, help you write down the data received from the sensor and send it to your application.  You can play the recorded data again and again, and if you wish, save them to a file and return to debugging after a while. <br><br>  To get started with Kinect Studio, you <a href="http://msdn.microsoft.com/en-us/library/jj131051">connect</a> to the application and select a sensor.  Now everything is ready to start recording <i>(recording)</i> sensor data or injection <i>(injection) of the</i> stored data. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/739/5ae/dc3/7395aedc33ab247c0cf4fbd4ce7d51f8.png" alt="image"><br><br><h5>  Afterword </h5><br>  The review article, which was not even planned, underwent more than one change during its work, and as a result was divided into three parts.  In them, I tried to collect material that I found in the vast world wide web.  But still the main source of knowledge was and remains MSDN.  Now Kinect is more like a kitten who is just learning to crawl than a product that can be perceived not only as just for fun.  I myself treat him with a certain skepticism.  But who knows what will happen tomorrow.  Now Kinect shows good results in the gaming industry, and Kinect for Windows opens up room for creativity for developers around the world. </div><p>Source: <a href="https://habr.com/ru/post/151296/">https://habr.com/ru/post/151296/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../151289/index.html">Work vs Freelance vs Business</a></li>
<li><a href="../151290/index.html">New Yandex.Money Terms of Service - FAQ</a></li>
<li><a href="../151291/index.html">What will Apple show on September 12th?</a></li>
<li><a href="../151292/index.html">Easy way to fail testing</a></li>
<li><a href="../151294/index.html">The illusion of effective development: management</a></li>
<li><a href="../151297/index.html">In the screenshots of the game WoW found digital watermarks (userID, time, realm)</a></li>
<li><a href="../151299/index.html">Programmer's Day: how it became my holiday</a></li>
<li><a href="../151300/index.html">3D transformations using CSS</a></li>
<li><a href="../151302/index.html">Fighting RSI - Truly Ergonomic 209 Keyboard Review</a></li>
<li><a href="../151303/index.html"><? = date (z) -255?: 'Happy programmer day';</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>