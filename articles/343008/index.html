<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Development of sustainability strategies</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the previous article, I described several evolutionary strategies (ES) algorithms that help optimize the parameters of a function without the need ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Development of sustainability strategies</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/mc/ia/4y/mcia4ymddcwm0g3-uad-uwik-ee.jpeg"></div><br><p>  In the <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/">previous article,</a> I described several evolutionary strategies (ES) algorithms that help optimize the parameters of a function without the need to explicitly calculate gradients.  When solving learning problems with reinforcement (reinforcement learning, RL), these algorithms can be used to search for suitable sets of model parameters for a neural network agent (neural network agent).  In this article I will discuss the use of ES in some RL-tasks, and also describe the methods for finding more stable and stable policies. </p><a name="habracut"></a><br><h2 id="evolyucionnye-strategii-v-obuchenii-s-podkrepleniem">  Evolutionary learning strategies with reinforcement </h2><br><p>  Since RL-algorithms need to send a reinforcement signal (reward signal) to the agent each time, only the resulting cumulative reinforcement received by the agent after it has been run in the environment is important for these algorithms.  In many cases, we only know the output at the end of the task, for example, did the agent succeed, did the robot pick up the object, did the agent survive, and so on. In all these tasks, the ES can be more efficient than the traditional RL.  Below I have given a pseudocode in which the agent's run in the <a href="https://gym.openai.com/docs/">OpenAI Gym</a> environment is encapsulated.  Here we are only interested in cumulative reinforcement: </p><br><pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">rollout</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(agent, env)</span></span></span><span class="hljs-function">:</span></span> obs = env.reset() done = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> total_reward = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> done: a = agent.get_action(obs) obs, reward, done = env.step(a) total_reward += reward <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> total_reward</code> </pre> <br><p>  You can define <code>rollout</code> as a target function that maps the parameters of an agent model with fitness scores, and use an ES solver (solver) to find a suitable set of parameters, as described in the previous <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/">article</a> : </p><br><pre> <code class="hljs pgsql">env = gym.make(<span class="hljs-string"><span class="hljs-string">'worlddomination-v0'</span></span>) # use our favourite ES solver = EvolutionStrategy() <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: # ask the ES <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> give <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> params solutions = solver.ask() # <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> <span class="hljs-keyword"><span class="hljs-keyword">array</span></span> <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> hold the results fitlist = np.zeros(solver.popsize) # evaluate <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">each</span></span> given solution <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(solver.popsize): # init the agent <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> a solution agent = Agent(solutions[i]) # rollout env <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> this agent fitlist[i] = rollout(agent, env) # give scores results back <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> ES solver.tell(fitness_list) # <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> best param &amp; fitness <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> ES bestsol, bestfit = solver.result() # see <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> our task <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> solved <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> bestfit &gt; MY_REQUIREMENT: break</code> </pre> <br><h2 id="deterministicheskie-i-stohasticheskie-politiki">  Deterministic and stochastic policies </h2><br><p>  The result of monitoring the environment is the input data for the agent, and the output is its action in each step during the run inside the environment.  We can model the agent as we want and use the methods from the rules set in the code, decision trees and linear functions for recurrent neural networks.  In this article, I will use a simple feed-forward network with two hidden levels so that the result of the agent‚Äôs observation of the medium (vector <strong>x</strong> ) is directly translated into action (vector <strong>y</strong> ): </p><br><p>  h <sub>1</sub> = f <sub>h</sub> (W <sub>1</sub> x + b <sub>1</sub> ) <br>  h <sub>2</sub> = f <sub>h</sub> (W <sub>2</sub> h <sub>1</sub> + b <sub>2</sub> ) <br>  y = f <sub>out</sub> (W <sub>out</sub> h <sub>2</sub> + b <sub>out</sub> ) </p><br><p>  The activation functions of f <sub>h</sub> and f <sub>out</sub> can be <code>tanh</code> , <code>sigmoid</code> , <code>relu</code> or any other.  In all my experiments, I used <code>tanh</code> .  If there is such a need, then in the output level it is possible to take f <sub>out</sub> as the pass-through function without nonlinearities.  If we concatenate all the weights and error parameters (bias parameters) into a single vector W, then we see that the neural network described above is a deterministic function y = F (x, W).  Then you can use ES to find the solution W using the search cycle described in the previous article. </p><br><p>  And if we don‚Äôt want the agent‚Äôs policy to be deterministic?  For some tasks, even as simple as a stone-scissors-paper, the optimal policy is a random action.  That is, the agent must be trained in stochastic politics.  One of the ways to make y = F (x, W) into a stochastic policy is to make W random.  Each model parameter w <sub>i</sub> ‚ààW can be a random value derived from the normal distribution N (Œº <sub>i</sub> , œÉ <sub>i</sub> ). </p><br><p>  This kind of stochastic neural network is called a <em><a href="http://edwardlib.org/tutorials/bayesian-neural-network">Bayesian neural network</a></em> .  This is a network with a preliminary distribution of weights.  In our case, the parameters of the model for which we are looking for a solution are a set of vectors Œº and œÉ, not weights W. With each pass of the neural network, we from N (Œº, œÉI) get a new W. There are many <a href="https://arxiv.org/abs/1703.02910">interesting</a> works on <a href="">using</a> Bayes neural networks to solve different tasks, as well as the <a href="http://bayesiandeeplearning.org/">problems of</a> learning <a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.704.7138%26rep%3Drep1%26type%3Dpdf">these</a> networks.  ES can be used to directly find solutions of a stochastic policy by setting Œº and œÉ instead of W as the solution space. </p><br><p>  Stochastic networks are often found in works on RL.  For example, in the <a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization (PPO)</a> algorithm, the last level is a set of parameters Œº and œÉ, as well as an action selected from N (Œº, œÉI).  Adding <a href="https://arxiv.org/abs/1707.06347">noise</a> to the parameters can stimulate the agent to explore the environment and avoid a local optimum. </p><br><p>  I found that when an agent has to explore the environment, we often don‚Äôt need the vector W to be completely random - just bias is enough.  In the difficult tasks associated with the movement, for example in the environment of <a href="https://blog.openai.com/roboschool/">roboschool</a> , I often have to use ES to find a stochastic policy, in which only the error parameters are extracted from the normal distribution. </p><br><h2 id="razvivayuschiesya-politiki-ustoychivosti-dlya-dvunogogo-hodoka">  Emerging resilience policies for bipedal walker </h2><br><p>  This is one of the areas where ES is useful for finding sustainability policies.  I want to control the balance between data efficiency and policy sustainability over several random attempts.  I experienced ES in a great environment <a href="https://gym.openai.com/envs/BipedalWalkerHardcore-v2/">BipedalWalkerHardcore-v2</a> , developed by <a href="https://twitter.com/robo_skills">Oleg Klimov</a> .  The environment uses the physics engine <a href="">Box2D Physics Engine</a> , which was used in <a href="">Angry Birds</a> . </p><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: static; visibility: visible; display: block; transform: rotate(0deg); width: 500px; margin: 10px auto; max-width: 100%; min-width: 220px;" data-tweet-id="889215446150291458"></twitter-widget><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><p>  <em>Our agent decided <a href="https://gym.openai.com/envs/BipedalWalkerHardcore-v2/">BipedalWalkerHardcore-v2</a> .</em> </p><br><p>  In this environment, our agent studied the walking policy without falling over a randomly generated obstacle course with a time limit.  24 input signals were used: 10 lidar sensors, angles and contact with the surface.  The agent does not know where he is in the track.  The action space consists of four continuous values ‚Äã‚Äãcontrolling the torques of the four motors.  The total reinforcement (total reward) is calculated on the basis of the entire distance traveled by the agent.  If the agent passes the whole route, he earns more than 300 points.  True, some points will be deducted depending on the amount of applied torque, so energy consumption is also a limitation. </p><br><p>  In <a href="https://gym.openai.com/envs/BipedalWalkerHardcore-v2/">BipedalWalkerHardcore-v2, the</a> problem is considered <em>solved</em> if the agent scores an average of 300 or more points for 100 consecutive random attempts.  Although using the RL algorithm, it is relatively easy to train an agent to successfully complete the entire route, but it is much harder to get to pass efficiently and with a stable result.  The task turns out quite interesting.  As far as I know, as of October 2017, my agent passes the route best of all. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/5d3/c7e/08e/5d3c7e08e26693a5956523d0775444e9.gif" alt="image"><br>  <em>Start.</em>  <em>Learning to walk.</em> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/331/ed7/9f3/331ed79f3ff8564fed7437e6f109f7f7.gif" alt="image"><br>  <em>Learning to correct mistakes, but it turns out more slowly ...</em> </p><br><p>  Since for each attempt a new random route is generated, sometimes the route is easy and sometimes very difficult.  We do not need, in the course of natural selection, agents with weak politicians, who were lucky to be on the easy track, pass into the next generation.  In addition, agents with good policies should be able to prove that they are no worse than others.  So I took the <em>average</em> result of 16 random runs as an agency episode, and used the average value of accumulative reinforcements over 16 runs as a fitness score. </p><br><p>  You can look at it from the other side and see that although we tested the agent on 100 attempts, but the attempts were single, so the test task does not correspond to the training task for which we optimized our system.  If in the stochastic environment averaged repeatedly each agent in the population, then the gap between the training and test sets can be reduced.  If you can retrain under the training set, then you can retrain and under the test set, especially in RL with this <a href="https://twitter.com/jacobandreas/status/924356906344267776">there are no difficulties</a> :) </p><br><p>  Of course, the efficiency of the algorithm data is now 16 times worse.  But the final policy has become much more stable.  When I tested the final policy on 100 consecutive random attempts, it took an average of more than 300 points to go through this environment.  Without this averaging method, the best agent on 100 attempts could score ‚àº220‚Äì230 points.  As far as I know, our decision was the first to go through this Wednesday (as of October 2017). </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/421/405/f97/421405f977acf341175b936fb753e471.gif" alt="image"></p><br><p><br>  <em>The winning solution was studied using the <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/">PEPG</a> using an average of 16 runs per episode.</em> </p><br><p>  I also used <a href="https://arxiv.org/abs/1707.06347">PPO</a> , a great policy gradient algorithm for RL.  To the best of my ability, I tried to set it up so that it worked well in my task.  I managed to achieve only ‚àº240-250 points on 100 random attempts.  But I am sure that someone will still be able to use PPO or another RL-algorithm to go through this environment. </p><br><p>  In real situations, when we need secure policies, a useful and highly effective function is to manage the balance between data efficiency and policy sustainability.  Theoretically, if the computing power were enough, one could even average the data over the 100 necessary runs and optimize the bipedal walker right at the requirements level.  Professional engineers in device design often have to consider the requirements of the quality control service and safety factors.  They need to be taken into account when teaching agents to politicians who can influence the real world around them. </p><br><p>  Several solutions found by ES: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/620/7f0/2a1/6207f02a10e9097986bd50ee01f20f61.gif" alt="image"><br>  <em><a href="">CMA-ES</a> solution.</em> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/559/936/9ab/5599369ab2e7207129156bb826e294c4.gif" alt="image"><br>  <em><a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/">OpenAI-ES</a> solution.</em> </p><br><p>  I also trained the agent using a network that uses stochastic policies and has high initial noise parameters, so the agent saw noise everywhere, even his movements were noisy.  As a result, the agent was trained to solve the problem, even if there was no confidence in the accuracy of its input and output signals (although he could not score more than 300 points): </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/c64/3a2/3d4/c643a23d4588f9065a6a97c164a35e19.gif" alt="image"><br>  <em>Two-legged walker using stochastic policies.</em> </p><br><h2 id="hvatayuschiy-robot-manipulyator-kuka">  Kuka grab robot arm </h2><br><p>  I tried to use a combination of ES and averaging techniques to solve a simpler problem with the Kuka robot manipulator.  This medium is available in the <a href="https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym/pybullet_envs/bullet">pybullet</a> environment.  The <a href="https://www.kuka.com/en-de/products">Kuka</a> model used in the simulator corresponds to the real Kuka manipulator.  In this task, the agent gets the <a href="https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/kukaGymEnv.py">coordinates of the</a> object. </p><br><p>  In more advanced RL environments, an agent may be required to perform actions based on pixel input signals, but, in principle, we can combine this simplified model with a pre-trained convolutional neural network and then we can also calculate the coordinates. </p><br><p><img src="https://cdn.rawgit.com/hardmaru/pybullet_animations/8a6ccaf5/anim/kuka/kuka.gif" alt="image"><br>  <em>The task of grabbing the manipulator using a stochastic policy.</em> </p><br><p>  If an agent successfully grabs an object, then it receives 10,000 points, and if not, then 0. Part of points is deducted for energy consumption.  If we average reinforcements over 16 random attempts, we can optimize ES in terms of sustainability.  However, in the end, I was able to get a policy in which the object is grabbed in about 70-75% of cases under deterministic and stochastic policies.  There is still something to strive for. </p><br><h2 id="obuchaem-minitaur-neskolkim-zadacham">  We teach Minitaur several tasks </h2><br><p>  If we learn to simultaneously perform several complex tasks, then we begin to perform better and single tasks.  For example, Shaolin monks, lifting weights, standing on the tops of the columns, much better balance without burdening weights.  If you try not to spill water from a cup while driving a car at a speed of 140 km / h on a mountain road, you will be an excellent driver for illegal street racing.  We can also train agents to perform several tasks at once, and then agents will master more stable policies. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e59/2c3/e36/e592c3e36e5742348eac723a4a894891.gif" alt="image"><br>  <em>Shaolin agents.</em> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/0c0/7c2/039/0c07c2039ed402becdaaae9863c92800.gif" alt="image"><br>  <em>Drift training.</em> </p><br><p>  Recent work devoted to <a href="https://arxiv.org/abs/1710.03748">self-playing</a> agents proves that agents who have mastered difficult tasks like sumo wrestling (and this sport requires many skills) can perform simple tasks like wind resistance when walking without additional training.  <a href="https://twitter.com/erwincoumans/status/924352109511819264">Erwin Kumans</a> recently tried to experiment with adding a <a href="https://twitter.com/erwincoumans/status/924352109511819264">duck</a> on the lid of a Minitaur that is learning to walk.  If the duck falls, the task is not counted.  So we can hope that such additions to the task will help to translate the studied policies from the simulator to the real Minitaur.  I took one of the <a href="https://gist.github.com/erwincoumans/c579e076cbaf7c76caa9a42829408e2e">examples</a> and experimented with Minitaur and a duck, using ES for training. </p><br><p><img src="https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/minitaur/minitaur_faster.gif" alt="image"><br>  <em>CMA-ES <a href="http://pybullet.org/">pacing policy</a> in <a href="http://pybullet.org/">pybullet</a> .</em> </p><br><p><img src="https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/minitaur/real_minitaur.gif" alt="image"><br>  <em>A real Minitaur from <a href="https://www.ghostrobotics.io/">Ghost Robotics</a> .</em> </p><br><p>  The Minitaur model in <a href="https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/minitaur.py">pybullet</a> is modeled on the present Minitaur.  However, policies learned in an ideal virtual environment usually do not work in the real world.  She might not even be able to generalize small additions to the problem inside the simulator.  For example, in the previous video, Minitaur was trained to go forward (with the help of CMA-ES), but this policy does not always allow you to move a duck across the room if you put it in the simulator on top of a robot. </p><br><p><img src="https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/minitaur/duck_notrain.gif" alt="image"><br>  <em>The policy of walking works with a duck.</em> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/77a/e1f/072/77ae1f07205cdc860c8a08e304d40389.gif" alt="image"><br>  <em>The policy studied with the help of a duck.</em> </p><br><p>  Politics, studied on a simple walk without a duck, still somehow works when you put a duck on a robot, that is, this does not complicate the task too much.  The duck is stable, so Minitaur was not so hard not to drop it.  I tried to replace the duck with a ball in order to make the task very difficult. </p><br><p><img src="https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/minitaur/ball_cheating.gif" alt="image"></p><br><p><img src="https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/minitaur/ball_cheating.gif" alt="image"><br>  <em>Learning to cheat.</em> </p><br><p>  But this did not lead to the immediate emergence of a stable balancing policy.  Instead, the CMA-ES has developed a policy that technically allows you to move the ball, rolling it into the recess between the legs and holding it there.  It can be concluded that the algorithm, which seeks a solution in accordance with its goal (objective-driven search algorithm), learns to use any structural disadvantages of the environment in order to accomplish the task. </p><br><p><img src="https://cdn.rawgit.com/hardmaru/pybullet_animations/f6f7fcd7/anim/minitaur/ball_stoc.gif" alt="image"><br>  <em>Stochastic policy learned on the ball.</em> </p><br><p><img src="https://cdn.rawgit.com/hardmaru/pybullet_animations/8a6ccaf5/anim/minitaur/duck_stoc.gif" alt="image"><br>  <em>The same policy, but with a duck.</em> </p><br><p>  After I made the ball smaller, CMA-ES was able to find a stochastic policy that allows you to walk and at the same time balance the ball.  This policy has been transferred to a simpler duck task.  I hope that in the future such methods of additions to the tasks will be useful for transferring experience to real robots. </p><br><h2 id="estool">  Estool </h2><br><p>  One of the important features of ES is that calculations can be parallelized across several workers working in different execution threads on different CPU cores or even on <a href="https://blog.openai.com/evolution-strategies/">different machines</a> . </p><br><p>  Python's <a href="https://docs.python.org/2/library/multiprocessing.html">multiprocessing</a> library allows you to run processes in parallel.  I prefer to run separate Python processes for each task using the Message Passing Interface (MPI) and <a href="http://mpi4py.scipy.org/docs/">mpi4py</a> .  This allows you to bypass the <a href="https://ru.wikipedia.org/wiki/Global_Interpreter_Lock">global blocking of the interpreter</a> and be sure that each process will receive its own numpy and gym sandbox instances, which is important when it comes to initializing random number generators. </p><br><p><img src="https://cdn.rawgit.com/hardmaru/pybullet_animations/8a6ccaf5/anim/robo/roboschool.gif" alt="image"><br>  <em>Roboschool Hopper, Walker, Ant.</em> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/f7a/11a/f7a/f7a11af7abcfbb2a9630c0b93d505d52.gif" alt="image"><br>  <em>Roboschool Reacher.</em> </p><br><p>  <em>Agents with <code>estool</code> were trained in various <a href="https://blog.openai.com/roboschool/">roboschool</a> tasks.</em> </p><br><p>  I implemented a simple <code>estool</code> tool <code>estool</code> , using the <code>es.py</code> library described in the previous <a href="http://blog.otoro.net/2017/10/29/visual-evolution-strategies/">article,</a> trains simple feed-forward policy networks to perform GL-based RL tasks with a continuous control.  I used <code>estool</code> to simplify learning in all the experiments described above, as well as in various tasks with constant control inside the gym and roboschool.  <code>estool</code> uses MPI for distributed processing, so it doesn‚Äôt take a lot of gestures to spread workers across different machines. </p><br><h2 id="estool-s-pybullet">  ESTool with pybullet </h2><br><p>  <a href="https://github.com/hardmaru/estool/">Github</a> </p><br><p>  In addition to the gym and roboschool environments, <code>estool</code> works well with most <a href="http://pybullet.org/">pybullet</a> gym environments.  You can modify the existing pybullet-environment to get more suitable for your tasks.  For example, I effortlessly made a medium with Minitaur wearing a ball (in the <code>custom_envs</code> directory in the repository).  The ease of setting up environments makes it easy to check new ideas.  If you want to implement 3D models from other software packages like <a href="http://gazebosim.org/tutorials/%3Ftut%3Dros_urdf">ROS</a> or <a href="https://www.blender-models.com/model-downloads/mechanicalelectronical/robotics/id/star-wars-pit-droid/">Blender</a> , then you can create new interesting pybullet-environments and suggest others to go through them. </p><br><p>  Many models and environments in pybullet, for example Kuka and Minitaur, are modeled in the image and likeness of real robots to transfer current knowledge of the trained algorithms to them.  In fact, in many recent studies ( <a href="https://stanfordvl.github.io/ntp/">1</a> , <a href="https://sites.google.com/view/multi-task-domain-adaptation">2</a> , <a href="https://sermanet.github.io/imitate/">3</a> , <a href="https://research.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html">4</a> ), the pybullet is used to control knowledge transfer experiments. </p><br><p>  But in order to experiment with the transfer of knowledge from simulators to real devices, you do not need to acquire expensive robots.  In pybullet there is a <a href="https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/racecar.py">racecar</a> model, created based on the hardware open source <a href="https://mit-racecar.github.io/">MIT racecar</a> set.  There is even a pybullet environment that mounts a <a href="https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/racecarZEDGymEnv.py">virtual camera</a> on a virtual race car, so that the agent has a virtual screen as an input to the surveillance tool. </p><br><p>  We will first try a simpler version, where the car needs only to study the policy of moving to a giant ball.  In the <a href="https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/bullet/racecarGymEnv.py">RacecarBulletEnv-v0</a> environment, the agent receives the relative coordinates of the ball at the input, and at the output - continuous actions to control the speed of the motor and the direction of the rudder.  The task is simple, learning on a 2014 Macbook Pro (with an eight-core processor) takes 5 minutes (50 generations).  If you use <code>estool</code> , this command will start training in eight processes and assign 4 tasks to each process, resulting in 32 workers.  For policy development, CMA-ES is used: </p><br><pre> <code class="hljs mel"><span class="hljs-keyword"><span class="hljs-keyword">python</span></span> train.py bullet_racecar -o cma -n <span class="hljs-number"><span class="hljs-number">8</span></span> -t <span class="hljs-number"><span class="hljs-number">4</span></span></code> </pre> <br><p>  The current learning result, as well as the found model parameters, will be stored in the <code>log</code> subdirectory.  Run this command to visualize the agent in the environment using the best policy found: </p><br><pre> <code class="hljs pgsql">python model.py bullet_racecar <span class="hljs-keyword"><span class="hljs-keyword">log</span></span>/bullet_racecar.cma<span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.32</span></span>.best.json</code> </pre> <br><p><img src="https://habrastorage.org/getpro/habr/post_images/c2c/61b/028/c2c61b02830946aac2f5298834a1007a.gif" alt="image"><br>  <em>Wednesday pybullet racecar, based on <a href="https://mit-racecar.github.io/">MIT Racecar</a> .</em> </p><br><p>  In the simulator, the mouse cursor can move the ball and even move the car. </p><br><p>  Using the IPython notepad <code>plot_training_progress.ipynb</code> you can display the learning history of all generations of agents.  For each generation, you can see the best and worst scores, as well as the average result for the entire population. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/672/3e5/1e5/6723e51e5a8d1794893ff9523f465210.svg" alt="image"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/076/bb9/077/076bb907779afcbff97a83128d35e78b.svg" alt="image"></p><br><p>  The standard motion task is similar to that used in a roboschool like Inverted Pendulum.  Also in the pybullet are Hopper, Walker, HalfCheetah, Ant and Humanoid.  I developed a policy for Ant, which with the help of PEPG per hour reaches 3000 points on a multi-core machine, with a population of 256 agents: </p><br><pre> <code class="hljs mel"><span class="hljs-keyword"><span class="hljs-keyword">python</span></span> train.py bullet_ant -o pepg -n <span class="hljs-number"><span class="hljs-number">64</span></span> -t <span class="hljs-number"><span class="hljs-number">4</span></span></code> </pre> <br><p><img src="https://habrastorage.org/getpro/habr/post_images/f4e/7f6/3c0/f4e7f63c045e1b9f312565ed2b8e9b35.gif" alt="image"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/601/0ca/380/6010ca3807332624e00b28e45dc065ca.gif" alt="image"></p><br><p>  <em>An example of the run on <a href="https://github.com/bulletphysics/bullet3/blob/master/examples/pybullet/gym/pybullet_envs/gym_locomotion_envs.py">AntBulletEnv</a> .</em>  <em>With the help of <code>gym.wrappers.Monitor</code> you can record runs in MP4-video.</em> </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/292/f11/82c/292f1182c87ec0af88a3f98cc54402c3.svg" alt="image"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/292/f11/82c/292f1182c87ec0af88a3f98cc54402c3.svg" alt="image"></p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  In this article, I explained how to use ES to develop policies for agents of cross-cutting neural networks in order to perform various RL tasks with constant control defined by the gym interface.  I described the <code>estool</code> tool, it allows me using the MPI framework in a distributed computing environment to quickly test ES algorithms with different settings. </p><br><p>  So far I have described only the methods of teaching agents through trial and error.  This form of learning from scratch is called reinforcement learning without using models ( <em>model-free</em> ).  In the next article (if I write it at all) I will talk more about model-based learning, when the agent learns to use a previously trained model to perform the current task.  And yes, I will again use evolutionary algorithms. </p><br><h2 id="interesnye-ssylki">  Interesting links </h2><br><p>  <a href="https://github.com/hardmaru/estool">Estool</a> <br>  <a href="http://tuvalu.santafe.edu/~erica/stable.pdf">Stable or stable?</a>  <a href="http://tuvalu.santafe.edu/~erica/stable.pdf">What is the difference?</a> <br>  <a href="https://gym.openai.com/docs/">OpenAI Gym</a> documentation <br>  <a href="https://blog.openai.com/evolution-strategies/">Evolutionary strategies as a scalable alternative to reinforcement learning</a> <br>  <a href="http://edwardlib.org/">Edward - a library for probabilistic modeling, conclusions and criticism</a> <br> <a href="https://www.youtube.com/watch%3Fv%3DFD8l2vPU5FY">  </a> <br> <a href="https://gym.openai.com/envs/BipedalWalkerHardcore-v2/">BipedalWalkerHardcore-v2</a> <br> <a href="https://blog.openai.com/roboschool/">roboschool</a> <br> <a href="http://pybullet.org/">pybullet</a> <br> <a href="https://arxiv.org/abs/1710.03748">Emergent Complexity     </a> <br> <a href="https://arxiv.org/abs/1710.03748">GraspGAN</a> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/343008/">https://habr.com/ru/post/343008/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../342998/index.html">"Correct" command structure for DevOps</a></li>
<li><a href="../343000/index.html">Diagnostics of industrial electric motors and generators on the spectrum of current consumption and prevention of accidents</a></li>
<li><a href="../343002/index.html">Review of music software code defects. Part 4. Ardour</a></li>
<li><a href="../343004/index.html">The second wave that covered us. The standard that was waiting</a></li>
<li><a href="../343006/index.html">Russian AI Cup: Participant's Toolkit</a></li>
<li><a href="../343010/index.html">Renga BIM API</a></li>
<li><a href="../343012/index.html">Scrum is not only in development - we use a flexible methodology in organizing an IT festival for children and parents</a></li>
<li><a href="../343014/index.html">Slight discrepancy</a></li>
<li><a href="../343016/index.html">Ray casting in the web world today</a></li>
<li><a href="../343018/index.html">13 conclusions that I made, after 4 years of using Ext JS</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>