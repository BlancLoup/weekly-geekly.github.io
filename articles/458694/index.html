<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Is the Python GIL already dead?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! Next Monday, classes will begin in the new Python Developer group , which means that we have time to publish another interesting material, whic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Is the Python GIL already dead?</h1><div class="post__text post__text-html js-mediator-article">  Hello!  Next Monday, classes will begin in the new <a href="https://otus.pw/IYvU/">Python Developer group</a> , which means that we have time to publish another interesting material, which we will now do.  Enjoy your reading. <br><br><img src="https://habrastorage.org/webt/jb/cq/wj/jbcqwjrmctxos6x_uzhptngfd9y.png"><br><br>  Back in 2003, Intel released the new Pentium 4 ‚ÄúHT‚Äù processor.  This processor overclocked to 3GHz and supported hyperthreading technology. <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/9d/z1/es/9dz1esccmgms80liftaeolcqiui.jpeg"><br><br>  In the following years, Intel and AMD struggled to achieve the highest performance of desktop computers, increasing bus speed, L2 cache size and decreasing matrix size to minimize latency.  In 2004, the HT model with a frequency of 3 GHz was replaced by 580 Prescott models with overclocking to 4 GHz. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/AmwzUrL3vMc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  It seemed that to go ahead you just had to increase the clock frequency, but the new processors suffered from high power consumption and heat dissipation. <br><br>  Does your desktop processor have 4 GHz today?  It is unlikely, because the path to improved performance ultimately led through increased bus speed and an increase in the number of cores.  In 2006, Intel Core 2 replaced the Pentium 4 and had a much lower clock speed. <br><br>  In addition to the release of multi-core processors for a wide user audience in 2006, something else happened.  Python 2.5 finally saw the light!  It has already been delivered with the beta version of the with keyword, which you all know and love. <br><br>  Python 2.5 had one major limitation when it came to using Intel Core 2 or AMD Athlon X2. <br>  It was a GIL. <br><br><h2>  What is GIL? </h2><br>  GIL (Global Interpreter Lock) is a Boolean value in the Python interpreter, protected by a mutex.  The blocking is used in the main CPython bytecode calculation loop to determine which stream is currently executing instructions. <br><br>  CPython supports the use of multiple streams in the same interpreter, but streams must request access to GIL in order to perform low-level operations.  In turn, this means that Python developers can use asynchronous code, multithreading and no longer worry about blocking any variables or processor-level faults during deadlocks. <br><br>  GIL simplifies multithreaded programming in Python. <br><br><img src="https://habrastorage.org/webt/lg/yz/3h/lgyz3hoq07fkumzp4axuuiqxplk.gif"><br><br>  GIL also tells us that while CPython can be multi-threaded, only one thread can be executed at any time.  This means that your quad-core processor does something like this (except for the blue screen, I hope). <br><br>  The current version of GIL <a href="https://github.com/python/cpython/commit/074e5ed974be65fbcfe75a4c0529dbc53f13446f">was written in 2009</a> to support asynchronous functions and remained intact even after many attempts to remove it in principle or change the requirements for it. <br><br>  Any suggestion to remove GIL was justified by the fact that global interpreter locking should not degrade the performance of a single-threaded code.  Anyone who tried to include hyper-threading in 2003 will understand what <a href="https://arstechnica.com/features/2002/10/hyperthreading/">I'm talking about</a> . <br><br><h2>  Refusal from GIL in CPython </h2><br>  If you really want to parallelize the code on CPython, you will have to use several processes. <br><br>  In CPython 2.6, the <i><a href="https://docs.python.org/2/library/multiprocessing.html">multiprocessing</a></i> module has been added to the standard library.  Multiprocessing disguised the spawning of processes in CPython (each process with its own GIL). <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> multiprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Process <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">f</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(name)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'hello'</span></span>, name <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: p = Process(target=f, args=(<span class="hljs-string"><span class="hljs-string">'bob'</span></span>,)) p.start() p.join()</code> </pre> <br><br>  Processes are created, commands are sent to them using compiled modules and Python functions, and then they are attached to the main process again. <br><br>  Multiprocessing also supports the use of variables through a queue or channel.  It has a lock object, which is used to lock objects in the main process and write from other processes. <br><br>  Multiprocessing has one major drawback.  It carries a significant computational load, which affects both processing time and memory usage.  The launch time of CPython even without no-site is 100-200 ms (take a look at <a href="https://hackernoon.com/which-is-the-fastest-version-of-python-2ae7c61a6b2b">https://hackernoon.com/which-is-the-fastest-version-of-python-2ae7c61a6b2b</a> to learn more). <br><br>  As a result, you may have parallel code on CPython, but you still need to carefully plan the work of long processes that share several objects. <br><br>  Another alternative is to use a third-party package, such as Twisted. <br><br><h2>  PEP554 and the death of GIL? </h2><br>  So let me remind you that multithreading in CPython is simple, but in reality it is not parallelization, but multiprocessing is parallel, but entails significant overhead. <br><br>  <i>What if there is a better way?</i> <br>  The key to GIL traversal is in the name, the global interpreter lock is part of the global interpreter state.  CPython processes can have several interpreters and, consequently, several locks, but this function is rarely used, since it can only be accessed through the C-API. <br><br>  One of the features of CPython 3.8 is PEP554, the implementation of subinterpreters and APIs with the new <code>interpreters</code> module in the standard library. <br><br>  This allows you to create multiple interpreters from Python within the same process.  Another new feature of Python 3.8 is that all interpreters will have their own GIL. <br><br><img src="https://habrastorage.org/webt/bq/nc/m2/bqncm29jhm-ytakgrlkasbfe_6y.png"><br><br>  Since the state of the interpreter contains an area allocated in memory, a collection of all pointers to Python objects (local and global), the subinterpreters in PEP554 cannot access the global variables of other interpreters. <br><br>  Like multiprocessing, sharing by interpreters of objects consists in their serialization and use of the IPC form (network, disk, or shared memory).  There are many ways to serialize objects in Python, such as the <code>marshal</code> module, the <code>pickle</code> module, or more standardized methods, such as <code>json</code> or <code>simplexml</code> .  Each of them has its pros and cons, and they all give a computational load. <br><br>  It would be best to have a common memory space that can be modified and controlled by a specific process.  Thus, objects can be sent by the main interpreter and received by another interpreter.  This will be the managed memory space for searching for PyObject pointers that each interpreter can access, and the main process will manage the locks. <br><br><img src="https://habrastorage.org/webt/be/ww/d8/bewwd8ju-3akmyhs7ujq7xmyliy.png"><br><br>  An API for this is still being developed, but it will probably look something like this: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> _xxsubinterpreters <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> interpreters <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> threading <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> textwrap <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tw <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> marshal <span class="hljs-comment"><span class="hljs-comment"># Create a sub-interpreter interpid = interpreters.create() # If you had a function that generated some data arry = list(range(0,100)) # Create a channel channel_id = interpreters.channel_create() # Pre-populate the interpreter with a module interpreters.run_string(interpid, "import marshal; import _xxsubinterpreters as interpreters") # Define a def run(interpid, channel_id): interpreters.run_string(interpid, tw.dedent(""" arry_raw = interpreters.channel_recv(channel_id) arry = marshal.loads(arry_raw) result = [1,2,3,4,5] # where you would do some calculating result_raw = marshal.dumps(result) interpreters.channel_send(channel_id, result_raw) """), shared=dict( channel_id=channel_id ), ) inp = marshal.dumps(arry) interpreters.channel_send(channel_id, inp) # Run inside a thread t = threading.Thread(target=run, args=(interpid, channel_id)) t.start() # Sub interpreter will process. Feel free to do anything else now. output = interpreters.channel_recv(channel_id) interpreters.channel_release(channel_id) output_arry = marshal.loads(output) print(output_arry)</span></span></code> </pre> <br><br>  This example uses NumPy.  The numpy array is sent over the channel, it is serialized using the <code>marshal</code> module, then the subinterpreter processes the data (on a separate GIL), so there may be a parallelization problem associated with the CPU, which is ideal for subinterpreters. <br><br><h4>  <b>It looks ineffective</b> </h4><br>  The <code>marshal</code> module works really fast, but not as fast as sharing objects directly from memory. <br><br>  PEP574 introduces the new <a href="https://www.python.org/dev/peps/pep-0574/">pickle</a> protocol <a href="https://www.python.org/dev/peps/pep-0574/">(v5)</a> , which supports the ability to process memory buffers separately from the rest of the pickle stream.  As for big data objects, serialization of them all in one breath and deserialization from the subinterpreter will add a large amount of overhead. <br><br>  The new API can be implemented (purely hypothetical) as follows - <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> _xxsubinterpreters <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> interpreters <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> threading <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> textwrap <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tw <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pickle <span class="hljs-comment"><span class="hljs-comment"># Create a sub-interpreter interpid = interpreters.create() # If you had a function that generated a numpy array arry = [5,4,3,2,1] # Create a channel channel_id = interpreters.channel_create() # Pre-populate the interpreter with a module interpreters.run_string(interpid, "import pickle; import _xxsubinterpreters as interpreters") buffers=[] # Define a def run(interpid, channel_id): interpreters.run_string(interpid, tw.dedent(""" arry_raw = interpreters.channel_recv(channel_id) arry = pickle.loads(arry_raw) print(f"Got: {arry}") result = arry[::-1] result_raw = pickle.dumps(result, protocol=5) interpreters.channel_send(channel_id, result_raw) """), shared=dict( channel_id=channel_id, ), ) input = pickle.dumps(arry, protocol=5, buffer_callback=buffers.append) interpreters.channel_send(channel_id, input) # Run inside a thread t = threading.Thread(target=run, args=(interpid, channel_id)) t.start() # Sub interpreter will process. Feel free to do anything else now. output = interpreters.channel_recv(channel_id) interpreters.channel_release(channel_id) output_arry = pickle.loads(output) print(f"Got back: {output_arry}")</span></span></code> </pre> <br><h4>  <b>It looks like a template.</b> </h4><br>  In essence, this example is built on the use of low-level subinterpreters API.  If you have not used the <code>multiprocessing</code> library, some problems will seem familiar to you.  This is not as simple as stream processing; you cannot simply, say, run this function with such a list of input data in separate interpreters (for now). <br><br>  As soon as this PEP merges with others, I think we will see several new APIs in PyPi. <br><br><h3>  How much overhead does the subinterpreter have? </h3><br>  <b>Short answer:</b> More than flow, less than process. <br>  <b>Long answer: The</b> interpreter has its own state, therefore it will need to clone and initialize the following, despite the fact that PEP554 simplifies the creation of subinterpreters: <br><br><ul><li>  Modules in the <code>__main__</code> and <code>importlib</code> ; </li><li>  The contents of the <code>sys</code> dictionary; </li><li>  Built-in functions ( <code>print()</code> , <code>assert</code> , etc.); </li><li>  Streams; </li><li>  Kernel configuration </li></ul><br><br>  The kernel configuration can be easily cloned from memory, but with imported modules, things are not that simple.  Importing modules to Python is slow, so if creating a subinterpreter means importing modules to a different namespace each time, the benefits are reduced. <br><br><h3>  How about asyncio? </h3><br>  The existing implementation of the <code>asyncio</code> event <code>asyncio</code> in the standard library creates stack frames for evaluation, and also <code>asyncio</code> state in the main interpreter (and, therefore, shares GIL). <br><br>  After combining PEP554, probably already in Python 3.9, an alternative implementation of the event loop can be used (although no one has yet done this), which in parallel runs asynchronous methods in the subinterpreters. <br><br><h3>  It sounds cool, and wrap me! </h3><br>  Well, not quite. <br>  Since CPython has been working on one interpreter for so long, many parts of the code base use ‚ÄúRuntime State‚Äù instead of ‚ÄúInterpreter State‚Äù, so if PEP554 was introduced now, there would still be a lot of problems. <br><br>  For example, the state of the garbage collector (in versions 3.7 &lt;) belongs to the runtime environment. <br><br>  In the changes during the PyCon sprints, the state of the garbage collector <a href="https://github.com/python/cpython/pull/13219">began to move</a> to the interpreter, so that each subinterpreter will have its own garbage collector (as it should have been). <br><br>  Another problem is that there are some ‚Äúglobal‚Äù variables that were delayed in order in the CPython code base along with many extensions in C. Therefore, when people suddenly began to correctly parallelize their code, we saw some problems. <br><br>  Another problem is that the file handles belong to the process, so if you have a file open for writing in one interpreter, the subinterpreter will not be able to access this file (without further changes to CPython). <br><br>  In short, there are still many problems to be solved. <br><br><h2>  Conclusion: Is GIL really no longer relevant? </h2><br>  GIL will still be used for single-threaded applications.  Therefore, even if you follow PEP554, your single-threaded code will not suddenly become parallel. <br>  If you want to write parallel code in Python 3.8, you will have paralleling problems with the processor, but this is also a ticket to the future! <br><br><h2>  When? </h2><br>  Pickle v5 and memory sharing for multiprocessing are likely to be in Python 3.8 (October 2019), and subinterpreters will appear between versions 3.8 and 3.9. <br>  If you have a desire to play with the presented examples, then I created a separate branch with all the necessary code: <a href="">https://github.com/tonybaloney/cpython/tree/subinterpreters.</a> <br><br>  What do you think about this?  Write your comments and see you on the course. </div><p>Source: <a href="https://habr.com/ru/post/458694/">https://habr.com/ru/post/458694/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458680/index.html">Why do we need UML? Or how to save time and nerves</a></li>
<li><a href="../458684/index.html">ICANN removed the .org price threshold - why the IT community is against, and what will happen next</a></li>
<li><a href="../458686/index.html">Compilation @pythonetc, June 2019</a></li>
<li><a href="../45869/index.html">Control-navigation</a></li>
<li><a href="../458690/index.html">Automate it! How we improved integration testing</a></li>
<li><a href="../458696/index.html">Texturing, or what you need to know to become an artist on surfaces. Part 3. PBR and materials</a></li>
<li><a href="../458698/index.html">The path of peace and the path of war in IT projects</a></li>
<li><a href="../4587/index.html">Digital TV waiting for the golden mountains after 2017</a></li>
<li><a href="../45870/index.html">Havana Railway Station through the eyes of a tourist</a></li>
<li><a href="../458702/index.html">Sled dogs: what you need to know about them, and how they were taken</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>