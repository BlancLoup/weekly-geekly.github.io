<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>USE, RED, PgBouncer, its settings and monitoring</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We started updating the monitoring service for PgBouncer in our service and decided to do a bit of combing. To make everything fit, we pulled the most...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>USE, RED, PgBouncer, its settings and monitoring</h1><div class="post__text post__text-html js-mediator-article"><img align="left" width="359" src="https://habrastorage.org/webt/l4/xy/iz/l4xyize9ztzrmf303fot3iylnc8.png" alt="Pgbouncer use red"><br><p>  We started updating the monitoring service for PgBouncer in our service and decided to do a bit of combing.  To make everything fit, we pulled the most famous monitoring performance methodologies: Brendan Gregg‚Äôs USE (Utilization, Saturation, Errors) and Tom Wilkie‚Äôs RED (Requests, Errors, Durations). </p><br><p>  Under the cut, there is a story with graphs about how pgbouncer works, what configuration handles it has, and how to choose the right metrics for its monitoring using USE / RED. </p><a name="habracut"></a><br><h2 id="snachala-pro-sami-metody">  First, about the methods themselves </h2><br><p>  Although these methods are quite well-known ( <a href="https://habr.com/search/%3Fq%3DError%2BDuration%2Butilization%2Bsaturation">they were already on Habr√©, though not very detailed</a> ), but not that they are widespread in practice. </p><br><h3 id="use">  USE </h3><br><blockquote>  For each resource, follow recycling, saturation and errors. <br>  Brendan gregg </blockquote><p>  Here, a <strong>resource</strong> is any individual physical component - CPU, disk, bus, etc.  But not only - the performance of some software resources can also be considered by this method, in particular, virtual resources, such as containers / cgroups with limits, are also conveniently considered. </p><br><p>  <strong>U - Recycling</strong> : either the percentage of time (from the observation interval) when the resource was occupied by useful work.  Like, for example, CPU utilization or disk utilization, 90% means that 90% of the time was occupied with something useful) or, for resources such as memory, this is the percentage of memory used. </p><br><p>  In any case, 100% recycling means that the resource can not be used more than now.  And either the work will be stuck waiting for release / sent to the queue, or there will be errors.  These two scenarios are covered by the corresponding two remaining USE metrics: </p><br><p>  <strong>S - Saturation</strong> , it's saturation: a measure of the amount of "delayed" / queued work. </p><br><p>  <strong>E - Errors</strong> : just count the number of failures.  Errors / failures affect performance, but they may not be immediately noticeable due to rerai of fired operations or fault tolerance mechanisms with backup devices, etc. </p><br><h3 id="red">  RED </h3><br><p>  Tom Wilkie (now working at Grafana Labs) was frustrated by the USE methodology, or rather, by its poor applicability in some cases and inconsistencies in practice.  How, for example, to measure the saturation of memory?  Or how to measure errors in the system bus in practice? </p><br><blockquote>  Linux, it turns out, really sucks error counters. <br>  T. Wilkie </blockquote><p>  In short, to monitor performance and the behavior of microservices, he suggested another, suitable method: measure, again, three indicators: </p><br><p>  <strong>R - Rate</strong> : the number of requests per second. <br>  <strong>E - Errors</strong> : how many requests returned an error. <br>  <strong>D - Duration</strong> : time taken to process the request.  It is also latency, "latency" (¬© Sveta Smirnova :), response time, etc. </p><br><p>  In general, USE is more suitable for monitoring resources, and RED - services and their workload / payload. </p><br><h2 id="pgbouncer">  PgBouncer </h2><br><p>  being a service, it also has all sorts of internal limits and resources.  The same can be said about Postgres, which clients access via this PgBouncer.  Therefore, for full monitoring in such a situation, both methods are needed. </p><br><p>  To figure out how to attach these methods to a bouncer, you need to understand the details of its device.  It is not enough to monitor it as a black-box - ‚Äúis the pgbouncer process alive‚Äù or ‚Äúis the port open‚Äù, since  in case of problems, it will not give an understanding of what it was and how it broke down and what to do. </p><br><p>  What it does in general, what PgBouncer looks like from the client‚Äôs point of view: </p><br><ol><li>  client connects </li><li>  [client makes a request - gets an answer] x how many times he needs </li></ol><br><p>  Here I am a pictorial diagram of the corresponding client states from the point of view of PgBoucer: <br><img src="https://habrastorage.org/webt/zb/mh/ot/zbmhotvidvuxnsbzp04-bqtgqhk.jpeg"></p><br><p> In the login process, authorization can occur either locally (files, certificates, and even PAM and hba from new versions), or remotely ‚Äî that is,  in the database itself to which the connection is attempted.  Thus, the login state has an additional substate.  Let's call it <code>Executing</code> to indicate that <code>auth_query</code> is <code>auth_query</code> in the database at this time: <br><img src="https://habrastorage.org/webt/e4/ib/pb/e4ibpbf6ef5q9xcdy2fantydkc4.png"></p><br><p>  But these client connections are actually matched with backend / upstream database connections that PgBouncer opens within the pool and holds a limited amount.  And they give the client such a connection only for a while - for the duration of the session, transaction or request, depending on the type of pooling (determined by the <code>pool_mode</code> setting).  The most commonly used transaction pooling (we will mostly discuss this further) is when the connection is issued to the client for one transaction, and the rest of the time the client is not connected to the server after the fact.  Thus, the client‚Äôs ‚Äúactive‚Äù state doesn‚Äôt tell us much, and we‚Äôll break it down into the substates: <br><img src="https://habrastorage.org/webt/uv/q4/tj/uvq4tjzbbauunpwzhboxc8s3pgu.png"></p><br><p>  Each such client enters its own pool of connections, which will be issued for use by these connections to Postgres.  This is the main task of PgBouncer to limit the number of connections to Postgres. </p><br><p>  Due to the limited server connections, a situation may arise when the client needs to fulfill the request right now, but there is no free connection now.  Then the client is queued and its connection enters the <code>CL_WAITING</code> state.  Thus, the state diagram must be supplemented: <br><img src="https://habrastorage.org/webt/2v/ny/yu/2vnyyuhlqher6cc5q5izwjtu7te.png"><br>  Since this can happen in the case when the client only logs in and needs to execute the request for authorization, the state <code>CL_WAITING_LOGIN</code> . </p><br><p>  If you now look from the back side - from the server connections side, then they, respectively, are in such states: when authorization occurs immediately after the connection - <code>SV_LOGIN</code> , issued and (possibly) used by the client - <code>SV_ACTIVE</code> , or freely - <code>SV_IDLE</code> . </p><br><h2 id="use-dlya-pgbouncer">  USE for PgBouncer </h2><br><p>  Thus, we come to the (naive version) Utilization of a specific pool: </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">Pool</span></span> utiliz =    /  </code> </pre> <br><p>  PgBouncer has a special pgbouncer service database in which there is a <code>SHOW POOLS</code> that shows the current states of the connections of each pool: <br><img src="https://habrastorage.org/webt/xz/_o/eb/xz_oebvf-0yahuvdfzrxx3iass0.png"><br>  There are 4 client connections open and all are <code>cl_active</code> .  Of the 5 server connections, there are 4 <code>sv_active</code> and one in the new state <code>sv_used</code> . </p><br><div class="spoiler">  <b class="spoiler_title">What is sv_used actually about different pgbouncer settings that are not related to monitoring?</b> <div class="spoiler_text"><p>  So <code>sv_used</code> means not "connection is used," as you might have thought, but "connection was once used and not used for a long time."  The fact is that by default, PgBouncer uses server connections in LIFO mode - i.e.  the newly released connections are used first, then recently used ones, etc.  gradually moving to long-used compounds.  Accordingly, server connections from the bottom of such a stack can be "rotten".  And before using them one should check for liveliness, which is done with the help of <code>server_check_query</code> , while they are checked the state will be <code>sv_tested</code> . </p><br><p>  Documentation states that LIFO is enabled by default, because  then "a small number of connections gets the most load. And this gives the best performance when there is one server serving the database for pgbouncer", i.e.  as if in the most typical case.  I believe that the potential performance boost is due to the savings on switching performance between multiple backend processes.  But it was not possible to find out reliably, since  This implementation detail has been in existence for&gt; 12 years and goes beyond commit history on a githaba and the depth of my interest =) </p><br><p>  So, it seemed to me strange and <code>server_check_delay</code> with the current realities that the default value of the <code>server_check_delay</code> setting, which determines that the server had not been used for a long time and should be checked before giving to the client, is 30 seconds.  This is despite the fact that by default, tcp_keepalive is enabled at the same time with the default settings - start checking the keep alive connection with samples 2 hours after its idle'ing. <br>  It turns out that in the situation of a burst / burst of client connections that want to perform something on the server, an additional delay is added to the <code>server_check_query</code> , which though " <code>SELECT 1;</code> it can still take ~ 100 microseconds, and if you just put the <code>server_check_query = ';'</code>  you can save ~ 30 microseconds =) </p><br><p>  But the assumption that doing work in just a few connections = on several "main" backend postgres processes will be more efficient, it seems to me doubtful.  A postgres worker process caches (meta) information about each table that was accessed in this connection.  If you have a large number of tables, this relcache can grow greatly and take up a lot of memory, up to and including swapping pages of the 0_o process.  To bypass this, the <code>server_lifetime</code> setting (by default - 1 hour) is suitable, by which the server connection will be closed for rotation.  But on the other hand, there is a <code>server_round_robin</code> setting that will switch the mode of using connections from LIFO to FIFO, spreading client requests across server connections more evenly. </p></div></div><br><p>  Naively removing metrics from <code>SHOW POOLS</code> (by some prometheus exporter) we can plot these states: </p><br><p><img src="https://habrastorage.org/webt/i8/8f/-x/i88f-xpwo_2hj7z6iq6qbnatsju.png"></p><br><p>  But to get to the disposal you need to answer a few questions: </p><br><ul><li>  What is the size of the pool? </li><li>  How to count how many compounds are used?  In jokes or in time, on average or in peak? </li></ul><br><h3 id="razmer-pula">  Pool size </h3><br><p>  It's all complicated, as in life.  In total, there are already five limit settings in the pbbaunser! </p><br><ul><li>  <code>pool_size</code> can be set for each base.  A separate pool is created for each DB / user pair, i.e.  from any <em>additional</em> user, you can create another <code>pool_size</code> pool_size backend / worker.  Since  if <code>pool_size</code> not set, it is folded in <code>default_pool_size</code> , which is by default 20, then it turns out that each user with connection rights to the database (and working through pgbouncer) can potentially create 20 Postgres processes, which is not much.  But if you have many different user bases or the bases themselves, and the pools are not registered with a fixed user, i.e.  will be created on the fly (and then deleted by <code>autodb_idle_timeout</code> ), then it can be dangerous =) <br><blockquote>  Perhaps it is worth leaving the <code>default_pool_size</code> small, just for fire. <br></blockquote></li><li>  <code>max_db_connections</code> - just needed to limit the total number of connections to one database, since  otherwise, badly behaving, clients can create a lot of backends / postgres processes.  And by default here - unlimited ¬Ø_ („ÉÑ) _ / ¬Ø <br><blockquote>  It may be worth changing the default <code>max_db_connections</code> , for example, you can focus on the <code>max_connections</code> your Postgres (by default 100).  But if you have a lot of PgBouncer's ... <br></blockquote></li><li>  <code>reserve_pool_size</code> - in fact, if <code>pool_size</code> all used, then PgBouncer can open several more connections to the base.  I understand this is done to cope with a surge load.  We will come back to this. </li><li>  <code>max_user_connections</code> - This, on the contrary, is the limit of connections from one user to all databases, i.e.  true if you have several bases and they go under the same user. </li><li>  <code>max_client_conn</code> - how many client connections a PgBouncer will accept at all.  The default, as is customary, has a very strange value - 100. That is,  it is assumed that if suddenly more than 100 clients are breaking, they just need to send the <code>reset</code> and almost everything silently at the TCP level (well, in the logs, it must be admitted, there will be "no more connections allowed (max_client_conn)"). <br><blockquote>  Perhaps it is worth making <code>max_client_conn &gt;&gt; SUM ( pool_size' )</code> , for example, 10 times more. <br></blockquote></li></ul><br><p>  In addition to <code>SHOW POOLS</code> service pseudo-base pgbouncer also provides the <code>SHOW DATABASES</code> , which shows the limits actually applied to a specific pool: <br><img src="https://habrastorage.org/webt/1v/lv/4h/1vlv4hviyhxz1pbh9puc6xxim9o.jpeg"></p><br><h3 id="servernye-soedineniya">  Server connections </h3><br><p>  Once again - how to measure how many compounds are used? <br>  In jokes on average / peak / in time? </p><br><p>  In practice, it is quite problematic to monitor the use of pools with a bouncer by common tools, since  pgbouncer itself provides only a momentary picture, and how often do not do a survey, there is still the possibility of a wrong picture due to sampling.  Here is a real example when, depending on when the exporter was working out - at the beginning of a minute or at the end - the picture of both open and used compounds changes fundamentally: </p><br><p><img src="https://habrastorage.org/webt/i3/ch/qb/i3chqbtvyp3p6nm0bayw62pf3hq.png"></p><br><p>  Here all the changes in the load / use of the connections are just a fiction, an artifact of the statistics collector restarts.  Here you can look at the graphs of connections in Postgres for this time and on the file descriptors of the bouncer and PG - no changes: </p><br><p><img src="https://habrastorage.org/webt/n7/bh/4r/n7bh4r_2h21ypaxhtr7njv2u8xa.png"></p><br><p>  Let's return to the issue of recycling.  We in our service decided to use a combined approach - we sample <code>SHOW POOLS</code> once a second, and once a minute we render both the average and maximum number of connections in each state: </p><br><p><img src="https://habrastorage.org/webt/ea/v5/ei/eav5eimcl7fofctvc24oaymzsu4.png"></p><br><p>  And if we divide the number of these active state connections by the size of the pool, we get the average and peak utilization of this pool and we can alert if it is close to 100%. </p><br><p>  In addition, PgBouncer has a <code>SHOW STATS</code> command that will show usage statistics for each proxied database: <br><img src="https://habrastorage.org/webt/mo/wz/_q/mowz_qavy_zspkljbvnehbj1bpc.png"><br>  We are most interested in the <code>total_query_time</code> column - the time spent by all connections in the process of executing queries in postgres.  And since version 1.8, there is also a <code>total_xact_time</code> metric - time spent in transactions.  Based on these metrics, we can build the utilization of server connection time; this indicator is not subject, in contrast to the calculations calculated from the state of connections, to sampling problems, since  these <code>total_..._time</code> counters are cumulative and do not miss anything: </p><br><p><img src="https://habrastorage.org/webt/p3/1l/iv/p31liv1gccpj3rxnxav-nbelck0.png"><br>  Compare <br><img src="https://habrastorage.org/webt/yk/rt/gd/ykrtgdu5s8_pcltl3w3mmcuj4oo.png"><br>  It can be seen that sampling does not show all moments of high ~ 100% utilization, and query_time shows. </p><br><h3 id="saturation-i-pgbouncer">  Saturation and PgBouncer </h3><br><p>  Why do we need to keep track of Saturation, because by high utilization it is already clear that everything is bad? </p><br><p>  The problem is that no matter how measured the recycling, even the accumulated counters cannot show the local 100% use of the resource if it occurs only at very short intervals.  For example, you have any crowns or other synchronous processes that can simultaneously begin to make requests to the base on command.  If these requests are short, then recycling, measured on the scale of a minute or even a second, may be low, but at the same time these requests were forced to wait for the queue to be executed.  It seems that the situation is not 100% CPU usage and high Load average - it seems that CPU time is still there, but nevertheless many processes are waiting in the queue for execution. </p><br><p>  How can you track such a situation - well, again, we can simply count the number of clients in the <code>cl_waiting</code> state according to <code>SHOW POOLS</code> .  In a normal situation, such - zero, and more than zero means overflowing this pool: </p><br><p><img src="https://habrastorage.org/webt/q1/tg/tj/q1tgtjcqgrqpavfpkf0kyg31hjq.png"></p><br><p>  There remains a problem with the fact that <code>SHOW POOLS</code> can only be sampled, and in a situation with synchronous crowns or something like that, we can simply skip and not see such waiting clients. </p><br><p>  You can use this trick, pgbouncer can detect 100% use of the pool itself and open a backup pool.  Two settings are responsible for this: <code>reserve_pool_size</code> ‚Äî for its size, as I said, and <code>reserve_pool_timeout</code> ‚Äî how many seconds a client must have been <code>waiting</code> before using the backup pool.  Thus, if we see on the server connection graph that the number of connections open before Postgres is greater than pool_size, then the pool saturation was like this: <br><img src="https://habrastorage.org/webt/ne/ec/hn/neechnkp9sffd8g3rjov_3jjijm.png"><br>  Obviously, something like kroons makes many requests once per hour and completely occupies the pool.  And even though we don‚Äôt see the moment when <code>active</code> connections exceed the <code>pool_size</code> limit, the pgbouncer still had to open additional connections. </p><br><p>  Also on this graph, the work of setting up <code>server_idle_timeout</code> is clearly visible - after how much cease to keep and close connections that are not used.  By default, this is 10 minutes, which we see on the chart - after <code>active</code> peaks at exactly 5:00, 6:00, etc.  (according to cron <code>0 * * * *</code> ), connections hang <code>idle</code> + <code>used</code> another 10 minutes and close. </p><br><p>  If you live on the cutting edge of progress and have updated PgBouncer over the past 9 months, you can find the <code>total_wait_time</code> column in <code>SHOW STATS</code> , which shows saturation best of all.  cumulatively considers the time spent by customers in the <code>waiting</code> state.  For example, here - the <code>waiting</code> appeared at 16:30: <br><img src="https://habrastorage.org/webt/ck/-q/qt/ck-qqth3dhvsqsw1zzeuvkjg4wa.png"><br>  And <code>wait_time</code> , which is comparable and clearly affects the <code>average query time</code> , can be seen from 15:15 and almost to 19: <br><img src="https://habrastorage.org/webt/gn/av/bb/gnavbbmcfchzhkt0d0egcybthzc.png"></p><br><p>  However, monitoring client connection states is still very useful, because  allows you to find out not only the fact that all connections are spent on such a database and customers have to wait, but also because <code>SHOW POOLS</code> split into separate pools by users, and <code>SHOW STATS</code> does not, allows you to find out exactly which clients used all connections to a given base, by the <code>sv_active</code> column of the corresponding pool.  Or by metric </p><br><pre> <code class="hljs pgsql">sum_by(<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">database</span></span>, metric(<span class="hljs-type"><span class="hljs-type">name</span></span>="pgbouncer.clients.count", state="active-link")):</code> </pre> <br><p><img src="https://habrastorage.org/webt/k_/0s/lf/k_0slfupzfrdz4npoiz8kbxgpko.png"></p><br><p>  We went even further into okmeter and added a breakdown of the connections used to the IP addresses of the clients who opened and used them.  This allows you to understand which application instances behave differently: <br><img src="https://habrastorage.org/webt/dk/fr/5j/dkfr5j_yvxgmm2f0b5dvru4b9nk.png"><br>  Here we see aypishniki specific kubernetes pods with which you need to deal. </p><br><h3 id="errors">  Errors </h3><br><p>  There is nothing particularly tricky here: pgbouncer writes logs in which it reports errors if the limit of client connections is reached, the timeout for connecting to the server, etc.  So far we haven't gotten to the pgbouncer logs :( </p><br><h2 id="red-dlya-pgbouncer">  RED for PgBouncer </h2><br><p>  While USE is more focused on performance, in the sense of bottlenecks, RED, in my opinion, is more about the characteristics of incoming and outgoing traffic in general, and not about bottlenecks.  That is, RED answers the question - does everything work fine, and if not, then USE will help you to understand the problem. </p><br><h2 id="requests">  Requests </h2><br><p>  It would seem that everything is quite simple for a SQL database and for a proxy / connection puller to such a database - clients execute SQL statements, which are Requests.  From <code>SHOW STATS</code> we take <code>total_requests</code> and build a graph of its time derivative. </p><br><pre> <code class="hljs lisp">rate(<span class="hljs-name"><span class="hljs-name">metric</span></span>(<span class="hljs-name"><span class="hljs-name">name=</span></span><span class="hljs-string"><span class="hljs-string">"pgbouncer.total_requests"</span></span>, database: <span class="hljs-string"><span class="hljs-string">"*"</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/fa/7u/o2/fa7uo2r8b6_4y6z9e2bounudzmw.png"></p><br><p>  But in fact there are different modes of pulling, and the most common is transactions.  The unit of operation of this mode is a transaction, not a request.  In accordance with this, starting from version 1.8, Pgbouner has already provided two other statistics - <code>total_query_count</code> , instead of <code>total_requests</code> , and <code>total_xact_count</code> - the number of past transactions. </p><br><p>  Now workload can be characterized not only in terms of the number of completed requests / transactions, but you can, for example, look at the average number of requests per transaction into different databases, dividing one into another </p><br><pre> <code class="hljs lisp">rate(<span class="hljs-name"><span class="hljs-name">metric</span></span>(<span class="hljs-name"><span class="hljs-name">name=</span></span><span class="hljs-string"><span class="hljs-string">"total_requests"</span></span>, database=<span class="hljs-string"><span class="hljs-string">"*"</span></span>)) / rate(<span class="hljs-name"><span class="hljs-name">metric</span></span>(<span class="hljs-name"><span class="hljs-name">name=</span></span><span class="hljs-string"><span class="hljs-string">"total_xact"</span></span>, database=<span class="hljs-string"><span class="hljs-string">"*"</span></span>))</code> </pre> <br><p><img src="https://habrastorage.org/webt/xd/uh/3w/xduh3wsb-bww1tysfwtdhcw-seg.png"></p><br><p>  Here we see obvious changes in the load profile, which may be the reason for the change in performance.  And if they were looking only at the rate of transactions or requests, they could not see it. </p><br><h2 id="red-errors">  RED Errors </h2><br><p>  It is clear that RED and USE overlap in error monitoring, but it seems to me that errors in USE are mostly about request processing errors due to 100% utilization, i.e.  when the service refuses to take more work.  And errors for RED would be better to measure errors from the point of view of the client, client requests.  That is, not only in a situation when the pool in PgBouncer is full or another limit has been triggered, but also when request timeouts have been triggered, such as "canceling statement due to statement timeout", cancel'y and rollback and transactions by the client itself, t. e.  higher-level, closer to the business logic types of errors. </p><br><h2 id="durations">  Durations </h2><br><p>  <code>SHOW STATS</code> with cumulative <code>total_xact_time</code> , <code>total_query_time</code> and <code>total_wait_time</code> counters will help us again, dividing that by the number of requests and transactions, respectively, we get the average request time, average transaction time, average wait time per transaction.  I already showed the schedule about the first and third: <br><img src="https://habrastorage.org/webt/gn/av/bb/gnavbbmcfchzhkt0d0egcybthzc.png"></p><br><p>  What is there more to get?  The well-known anti-pattern in working with the database and Postgres in particular, when an application opens a transaction, makes a request, then starts (long) to process its results or worse, goes to some other service / base and makes there requests.  All this time, the transaction "hangs" in the open state, the service then returns and makes some more requests, updates in the database and only then closes the transaction.  For postgres it is especially unpleasant, because  pg workers are expensive.  So, we can monitor when such an application resides <code>idle in transaction</code> in the post- <code>state</code> itself - in the <code>state</code> column in <code>pg_stat_activity</code> , but there all the same described problems with sampling, since  <code>pg_stat_activity</code> gives only the current picture.  In PgBouncer, we can subtract the time spent by customers in <code>total_query_time</code> requests from the time spent in transactions <code>total_xact_time</code> - this will be the time of such idling.  If the result is still divided by <code>total_xact_time</code> , then it will be normalized: the value 1 corresponds to the situation when customers are <code>idle in transaction</code> 100% of the time.  And with such a normalization, it makes it easy to understand how bad everything is: </p><br><p><img src="https://habrastorage.org/webt/t9/kn/io/t9knioh2ckzd_photgqvcq543x4.png"></p><br><p>  In addition, returning to Duration, the <code>total_xact_time - total_query_time</code> can be divided by the number of transactions to see how much the average idle application is per transaction. </p><br><hr><br><p>  In my opinion, USE / RED methods are most useful for structuring which metrics you are shooting and why.  Since we are monitoring full-time and we have to do monitoring for various components of the infrastructure, these methods help us to remove the correct metrics, make the right schedules and triggers for our clients. </p><br><p>  <em>Good monitoring cannot be done right away, this is an iterative process.</em>  <em>We in <a href="https://okmeter.io/%3Futm_source%3Dhabr%26utm_medium%3Dhabr-post%26utm_campaign%3Dblog%26utm_content%3Dpgbouncer_use_red">okmeter.io</a> just continuous monitoring (a lot of things, but tomorrow will be better and more detailed :)</em> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/420429/">https://habr.com/ru/post/420429/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420415/index.html">Everything you wanted to know about testing Wi-Fi adapters, but were afraid to ask</a></li>
<li><a href="../420419/index.html">Runners for those who love humiliation or how we changed and modified PixJam</a></li>
<li><a href="../420423/index.html">Ground transition interface problems</a></li>
<li><a href="../420425/index.html">Theory and practice of using HBase</a></li>
<li><a href="../420427/index.html">How to stretch images without losing quality in Photoshop in a quick way</a></li>
<li><a href="../420431/index.html">Mars. A practical terraforming guide for housewives</a></li>
<li><a href="../420433/index.html">‚ÄúFriday format‚Äù: musical roads - what it is and why they are not in Russia</a></li>
<li><a href="../420435/index.html">Quick start with ARM Mbed: development on modern microcontrollers for beginners</a></li>
<li><a href="../420437/index.html">Practical familiarity with the package manager for Kubernetes - Helm</a></li>
<li><a href="../420439/index.html">Fintech Digest: Fintech investments reached $ 57 billion, transaction speed increases, and cost decreases</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>