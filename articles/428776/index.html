<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>New realization of curiosity in AI. Training with a reward that depends on the difficulty of predicting the outcome</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Progress in the game "The Revenge of Montezuma" was considered by many as synonymous with achievements in the field of research of an unfamiliar envir...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>New realization of curiosity in AI. Training with a reward that depends on the difficulty of predicting the outcome</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/49b/e3e/fbf/49be3efbf10821888431e9529873176a.svg" width="780"><br>  <i><font color="gray">Progress in the game "The Revenge of Montezuma" was considered by many as synonymous with achievements in the field of research of an unfamiliar environment</font></i> <br><br>  We developed a <b><a href="https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/">random</a></b> Network Distillation (RND) prediction-based method that encourages reinforcement learning agents to explore their surroundings through curiosity.  This method for the first time exceeded the average results of a person in the computer game <a href="https://www.retrogames.cz/play_124-Atari2600.php%3Flanguage%3DEN">"Revenge of Montezuma"</a> (except for an anonymous <a href="https://openreview.net/forum%3Fid%3DHyxGB2AcY7">application</a> in the ICLR, where the result is worse than ours).  <b>RND demonstrates cutting-edge efficiency, periodically finds all 24 rooms and passes the first level without prior demonstration and not having access to the basic state of the game.</b> <br><a name="habracut"></a><br>  The RND method stimulates an agent to transition to unfamiliar states by measuring the difficulty of predicting the result of applying a constant random neural network to state data.  If the condition is unfamiliar, then the final result is difficult to predict, and therefore the reward is high.  The method can be applied to any learning algorithm with reinforcement; it is simple to implement and effective in scaling.  Below is a link to the implementation of RND, which reproduces the results from our article. <br><br><blockquote>  <a href="https://arxiv.org/abs/1810.12894">The text of the scientific article</a> , <a href="https://github.com/openai/random-network-distillation">code</a> </blockquote>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/40VZeFppDEM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h1>  Results in the game "Revenge of Montezuma" </h1><br>  To achieve the desired goal, the agent must first examine what actions are possible in this environment and what constitutes progress towards the goal.  Many reward cues in games provide a curriculum, so even simple research strategies are enough to achieve a goal.  In the <a href="https://www.nature.com/articles/nature14236">initial work with the presentation of DQN,</a> ‚ÄúRevenge of Montezuma‚Äù was the <b>only game where DQN showed a result of 0% of the average human score (4700)</b> .  Simple intelligence strategies are unlikely to collect any rewards and will find no more than a few rooms at a level.  Since then, progress in the game ‚ÄúRevenge of Montezuma‚Äù has been widely regarded as synonymous with advances in the field of exploring an unfamiliar environment. <br><br>  Significant progress was achieved in <a href="https://arxiv.org/abs/1606.01868">2016</a> by combining DQN with a bonus on the counter, with the result that the agent managed to find 15 rooms and get the highest score of 6600 with an average of about 3700. Since then, <a href="https://arxiv.org/abs/1805.11593">significant</a> <a href="https://arxiv.org/abs/1805.11592">improvements are</a> achieved only with the help of <a href="https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/">demonstrations</a> from <a href="https://arxiv.org/abs/1809.03447">expert</a> people or by accessing the base states of the <a href="https://blog.openai.com/learning-montezumas-revenge-from-a-single-demonstration/">emulator</a> . <br><br>  We conducted a large-scale RND experiment with 1024 workers, receiving an <b>average result of 10,000 over 9 starts</b> and the <b>best average result of 14,500</b> .  In each case, the agent found 20‚àí22 rooms.  In addition, one of the smaller in scale, but longer start (out of 10) shows the <b>maximum result of 17,500, which corresponds to the passage of the first level and the finding of all 24 rooms</b> .  The graph below compares these two experiments, showing the average value depending on the update parameters. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cde/262/bde/cde262bde2a497752d59599ba524d41b.svg" width="780"><br><br>  The visualization below shows the course of the experiment on a smaller scale.  The agent, under the influence of curiosity, opens up new rooms and finds ways to score points. During training, this external reward causes him to return to these rooms later. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/animated-pyramid_10-29e.mp4" type="video/mp4"></video></div></div></div><br>  <i><font color="gray">Agent-detected rooms and average result during training.</font></i>  <i><font color="gray">The degree of transparency of the room corresponds to how many times out of 10 passes of the agent it was detected.</font></i>  <i><font color="gray"><a href="">Video</a></font></i> <br><br><h1>  <a href="https://pathak22.github.io/large-scale-curiosity/">Large-scale curiosity study study</a> </h1><br>  Prior to the development of RND, we, along with staff from the University of California at Berkeley, investigated learning without any environmental rewards.  Curiosity provides an easier way to teach agents to interact with <i>any</i> environment, rather than using a specially designed reward function for a specific task, which is not yet a fact, which corresponds to the solution of the problem.  In projects like <a href="https://github.com/mgbellemare/Arcade-Learning-Environment">ALE</a> , <a href="https://github.com/openai/universe">Universe</a> , <a href="https://github.com/Microsoft/malmo">Malmo</a> , <a href="https://github.com/openai/gym">Gym</a> , <a href="https://github.com/openai/retro/">Gym Retro</a> , <a href="https://github.com/Unity-Technologies/ml-agents">Unity</a> , <a href="https://github.com/deepmind/lab">DeepMind Lab</a> , <a href="https://github.com/facebookresearch/CommAI-env">CommAI</a> , a large number of simulated media are opened to the agent through a standardized interface.  An agent using a generic reward function that is not specific to a particular environment can acquire a basic level of competence in a wide range of environments.  This allows him to determine useful behavior even in the absence of well-designed rewards. <br><br><blockquote>  <a href="https://arxiv.org/abs/1808.04355">The text of the scientific article</a> , <a href="https://github.com/openai/large-scale-curiosity">code</a> </blockquote><br>  In the standard training settings with reinforcements at each discrete time step, the agent sends the action to the environment, and it responds, giving the agent a new observation, a switch reward and an episode end indicator.  In our <a href="https://arxiv.org/abs/1808.04355">previous article,</a> we set up the environment <a href="https://arxiv.org/abs/1606.01868">to issue</a> <a href="https://pathak22.github.io/noreward-rl/">only the</a> following observation.  There, the agent studies the predictor model of the next state based on his experience and uses the prediction error as an internal reward.  As a result, he is attracted by unpredictability.  For example, changing a game account is rewarded only if the score is displayed on the screen and the change is difficult to predict.  An agent usually finds interactions with new objects useful, since the results of such interactions are usually more difficult to predict than other aspects of the environment. <br><br>  Like <a href="https://arxiv.org/abs/1507.00814">other</a> <a href="https://pathak22.github.io/noreward-rl/">researchers</a> , we tried to avoid modeling all aspects of the environment, regardless of whether they are relevant or not, choosing observation features for modeling.  Surprisingly, we found that even random functions work well. <br><br><h1>  What are the curious agents doing? </h1><br>  We tested our agent in more than 50 different environments and observed a range of competencies from seemingly random actions to conscious interaction with the environment.  To our surprise, in some cases, the agent managed to get through the game, although he was not informed of the goal through an external reward. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Internal reward at the beginning of training</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/breakout_int_rew_440.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Domestic reward jump at first level pass</font></i> <br><br>  <b>Breakout</b> - internal reward jumps, when the agent sees a new block configuration at an early stage of training and when he passes a level for the first time after training for several hours. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/BowlingSmaller.mp4" type="video/mp4"></video></div></div></div><br>  <b>Pong</b> - we trained the agent to control both platforms simultaneously, and he learned to keep the ball in play, which led to protracted fights.  Even when training against an in-game AI, the agent tried to extend the game as much as possible, rather than win. <br><br>  <b><a href="">Bowling</a></b> - the agent learned to play the game better than other agents who were trained to directly maximize the external reward.  We think this is happening, because the agent is attracted by the hardly predictable blinking of the scoreboard after the throws. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Mario.mp4" type="video/mp4"></video></div></div></div><br>  <b>Mario</b> - internal reward is particularly well consistent with the goal of the game: advance through the levels.  The agent is rewarded for searching for new areas, since the details of the newly found area cannot be predicted.  As a result, the agent found 11 levels, found secret rooms and even defeated bosses. <br><br><h1>  The problem of noisy TV </h1><br>  As a gambler on a slot machine, attracted by random results, an agent sometimes falls into the trap of his curiosity as a result of the ‚Äúproblem of a noisy TV‚Äù.  The agent finds a source of chance in the environment and continues to observe it, always experiencing a high internal reward for such transitions.  An example of such a trap is watching TV that reproduces static noise.  We demonstrate this literally by placing an agent in a Unity maze with a TV that plays random channels. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agent in a maze with noisy TV</font></i> <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/Navigation_withoutTV.mp4" type="video/mp4"></video></div></div></div>  <i><font color="gray">Agent in the maze without a noisy TV</font></i> <br><br>  Theoretically, the problem of noisy TV is really serious, but we still expected that in much deterministic environments like Revenge of Montezuma, curiosity would force the agent to find rooms and interact with objects.  We tried several options for predicting the next state based on curiosity, combining a bonus for research with a score from the game. <br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/montezuma.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/pitfall.mp4" type="video/mp4"></video></div></div></div><br><br><div class="oembed"><div><div style="left: 0; width: 100%; height: 0; position: relative; padding-bottom: 56.25%;"><video controls="" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute;">  Your browser does not support HTML5 video. <source src="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/prediction-based-rewards/privateeye.mp4" type="video/mp4"></video></div></div></div><br>  In these experiments, the agent controls the environment through a noise controller, which with some probability repeats the last action instead of the current one.  This setup with repeatable sticky actions has been <a href="https://arxiv.org/abs/1709.06009">proposed</a> as a best practice for training agents in fully deterministic games, such as Atari, to prevent memorization.  Sticky actions make the transition from room to room unpredictable. <br><br><h1>  Random distillation of the network </h1><br>  Since the prediction of the next state is inherently susceptible to the problem of a noisy TV, we have identified the following relevant sources of prediction errors: <br><br><ul><li>  <b>Factor 1</b> .  The prediction error is high if the predictor fails to summarize from the previously discussed examples.  New experience corresponds to a high prediction error. </li><li>  <b>Factor 2</b> .  The prediction error is high due to the stochastic prediction goal. </li><li>  <b>Factor 3</b> .  The prediction error is high due to the lack of information necessary for the prediction, or because the class of predictor models is too limited to fit the complexity of the objective function. </li></ul><br>  We determined that factor 1 is a useful source of errors, since it quantifies the novelty of the experience, while factors 2 and 3 lead to the problem of a noisy TV.  To avoid factors 2 and 3, we developed RND - a new research bonus based on <b>predicting the issuance of a constant and randomly initialized neural network in the next state, taking into account the next state itself</b> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db6/ac9/7fc/db6ac97fc37b0914e1a62145f855820c.svg" width="780"><br><br>  Intuition suggests that prognostic models have a low error in predicting the states in which she was trained.  In particular, the agent's predictions about the issuance of a randomly initialized neural network will be less accurate in the new states than in the states that the agent has often met before.  The advantage of using the synthetic prediction problem is that it can be deterministic (bypassing factor 2), and inside the function class, the predictor can choose a predictor of the same architecture as the target network (bypassing factor 3).  This saves RND from the problem of noisy TV. <br><br>  We combined the research bonus with external rewards through a proximal policy optimization ( <a href="https://blog.openai.com/openai-baselines-ppo/">PPO</a> ) variation of the closest policy, which uses <b>two values ‚Äã‚Äãfor two reward streams</b> .  This allows you to use different discounts for different rewards and to combine episodic and non-episodic rewards.  <b>Thanks to this additional flexibility, our best agent often finds 22 out of 24 rooms on the first level in Revenge of Montezuma, and sometimes passes the first level after finding the remaining two rooms.</b>  The same method demonstrates record performance in Venture and Gravitar games. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/043/51a/ee8/04351aee8d6be917caf1994b968e04b9.svg" width="780"><br>  The visualization below shows the internal reward schedule in the Episode "Revenge of Montezuma", where the agent finds the torch for the first time. <br><br><img src="https://habrastorage.org/webt/hu/vg/px/huvgpxpbzzc3-reechovxyhjcjs.gif"><br><br><h1>  Important competent implementation </h1><br>  To choose a good algorithm, it is important to consider general considerations, such as susceptibility to the problem of a noisy TV.  However, we found that seemingly very small changes in our simple algorithm strongly affect its efficiency: from an agent who cannot leave the first room to an agent who passes the first level.  To add stability to learning, we avoided saturating the signs and brought internal rewards to a predictable range.  We also noticed <b>significant improvements in the efficiency of the RND each time we found and fixed a bug</b> (our favorite includes accidental zeroing of the array, which caused external rewards to be regarded as non-episodic; we realized this only after thinking about the external function of which looked suspiciously periodic).  Correcting these details has become an important part of achieving high performance even when using algorithms that are conceptually similar to previous work.  This is one of the reasons why it is better to choose simple algorithms if possible. <br><br><h1>  Future work </h1><br>  We offer the following areas for further research: <br><br><ul><li>  Analysis of the benefits of various research methods and the search for new ways of combining them. </li><li>  Learning a curious agent in many different environments without reward and learning to transfer to a target environment with rewards. </li><li>  Global intelligence, including coordinated decisions on long time horizons. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/428776/">https://habr.com/ru/post/428776/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../428766/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ337 (October 29 - November 4, 2018)</a></li>
<li><a href="../428768/index.html">In three articles on least squares: literacy on the theory of probability</a></li>
<li><a href="../428770/index.html">Keyboard macros for everyday tasks</a></li>
<li><a href="../428772/index.html">Democratization of data in Uber</a></li>
<li><a href="../428774/index.html">GPS firewall for data centers - why you need it and how it works</a></li>
<li><a href="../428778/index.html">See invisible. Near infrared range (0.9-1.7¬µm)</a></li>
<li><a href="../428782/index.html">Microsoft has released a Linux version of the utility ProcDump</a></li>
<li><a href="../428784/index.html">The Ministry of Communications proposed to create alternative social networks and instant messengers</a></li>
<li><a href="../428786/index.html">Quantum processor based on spin resonance and manipulations with singlet-triplet system</a></li>
<li><a href="../428788/index.html">Under the hood of Bitfury Clarke - how our new mining chip works</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>