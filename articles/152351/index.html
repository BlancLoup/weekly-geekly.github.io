<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Problems in the work of services September 24-25</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="First of all, we want to make a formal apology for the biggest downtime in the history of Selektel. Below we will try to restore in detail the chronol...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Problems in the work of services September 24-25</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/eee/9a3/604/eee9a3604c29c57e63136acbc49eed0c.png" alt="image"><br>  First of all, we want to make a formal apology for the biggest downtime in the history of Selektel.  Below we will try to restore in detail the chronology of events, tell about what has been done to prevent such situations in the future, as well as compensation for clients affected by these problems. <br><a name="habracut"></a><br><br><h4>  First failure </h4><br>  The problems began on the evening of Monday September 24 ( <b>downtime 22:00 - 23:10</b> ).  From the side it looked like a loss of connectivity with the Petersburg segment of the network.  This failure caused problems in all of our Internet services in St. Petersburg;  Moscow network segment, as well as local ports of servers continued to work.  Also DNS (ns1.selectel.org and ns2.selectel.org), which are located in St. Petersburg, Moscow DNS (ns3.selectel.org) were not available, this failure did not affect.  Due to the lack of connectivity, access to the site and the control panel was lost, telephony accounted for the main load, and therefore many clients could not wait for a response. <br><br>  When analyzing the situation, we were able to immediately establish that the problem was caused by the incorrect operation of the aggregation level switches, which are two Juniper EX4500s combined into a single virtual chassis.  Visually, everything looked quite workable, but when connected to the console, a lot of messages were detected, which, however, did not allow to determine the exact cause of the problem. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre> Sep 24 22:02:02 chassism [903]: CM_TSUNAMI: i2c read on register (56) failed
 Sep 24 22:02:02 chassism [903]: cm_read_i2c errno: 16, device: 292
</pre><br>  In fact, all optical 10G Ethernet ports in the aggregation level switch chassis stopped working. <br><br><pre> Sep 24 22:01:49 chassisd [952]: CHASSISD_IFDEV_DETACH_PIC: ifdev_detach_pic (0/3)
 Sep 24 22:01:49 craftd [954]: Minor alarm set, FPC 0 PEM 0 Removed
 Sep 24 22:01:49 craftd [954]: Minor alarm set, FPC 0 PEM 1 Removed
</pre><br>  After rebooting, everything worked steadily.  Since the network configuration had not changed for a long time, and no work was done before the accident, we decided that this is a one-time problem.  Unfortunately, after only 45 minutes, the same switches stopped responding again, and were reset again ( <b>23:55 - 00:05</b> ). <br><br><h4>  Decrease switch priority in virtual chassis </h4><br>  Since in both cases the first one of the two switches in the virtual chassis failed, and only the second one failed to work, the assumption was made that the problem lies in it.  The virtual chassis was reconfigured in such a way that the second switch became the main one, while the other remained only as a reserve.  In the intervals between operations, the switches were reset again ( <b>00:40 - 00:55</b> ) <br><br><h4>  Disassembled virtual chassis, all links are transferred to one switch </h4><br>  After about an hour, another failure showed that the actions performed were not enough.  After releasing and sealing part of the port capacity, we decided to completely disconnect the failed device from the virtual chassis and transfer all links to a ‚Äúhealthy‚Äù switch.  At about 4:30 am it was done ( <b>02:28 - 03:01, 03:51 - 04:30</b> ). <br><br><h4>  Replacing the switch with a spare </h4><br>  However, an hour later, this switch stopped working.  For the time while he was still working from the reserve, exactly the same completely new switch was taken, installed and configured.  All traffic has been transferred to it.  Connectivity has appeared - the network has earned ( <b>05:30 - 06:05</b> ) <br><br><h4>  JunOS update </h4><br>  After 3 hours, around 9 am, everything happened again.  We decided to install a different version of the operating system (JunOS) on the switch.  After the update, everything worked ( <b>08:44 - 09:01</b> ) <br><br><h4>  Fiber break between data centers </h4><br>  Closer to 12:00 all cloud servers were launched.  But at 12:45 pm there was a damage to the optical signal in the cable, which combined network segments in different data centers.  At this point, due to the withdrawal of one of the two reference switches, the network worked only along one main route, the backup was disconnected.  This led to a loss of connectivity in the cloud between the host machines and the storage system (storage network), as well as to the inaccessibility of servers located in one of the St. Petersburg data centers. <br><br>  After the emergency team arrived at the place of cable damage, it turned out that the cable had been fired from an air rifle by hooligans who had been caught and transferred to the police. <br><img src="https://habrastorage.org/getpro/habr/post_images/a96/3b6/556/a963b655655db41b3895a34c833a394c.jpg" alt="image"><br>  Switching to the second channel was our obvious action, without waiting for the fiber to recover through the first channel.  This was done quickly enough, but everything just worked, as the switch hangs again.  ( <b>12:45 - 13:05</b> ) <br><br><h4>  Optical SFP + Transceivers </h4><br>  This time, in the new version of JunOS, intelligible messages appeared in the logs and it was possible to find a complaint about the inability to read the service information of one of the SFP + modules, <br><br><pre> Sep 25 13:01:06 chassism [903]: CM_TSUNAMI [FPC: 0 PIC: 0 Port: 18]: Failed to read SFP + ID EEPROM
 Sep 25 13:01:06 chassism [903]: xcvr_cache_eeprom: xcvr_read_eeprom failed - link: 18 pic_slot: 0
</pre><br>  After removing this module, the network has recovered.  We assumed that the problem was in this transceiver and the reaction to it from the switch, as this transceiver visited each of the 3 switches, which we consistently replaced before. <br><br>  However, after 3 hours, the situation repeated itself.  This time in the messages there was no indication of a failed module, we immediately decided to replace all the transvers with new ones from the reserve, but this did not help either.  We started watching all the transceivers in turn, pulling out one at a time, another problem transceiver was found already from the new batch.  Making sure that the problem with the switches was resolved, we re-crossed the intranet connections to go to the basic work scheme ( <b>16:07 - 16:31, 17:39 - 18:04, 18:22 - 18:27</b> ) <br><br><h4>  Recovery of cloud servers </h4><br>  Since the scale of the problem was initially unclear, we tried several times to raise cloud servers.  The machines located on the new storage (the beginning of the uuids for SR: d7e ... and e9f ...) survived the first accidents only as the inaccessibility of the Internet.  Cloud servers on old storages, alas, received an I / O Error for disks.  At the same time, very old virtual machines have switched to read only mode.  Machines of a new generation have an error = panic setting in fstab, which terminates the machine in case of an error.  After several restarts, unfortunately, there was a situation when preparing the hosts for launching the VM took an inadmissibly long time (the massive IO error for LVM is rather unpleasant; in some cases, the dying virtual machine turns into a zombie, and their capture and completion requires manual work every time) .  It was decided to restart the hosts on power.  This caused a reboot for virtual machines from new storage, which we really didn‚Äôt want to do, but allowed us to significantly (at least three times) reduce the launch time of all the others.  The repositories themselves were without network activity and with intact data. <br><br><h4>  Measures taken </h4><br>  Despite the fact that there was a reserve of equipment in the data centers, the network was built with redundancy, as well as a number of other factors ensuring stability and uninterrupted operation, the situation became unexpected for us. <br><br>  As a result, it was decided to implement the following activities: <br><ol><li>  Enhanced verification of optical transceivers and network equipment in the test environment; </li><li>  Armored Kevlar fiber-optic cable in places with risk of damage due to bullying; </li><li>  Accelerating the completion of cloud server infrastructure upgrades. </li></ol><br><br><h4>  Compensation </h4><br>  A question that interests everyone.  The table below shows the amount of compensation for various types of services as a percentage of the cost of the service provided per month, in accordance with the SLA. <br><br>  Given that formally the downtime of the services was less than that indicated in the table (the connectivity sometimes appeared and disappeared), it was decided to round down the downtime. <br><table><tbody><tr><th>  Service </th><th>  Downtime </th><th>  Compensation </th></tr><tr><td>  Virtual dedicated server </td><td>  11 o'clock </td><td>  thirty% </td></tr><tr><td>  Dedicated server / arbitrary configuration server </td><td>  11 o'clock </td><td>  thirty% </td></tr><tr><td>  Equipment placement </td><td>  11 o'clock </td><td>  thirty% </td></tr><tr><td>  CMS hosting </td><td>  11 o'clock </td><td>  thirty% </td></tr><tr><td>  Cloud servers </td><td>  24 hours </td><td>  50% </td></tr><tr><td>  Cloud storage </td><td>  11 o'clock </td><td>  50% </td></tr></tbody></table><br><br>  Once again, we apologize to all who are stung by this incident.  We perfectly understand how negative network inaccessibility affects clients, but somehow they could not speed up the solution of problems due to the fact that the situation was nonstandard.  We took all possible actions to fix the problems as quickly as possible, but unfortunately, it was not possible to determine the exact problem analytically and had to search for it by looking at all the possible options, which in turn took a lot of time. </div><p>Source: <a href="https://habr.com/ru/post/152351/">https://habr.com/ru/post/152351/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../152339/index.html">Medical reference for Android</a></li>
<li><a href="../152341/index.html">Google Play: 25 billion downloads and apps for $ 0.25</a></li>
<li><a href="../152345/index.html">WebServer as a test task</a></li>
<li><a href="../152347/index.html">Creative workshop: 80 creative tasks of a designer</a></li>
<li><a href="../152349/index.html">The early answer is currency rates, weather and traffic jams in the tips.</a></li>
<li><a href="../152353/index.html">100,000 passwords ieee.org for the whole month were in open access</a></li>
<li><a href="../152355/index.html">Autumn Technology Forum: focus on web development</a></li>
<li><a href="../152357/index.html">Car GPS tracker for MegaFon Autocontrol service</a></li>
<li><a href="../152363/index.html">Russian Post will work faster in 2 years</a></li>
<li><a href="../152365/index.html">Windows 8 - let it be SMEP!</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>