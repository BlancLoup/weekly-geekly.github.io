<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Bag of words and sentiment analysis on R</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article is based on (first part) of the Bag of Words Kaggle study task, but this is not a translation. The original task is done in Python. I wan...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Bag of words and sentiment analysis on R</h1><div class="post__text post__text-html js-mediator-article">  This article is based on (first part) of the <a href="http://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words">Bag of Words</a> Kaggle study task, but this is not a translation.  The original task is done in Python.  I wanted to evaluate the possibilities of the R language for processing natural language texts and at the same time try implementing the Random Forest implementation in the wrapper of the R package caret. <br><br>  The meaning of the task is to build a ‚Äúmachine‚Äù that will in a certain way handle film reviews in English and determine the tone of the review, relating it to one of two classes: negative / positive.  As a training sample, the task uses a data set with twenty-five thousand revisions from the IMDB, marked up by unknown volunteers. <br><a name="habracut"></a><br>  The code for the article and the task preparation can be found in my <a href="http://github.com/khmelkoff/BagOfWords">repository</a> on GitHub.  To do this, you will need R version 3.1.3 (if you ignore warnings from packages, then in 3.1.2 everything will work too).  Additionally, you must install the following packages: <br><br><ul><li>  <a href="http://cran.r-project.org/web/packages/tm/index.html">tm</a> - a set of tools for working with texts </li><li>  <a href="http://topepo.github.io/caret/index.html">caret</a> is a wrapper that supports more than 100 machine learning methods </li><li>  <a href="http://cran.r-project.org/web/packages/randomForest/index.html">randomForest</a> - a package that implements the algorithm of the same name </li><li>  SnowballC - auxiliary package for lemmatization in tm </li><li>  e1071 - package of additional functions for randomForest </li></ul><br>  The task in abbreviated form (a thousand reviews and about a hundred of the most common terms) can be done on a machine with a 32-bit OS, but the full version will require 64 bits and 16 GB of RAM.  During training, the model uses about 12 GB of physical memory. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      A training set is a tab-delimited data set.  The control sample also contains 25,000 revues, but of course without markup.  Files can be downloaded from <a href="http://www.kaggle.com/c/word2vec-nlp-tutorial/data">this</a> page.  This will require registration at Kaggle. <br><br><h4>  Preliminary data processing </h4><br>  After downloading and unpacking, I recommend to check the dimension, it should be 25000 rows in three columns.  I did not succeed immediately because of quotes in the text.  I experimented on the first record.  The corresponding review is quite long, 2304 characters, so I quote only its fragment: <br><br><pre>  It was originally released. "\" Moonwalker  This is a mi-kay message. .. " </pre><br><br>  The text has punctuation marks, HTML tags and special characters that you had to get rid of.  For starters, I cleared the HTML review.  Then he removed the punctuation marks, special characters, numbers.  Generally speaking, with such an analysis, some constructions from punctuation marks, like emoticons, it is better not to delete, but convert to words, but I didn‚Äôt do this, I just converted all the characters to lower case and made out individual words.  It remains to remove the words of general vocabulary that did not add meaning to the review.  I took a list of such words for English from the tm package.  For the sake of interest, I looked at the first 20: <br><br>  <i>‚ÄúI‚Äù ‚Äúme‚Äù ‚Äúmy‚Äù ‚Äúmyself‚Äù ‚Äúwe‚Äù ‚Äúour‚Äù ‚Äúours‚Äù ‚Äúwe‚Äù ‚Äúyou‚Äù ‚Äúyour‚Äù ‚Äúyours‚Äù ‚Äúyourself‚Äù ‚Äúyourselves‚Äù ‚Äúhe‚Äù ‚Äúhim‚Äù ‚Äúhis‚Äù ‚Äúhimself‚Äù "" She "" her "" hers "</i> <br><br>  Then I repeated the above operations for all 25,000 entries in the training set.  At the same time removed one-letter words left over from constructions like MJ's. <br><br><h4>  Bag of words </h4><br>  A bag of words (or Bag of Words) is a model of texts in a natural language, in which each document or text looks like an unordered set of words without knowing the links between them.  It can be represented as a matrix, each line in which corresponds to a separate document or text, and each column to a specific word.  The cell at the intersection of the row and column contains the number of occurrences of the word in the corresponding document. <br><br>  To prepare the word bag, I used the tm package.  This package as an object works with the so-called linguistic corpus of the first order - a collection of texts united by a common feature.  In our case - all texts are movie reviews.  In order to create a body, you first need to convert the texts into a vector, each element of which represents a separate document.  And then build on the basis of the housing matrix "document-term".  She will become a bag of words. <br><br>  First, I got a matrix of 25,000 documents and 73,759 terms.  Obviously, there were too many terms.  After lemmatization, in other words, bringing the word form to a lemma - the normal, vocabulary form, there are 49,549 terms left.  It was still a lot.  Therefore, I removed the words that are particularly rare in the reviews.  In the original assignment, 5,000 words were used, unfortunately, the sparsity parameter has a non-linear effect on the number of terms, so, varying it, I stopped at the number 5072. <br><br>  This is how the fragment of the Bag looked with the most frequently used words: <br><pre> Docs act
     1 0 0 1 2 1 0 3 0 0 1
     2 0 0 0 0 0 0 0 0 0 1
     3 0 1 0 0 0 0 1 0 1 0
     4 0 1 0 1 2 0 0 0 0 1
</pre><br><h4>  Model training </h4><br>  To build the model, I chose the randomForest package in the caret wrapper.  Caret allows you to flexibly manage the learning process, has built-in capabilities for data preprocessing and quality control of training.  Besides with caret you can use several processor cores at once. <br><br>  On a real sample (25000 x 5072) with a specified number of trees, nree = 100, training took about 110 hours on a computer with OS X 10.8, 2.3 GHz Intel Core i7, 16 GB 1600 MHz DDR3. <br><br>  I note that when using caret, three models with different mtry values ‚Äã‚Äãare trained at once, and then the optimal one is chosen according to the Accuracy indicator.  mtry sets the number of terms that are randomly selected with each branch of the tree. <br><br>  By default, when training a model, it is supposed to use bootstrapping with 25 cycles.  In order to save computer time, I replaced it with a sliding control with five groups, i.e.  in each cycle, 80% is used for training, 20% for control. <br><br>  The best model for a reduced sample showed an accuracy of 0.71.  This meant that approximately 29% of all reviews as a result would be classified incorrectly.  But it is on the control sample, even with a sliding control.  On the test sample, the accuracy will be slightly worse.  But for our abbreviated data set this is not so bad. <br><br>  The varImp function from the caret package provides a list of indicators used to train the model, in descending order of importance.  The first 10 looked quite expected: <br><br>  term rate <br><ol><li>  great 100.00000 </li><li>  bad 81.79039 </li><li>  movi 59.40987 </li><li>  film 53.80526 </li><li>  even 42.43621 </li><li>  love 39.98373 </li><li>  time 38.14139 </li><li>  best 36.50986 </li><li>  one 36.36246 </li><li>  like 35.85307 </li></ol><br><h4>  Kaggle forecasting and testing </h4><br>  I processed the test sample in the same way as the training sample with one exception - the dictionary obtained from the training sample was used to select terms in the test one. <br><br>  After I had a matrix of documents / terms, I converted it into a data frame, which I sent to predict () for prediction using a trained model.  The forecast was saved in csv and sent via Kaggle via the form on the website. <br><br><img src="https://habrastorage.org/files/6fc/0dd/699/6fc0dd6993b546f2851b7ede514f098b.png"><br><br>  The result is quite modest, 231 out of 277, but still my ‚Äúcar‚Äù was wrong about one in six cases, which inspires some optimism. <br><br>  As it turned out, the possibilities of R are quite enough for processing natural language texts.  Unfortunately, I do not have data on how much faster the task is performed in Python.  The tm package is not the only available tool, R has interfaces to <a href="http://opennlp.apache.org/">OpenNLP</a> , <a href="http://www.cs.waikato.ac.nz/ml/weka/">Weka</a> and a number of packages that allow you to solve individual problems.  More or less current tool list can be found <a href="http://cran.r-project.org/web/views/NaturalLanguageProcessing.html">here</a> .  There is even a Google implementation of <a href="http://code.google.com/p/word2vec/">word2vec</a> and, despite the fact that it is still in development, you can try to make the second part of the Kaggle task, where word2vec is a central technological component. </div><p>Source: <a href="https://habr.com/ru/post/255143/">https://habr.com/ru/post/255143/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../255133/index.html">How I fumbled with one button different data in the Android application</a></li>
<li><a href="../255135/index.html">Reverse Engineering ESP8266 - Part 1</a></li>
<li><a href="../255137/index.html">Exploring JavaScript Symbols. Symbol - new data type in javascript</a></li>
<li><a href="../255139/index.html">Collect the best of two worlds - frameworks and CMS (part 2)</a></li>
<li><a href="../255141/index.html">R-prong electrocardiogram as a parameter of the tree of Pythagoras</a></li>
<li><a href="../255145/index.html">PHP Recipes programming. 3rd ed</a></li>
<li><a href="../255151/index.html">Meet ReSharper C ++</a></li>
<li><a href="../255153/index.html">Reverse Engineering ESP8266 - part 2</a></li>
<li><a href="../255155/index.html">12 balls</a></li>
<li><a href="../255163/index.html">DevCon 2015: announcement of a master class on Unity 5 from the creators of the platform</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>