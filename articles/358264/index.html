<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Kubernetes-ha. Deploy Kubernetes failover cluster with 5 masters</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 

 In this publication, I would like to talk about the Kubernetes High Availability Cluster (HA). 



 Table of contents: 



1. Introduction 
...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Kubernetes-ha. Deploy Kubernetes failover cluster with 5 masters</h1><div class="post__text post__text-html js-mediator-article">  Hello! <br><br>  In this publication, I would like to talk about the Kubernetes High Availability Cluster (HA). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/119/e98/869/119e988691435f6a2706feee1a8f0c0d.png" alt="image">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Table of contents: <br><br><ol><li>  <a href="https://habr.com/ru/post/358264/">Introduction</a> </li><li>  <a href="https://habr.com/ru/post/358264/">List of software used</a> </li><li>  <a href="https://habr.com/ru/post/358264/">Host List and Assignment</a> </li><li>  <a href="https://habr.com/ru/post/358264/">Principle of operation and deployment</a> </li><li>  <a href="https://habr.com/ru/post/358264/">Preparing the OS for deployment.</a>  <a href="https://habr.com/ru/post/358264/">Install docker, kubeadm, kubelet and kubectl</a> </li><li>  <a href="https://habr.com/ru/post/358264/">Preparation of the configuration script</a> </li><li>  <a href="https://habr.com/ru/post/358264/">Creating etcd cluster</a> </li><li>  <a href="https://habr.com/ru/post/358264/">Wizard initialization with kubeadm</a> </li><li>  <a href="https://habr.com/ru/post/358264/">CIDR setup</a> </li><li>  <a href="https://habr.com/ru/post/358264/">Initialize the rest of the master</a> </li><li>  <a href="https://habr.com/ru/post/358264/">Configuring keepalived and virtual IP</a> </li><li>  <a href="https://habr.com/ru/post/358264/">Adding working nodes to the cluster</a> </li><li>  <a href="https://habr.com/ru/post/358264/">Install ingress-nginx</a> </li><li>  Additionally <br><ul><li>  <a href="https://habr.com/ru/post/358264/">Dashboard</a> </li><li>  <a href="https://habr.com/ru/post/358264/">Heapster</a> </li></ul><br></li></ol><a name="habracut"></a><br><a name="introduction"></a>  <strong>Introduction</strong> <br><br>  At the new place of work I had to face an interesting task, namely: to deploy a highly accessible kubernetes cluster.  The main message of the task was to achieve maximum resiliency of the cluster in the event of failure of physical machines. <br><br>  A small introduction: <br><br>  I got a job with a project with a minimum amount of documentation and one deployed stand, on which the individual components of this project ‚Äúhung out‚Äù in docker containers.  Four fronts for different services, launched through pm2, also worked at this stand. <br><br>  After I was able to figure out the scheme of services and the logic of their work, it was up to the choice of the infrastructure on which the project would work.  After all the discussions, we stopped at two versions of the development of events.  The first is to stuff everything into lxc containers and steer everything with the help of ansible.  The second is to leave everything at docker and try k8s in work. <br>  In the first version, most of the projects in our company work.  However, in this case, we decided to leave everything to the docker, but put the project in a failover cluster using kubernetes. <br><br>  For enhanced fault tolerance, it was decided to deploy the cluster with five master nodes. <br><br>  According to the table in the etcd documentation on <a href="https://coreos.com/etcd/docs/latest/v2/admin_guide.html">the CoreOS site</a> , <br><br><img src="https://habrastorage.org/webt/pi/22/lc/pi22lczm4hcv7vjz_bld6yesydg.png"><br><br>  It is recommended to have an odd number of members in the cluster.  In order for the cluster to continue working after the failure of one member (in our case, the master kubernetes), you need at least 3 machines.  In order for the cluster to work after losing 2 machines, they need to have 5. We decided to play it safe and deploy a version with 5 masters. <br><br>  Kubernetes has very detailed official documentation, although, in my opinion, quite complicated;  especially when faced with this product for the first time. <br><br>  It is bad that in the documents the work schemes are mainly described when there is only one node with the master role in the cluster.  There is also not much information on the work of the cluster in the HA mode on the Internet, and in my Russian part, in my opinion, it is not at all.  So I decided to share my experience.  Perhaps he is useful to someone.  So, I'll start: <br><a name="deployshema"></a><br>  The main idea was spied on <a href="https://github.com/cookeem/kubeadm-ha">githab from cookeem</a> .  In general, I implemented it, correcting most of the flaws in the configs, increasing the number of master nodes in the cluster to five.  All of the following configs and scripts can be downloaded from <a href="https://github.com/rjeka/kubernetes-ha">my repository on GitHub</a> . <br><br><h4>  Brief outline and description of the deployment architecture </h4><img src="https://habrastorage.org/webt/qo/mi/un/qomiunfreqwd2oyor5h-hjrjzm8.png"><br><br>  The whole essence of the scheme is as follows: <br><br><ul><li>  create etcd cluster </li><li>  With the help of kubeadm init we create the first master certificates, keys, etc. </li><li>  using the generated configuration files, we initialize the remaining 4 master nodes <br></li><li>  configure the nginx balancer on each master node for the virtual address </li><li>  we change the address and port of the server API to a dedicated virtual address </li><li>  Add working nodes to the cluster </li></ul><br><a name="po"></a><br>  <strong>List of software used</strong> <br><ul><li>  linux: <br>  The choice of operating system.  Initially we wanted to try CoreOS in our work, but at the very moment of our choice the company that produced this OS was acquired by RedHat.  After acquiring CoreOS, RedHat did not announce its future plans for the acquired developments, so we were afraid to use it, due to possible licensing restrictions in the future. <br><br>  I chose Debian 9.3 (Stretch) simply because I was more used to working with it;  In general, there is not much difference in the choice of OS for Kubernetes.  The whole scheme below will work on any supported OS, from the <a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/">list in the official documentation for kubernetes</a> <br><br><ul><li>  Debian </li><li>  Ubuntu </li><li>  HypriotOS </li><li>  CentOS </li><li>  Rhel </li><li>  Fedora </li><li>  Container linux </li></ul></li><li>  containers: <br><br>  At the time of this writing, docker version 17.03.2-ce, build f5ec1e2 and docker-compose version 1.8.0 recommended by the documentation. </li><li>  Kubernetes v1.9.3 </li><li>  networks add-ons: flannel </li><li>  Balancer: nginx <br><br>  Virtual IP: keepalived Version: 1: 1.3.2-1 </li></ul><br><a name="hostslist"></a><br><h4>  Host list </h4><br><table><tbody><tr><th>  <b>Hostnames</b> </th><th>  <b>IP address</b> </th><th>  <b>Description</b> </th><th>  <b>Components</b> </th></tr><tr><td>  hb-master01 ~ 03 </td><td>  172.26.133.21 ~ 25 </td><td>  master nodes * 5 </td><td>  keepalived, nginx, etcd, kubelet, kube-apiserver, kube-scheduler, kube-proxy, kube-dashboard, heapster </td></tr><tr><td>  N \ A </td><td>  172.26.133.20 </td><td>  keepalived virtual IP </td><td>  N \ A </td></tr><tr><td>  hb-node01 ~ 03 </td><td>  172.26.133.26 ~ 28 </td><td>  Work nodes * 3 </td><td>  kubelet, kube-proxy </td></tr></tbody></table><br><a name="beforebegin"></a><br><h4>  Preparing the OS for deployment.  Install docker, kubeadm, kubelet and kubectl </h4><br>  Before starting the deployment, you need to prepare the system on all nodes of the cluster, namely: install the necessary packages, configure the firewall, disable the swap As they say, before you begin. <br><br><pre><code class="bash hljs">$ sudo -i :~<span class="hljs-comment"><span class="hljs-comment">#</span></span></code> </pre> <br>  If swap is used, then it needs to be disabled;  kubeadm does not support swap.  I immediately put the system without a swap partition. <br><br><pre> <code class="bash hljs">swapoff -a</code> </pre> <br>  Rules / etc / fstab.  Or manually <br><br><pre> <code class="bash hljs">vim /etc/fstab <span class="hljs-comment"><span class="hljs-comment"># swap was on /dev/sda6 during installation #UUID=5eb7202b-68e2-4bab-8cd1-767dc5a2ee9d none swap sw 0 0</span></span></code> </pre> <br>  Lyibo via sed <br><br><pre> <code class="bash hljs">sed -i <span class="hljs-string"><span class="hljs-string">'/ swap / s/^\(.*\)$/#\1/g'</span></span> /etc/fstab</code> </pre> <br>  In Debian 9, there is no selinux.  If it is in your distribution, then it needs to be translated into permissive mode <br><br>  If there are any rules in iptables, then they should be cleared.  During installation and configuration, Docker and kubernetes prescribe their firewall rules. <br><br>  On each node of the cluster, you must specify the correct hostname. <br><br><pre> <code class="bash hljs">vim /etc/hostname hb-master01</code> </pre><br>  This completes the preparation, restart before the next step. <br><br><pre> <code class="bash hljs">reboot</code> </pre> <br>  On the cluster machine, install the docker according to the instructions from the <a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/">kubernetes documentation</a> : <br><br><pre> <code class="bash hljs">apt-get update apt-get install -y \ apt-transport-https \ ca-certificates \ curl \ software-properties-common \ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add - add-apt-repository \ <span class="hljs-string"><span class="hljs-string">"deb https://download.docker.com/linux/</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(. /etc/os-release; echo "$ID")</span></span></span><span class="hljs-string"> \ </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(lsb_release -cs)</span></span></span><span class="hljs-string"> \ stable"</span></span> apt-get update &amp;&amp; apt-get install -y docker-ce=$(apt-cache madison docker-ce | grep 17.03 | head -1 | awk <span class="hljs-string"><span class="hljs-string">'{print $3}'</span></span>) docker-compose</code> </pre> <br>  Further we put kubeadm, kubelet and kubectl according to the same instruction. <br><br><pre> <code class="bash hljs">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl</code> </pre> <br>  Install keepalived: <br><br><pre> <code class="bash hljs">apt-get install keepalived systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> keepalived &amp;&amp; systemctl restart keepalived</code> </pre> <br>  For proper operation of CNI (Container Network Interface) you need to set / proc / sys / net / bridge / bridge-nf-call-iptables to 1 <br><br><pre> <code class="bash hljs">sysctl net.bridge.bridge-nf-call-iptables=1</code> </pre> <br><a name="startscript"></a><br><h4>  Preparation of the configuration script </h4><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/rjeka/kubernetes-ha.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ha</code> </pre> <br>  On each master node we prepare the script create-config.sh <br><br><pre> <code class="bash hljs">vim create-config.sh <span class="hljs-comment"><span class="hljs-comment">#!/bin/bash # local machine ip address export K8SHA_IPLOCAL=172.26.133.21 # local machine etcd name, options: etcd1, etcd2, etcd3, etcd4, etcd5 export K8SHA_ETCDNAME=etcd1 # local machine keepalived state config, options: MASTER, BACKUP. One keepalived cluster only one MASTER, other's are BACKUP export K8SHA_KA_STATE=MASTER # local machine keepalived priority config, options: 102, 101, 100, 99, 98. MASTER must 102 export K8SHA_KA_PRIO=102 # local machine keepalived network interface name config, for example: eth0 export K8SHA_KA_INTF=ens18 ####################################### # all masters settings below must be same ####################################### # master keepalived virtual ip address export K8SHA_IPVIRTUAL=172.26.133.20 # master01 ip address export K8SHA_IP1=172.26.133.21 # master02 ip address export K8SHA_IP2=172.26.133.22 # master03 ip address export K8SHA_IP3=172.26.133.23 # master04 ip address export K8SHA_IP4=172.26.133.24 # master05 ip address export K8SHA_IP5=172.26.133.25 # master01 hostname export K8SHA_HOSTNAME1=hb-master01 # master02 hostname export K8SHA_HOSTNAME2=hb-master02 # master03 hostname export K8SHA_HOSTNAME3=hb-master03 # master04 hostname export K8SHA_HOSTNAME4=hb-master04 # master04 hostname export K8SHA_HOSTNAME4=hb-master05 # keepalived auth_pass config, all masters must be same export K8SHA_KA_AUTH=55df7dc334c90194d1600c483e10acfr # kubernetes cluster token, you can use 'kubeadm token generate' to get a new one export K8SHA_TOKEN=4ae6cb.9dbc7b3600a3de89 # kubernetes CIDR pod subnet, if CIDR pod subnet is "10.244.0.0/16" please set to "10.244.0.0\\/16" export K8SHA_CIDR=10.244.0.0\\/16 ############################## # please do not modify anything below ##############################</span></span></code> </pre><br>  In the configuration file itself, cookeem has quite detailed comments, but still let's go over the main points: <br><br><div class="spoiler">  <b class="spoiler_title">Decrypt create-config.sh</b> <div class="spoiler_text">  # settings on the local machine of each node (each node has its own) <br>  <b>K8SHA_IPLOCAL</b> - the IP address of the node on which the script is configured <br>  <b>K8SHA_ETCDNAME</b> is the name of the local machine in the ETCD cluster, respectively on master01 - etcd1, master02 - etcd2, etc. <cut></cut><br>  <b>K8SHA_KA_STATE</b> - role in keepalived.  One MASTER node, all other BACKUP. <br>  <b>K8SHA_KA_PRIO</b> - keepalived priority, the master has 102 of the remaining 101, 100, ..... 98.  When the master falls with the number 102, its place is occupied by the node with the number 101, and so on. <br>  <b>K8SHA_KA_INTF</b> - keepalived network interface.  The name of the interface that will listen <br><br>  # general settings for all masternode are the same <br>  <b>K8SHA_IPVIRTUAL</b> = 172.26.133.20 - virtual IP cluster. <br>  <b>K8SHA_IP1 ... K8SHA_IP5 - IP</b> addresses of masters <br>  <b>K8SHA_HOSTNAME1 ... K8SHA_HOSTNAME5</b> - hostnames for masternotes.  An important point, under these names, kubeadm will generate certificates. <br>  <b>K8SHA_KA_AUTH</b> - password for keepalived.  You can set any <br>  <b>K8SHA_TOKEN</b> - cluster token.  You can generate the command <b>kubeadm token generate</b> <br>  <b>K8SHA_CIDR</b> - subnet address for pods.  I use flannel therefore CIDR 0.244.0.0/16.  Must be shielded - in the config should be K8SHA_CIDR = 10.244.0.0 \\ / 16 <br></div></div><br>  After all the values ‚Äã‚Äãare registered, on each master-quest, you need to run the create-config.sh script to create configs. <br><br><pre> <code class="bash hljs">kubernetes-ha<span class="hljs-comment"><span class="hljs-comment"># ./create-config.sh</span></span></code> </pre> <br><a name="etcd"></a><br><h4>  Creating etcd cluster </h4><br>  Based on the received configs, we create an etcd cluster <br><br><pre> <code class="bash hljs">docker-compose --file etcd/docker-compose.yaml up -d</code> </pre><br>  After the containers have been raised on all the wizards, we check the status of etcd <br><br><pre> <code class="bash hljs">docker <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -ti etcd etcdctl cluster-health member 3357c0f051a52e4a is healthy: got healthy result from http://172.26.133.24:2379 member 4f9d89f3d0f7047f is healthy: got healthy result from http://172.26.133.21:2379 member 8870062c9957931b is healthy: got healthy result from http://172.26.133.23:2379 member c8923ecd7d317ed4 is healthy: got healthy result from http://172.26.133.22:2379 member cd879d96247aef7e is healthy: got healthy result from http://172.26.133.25:2379 cluster is healthy</code> </pre><br><pre> <code class="bash hljs">docker <span class="hljs-built_in"><span class="hljs-built_in">exec</span></span> -ti etcd etcdctl member list 3357c0f051a52e4a: name=etcd4 peerURLs=http://172.26.133.24:2380 clientURLs=http://172.26.133.24:2379,http://172.26.133.24:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span> 4f9d89f3d0f7047f: name=etcd1 peerURLs=http://172.26.133.21:2380 clientURLs=http://172.26.133.21:2379,http://172.26.133.21:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span> 8870062c9957931b: name=etcd3 peerURLs=http://172.26.133.23:2380 clientURLs=http://172.26.133.23:2379,http://172.26.133.23:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span> c8923ecd7d317ed4: name=etcd2 peerURLs=http://172.26.133.22:2380 clientURLs=http://172.26.133.22:2379,http://172.26.133.22:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">true</span></span> cd879d96247aef7e: name=etcd5 peerURLs=http://172.26.133.25:2380 clientURLs=http://172.26.133.25:2379,http://172.26.133.25:4001 isLeader=<span class="hljs-literal"><span class="hljs-literal">false</span></span></code> </pre> <br>  If the cluster is all right, then move on.  If something is wrong, then we look at the logs <br><br><pre> <code class="bash hljs">docker logs etcd</code> </pre> <br><a name="firstmaster"></a><br><h4>  Initializing the first master node with kubeadm </h4><br>  On hb-master01, using kubeadm, we initialize the kubernetes cluster. <br><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml</code> </pre> <br>  If there is an error according to Kubelet version, then you need to add a key <br><br><pre> <code class="bash hljs">--ignore-preflight-errors=KubeletVersion</code> </pre> <br>  After the master is initialized, kubeadm will display the service information.  It will contain a token and a hash to initialize other cluster members.  Be sure to save the line of the form: <b>kubeadm join --token XXXXXXXXXXXXX 172.26.133.21:6443 --discovery-token-ca-cert-hash sha256: XXXXXXXXXXXXXXXXXXXXXXXXX</b> somewhere separately, since this information is output once;  if the tokens are lost, they will have to be regenerated. <br><br><pre> <code class="bash hljs">Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config sudo chown $(id -u):$(id -g) <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config You should now deploy a pod network to the cluster. Run <span class="hljs-string"><span class="hljs-string">"kubectl apply -f [podnetwork].yaml"</span></span> with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join --token XXXXXXXXXXXX 172.26.133.21:6443 --discovery-token-ca-cert-hash sha256:XXXXXXXXXXXXXXXXXXXXXXX</code> </pre><br>  Next, you need to set the environment variable, to be able to work with the cluster as root <br><br><pre> <code class="bash hljs">vim ~/.bashrc <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> KUBECONFIG=/etc/kubernetes/admin.conf</code> </pre> <br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br>  If you want to work as a regular user, then follow the instructions that appeared on the screen when the wizard initialized. <br><br><pre> <code class="bash hljs">To start using your cluster, you need to run the following as a regular user: mkdir -p <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config sudo chown $(id -u):$(id -g) <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config</code> </pre><br>  We check that everything was done correctly: <br><br><pre> <code class="bash hljs">kubectl get node NAME STATUS ROLES AGE VERSION hb-master01 NotReady master 22m v1.9.5      NotReady        cidr,  .</code> </pre><br><a name="cidr"></a><br><h4>  CIDR setup </h4><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml clusterrole <span class="hljs-string"><span class="hljs-string">"flannel"</span></span> created clusterrolebinding <span class="hljs-string"><span class="hljs-string">"flannel"</span></span> created serviceaccount <span class="hljs-string"><span class="hljs-string">"flannel"</span></span> created configmap <span class="hljs-string"><span class="hljs-string">"kube-flannel-cfg"</span></span> created daemonset <span class="hljs-string"><span class="hljs-string">"kube-flannel-ds"</span></span> created</code> </pre> <br>  Check that everything is OK <br><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kube-apiserver-hb-master01 1/1 Running 0 1h kube-system kube-controller-manager-hb-master01 1/1 Running 0 1h kube-system kube-dns-6f4fd4bdf-jdhdk 3/3 Running 0 1h kube-system kube-flannel-ds-hczw4 1/1 Running 0 1m kube-system kube-proxy-f88rm 1/1 Running 0 1h kube-system kube-scheduler-hb-master01 1/1 Running 0 1h</code> </pre> <br><a name="othermaster"></a><br><h4>  Initialize the rest of the master </h4><br>  Now, after our cluster is working with one node, it is time to introduce the remaining master codes into the cluster. <br><br>  To do this, with hb-master01, you need to copy the / etc / kubernetes / pki directory to the remote / etc / kubernetes / directory of each wizard.  To copy in the ssh settings, I temporarily allowed the connection root.  After copying the files, of course, this feature is disabled. <br><br>  On each of the remaining masters, set up an ssh server <br><br><pre> <code class="bash hljs">vim /etc/ssh/sshd_config PermitRootLogin yes systemctl restart ssh</code> </pre><br>  Copy files <br><br><pre> <code class="bash hljs">scp -r /etc/kubernetes/pki 172.26.133.22:/etc/kubernetes/ \ &amp;&amp; scp -r /etc/kubernetes/pki 172.26.133.23:/etc/kubernetes/ \ &amp;&amp; scp -r /etc/kubernetes/pki 172.26.133.24:/etc/kubernetes/ \ &amp;&amp; scp -r /etc/kubernetes/pki 172.26.133.25:/etc/kubernetes/</code> </pre><br>  Now on hb-master02, use kubeadm to start the cluster, make sure that pod kube-apiserver- is in working condition. <br><br><pre> <code class="bash hljs">kubeadm init --config=kubeadm-init.yaml Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube sudo cp -i /etc/kubernetes/admin.conf <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config sudo chown $(id -u):$(id -g) <span class="hljs-variable"><span class="hljs-variable">$HOME</span></span>/.kube/config You should now deploy a pod network to the cluster. Run <span class="hljs-string"><span class="hljs-string">"kubectl apply -f [podnetwork].yaml"</span></span> with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join --token xxxxxxxxxxxxxx 172.26.133.22:6443 --discovery-token-ca-cert-hash sha256:xxxxxxxxxxxxxxxxxxxxxxxxx</code> </pre><br>  Repeat on hb-master03, hb-master04, hb-master05 <br><br>  Check that all masters are initialized and work in a cluster. <br><br><pre> <code class="bash hljs">kubectl get nodes NAME STATUS ROLES AGE VERSION hb-master01 Ready master 37m v1.9.5 hb-master02 Ready master 33s v1.9.5 hb-master03 Ready master 3m v1.9.5 hb-master04 Ready master 17m v1.9.5 hb-master05 Ready master 19m v1.9.5</code> </pre><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system kube-apiserver-hb-master01 1/1 Running 0 6m kube-system kube-apiserver-hb-master02 1/1 Running 0 1m kube-system kube-apiserver-hb-master03 1/1 Running 0 1m kube-system kube-apiserver-hb-master04 1/1 Running 0 1m kube-system kube-apiserver-hb-master05 1/1 Running 0 10s kube-system kube-controller-manager-hb-master01 1/1 Running 0 6m kube-system kube-controller-manager-hb-master02 1/1 Running 0 1m kube-system kube-controller-manager-hb-master03 1/1 Running 0 1m kube-system kube-controller-manager-hb-master04 1/1 Running 0 1m kube-system kube-controller-manager-hb-master05 1/1 Running 0 9s kube-system kube-dns-6f4fd4bdf-bnxl8 3/3 Running 0 7m kube-system kube-flannel-ds-j698p 1/1 Running 0 6m kube-system kube-flannel-ds-mf9zc 1/1 Running 0 2m kube-system kube-flannel-ds-n5vbm 1/1 Running 0 2m kube-system kube-flannel-ds-q7ztg 1/1 Running 0 1m kube-system kube-flannel-ds-rrrcq 1/1 Running 0 2m kube-system kube-proxy-796zl 1/1 Running 0 1m kube-system kube-proxy-dz25s 1/1 Running 0 7m kube-system kube-proxy-hmrw5 1/1 Running 0 2m kube-system kube-proxy-kfjst 1/1 Running 0 2m kube-system kube-proxy-tpkbt 1/1 Running 0 2m kube-system kube-scheduler-hb-master01 1/1 Running 0 6m kube-system kube-scheduler-hb-master02 1/1 Running 0 1m kube-system kube-scheduler-hb-master03 1/1 Running 0 1m kube-system kube-scheduler-hb-master04 1/1 Running 0 48s kube-system kube-scheduler-hb-master05 1/1 Running 0 29s</code> </pre><br>  Create replicas of the kube-dns service.  On hb-master01 run <br><br><pre> <code class="bash hljs">kubectl scale --replicas=5 -n kube-system deployment/kube-dns</code> </pre> <br>  On all masternods, make a line with the number of api servers in the configuration file <br>  If you are using kubernetes versions greater than 1.9, you can skip this step. <br><pre> <code class="bash hljs">vim /etc/kubernetes/manifests/kube-apiserver.yaml - --apiserver-count=5 systemctl restart docker &amp;&amp; systemctl restart kubelet</code> </pre><br><a name="balancer"></a><br><h4>  Configuring keepalived and virtual IP </h4><br>  On all masternods we configure keepalived and nginx as a balancer <br><br><pre> <code class="bash hljs">systemctl restart keepalived docker-compose -f nginx-lb/docker-compose.yaml up -d</code> </pre><br>  Testing work <br><br><pre> <code class="bash hljs">curl -k https://172.26.133.21:16443 | wc -1 wc: invalid option -- <span class="hljs-string"><span class="hljs-string">'1'</span></span> Try <span class="hljs-string"><span class="hljs-string">'wc --help'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> more information. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 233 100 233 0 0 15281 0 --:--:-- --:--:-- --:--:-- 15533</code> </pre><br>  If 100% - then everything is OK. <br><br>  After we get a working virtual address, we specify it as the server API address. <br><br>  On hb-master01 <br><br><pre> <code class="bash hljs">kubectl edit -n kube-system configmap/kube-proxy server: https://172.26.133.20:16443</code> </pre><br>  Remove all kube-proxy pod to restart them with new parameters. <br><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces -o wide | grep proxy kubectl delete pod -n kube-system kube-proxy-XXX</code> </pre><br>  Check that all restartanuli <br><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces -o wide | grep proxy kube-system kube-proxy-2q7pz 1/1 Running 0 28s 172.26.133.22 hb-master02 kube-system kube-proxy-76vnw 1/1 Running 0 10s 172.26.133.23 hb-master03 kube-system kube-proxy-nq47m 1/1 Running 0 19s 172.26.133.24 hb-master04 kube-system kube-proxy-pqqdh 1/1 Running 0 35s 172.26.133.21 hb-master01 kube-system kube-proxy-vldg8 1/1 Running 0 32s 172.26.133.25 hb-master05</code> </pre><br><a name="worknode"></a><br><h4>  Adding working nodes to the cluster </h4><br>  At each work node, install docke, kubernetes and kubeadm, by analogy with the masters. <br>  Add a node to the cluster using tokens generated during initialization of hb-master01 <br><br><pre> <code class="bash hljs">kubeadm join --token xxxxxxxxxxxxxxx 172.26.133.21:6443 --discovery-token-ca-cert-hash sha256:xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx [preflight] Running pre-flight checks. [WARNING FileExisting-crictl]: crictl not found <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> system path [discovery] Trying to connect to API Server <span class="hljs-string"><span class="hljs-string">"172.26.133.21:6443"</span></span> [discovery] Created cluster-info discovery client, requesting info from <span class="hljs-string"><span class="hljs-string">"https://172.26.133.21:6443"</span></span> [discovery] Requesting info from <span class="hljs-string"><span class="hljs-string">"https://172.26.133.21:6443"</span></span> again to validate TLS against the pinned public key [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server <span class="hljs-string"><span class="hljs-string">"172.26.133.21:6443"</span></span> [discovery] Successfully established connection with API Server <span class="hljs-string"><span class="hljs-string">"172.26.133.21:6443"</span></span> This node has joined the cluster: * Certificate signing request was sent to master and a response was received. * The Kubelet was informed of the new secure connection details. Run <span class="hljs-string"><span class="hljs-string">'kubectl get nodes'</span></span> on the master to see this node join the cluster.</code> </pre><br>  We check that all working nodes are included in the cluster and they are available. <br><br><pre> <code class="bash hljs">kubectl get nodes NAME STATUS ROLES AGE VERSION hb-master01 Ready master 20h v1.9.5 hb-master02 Ready master 20h v1.9.5 hb-master03 Ready master 20h v1.9.5 hb-master04 Ready master 20h v1.9.5 hb-master05 Ready master 20h v1.9.5 hb-node01 Ready &lt;none&gt; 12m v1.9.5 hb-node02 Ready &lt;none&gt; 4m v1.9.5 hb-node03 Ready &lt;none&gt; 31s v1.9.5</code> </pre><br>  <b>Only on working nodes</b> in the /etc/kubernetes/bootstrap-kubelet.conf and /etc/kubernetes/kubelet.conf files we change <br>  the value of the server variable on our virtual IP <br><br><pre> <code class="bash hljs">vim /etc/kubernetes/bootstrap-kubelet.conf server: https://172.26.133.20:16443</code> </pre><br><pre> <code class="bash hljs">vim /etc/kubernetes/kubelet.conf server: https://172.26.133.20:16443</code> </pre><br><pre> <code class="bash hljs">systemctl restart docker kubelet</code> </pre> <br><br>  Then you can increase the performance of your cluster by adding new work nodes, as needed. <br><br><a name="ingress"></a>  <strong>Install ingress-nginx</strong> <br>  Ntgthm we have to install ingress. <br>  The following documentation is written in the kubernetes documentation about Ingress: <br>  <i>An API object that controls external access to services in a cluster, usually HTTP.</i> <i><br></i>  <i>Ingress can provide load balancing, SSL termination, and name-based shared hosting.</i> <br><br>  In general, then in more detail I can hardly describe.  Setting up ingress is material for a separate article; in the context of installing a cluster, I will only describe its installation. <br><pre> <code class="bash hljs">kubectl apply -f kube-ingress/mandatory.yaml namespace <span class="hljs-string"><span class="hljs-string">"ingress-nginx"</span></span> created deployment.extensions <span class="hljs-string"><span class="hljs-string">"default-http-backend"</span></span> created service <span class="hljs-string"><span class="hljs-string">"default-http-backend"</span></span> created configmap <span class="hljs-string"><span class="hljs-string">"nginx-configuration"</span></span> created configmap <span class="hljs-string"><span class="hljs-string">"tcp-services"</span></span> created configmap <span class="hljs-string"><span class="hljs-string">"udp-services"</span></span> created serviceaccount <span class="hljs-string"><span class="hljs-string">"nginx-ingress-serviceaccount"</span></span> created clusterrole.rbac.authorization.k8s.io <span class="hljs-string"><span class="hljs-string">"nginx-ingress-clusterrole"</span></span> configured role.rbac.authorization.k8s.io <span class="hljs-string"><span class="hljs-string">"nginx-ingress-role"</span></span> created rolebinding.rbac.authorization.k8s.io <span class="hljs-string"><span class="hljs-string">"nginx-ingress-role-nisa-binding"</span></span> created clusterrolebinding.rbac.authorization.k8s.io <span class="hljs-string"><span class="hljs-string">"nginx-ingress-clusterrole-nisa-binding"</span></span> configured deployment.extensions <span class="hljs-string"><span class="hljs-string">"nginx-ingress-controller"</span></span> created</code> </pre><br><pre> <code class="bash hljs">kubectl apply -f kube-ingress/service-nodeport.yaml service <span class="hljs-string"><span class="hljs-string">"ingress-nginx"</span></span> created</code> </pre> <br>  Check that ingress has risen: <br><pre> <code class="bash hljs">kubectl get all -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/default-http-backend-5c6d95c48-j8sd4 1/1 Running 0 5m pod/nginx-ingress-controller-58c9df5856-vqwst 1/1 Running 0 5m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/default-http-backend ClusterIP 10.109.216.21 &lt;none&gt; 80/TCP 5m service/ingress-nginx NodePort 10.96.229.115 172.26.133.20 80:32700/TCP,443:31211/TCP 4m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/default-http-backend 1 1 1 1 5m deployment.apps/nginx-ingress-controller 1 1 1 1 5m NAME DESIRED CURRENT READY AGE replicaset.apps/default-http-backend-5c6d95c48 1 1 1 5m replicaset.apps/nginx-ingress-controller-58c9df5856 1 1 1 5m</code> </pre> <br><br>  At this step, the cluster configuration is complete.  If you did everything correctly, you should get a fault-tolerant, working Kubernetes cluster with a fail-safe entry point and a balancer on the virtual address. <br><br>  Thank you for your attention, I will be glad to comment, or indications of inaccuracy.  You can also create an issue on github, I will try to respond quickly to them. <br><br>  Respectfully, <br>  Evgeny Rodionov <br><a name="dashboard"></a><br><h4>  Additionally </h4>  | <br>  <b>Installing the Kubernetes Dashboard</b> <br>  In Kubernetes, in addition to cli, there is not a bad toolbar.  It is installed very simply, instructions and documentation are on <a href="https://github.com/kubernetes/dashboard">GitHub</a> <br><br>  Commands can be executed on any of 5 masters.  I work with hb-master01 <br><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</code> </pre> <br>  Checking: <br><br><pre> <code class="bash hljs">kubectl get pods --all-namespaces -o wide | grep kubernetes-dashboard kube-system kubernetes-dashboard-5bd6f767c7-cz55w 1/1 Running 0 1m 10.244.7.2 hb-node03</code> </pre><br>  The panel is now available at: <br><blockquote>  <a href="https:kubernetes-dashboard:/proxy/">http: // localhost: 8001 / api / v1 / namespaces / kube-system / services / https: kubernetes-dashboard: / proxy /</a> </blockquote>  But in order to get at it, you need to forward the proxy from the local machine using the command <br><br><pre> <code class="bash hljs">kubectl proxy</code> </pre> <br>  This is not convenient for me, so I will use <a href="https://kubernetes.io/docs/concepts/services-networking/service/">NodePort</a> and will place the panel at <a href="https://172.26.133.20:30000/">https:</a> //172.26.133.20.030000 on the first available port in the range allocated to NodePort. <br><br><pre> <code class="bash hljs">kubectl -n kube-system edit service kubernetes-dashboard</code> </pre> <br>  Replace the type: ClusterIP value with type: NodePort and in the port section: add the nodePort value: 30000 <br><br><img src="https://habrastorage.org/webt/fn/ql/kr/fnqlkren3ltk88xzi8fqwi4vbxa.png"><br><br>  Next, create a user with the name admin-user and cluster administrator authority. <br><br><pre> <code class="bash hljs">kubectl apply -f kube-dashboard/dashboard-adminUser.yaml serviceaccount <span class="hljs-string"><span class="hljs-string">"admin-user"</span></span> created clusterrolebinding <span class="hljs-string"><span class="hljs-string">"admin-user"</span></span> created</code> </pre><br>  We get a token for the user admin-user <br><br><pre> <code class="bash hljs">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="hljs-string"><span class="hljs-string">'{print $1}'</span></span>) Name: admin-user-token-p8cxl Namespace: kube-system Labels: &lt;none&gt; Annotations: kubernetes.io/service-account.name=admin-user kubernetes.io/service-account.uid=0819c99c-2cf0-11e8-a281-a64625c137fc Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX</code> </pre><br>  Copy the token and go to: <br><blockquote>  <a href="https://172.26.133.20:30000/">https://172.26.133.20Â§©0000/</a> </blockquote>  Now we have access to the Kubernetes cluster control panel with admin privileges. <br><br><img src="https://habrastorage.org/webt/ry/sk/jh/ryskjhvlqcb_k_jmugsjjk8xs0u.png"><br><a name="heapster"></a><br><h4>  Heapster </h4><br>  Next, install Heapster.  It is a tool for monitoring the resources of all components of the cluster.  <a href="https://github.com/kubernetes/heapster">Project page on GitHub</a> <br><br>  Installation: <br><br><pre> <code class="bash hljs">git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/kubernetes/heapster.git <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> heapster kubectl create -f deploy/kube-config/influxdb/ deployment <span class="hljs-string"><span class="hljs-string">"monitoring-grafana"</span></span> created service <span class="hljs-string"><span class="hljs-string">"monitoring-grafana"</span></span> created serviceaccount <span class="hljs-string"><span class="hljs-string">"heapster"</span></span> created deployment <span class="hljs-string"><span class="hljs-string">"heapster"</span></span> created service <span class="hljs-string"><span class="hljs-string">"heapster"</span></span> created deployment <span class="hljs-string"><span class="hljs-string">"monitoring-influxdb"</span></span> created service <span class="hljs-string"><span class="hljs-string">"monitoring-influxdb"</span></span> created</code> </pre><br><pre> <code class="bash hljs">kubectl create -f deploy/kube-config/rbac/heapster-rbac.yaml clusterrolebinding <span class="hljs-string"><span class="hljs-string">"heapster"</span></span> created</code> </pre><br>  In a couple of minutes the information should go.  Checking: <br><br><pre> <code class="bash hljs">kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% hb-master01 166m 4% 1216Mi 31% hb-master02 135m 3% 1130Mi 29% hb-master03 142m 3% 1091Mi 28% hb-master04 193m 4% 1149Mi 29% hb-master05 144m 3% 1056Mi 27% hb-node01 41m 1% 518Mi 3% hb-node02 38m 0% 444Mi 2% hb-node03 45m 1% 478Mi 2%</code> </pre> <br>  Also metrics are available in the web interface. <br><br><img src="https://habrastorage.org/webt/lj/v7/bi/ljv7bithp-t13ga2oj0p6br7upu.png"><br><br>  Thanks for attention. <br><br>  <b>Use materials:</b> <br><br><ul><li>  <a href="https://github.com/cookeem/kubeadm-ha">kubeadm-highavailability - kubernetes high availability; based on kubeadm, for Kubernetes version 1.9.x / 1.7.x / 1.6.x</a> </li><li>  <a href="https://kubernetes.io/docs/home/%3Fpath%3Dusers%26persona%3Dapp-%2520developer%26level%3Dfoundational">Kubernetes Documentation</a> </li><li>  <a href="https://habrahabr.ru/post/258443/">Kubernetes Basics</a> </li><li>  <a href="http://alesnosek.com/blog/2017/02/14/accessing-kubernetes-pods-from-outside-of-the-cluster/">Accessing Kubernetes Pods from Outside of the Cluster</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/358264/">https://habr.com/ru/post/358264/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../358252/index.html">What I learned from personal experience over the years of freelancing</a></li>
<li><a href="../358256/index.html">What's New in PostgreSQL 11: Improved Casting</a></li>
<li><a href="../358258/index.html">How tickets in support turn into tickets in Jira</a></li>
<li><a href="../358260/index.html">Four ways to work with text UI in Unity</a></li>
<li><a href="../358262/index.html">Unreal Engine 4 Tutorial: Paint Filter</a></li>
<li><a href="../358268/index.html">New trend at IT interviews: whole days of unpaid homework</a></li>
<li><a href="../358274/index.html">Patch Tuesday: critical patches you might have missed</a></li>
<li><a href="../358276/index.html">Survey from Veeam Academy. Do you want to go through intensive C # in the summer?</a></li>
<li><a href="../358278/index.html">Guice Almighty: assistedinject, multibindings, generics</a></li>
<li><a href="../358282/index.html">Messenger problems: vanishing messages in Signal for Mac are not permanently deleted</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>