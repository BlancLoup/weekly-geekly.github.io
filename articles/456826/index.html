<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Do-it-yourself autoscaling using AWX, Ansible, haproxy and KROK Clouds</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Some time ago we did agentless monitoring and alarms to it. This is similar to CloudWatch in AWS with a compatible API. Now we are working on balancer...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Do-it-yourself autoscaling using AWX, Ansible, haproxy and KROK Clouds</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/6f/nt/tj/6fnttjgdk8vat9bbsahsnck_rfa.jpeg" alt="image"></p><br><p>  Some time ago we did agentless monitoring and alarms to it.  This is similar to CloudWatch in AWS with a compatible API.  Now we are working on balancers and automatic scaling.  But while we do not provide such a service, we offer our customers to do it themselves, using our monitoring and tags (AWS Resource Tagging API) as a data source as a simple service discovery.  How to do this will show in this post. </p><a name="habracut"></a><br><p>  An example of a minimal infrastructure of a simple web service: DNS -&gt; 2 balancers -&gt; 2 backend.  This infrastructure can be considered the minimum required for fault-tolerant work and for maintenance.  For this reason, we will not ‚Äúcompress‚Äù even more strongly this infrastructure, leaving, for example, only one backend.  But I would like to increase the number of backend servers and reduce back to two.  This will be our task.  All examples are available in the <a href="https://github.com/Ubun1/c2-autoscaling">repository</a> . </p><br><h3 id="bazovaya-infrastruktura">  Basic infrastructure </h3><br><p>  We will not dwell in detail on setting up the above infrastructure, we will show only how to create it.  We prefer to deploy the infrastructure using Terraform.  It helps to quickly create everything you need (VPC, Subnet, Security Group, VMs) and repeat this procedure time after time. </p><br><p>  Script to raise the basic infrastructure: </p><br><div class="spoiler">  <b class="spoiler_title">main.tf</b> <div class="spoiler_text"><pre><code class="bash hljs">variable <span class="hljs-string"><span class="hljs-string">"ec2_url"</span></span> {} variable <span class="hljs-string"><span class="hljs-string">"access_key"</span></span> {} variable <span class="hljs-string"><span class="hljs-string">"secret_key"</span></span> {} variable <span class="hljs-string"><span class="hljs-string">"region"</span></span> {} variable <span class="hljs-string"><span class="hljs-string">"vpc_cidr_block"</span></span> {} variable <span class="hljs-string"><span class="hljs-string">"instance_type"</span></span> {} variable <span class="hljs-string"><span class="hljs-string">"big_instance_type"</span></span> {} variable <span class="hljs-string"><span class="hljs-string">"az"</span></span> {} variable <span class="hljs-string"><span class="hljs-string">"ami"</span></span> {} variable <span class="hljs-string"><span class="hljs-string">"client_ip"</span></span> {} variable <span class="hljs-string"><span class="hljs-string">"material"</span></span> {} provider <span class="hljs-string"><span class="hljs-string">"aws"</span></span> { endpoints { ec2 = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${var.ec2_url}</span></span></span><span class="hljs-string">"</span></span> } skip_credentials_validation = <span class="hljs-literal"><span class="hljs-literal">true</span></span> skip_requesting_account_id = <span class="hljs-literal"><span class="hljs-literal">true</span></span> skip_region_validation = <span class="hljs-literal"><span class="hljs-literal">true</span></span> access_key = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${var.access_key}</span></span></span><span class="hljs-string">"</span></span> secret_key = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${var.secret_key}</span></span></span><span class="hljs-string">"</span></span> region = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${var.region}</span></span></span><span class="hljs-string">"</span></span> } resource <span class="hljs-string"><span class="hljs-string">"aws_vpc"</span></span> <span class="hljs-string"><span class="hljs-string">"vpc"</span></span> { cidr_block = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${var.vpc_cidr_block}</span></span></span><span class="hljs-string">"</span></span> } resource <span class="hljs-string"><span class="hljs-string">"aws_subnet"</span></span> <span class="hljs-string"><span class="hljs-string">"subnet"</span></span> { availability_zone = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${var.az}</span></span></span><span class="hljs-string">"</span></span> vpc_id = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${aws_vpc.vpc.id}</span></span></span><span class="hljs-string">"</span></span> cidr_block = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${cidrsubnet(aws_vpc.vpc.cidr_block, 8, 0)}</span></span></span><span class="hljs-string">"</span></span> } resource <span class="hljs-string"><span class="hljs-string">"aws_security_group"</span></span> <span class="hljs-string"><span class="hljs-string">"sg"</span></span> { name = <span class="hljs-string"><span class="hljs-string">"auto-scaling"</span></span> vpc_id = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${aws_vpc.vpc.id}</span></span></span><span class="hljs-string">"</span></span> ingress { from_port = 22 to_port = 22 protocol = <span class="hljs-string"><span class="hljs-string">"tcp"</span></span> cidr_blocks = [<span class="hljs-string"><span class="hljs-string">"0.0.0.0/0"</span></span>] } ingress { from_port = 80 to_port = 80 protocol = <span class="hljs-string"><span class="hljs-string">"tcp"</span></span> cidr_blocks = [<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${cidrsubnet(aws_vpc.vpc.cidr_block, 8, 0)}</span></span></span><span class="hljs-string">"</span></span>] } ingress { from_port = 8080 to_port = 8080 protocol = <span class="hljs-string"><span class="hljs-string">"tcp"</span></span> cidr_blocks = [<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${cidrsubnet(aws_vpc.vpc.cidr_block, 8, 0)}</span></span></span><span class="hljs-string">"</span></span>] } egress { from_port = 0 to_port = 0 protocol = <span class="hljs-string"><span class="hljs-string">"-1"</span></span> cidr_blocks = [<span class="hljs-string"><span class="hljs-string">"0.0.0.0/0"</span></span>] } } resource <span class="hljs-string"><span class="hljs-string">"aws_key_pair"</span></span> <span class="hljs-string"><span class="hljs-string">"key"</span></span> { key_name = <span class="hljs-string"><span class="hljs-string">"auto-scaling-new"</span></span> public_key = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${var.material}</span></span></span><span class="hljs-string">"</span></span> } resource <span class="hljs-string"><span class="hljs-string">"aws_instance"</span></span> <span class="hljs-string"><span class="hljs-string">"compute"</span></span> { count = 5 ami = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${var.ami}</span></span></span><span class="hljs-string">"</span></span> instance_type = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${count.index == 0 ? var.big_instance_type : var.instance_type}</span></span></span><span class="hljs-string">"</span></span> key_name = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${aws_key_pair.key.key_name}</span></span></span><span class="hljs-string">"</span></span> subnet_id = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${aws_subnet.subnet.id}</span></span></span><span class="hljs-string">"</span></span> availability_zone = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${var.az}</span></span></span><span class="hljs-string">"</span></span> security_groups = [<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${aws_security_group.sg.id}</span></span></span><span class="hljs-string">"</span></span>] } resource <span class="hljs-string"><span class="hljs-string">"aws_eip"</span></span> <span class="hljs-string"><span class="hljs-string">"pub_ip"</span></span> { instance = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${aws_instance.compute.0.id}</span></span></span><span class="hljs-string">"</span></span> vpc = <span class="hljs-literal"><span class="hljs-literal">true</span></span> } output <span class="hljs-string"><span class="hljs-string">"awx"</span></span> { value = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${aws_eip.pub_ip.public_ip}</span></span></span><span class="hljs-string">"</span></span> } output <span class="hljs-string"><span class="hljs-string">"haproxy_id"</span></span> { value = [<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${slice(aws_instance.compute.*.id, 1, 3)}</span></span></span><span class="hljs-string">"</span></span>] } output <span class="hljs-string"><span class="hljs-string">"awx_id"</span></span> { value = <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${aws_instance.compute.0.id}</span></span></span><span class="hljs-string">"</span></span> } output <span class="hljs-string"><span class="hljs-string">"backend_id"</span></span> { value = [<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">${slice(aws_instance.compute.*.id, 3, 5)}</span></span></span><span class="hljs-string">"</span></span>] }</code> </pre> </div></div><br><p>  All entities described in this configuration seem to be understandable to the ordinary user of modern clouds.  We place the variables specific to our cloud and for a specific task into a separate file - terraform.tfvars: </p><br><div class="spoiler">  <b class="spoiler_title">terraform.tfvars</b> <div class="spoiler_text"><pre> <code class="bash hljs">ec2_url = <span class="hljs-string"><span class="hljs-string">"https://api.cloud.croc.ru"</span></span> access_key = <span class="hljs-string"><span class="hljs-string">"project:user@customer"</span></span> secret_key = <span class="hljs-string"><span class="hljs-string">"secret-key"</span></span> region = <span class="hljs-string"><span class="hljs-string">"croc"</span></span> az = <span class="hljs-string"><span class="hljs-string">"ru-msk-vol51"</span></span> instance_type = <span class="hljs-string"><span class="hljs-string">"m1.2small"</span></span> big_instance_type = <span class="hljs-string"><span class="hljs-string">"m1.large"</span></span> vpc_cidr_block = <span class="hljs-string"><span class="hljs-string">"10.10.0.0/16"</span></span> ami = <span class="hljs-string"><span class="hljs-string">"cmi-3F5B011E"</span></span></code> </pre> </div></div><br><p>  Launch Terraform: </p><br><div class="spoiler">  <b class="spoiler_title">terraform apply</b> <div class="spoiler_text"><pre> <code class="bash hljs">yes yes | terraform apply -var client_ip=<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(curl -s ipinfo.io/ip)</span></span></span><span class="hljs-string">/32"</span></span> -var material=<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$(cat &lt;ssh_publick_key_path&gt;)</span></span></span><span class="hljs-string">"</span></span></code> </pre> </div></div><br><h3 id="nastroyka-monitoringa">  Monitoring setup </h3><br><p>  The VMs launched above are automatically monitored by our cloud.  It is the data of this monitoring that will be the source of information for future auto-scaling.  Relying on certain metrics we can increase or decrease capacity. </p><br><p>  Monitoring in our cloud allows you to configure alarms on various conditions for different metrics.  It is very convenient.  We do not need to analyze the metrics for some intervals and make a decision - this will make the monitoring of the cloud.  In this example, we will use alarms on CPU metrics, but in our monitoring you can also configure them for such metrics as network utilization (speed / pps), disk utilization (speed / iops). </p><br><div class="spoiler">  <b class="spoiler_title">cloudwatch put-metric-alarm</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> CLOUDWATCH_URL=https://monitoring.cloud.croc.ru <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> instance_id <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> &lt;backend_instance_ids&gt;; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> \ aws --profile &lt;aws_cli_profile&gt; --endpoint-url <span class="hljs-variable"><span class="hljs-variable">$CLOUDWATCH_URL</span></span> \ cloudwatch put-metric-alarm \ --alarm-name <span class="hljs-string"><span class="hljs-string">"scaling-low_</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$instance_id</span></span></span><span class="hljs-string">"</span></span> \ --dimensions Name=InstanceId,Value=<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$instance_id</span></span></span><span class="hljs-string">"</span></span> \ --namespace <span class="hljs-string"><span class="hljs-string">"AWS/EC2"</span></span> --metric-name CPUUtilization --statistic Average \ --period 60 --evaluation-periods 3 --threshold 15 --comparison-operator LessThanOrEqualToThreshold; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> instance_id <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> &lt;backend_instance_ids&gt;; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> \ aws --profile &lt;aws_cli_profile&gt; --endpoint-url <span class="hljs-variable"><span class="hljs-variable">$CLOUDWATCH_URL</span></span> \ cloudwatch put-metric-alarm\ --alarm-name <span class="hljs-string"><span class="hljs-string">"scaling-high_</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$instance_id</span></span></span><span class="hljs-string">"</span></span> \ --dimensions Name=InstanceId,Value=<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$instance_id</span></span></span><span class="hljs-string">"</span></span> \ --namespace <span class="hljs-string"><span class="hljs-string">"AWS/EC2"</span></span> --metric-name CPUUtilization --statistic Average\ --period 60 --evaluation-periods 3 --threshold 80 --comparison-operator GreaterThanOrEqualToThreshold; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre> <br><p>  Description of some parameters that may not be clear: </p><br><p>  --profile is an aws-cli settings profile, described in ~ / .aws / config.  Typically, different profiles are given different access keys. </p><br><p>  --dimensions - the parameter determines for which resource an alarm will be created, in the example above, for an instance with an identifier from the variable $ instance_id. </p><br><p>  --namespace - namespace from which the monitoring metric will be selected. </p><br><p>  --metric-name - the name of the monitoring metric. </p><br><p>  --statistic ‚Äî name of the method of aggregation of metric values </p><br><p>  --period - time interval between events of monitoring values ‚Äã‚Äãcollection. </p><br><p>  --evaluation-periods - the number of intervals required to trigger an alarm. </p><br><p>  --threshold - threshold metric for assessing the state of the alarm. </p><br><p>  --comparison-operator - a method that is used to estimate the value of a metric relative to a threshold value. </p></div></div><br><p>  In the example above, two alarms are created for each backend instance.  Scaling-low- &lt;instance-id&gt; will switch to the Alarm state when the CPU is loaded less than 15% for 3 minutes.  Scaling-high- &lt;instance-id&gt; will switch to the Alarm state when the CPU is loaded for more than 80% for 3 minutes. </p><br><h3 id="nastroyka-tegov">  Tag customization </h3><br><p>  After setting up monitoring, we face the following task - the detection of instances and their names (service discovery).  We need to somehow understand how much backend instances we have now running, and also need to know their names.  In the world outside the cloud, for example, the consul and consul template would be well suited for generating a balancer config.  But in our cloud there are tags.  Tags will help us categorize resources.  By requesting information on a specific tag (describe-tags), we can understand how many instances we have in the pool now and what id they have.  By default, the unique id of the instance is used as the hostname.  Thanks to the internal DNS that runs inside the VPC, these id / hostname are resolved to the internal ip instances. </p><br><p>  Set tags for backend instances and balancers: </p><br><div class="spoiler">  <b class="spoiler_title">ec2 create-tags</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> EC2_URL=<span class="hljs-string"><span class="hljs-string">"https://api.cloud.croc.ru"</span></span> aws --profile &lt;aws_cli_profile&gt; --endpoint-url <span class="hljs-variable"><span class="hljs-variable">$EC2_URL</span></span> \ ec2 create-tags --resources <span class="hljs-string"><span class="hljs-string">"&lt;awx_instance_id&gt;"</span></span> \ --tags Key=env,Value=auto-scaling Key=role,Value=awx <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> &lt;backend_instance_ids&gt;; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> \ aws --profile &lt;aws_cli_profile&gt; --endpoint-url <span class="hljs-variable"><span class="hljs-variable">$EC2_URL</span></span> \ ec2 create-tags --resources <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$i</span></span></span><span class="hljs-string">"</span></span> \ --tags Key=env,Value=auto-scaling Key=role,Value=backend ; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> &lt;haproxy_instance_ids&gt;; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> \ aws --profile &lt;aws_cli_profile&gt; --endpoint-url <span class="hljs-variable"><span class="hljs-variable">$EC2_URL</span></span> \ ec2 create-tags --resources <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$i</span></span></span><span class="hljs-string">"</span></span> \ --tags Key=env,Value=auto-scaling Key=role,Value=haproxy; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span>;</code> </pre> <br><p>  Where: </p><br><p>  --resources - list of resource identifiers that will be tagged. </p><br><p>  --tags - a list of key-value pairs. </p></div></div><br><p>  An example describe-tags is available in the CRIC Cloud <a href="http://docs.website.cloud.croc.ru/en/api/ec2/tags/DescribeTags.html">documentation</a> . </p><br><h3 id="nastroyka-avtoskeylinga">  Autoscaling setting </h3><br><p>  Now, when the cloud is monitoring, and we know how to work with tags, we can only poll the status of configured alarms for their triggering.  Here we need an entity that will be engaged in periodic polling of monitoring and launching tasks for creating / deleting instances.  Here you can use various automation tools.  We will use AWX.  AWX is an open-source version of commercial <a href="https://github.com/ansible/awx">Ansible Tower</a> , a product for centralized management of Ansible infrastructure.  The main task is to periodically run our ansible playbook. </p><br><p>  An example of the AWX deployment is available on the <a href="">wiki</a> page in the official repository.  AWX configuration is also described in the Ansible Tower documentation.  In order for the AWX service to start launching a custom playbook, you need to configure it by creating the following entities: </p><br><ul><li>  redentials of three types: <br>  <em>- AWS credentials - to authorize operations related to the CRIC Cloud.</em> <em><br></em>  - Machine credentials - ssh keys for access to newly created instances. <br>  - SCM credentials - for authorization in the version control system. </li><li>  Project - an entity that will tilt the git repository from the playbook. </li><li>  Scripts - script dynamic inventory for ansible. </li><li>  Inventory is an entity that will invoke the dynamic inventory script before launching the playbook. </li><li>  Template - configuration of a specific playbook call, consists of a set of Credentials, Inventory and playbook from Project. </li><li>  Workflow - a sequence of playbooks calls. </li></ul><br><p>  The process of autoscaling can be divided into two parts: </p><br><ul><li>  scale_up - create an instance when at least one high alarm is triggered; </li><li>  scale_down is the instance termination if a low alarm worked for it. </li></ul><br><p>  Within the scale_up part you will need: </p><br><ul><li>  interrogate the cloud monitoring service about the presence of high alarms in the "Alarm" state; </li><li>  stop scale_up ahead of time if all high alarms are in the "OK" state; </li><li>  create a new instance with the necessary attributes (tag, subnet, security_group, etc.); </li><li>  create high and low alarms for the running instance; </li><li>  configure our application inside a new instance (in our case it will be just nginx with a test page); </li><li>  update the haproxy configuration, make a reload so that new requests start to go to the new instance. </li></ul><br><div class="spoiler">  <b class="spoiler_title">create-instance.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">--- - name: get alarm statuses describe_alarms: region: "croc" alarm_name_prefix: "scaling-high" alarm_state: "alarm" register: describe_alarms_query - name: stop if no alarms fired fail: msg: zero high alarms in alarm state when: describe_alarms_query.meta | length == 0 - name: create instance ec2: region: "croc" wait: yes state: present count: 1 key_name: "{{ hostvars[groups['tag_role_backend'][0]].ec2_key_name }}" instance_type: "{{ hostvars[groups['tag_role_backend'][0]].ec2_instance_type }}" image: "{{ hostvars[groups['tag_role_backend'][0]].ec2_image_id }}" group_id: "{{ hostvars[groups['tag_role_backend'][0]].ec2_security_group_ids }}" vpc_subnet_id: "{{ hostvars[groups['tag_role_backend'][0]].ec2_subnet_id }}" user_data: | #!/bin/sh sudo yum install epel-release -y sudo yum install nginx -y cat &lt;&lt;EOF &gt; /etc/nginx/conf.d/dummy.conf server { listen 8080; location / { return 200 '{"message": "$HOSTNAME is up"}'; } } EOF sudo systemctl restart nginx loop: "{{ hostvars[groups['tag_role_backend'][0]] }}" register: new - name: create tag entry ec2_tag: ec2_url: "https://api.cloud.croc.ru" region: croc state: present resource: "{{ item.id }}" tags: role: backend loop: "{{ new.instances }}" - name: create low alarms ec2_metric_alarm: state: present region: croc name: "scaling-low_{ item.id }}" metric: "CPUUtilization" namespace: "AWS/EC2" statistic: Average comparison: "&lt;=" threshold: 15 period: 300 evaluation_periods: 3 unit: "Percent" dimensions: {'InstanceId':"{{ item.id }}"} loop: "{{ new.instances }}" - name: create high alarms ec2_metric_alarm: state: present region: croc name: "scaling-high_{{ item.id }}" metric: "CPUUtilization" namespace: "AWS/EC2" statistic: Average comparison: "&gt;=" threshold: 80.0 period: 300 evaluation_periods: 3 unit: "Percent" dimensions: {'InstanceId':"{{ item.id }}"} loop: "{{ new.instances }}"</code> </pre> </div></div><br><p>  In create-instance.yaml, an instance is created with the correct parameters, this instance is tagged and the necessary alarms are created.  Also through user-data the script of installation and the nginx settings is transferred.  User-data is processed by the cloud-init service, which allows for flexible customization of the instance during startup, without resorting to other automation tools. </p><br><p>  The update-lb.yaml re-creates the /etc/haproxy/haproxy.cfg file on the haproxy instance and reload the haproxy service: </p><br><div class="spoiler">  <b class="spoiler_title">update-lb.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">- name: update haproxy configs template: src: haproxy.cfg.j2 dest: /etc/haproxy/haproxy.cfg - name: add new backend host to haproxy systemd: name: haproxy state: restarted</code> </pre> </div></div><br><p>  Where haproxy.cfg.j2 is the haproxy service configuration file template: </p><br><div class="spoiler">  <b class="spoiler_title">haproxy.cfg.j2</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"># {{ ansible_managed }} global log /dev/log local0 log /dev/log local1 notice chroot /var/lib/haproxy stats timeout 30s user haproxy group haproxy daemon defaults log global mode http option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 frontend loadbalancing bind *:80 mode http default_backend backendnodes backend backendnodes balance roundrobin option httpchk HEAD / {% for host in groups['tag_role_backend'] %} server {{hostvars[host]['ec2_id']}} {{hostvars[host]['ec2_private_ip_address']}}:8080 check {% endfor %}</code> </pre> </div></div><br><p>  Since the option httpchk option is defined in the backend section of the haproxy config, the haproxy service will self-poll the backend instances and balance the traffic only between those who have passed the health check. </p><br><p>  In the scale_down part you need: </p><br><ul><li>  check state of low alarm; </li><li>  to finish the play ahead of schedule if there are no low alarms in the "Alarm" state; </li><li>  To terminate all instances that have a low alarm in the "Alarm" state; </li><li>  prohibit the termination of the last pair of instances, even if their alarms are in the "Alarm" state; </li><li>  remove load balancer from instances that we have removed. </li></ul><br><div class="spoiler">  <b class="spoiler_title">destroy-instance.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">- name: look for alarm status describe_alarms: region: "croc" alarm_name_prefix: "scaling-low" alarm_state: "alarm" register: describe_alarms_query - name: count alarmed instances set_fact: alarmed_count: "{{ describe_alarms_query.meta | length }}" alarmed_ids: "{{ describe_alarms_query.meta }}" - name: stop if no alarms fail: msg: no alarms fired when: alarmed_count | int == 0 - name: count all described instances set_fact: all_count: "{{ groups['tag_role_backend'] | length }}" - name: fail if last two instance remaining fail: msg: cant destroy last two instances when: all_count | int == 2 - name: destroy tags for marked instances ec2_tag: ec2_url: "https://api.cloud.croc.ru" region: croc resource: "{{ alarmed_ids[0].split('_')[1] }}" state: absent tags: role: backend - name: destroy instances ec2: region: croc state: absent instance_ids: "{{ alarmed_ids[0].split('_')[1] }}" - name: destroy low alarms ec2_metric_alarm: state: absent region: croc name: "scaling-low_{{ alarmed_ids[0].split('_')[1] }}" - name: destroy high alarms ec2_metric_alarm: state: absent region: croc name: "scaling-high_{{ alarmed_ids[0].split('_')[1] }}"</code> </pre> </div></div><br><p>  In destroy-instance.yaml, the alarm is deleted, the instance and its tag are terminated, and the conditions of the last instances are prohibited from being terminated. </p><br><p>  We explicitly remove tags after deleting instances due to the fact that after an instance is deleted, tags associated with it are removed pending and are available for another minute. <br>  Awx. </p><br><h3 id="nastroyka-zadach-shablonov">  Setting up tasks, templates </h3><br><p>  The following tasks set will create the necessary entities in AWX: </p><br><div class="spoiler">  <b class="spoiler_title">awx-configure.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">--- - name: Create tower organization tower_organization: name: "scaling-org" description: "scaling-org organization" state: present - name: Add tower cloud credential tower_credential: name: cloud description: croc cloud api creds organization: scaling-org kind: aws state: present username: "{{ croc_user }}" password: "{{ croc_password }}" - name: Add tower github credential tower_credential: name: ghe organization: scaling-org kind: scm state: present username: "{{ ghe_user }}" password: "{{ ghe_password }}" - name: Add tower ssh credential tower_credential: name: ssh description: ssh creds organization: scaling-org kind: ssh state: present username: "ec2-user" ssh_key_data: "{{ lookup('file', 'private.key') }}" - name: Add tower project tower_project: name: "auto-scaling" scm_type: git scm_credential: ghe scm_url: &lt;repo-name&gt; organization: "scaling-org" scm_branch: master state: present - name: create inventory tower_inventory: name: dynamic-inventory organization: "scaling-org" state: present - name: copy inventory script to awx copy: src: "{{ role_path }}/files/ec2.py" dest: /root/ec2.py - name: create inventory source shell: | export SCRIPT=$(tower-cli inventory_script create -n "ec2-script" --organization "scaling-org" --script @/root/ec2.py | grep ec2 | awk '{print $1}') tower-cli inventory_source create --update-on-launch True --credential cloud --source custom --inventory dynamic-inventory -n "ec2-source" --source-script $SCRIPT --source-vars '{"EC2_URL":"api.cloud.croc.ru","AWS_REGION": "croc"}' --overwrite True - name: Create create-instance template tower_job_template: name: "create-instance" job_type: "run" inventory: "dynamic-inventory" credential: "cloud" project: "auto-scaling" playbook: "create-instance.yaml" state: "present" register: create_instance - name: Create update-lb template tower_job_template: name: "update-lb" job_type: "run" inventory: "dynamic-inventory" credential: "ssh" project: "auto-scaling" playbook: "update-lb.yaml" credential: "ssh" state: "present" register: update_lb - name: Create destroy-instance template tower_job_template: name: "destroy-instance" job_type: "run" inventory: "dynamic-inventory" project: "auto-scaling" credential: "cloud" playbook: "destroy-instance.yaml" credential: "ssh" state: "present" register: destroy_instance - name: create workflow tower_workflow_template: name: auto_scaling organization: scaling-org schema: "{{ lookup('template', 'schema.j2')}}" - name: set scheduling shell: | tower-cli schedule create -n "3min" --workflow "auto_scaling" --rrule "DTSTART:$(date +%Y%m%dT%H%M%SZ) RRULE:FREQ=MINUTELY;INTERVAL=3"</code> </pre> </div></div><br><p>  The previous snippet will create a template for each of the used ansible playbooks.  Each template configures the launch of a playbook with a set of defined credentials and inventory. </p><br><p>  To build a pipe for calls to playbooks will allow the workflow template.  The autoflow scans workflow configuration is shown below: </p><br><div class="spoiler">  <b class="spoiler_title">schema.j2</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">- failure_nodes: - id: 101 job_template: {{ destroy_instance.id }} success_nodes: - id: 102 job_template: {{ update_lb.id }} id: 103 job_template: {{ create_instance.id }} success_nodes: - id: 104 job_template: {{ update_lb.id }}</code> </pre> </div></div><br><p>  The previous template presents the workflow scheme, i.e.  template execution sequence.  In this workflow, each next step (success_nodes) will be executed only if the previous one is successful.  Graphic representation of the workflow presented in the picture: <br><img src="https://habrastorage.org/webt/po/l-/vb/pol-vbfpulv4hoimk99mvbtig-u.png" alt="workflow"></p><br><p>  As a result, a generic workflow was created that performs a create-instace playbook and, depending on the execution status, destroy-instance and / or update-lb playbook.  Combined workflow is convenient to run on a predetermined schedule.  The autoscaling process will run every three minutes, starting and terminating the instances depending on the state of the alarms. </p><br><h3 id="testirovanie-raboty">  Testing work </h3><br><p>  Now check the work of the configured system.  First, install the wrk-utility for http benchmarking. </p><br><div class="spoiler">  <b class="spoiler_title">wrk install</b> <div class="spoiler_text"><pre> <code class="bash hljs">ssh -A ec2-user@&lt;aws_instance_ip&gt; sudo su - <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /opt yum groupinstall <span class="hljs-string"><span class="hljs-string">'Development Tools'</span></span> yum install -y openssl-devel git git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/wg/wrk.git wrk <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> wrk make install wrk /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/bin <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span></code> </pre> </div></div><br><p>  We use cloud monitoring to monitor the use of instance resources during load: </p><br><div class="spoiler">  <b class="spoiler_title">monitoring</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">function CPUUtilizationMonitoring() { local AWS_CLI_PROFILE="&lt;aws_cli_profile&gt;" local CLOUDWATCH_URL="https://monitoring.cloud.croc.ru" local API_URL="https://api.cloud.croc.ru" local STATS="" local ALARM_STATUS="" local IDS=$(aws --profile $AWS_CLI_PROFILE --endpoint-url $API_URL ec2 describe-instances --filter Name=tag:role,Values=backend | grep -i instanceid | grep -oE 'i-[a-zA-Z0-9]*' | tr '\n' ' ') for instance_id in $IDS; do STATS="$STATS$(aws --profile $AWS_CLI_PROFILE --endpoint-url $CLOUDWATCH_URL cloudwatch get-metric-statistics --dimensions Name=InstanceId,Value=$instance_id --namespace "AWS/EC2" --metric CPUUtilization --end-time $(date --iso-8601=minutes) --start-time $(date -d "$(date --iso-8601=minutes) - 1 min" --iso-8601=minutes) --period 60 --statistics Average | grep -i average)"; ALARMS_STATUS="$ALARMS_STATUS$(aws --profile $AWS_CLI_PROFILE --endpoint-url $CLOUDWATCH_URL cloudwatch describe-alarms --alarm-names scaling-high-$instance_id | grep -i statevalue)" done echo $STATS | column -s ',' -o '|' -N $(echo $IDS | tr ' ' ',') -t echo $ALARMS_STATUS | column -s ',' -o '|' -N $(echo $IDS | tr ' ' ',') -t } export -f CPUUtilizationMonitoring watch -n 60 bash -c CPUUtilizationMonitoring</code> </pre> </div></div><br><p>  The previous script every 60 seconds collects information about the average value of the CPUUtilization metric for the last minute and polls the alarm status for the backend instances. </p><br><p>  Now you can run wrk and look at resource utilization of backend instances under load: </p><br><div class="spoiler">  <b class="spoiler_title">wrk run</b> <div class="spoiler_text"><pre> <code class="bash hljs">ssh -A ec2-user@&lt;awx_instance_ip&gt; wrk -t12 -c100 -d500s http://&lt;haproxy_instance_id&gt; <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span></code> </pre> </div></div><br><p>  The last command will start the benchmark for 500 seconds, using 12 threads and opening 100 http connections. </p><br><p>  Over the course of time, the monitoring script should show that during the benchmarking period, the statistics value of the CPUUtilization metric increases until it reaches 300%.  180 seconds after the start of the benchmark, the StateValue flag should switch to the Alarm state.  Once every two minutes, autoscaling workflow starts.  By default, parallel execution of the same workflow is prohibited.  That is, every two minutes the task to execute the workflow will be added to the queue and will be launched only after the previous one is completed.  Thus, while wrk is running, there will be a steady increase in resources until the high alarms of all backend instances go into the OK state.  Upon completion, wrk scale_down workflow terminates all backend instances except two. </p><br><p>  An example of the output of the monitoring script: </p><br><div class="spoiler">  <b class="spoiler_title">monitoring results</b> <div class="spoiler_text"><pre> <code class="plaintext hljs"># start test i-43477460 |i-AC5D9EE0 "Average": 0.0 | "Average": 0.0 i-43477460 |i-AC5D9EE0 "StateValue": "ok"| "StateValue": "ok" # start http load i-43477460 |i-AC5D9EE0 "Average": 267.0 | "Average": 111.0 i-43477460 |i-AC5D9EE0 "StateValue": "ok"| "StateValue": "ok" # alarm state i-43477460 |i-AC5D9EE0 "Average": 267.0 | "Average": 282.0 i-43477460 |i-AC5D9EE0 "StateValue": "alarm"| "StateValue": "alarm" # two new instances created i-1E399860 |i-F307FB00 |i-43477460 |i-AC5D9EE0 "Average": 185.0 | "Average": 215.0 | "Average": 245.0 | i-1E399860 |i-F307FB00 |i-43477460 |i-AC5D9EE0 "StateValue": "insufficient_data"| "StateValue": "insufficient_data"| "StateValue": "alarm"| "StateValue": "alarm" # only two instances left after load has been stopped i-935BAB40 |i-AC5D9EE0 "Average": 0.0 | "Average": 0.0 i-935BAB40 |i-AC5D9EE0 "StateValue": "ok"| "StateValue": "ok"</code> </pre> </div></div><br><p>  Also in the CROC Cloud, it is possible to view the graphs used in the monitoring post on the instance page on the corresponding tab. </p><br><p>  Alarm view is available on the monitoring page on the alarm tab. </p><br><h3 id="zaklyuchenie">  Conclusion </h3><br><p>  Autospeaking is quite a popular scenario, but, unfortunately, it is not in our cloud yet (but only so far).  However, we have quite a lot of powerful API to do similar and many other things, using popular, one can say almost standard, tools such as: Terraform, ansible, aws-cli and others. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/456826/">https://habr.com/ru/post/456826/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456810/index.html">The first. A story without which Tesla would not exist</a></li>
<li><a href="../456812/index.html">What is there at ITMO University - IT-festivals, hackathons, conferences and open seminars</a></li>
<li><a href="../456818/index.html">Sysadmin in neaytishnoy company. The unbearable burden of being?</a></li>
<li><a href="../45682/index.html">cry of the soul and manifest</a></li>
<li><a href="../456820/index.html">Clay ‚Üí Brick ‚Üí Furnace</a></li>
<li><a href="../456828/index.html">Tuning vias of printed circuit boards</a></li>
<li><a href="../45683/index.html">ExtJS and Hosting Base Files in CDN</a></li>
<li><a href="../456830/index.html">Vivaldi 2.6 - Summer Joys</a></li>
<li><a href="../456836/index.html">Selection: 5 non-obvious competitive analysis tools that you might not know about</a></li>
<li><a href="../456840/index.html">SDL 2 Tutorial: Lesson 6 - Primitives</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>