<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The classification of land cover using eo-learn. Part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The classification of land cover using eo-learn. Part 2 


 Part 1 


 Moving from data to results without leaving your computer 



 A stack of image...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The classification of land cover using eo-learn. Part 2</h1><div class="post__text post__text-html js-mediator-article"><h1 id="klassifikaciya-pokrova-zemli-pri-pomoschi-eo-learn-chast-2">  The classification of land cover using eo-learn.  Part 2 </h1><br><p>  <a href="https://habr.com/ru/post/452284/">Part 1</a> </p><br><p>  Moving from data to results without leaving your computer </p><br><p><img src="https://habrastorage.org/webt/hq/kv/ie/hqkviehem-itsqlacwosv2hjdco.png"><br>  <em>A stack of images of a small zone in Slovenia, and a map with a classified cover of land, obtained using the methods described in the article.</em> </p><a name="habracut"></a><br><h2 id="predislovie">  Foreword </h2><br><p>  The second part of a series of articles about the classification of land cover using the eo-learn library.  We remind you that in the first article the following was demonstrated: </p><br><ul><li>  The division of the AOI (area of ‚Äã‚Äãinterest) into fragments called EOPatch </li><li>  Acquire images and cloud masks from Sentinel-2 satellites </li><li>  Calculation of additional information, such as <a href="https://www.sentinel-hub.com/eoproducts/ndwi-normalized-difference-water-index">NDWI</a> , <a href="https://www.sentinel-hub.com/eoproducts/ndvi-normalized-difference-vegetation-index">NDVI</a> </li><li>  Creating a reference mask and adding it to the source data </li></ul><br><p>  In addition, we conducted a superficial study of the data, which is an extremely important step before the start of immersion in machine learning.  The aforementioned tasks were supplemented with <a href="https://github.com/sentinel-hub/eo-learn/blob/934b4e6328706b4d44905d54e58b5fa7dc267ec1/examples/land-cover-map/SI_LULC_pipeline.ipynb">an example in the form of a notebook of Jupyter Notebook</a> , which now contains material from this article. </p><br><p>  In this article, we will complete the data preparation, as well as build the first model for building land cover maps for Slovenia in 2017. </p><br><h2 id="podgotovka-dannyh">  Data preparation </h2><br><p>  The amount of code that is directly related to machine learning is rather small compared to the full program.  The lion's share of work consists in clearing data, manipulating data in such a way as to ensure seamless use with the classifier.  Further this part of work will be described. </p><br><p><img src="https://habrastorage.org/webt/gd/sj/4i/gdsj4iapqdgkldjwowfx-6p7bgg.jpeg"></p><br><p>  <em>The machine learning pipeline diagram, which shows that the code itself using ML is a small part of the whole process.</em>  <em><a href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">A source</a></em> </p><br><h3 id="filtraciya-oblachnyh-izobrazheniy">  Cloud Filtering </h3><br><p>  Clouds are entities that usually appear on a scale larger than our average EOPatch (1000x1000 pixels, 10m resolution).  This means that any area can be completely covered with clouds on random dates.  Such images do not contain useful information and only consume resources, so we skip them, based on the ratio of valid pixels to the total number, and set the threshold.  We can call valid all pixels that are not classified as clouds and are inside the satellite image.  Note also that we do not use the masks supplied with the Sentinel-2 images, since they are calculated at the level of full images (the size of the full S2 image is 10980x10980 pixels, approximately 110x110 km), which means for the most part are not needed for our AOI.  To define clouds, we will use the algorithm from the <a href="https://medium.com/sentinel-hub/sentinel-hub-cloud-detector-s2cloudless-a67d263d3025">s2cloudless</a> package to get the cloud pixel mask. </p><br><p>  In our notebook, the threshold is set to 0.8, so we only select images filled with normal data by 80%.  This may sound like a fairly high value, but since clouds are not too big a problem for our AOI, we can afford it.  It is worth considering that such an approach cannot be thoughtlessly applied to any point on the planet, since your chosen terrain may be covered with clouds in a significant part of the year. </p><br><h3 id="temporalnaya-interpolyaciya">  Temporal interpolation </h3><br><p>  Due to the fact that images on some dates may be missed, as well as due to non-permanent dates of receiving images by AOI, the lack of data is a very frequent phenomenon in the field of Earth observation.  One way to solve this problem is to overlay the mask of pixel validity (from the previous step) and interpolate the values ‚Äã‚Äãfor "fill holes".  As a result of the interpolation process, the missing pixel values ‚Äã‚Äãcan be calculated to create an EOPatch, which contains snapshots of evenly distributed days.  In this example, we used linear interpolation, but there are other methods, some of which are already <a href="https://eo-learn.readthedocs.io/en/latest/eolearn.features.interpolation.html">implemented</a> in eo-learn. </p><br><p><img src="https://habrastorage.org/webt/uf/va/ho/ufvahoeiz3u3shfdshioiuommlo.png"><br>  <em>On the left is a stack of Sentinel-2 snapshots from a randomly selected AOI.</em>  <em>Transparent pixels imply missing data due to clouds.</em>  <em>The image on the right shows the stack after interpolation, taking into account cloud masks.</em> </p><br><p>  Temporal information is extremely important in the classification of cover, and even more important in the problem of determining the germinating culture.  This is all due to the fact that a large amount of information about the land cover is concealed in how the plot changes throughout the year.  For example, when viewing interpolated NDVI values, you can see that the values ‚Äã‚Äãon the territories of forests and fields reach their maximums in the spring / summer and fall strongly in the autumn / winter, while the water and artificial surfaces keep these values ‚Äã‚Äãapproximately constant throughout the year.  Artificial surfaces have slightly higher NDVI values ‚Äã‚Äãcompared to water, and partly repeat the development of forests and fields, because in cities you can often find parks and other vegetation.  You should also consider the limitations associated with the resolution of images - often in the area that covers one pixel, you can observe several types of coverage simultaneously. </p><br><p><img src="https://habrastorage.org/webt/cj/mx/8s/cjmx8sdns2mzfv5nq9hko7apno4.png"><br>  <em>The temporal development of NDVI values ‚Äã‚Äãfor pixels from specific types of ground cover throughout the year</em> </p><br><h3 id="otricatelnaya-buferizaciya">  Negative buffering </h3><br><p>  Although the resolution of images in 10m is sufficient for a very wide range of tasks, the side effects of small objects are very significant.  Such objects are located on the border between different types of cover, and only one of these types is assigned to these pixels.  Because of this, there is extra noise in the input data when training the classifier, which degrades the result.  In addition, roads and other objects 1 pixel wide are present on the source map, although it is extremely difficult to identify them from images.  We apply 1px negative buffering to the reference map, removing almost all problem areas from the input data. </p><br><p><img src="https://habrastorage.org/webt/-m/u4/ep/-mu4epr9om3nqrmfdeoedefadqi.png"><br>  <em>AOI reference map before (left) and after (right) negative buffering</em> </p><br><h3 id="sluchaynyy-vybor-dannyh">  Random data selection </h3><br><p>  As mentioned in the last article, the full AOI is divided into approximately 300 fragments, each of which consists of ~ 1 million pixels.  This is quite an impressive amount of these very pixels, so we evenly take about 40,000 pixels for each EOPatch to get a data set of 12 million copies.  Since the pixels are taken evenly, a large number do not matter on the reference map, since these data are unknown (or were lost after the previous step).  It makes sense to weed out such data to simplify the training of the classifier, since we do not need to teach him to identify the "no data" label.  We repeat the same procedure for the test set, since such data artificially degrade the quality indicators of the classifier predictions. </p><br><h3 id="razdelenie-i-formirovanie-dannyh">  Separation and formation of data </h3><br><p> We divided the input data into training / testing sets in the ratio of 80/20%, respectively, at the EOPatch level, which guarantees that these sets do not overlap.  We also divide the pixels from the training set into sets for testing and cross-validation in the same way.  After separation, we get an array of <code>numpy.ndarray</code> dimension <code>(p,t,w,h,d)</code> , where: <br>  <em><code>p</code> is the number of <code>EOPatch</code> in the data set</em> <em><br></em>  <code>t</code> is the number of interpolated images for each EOPatch <br>  * <code>w, h, d</code> - width, height, and number of layers in images, respectively. </p><br><p>  After selecting the subsets, the width <code>w</code> corresponds to the number of selected pixels (eg 40000), with the dimension <code>h</code> equal to 1. The difference in the shape of the array does not change anything, this procedure is necessary only to simplify working with images. </p><br><p>  The data from the sensors and the <code>d</code> mask in any snapshot t define the input data for training, where there are such instances in the sum of <code>p*w*h</code> .  In order to convert the data into a form that is digestible for the classifier, we must reduce the dimension of the array from 5 to the matrix, the form <code>(p*w*h, d*t)</code> .  This is easily done using the following code: </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np p, t, w, h, d = features_array.shape <span class="hljs-comment"><span class="hljs-comment">#   t axis   1   3 features_array = np.moveaxis(features_array, 1, 3) #    features_array = features_array.reshape(p*w*h, t*d)</span></span></code> </pre> <br><p>  This procedure will allow you to make a prediction on the new data of the same form, and then convert them back and visualize them with standard means. </p><br><h3 id="sozdanie-modeli-dlya-mashinnogo-obucheniya">  Creating a model for machine learning </h3><br><p>  The optimal choice of the classifier is highly dependent on the specific task, and even with the right choice we must not forget about the parameters of a particular model, which need to be changed from task to task.  It is usually necessary to carry out a set of experiments with different sets of parameters in order to say exactly what is needed in a particular situation. </p><br><p>  In this series of articles, we use the <a href="https://lightgbm.readthedocs.io/en/latest/">LightGBM</a> package, because it is an intuitive, fast, distributed and productive framework for building models based on decision trees.  For the selection of hyper-parameters of the classifier, it is possible to use different approaches, such as a <a href="https://scikit-learn.org/stable/modules/grid_search.html">search by lattice</a> , which should be tested on a test set.  For the sake of simplicity, we skip this step and use the default parameters. </p><br><p><img src="https://habrastorage.org/webt/ik/ds/gr/ikdsgrz5mfdrifwakch1pvv1wey.png"><br>  <em>Decision trees in LightGBM.</em>  <a href="http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html">A source</a> </p><br><p>  The implementation of the model is quite simple, and since the data already comes in the form of a matrix, we simply feed this data to the input of the model and wait.  Congratulations!  Now you can tell everyone that you are learning machine and will be the most fashionable guy in the party, while your mother will be nervous about the uprising of robots and the death of humanity. </p><br><h2 id="validaciya-modeli">  Model validation </h2><br><p>  Training models in machine learning is easy.  The difficulty is to train them <strong>well</strong> .  For this, we need a suitable algorithm, a reliable reference card, and a sufficient amount of computing resources.  But even in this case, the results may not be what you wanted, so checking the classifier with error matrices and other metrics is absolutely necessary for at least some confidence in the results of your work. </p><br><h3 id="matrica-oshibok">  Error matrix </h3><br><p>  Error matrices are the first thing to look at when assessing the quality of classifiers.  They show the number of correctly and incorrectly predicted labels for each label from the reference map and vice versa.  Usually, a normalized matrix is ‚Äã‚Äãused, where all the values ‚Äã‚Äãin the lines are divided by the total amount.  This shows whether the classifier does not have an offset towards a certain type of cover relative to another </p><br><p><img src="https://habrastorage.org/webt/rg/3m/xd/rg3mxdktoqpvxy76j_jvfkgpzw4.png"><br>  <em>Two normalized error matrices of the trained model.</em> </p><br><p>  For most classes, the model shows good results.  For some classes errors occur due to unbalanced input data.  We see that the problem is, for example, bushes and water, for which a model often confuses pixel labels and identifies them incorrectly.  On the other hand, what is labeled as a bush or water is related quite well to the reference map.  From the following image, we can see that problems arise for classes that have a small number of training instances - mainly because of the small amount of data in our example, but this problem can occur in any real-world task. </p><br><p><img src="https://habrastorage.org/webt/4f/8l/qz/4f8lqz3q4xyxjyp3xp3e9usb_qg.png"></p><br><p>  <em>The frequency of occurrence of pixels of each class in the training set.</em> </p><br><h3 id="reciever-operating-characteristic---roc-krivaya">  Reciever Operating Characteristic - ROC curve </h3><br><p>  Classifiers predict labels with a certain confidence, but this threshold for a particular label can be changed.  The ROC curve shows the ability of the classifier to correctly predict when the sensitivity threshold is changed.  Usually this graph is used for <strong>binary</strong> systems, but it can be used in our case, if we count the characteristic "label against all others" for each class.  False-positive results are displayed along the x-axis (we need to minimize their number), while true-positive results are shown along the y-axis (we need to increase their number) at different thresholds.  A good classifier can be described by a curve, under which the area of ‚Äã‚Äãthe curve is maximum.  This indicator is also known as area under curve, AUC.  From the graphs of ROC curves, the same conclusions can be drawn about the insufficient number of examples of the ‚Äúbush‚Äù class, although the curve for water looks much better - this is because visually the water is very different from other classes, even with an insufficient number of examples in the data. </p><br><p><img src="https://habrastorage.org/webt/v2/b5/_c/v2b5_cp7omqsxgg9v-bqaqmlk8u.png"><br>  <em>ROC curves of the classifier, in the form of "one against all" for each class.</em>  <em>The numbers in parentheses are the AUC values.</em> </p><br><h3 id="vazhnost-priznakov">  Importance of symptoms </h3><br><p>  If you want to dig deeper into the subtleties of the classifier, you can look at the graph of the importance of signs (feature importance), which tells us which of the signs more strongly influenced the final result.  Some machine learning algorithms, for example, which we used in this article, return these values.  For other models, this metric needs to be read by itself. </p><br><p><img src="https://habrastorage.org/webt/x9/ui/d6/x9uid68f7bbim-g4nfusppu9cuu.png"><br>  <em>Matrix of importance of attributes for the classifier from the example</em> </p><br><p>  Although other signs in spring (NDVI) are generally more important, we see that there is an exact date when one of the signs (B2 is blue) is the most important.  If you look at the pictures, it turns out that the AOI was covered with snow during this period.  It can be concluded that snow reveals information about the underlying cover, which greatly helps the classifier to determine the type of surface.  It is worth remembering that this phenomenon is specific to the observed AOI and, in general, cannot be relied upon. </p><br><p><img src="https://habrastorage.org/webt/qv/h1/ak/qvh1ak0bil0qhgt0ax77j0vl1dc.png"><br>  <em>Part of AOI in the form of a 3x3 EOPatch mesh covered with snow</em> </p><br><h2 id="rezultaty-predskazaniy">  Prediction results </h2><br><p>  After validation, we better understand the advantages and disadvantages of our model.  If we are not satisfied with the current state of affairs, you can make changes to the pipeline and try again.  After optimizing the model, we define a simple EOTask, which accepts EOPatch and the classifier model, makes a prediction and applies it to the fragment. </p><br><p><img src="https://habrastorage.org/webt/bn/d4/8u/bnd48uzm8dp75_2rjgba0enxwlg.png"><br>  <em>Image Sentinel-2 (left), true (center) and prediction (right) for a random fragment of AOI.</em>  <em>You may notice some differences in the images that can be explained by applying negative buffering on the original map.</em>  <em>In general, the result for this example is satisfactory.</em> </p><br><p>  Further path is clear.  You must repeat the procedure for all fragments.  You can even export them in GeoTIFF format, and glue them together using <a href="https://www.gdal.org/gdal_merge.html">gdal_merge.py</a> . </p><br><p>  We uploaded glued GeoTIFF to our portal GeoPedia, you can see the results in detail <a href="https://www.geopedia.world/">here.</a> </p><br><p><img src="https://habrastorage.org/webt/mw/gu/jo/mwgujo8q9zai7myheycgaomcwni.png"><br>  <em>Screenshot prediction cover the land of Slovenia 2017 using the approach from this post.</em>  <em>Available in an interactive format at the link above.</em> </p><br><p>  You can also compare official data with the result of the classifier.  Note the difference between <em>land use</em> and <em>land cover</em> , which is often found in machine learning tasks - it is not always possible to easily display data from official registries to classes in nature.  As an example, we will show two airports in Slovenia.  The first is <a href="https://www.geopedia.world/">Levec, near the town of Celje</a> .  This airport is small, mainly used for private jets, and is covered with grass.  Officially, the territory is marked as an artificial surface, although the classifier is able to correctly identify the territory as grass, see below. </p><br><p><img src="https://habrastorage.org/webt/bj/ps/vo/bjpsvoabp90ud12fouuyg1dza3y.png"><br>  <em>Image Sentinel-2 (left), true (center), and prediction (right) for the area around the small sports airport.</em>  <em>The categorizer defines the landing strip as grass, although it is marked as an artificial surface in the actual data.</em> </p><br><p>  On the other hand, <a href="https://www.geopedia.world/">at the largest airport in Slovenia, Ljubljana</a> , zones marked as artificial surface on the map are roads.  In this case, the classifier distinguishes buildings, while correctly distinguishing grass and fields in the neighboring territory. </p><br><p><img src="https://habrastorage.org/webt/bs/oa/c-/bsoac-m7f9jdus4nqh7gl0v8ij0.png"><br>  <em>Image Sentinel-2 (left), true (center), and prediction (right) for the zone around Ljubljana.</em>  <em>The classifier determines the runway and the road, while correctly distinguishing grass and fields in the neighborhood</em> </p><br><p>  Voila! </p><br><p>  Now you know how to create a reliable model for the whole country!  Remember to add this to your resume. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/452378/">https://habr.com/ru/post/452378/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../452366/index.html">New CMOS-matrix expands the possibilities of shooting moving objects</a></li>
<li><a href="../452368/index.html">Fifteen Useful Details for Electronic Document Management</a></li>
<li><a href="../45237/index.html">Opera Software, Ukrainian meetings</a></li>
<li><a href="../452370/index.html">How a 3D printer helped a teenager affected by the bombing get a new hand</a></li>
<li><a href="../452372/index.html">Now, good developers are measured by views and subscribers. Is it bad?</a></li>
<li><a href="../452382/index.html">News of the week: Offline Runet Control Center, Bitcoin at $ 8,000, Vulnerability in Intel Processors</a></li>
<li><a href="../452384/index.html">The processor will accelerate the optics to 800 Gbps: how it works</a></li>
<li><a href="../452388/index.html">Sieve of Eratosthenes for O (n). Evidence</a></li>
<li><a href="../45239/index.html">Online service simplest image editing</a></li>
<li><a href="../452392/index.html">A selection of datasets for machine learning</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>