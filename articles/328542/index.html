<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>About memory, tags and coherence</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tagged memory ( tagged architecture ) gives an exotic opportunity to separate data from metadata. The price for this is not so great (at first glance)...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>About memory, tags and coherence</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/web/03a/743/b6b/03a743b6b6934601bcd48c4db10d855b.png"></div><br>  Tagged memory ( <a href="https://en.wikipedia.org/wiki/Tagged_architecture">tagged architecture</a> ) gives an exotic opportunity to separate data from metadata.  The price for this is not so great (at first glance), and the potential opportunities are impressive.  Under the cut try to figure it out. <br><a name="habracut"></a><br><h3>  Background </h3><br>  In a tagged architecture, each word of memory is accompanied by a tag ‚Äî one or more bits describing the data in this word.  By itself, this idea is very natural and originated at the very dawn of the computer industry. <br><br><h4>  Rice computer </h4><br>  Chronologically, apparently the first full-fledged computer with such an architecture was the <a href="http://www.princeton.edu/~adam/R1/r1rpt.html">R1</a> , which was developed from 1958 to 1961, began to give the first signs of life in 1959. Mostly Rice computer was remembered by the fact that its memory was collected on <a href="https://en.wikipedia.org/wiki/Williams_tube">CRT handsets</a> .  The image of a piece of memory on such a tube is presented in the title illustration. <br><br>  The hardware word (of which there were 32K, grouped by 64) had a size of 64 bits, of which: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  1 bit was used to debug the work of the tubes themselves, which were very unreliable storage.  In fact, every 64th handset was a CRT monitor, on which one could literally <i>peep the</i> contents of any of the adjacent handsets. <br><img src="https://habrastorage.org/web/ff4/67c/d45/ff467cd45a354b24ace8ca379ee94f2d.png"><br>  <i>Debugging on R1.</i> <br><br></li><li>  7 digits per <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D0%25B4_%25D0%25A5%25D1%258D%25D0%25BC%25D0%25BC%25D0%25B8%25D0%25BD%25D0%25B3%25D0%25B0">Hamming code</a> </li><li>  54 bits of data itself </li><li>  2 digits per tag.  Tags extended to code words as well as to data. </li></ul><br>  The tags on the code were used for debugging, for example, an instruction with a ‚Äú01‚Äù tag caused the registers to be printed. <br><br>  For data, everything is more complicated.  Tags marked arrays, for example, two-dimensional arrays had tags of the end of a line and the end of an array.  Also, the tag could mean the end of the iterations (vector), when the instruction was cycled on the vector. <br><br><h4>  Burroughs Large Systems </h4><br><img src="https://habrastorage.org/web/e80/d8c/376/e80d8c37649241beaa859e37593f18cf.png"><br>  Burroughs 5000 appeared in 1961.  Each 48-bit word was accompanied by a tag from one digit that determined whether it was code or data. <br><br>  Later on (1966, B6500), the use of tags was considered very successful, and the tag was extended to 3 bits.  At the same time, the 48th digit was immutable and still separated the code from the data.  Tags meant: <br><br><ul><li>  0 - any data except double precision </li><li>  2 - double precision </li><li>  4 - cycle index </li><li>  6 - uninitialized data </li></ul><br><ul><li>  1 - addresses on the stack </li><li>  3 - common tag for code </li><li>  5 - descriptors describing data <b>not</b> on the stack </li><li>  7 - procedure handle </li></ul><br>  This series has been very successful and commercially in terms of technology development.  The only serious drawback can be considered a strong focus on <a href="http://www.eah-jena.de/~kleine/history/languages/burroughs_B5500_ExtendedAlgol.pdf">Algol</a> .  With obsolescence, Algola is gone and Burroughs. <br><br><h4>  Lisp are machines. </h4><br>  Actively developed in 1973 ... 1987 after and during the boom of artificial intelligence systems in Lisp. <br><br>  Some of them had hardware tags, for example, the <a href="https://en.wikipedia.org/wiki/Symbolics">Symbolics 3600</a> (a 36-bit word containing 4‚Äì8 bit tags and 28‚Äì32 data bits). <br><br>  And some used the technique of <a href="https://en.wikipedia.org/wiki/Tagged_pointer">tagged pointers</a> when the tag is written either to the unused bits of the pointer or to the memory preceding the one the pointer is looking at. <br><br>  This technique is quite flourishing itself today, for example, in the form of <a href="https://habrahabr.ru/post/149012/">NSNumber</a> .  From a close to the author, a similar lisp approach (together with the lisp core) was implemented first in Kubl and then in <a href="https://en.wikipedia.org/wiki/Virtuoso_Universal_Server">Openlink Virtuoso</a> RDBMS. <br><br><h4>  Soviet Elbrus. </h4><br>  Was disassembled earlier in this <a href="https://habrahabr.ru/post/313376/">article</a> .  The 64-bit word was accompanied by 8 digits of the tag and 8 - Hamming code.  The developed tag system allowed dynamic typing.  Those.  there was a general addition instruction, the processor used the tags to determine the types of arguments, made the necessary casts, and started the summation of the desired type. <br><br>  In this case, if the argument was a function descriptor, the function was started, and if an indirect word was used, an indirect call was made.  And so on until it turned out exactly the value. <br><br>  Along with addresslessness, this provided a high compactness of the code. <br><br><h4>  Current Elbrus (three stacks - Sic!) </h4><br>  Elbrus, as far as the author knows, is the only evolving architecture with tagged memory. <br><br>  Each 4-byte word in memory, registers and tires is accompanied by a 2-bit tag.  The word tag encodes the following features [4, p. 98]: <br><br><ul><li>  0 - the word contains numeric information either by itself or <br>  being a format fragment </li><li>  1 - the word contains non-numeric information with the format of a single <br>  the words </li><li>  2 - the word contains a fragment of non-numeric format information <br>  double word </li><li>  3 - the word contains a fragment of non-numeric format information <br>  quadro words </li></ul><br>  Neither the length nor the type of numerical data is architecturally indistinguishable.  The semantic filling of a numeric variable is tracked by the compiler and manifests itself when it becomes the operand of an operation. <br><br>  The register file contains [4, p. 127] 256 registers of 84 bits each, having three fields.  The first two fields are for 32-bit storage. <br>  words with 2-bit tags, double word F64, mantissa real <br>  numbers, and the third field is reserved for storing a 16-bit order value.  Those.  2 * (32 + 2) + 16. <br><br>  According to [5, p. 9], [6, p. 10] tags are - <br><br><ul><li>  array handle </li><li>  object handle </li><li>  is empty </li><li>  numeric data </li></ul><br>  Apparently, this is deciphering the value of the tag, ‚Äúin the process of the path the dog could grow up‚Äù. <br><br>  The tag mechanism is used for hardware control of the correctness of the program, the costs are 25-30% (forum.ixbt).  Costs, apparently, are caused by the fact that the pointer can reach 256 bits. <br><br>  Swapping data comes with tags.  Physically tags are located in the memory allocated for ECC. <br><br>  So, they refused from the dynamic typing in the spirit of the Soviet Elbrus, but applied a modified object model, which, apart from the performance costs, greatly complicated the C / C ++ compiler, making it impossible to integrate into the LLVM, for example. <br><br><h3>  So. </h3><br>  We see that the tagged memory was used for the following purposes: <br><br><ul><li>  debugging </li><li>  performance control </li><li>  data driven programming </li></ul><br>  Without diminishing the importance of all of the above, it would be useful to introduce another class of information for tags ‚Äî hints to the processor, the presence of which can improve performance. <br><br><h3>  Current Element Base, <a href="https://ru.wikipedia.org/wiki/ECC-%25D0%25BF%25D0%25B0%25D0%25BC%25D1%258F%25D1%2582%25D1%258C">ESS</a> </h3><br>  In practice, DDR * SDRAM ECC-memory is widely used for servers with the <a href="http://technical_abbreviations.academic.ru/20724/SEC_DED">SECDED</a> class <a href="http://technical_abbreviations.academic.ru/20724/SEC_DED">code</a> (single-patch correction and double-error detection).  On memory modules, for every 8 chips, one more chip is added, which stores ECC codes of 8 bits for every 64 bits of main memory. <br><br><h3>  What if: </h3><br>  Suppose we are dealing with 64-bit words.  <i>On the existing element base,</i> we have an additional 8 digits for each word.  At the same time, we adhere to static typing quite in the spirit of C / C ++.  And do without the object model, which for the most part is needed when debugging, and you always have to pay for it. <br><br>  How can you use the discharges of 8 discharges?  For example, enter the following fields in the tag: <br><br><ol><li>  <b>Null</b>  Signals whether underlying data is initialized.  Used to control the correctness of the program.  Existing software traps are quite expensive, which is why they are mainly used for debugging.  The presence of hardware control would bring many benefits.  1 rank <br><br></li><li>  <b>Exec</b> .  Distinguishes executable code from data.  Used to avoid execution of the unenforceable.  1 rank <br><br></li><li>  <b>Type metadata</b> .  Sets the fact whether we are dealing with a chunk of data or a pointer.  1 rank  One could have done without it, since we have static typing.  On the other hand, hardware control of this kind would be very useful.  The use of this field entails the need for a tag modification instruction. <br><br></li><li>  <b>Attributes of type instance</b> .  For numeric data, this field contains information similar to the contents of the flags register at the time of the operation to calculate this data. <br><br>  In x86, for example, the following <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25B3%25D0%25B8%25D1%2581%25D1%2582%25D1%2580_%25D1%2584%25D0%25BB%25D0%25B0%25D0%25B3%25D0%25BE%25D0%25B2">FLAGS</a> register fields are responsible for this. <br><br><ol><li> CF - carry flag - overflow in unsigned integer arithmetic </li><li>  <s>PF - parity bit, not needed</s> </li><li>  <s>AF - for BCD, not needed</s> </li><li>  ZF - the result is 0 </li><li>  SF - less than 0 result </li><li>  OF - overflow occurred </li></ol><br>  In the superscalar core, the register of flags is also to be <a href="http://www.ixbt.com/cpu/cpu-pedia.shtml">renamed</a> , with various flags being renamed independently.  This creates certain difficulties and delays in reading the contents of the flags register. <br><br>  And we have the opportunity to place the flags of the result of a specific operation in its tag.  If the task is to squeeze into 3 bits, you can simply donate OF or set the NULL flag if the compiler considers it necessary. <br><br>  With regard to the work with a floating point, ZF &amp; SF will be useful, <a href="https://ru.wikipedia.org/wiki/NaN">NAN</a> can also be duplicated here. <br><br>  Pointers.  For them it would be useful to leave the CF flag, enter another one - the constancy of the data.  Data constancy does not mean that the data cannot be changed; it simply cannot be done through this pointer.  It also requires support (tag modification instructions) to remove / establish constancy. <br><br>  The presence of attributes of an instance of a type allows part of the comparison operations to be performed without recourse to an ALU.  And the separation of the comparison operation and the conditional transition slightly simplifies the life of the predictor of transitions. <br><br></li><li>  <b>Attributes of the place</b> .  This part of the tag refers not to the data, but to the place where they are located.  Data may be <br><br><ul><li>  in the program body as constants or as global variables, in this case the metadata of the place is provided by the compiler </li><li>  in the swap file </li><li>  in RAM </li><li>  in cache </li><li>  in the register </li></ul><br>  Everywhere along with the data is present and their tag. <br><br>  <i>When copying data to a register, their place attributes are lost.</i> <i><br></i>  <i>When copying from a register, place attributes are taken from a destination.</i> <i><br><br></i> <ol><li>  <i>Constancy</i>  The presence of this flag guarantees the hardware protection of a specific word against changes.  In C / C ++, constants are placed in a special write-protected segment.  Using the operating system, you can create a data segment and prohibit the change of its content by hardware, but this is expensive.  Having cheap word level hardware protection might be helpful.  <i>Traps</i> .  At the debug level, it is sometimes useful to catch changes in a particular word; this can be implemented in such a relatively cheap way. <br><br></li><li>  <i>Globality</i>  <b>Perhaps the most important.</b>  This flag determines whether the given word participates in the cache coherence support mechanism. </li></ol></li></ol><br><h3>  Cache coherence support. </h3><br>  At the moment, the whole (with rare exceptions - non-cached ranges) memory is covered by the mechanism of cache coherence support.  Although, for example, stack memory is used only by a specific thread, and it is unnecessary to share it with others (this may be required in exceptional cases under the responsibility of the programmer). <br><br>  This problem could be solved using the attributes of memory segments, but this is too expensive.  <a href="https://en.wikipedia.org/wiki/C_standard_library">CRT</a> , for example, allocates segments as it pleases and the programmer has no way to influence this. <br><br>  On the other hand, the amount of user data for which coherence support is required is not that large.  If it were possible to clearly indicate a sign of data coherence, this would markedly reduce the load on the bus.  Most system data, for example, <a href="https://en.wikipedia.org/wiki/Translation_lookaside_buffer">TLB</a> is not concerned, they should be global. <br><br>  Consider an example, the multiplication of matrices.  This refers to the matrix of large size, which are certainly not located on the stack. <br><br>  The <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC_%25D0%25A8%25D1%2582%25D1%2580%25D0%25B0%25D1%2581%25D1%2581%25D0%25B5%25D0%25BD%25D0%25B0">Strassen algorithm</a> recursively splits 2X2 matrices, then multiplies them using 7 multiplications instead of 8. The division continues until the submatrices become small (&lt;32 ... 128), then they are multiplied in a naive way.  Has difficulty <img src="https://upload.wikimedia.org/math/d/2/d/d2d58d7a0716cbcf45e9f2f5a1fde024.png" alt="image">  and suffers from instability. <br><br>  At first glance, it seems that in the process of computing all the used memory should be coherent.  But, <br><br><ul><li>  when naively multiplying small submatrices, no coherence is required at all </li><li>  when working with 2X2 matrices, all the work inside the submatrices has already been completed and all data is used within one thread <br></li><li>  in fact, memory coherence is not needed for this task </li></ul><br>  The same is true of the most well-parallelized algorithms - the interaction between the threads occurs only when merging partially calculated results.  And the interaction between threads is reduced to the barriers of computation. <br><br>  The reason is that synchronization of threads is expensive, incompatible with efficient calculations. <br><br>  What can be a living example of mass use of shared memory? <br>  Parallel work with the graph, where the mask of attendance of the ribs is divided.  It was possible to have a separate mask for each thread, but this is less effective in terms of using the cache.  As a result, masks are changed by interlocked operations.  But interlocked operations are a clear indication of the mechanism for supporting coherence. <br><br>  Suppose we have a mechanism that allows a programmer to explicitly tell the compiler what data he wants to see in a coherent state, and what data he doesn't want.  In C / C ++, this could be a volatile modifier.  At the moment, this modifier is used to remove a variable from under optimization, the compiler avoids its placement in registers. <br><br>  In what situation can there be problems with incoherent data? <br><br><ul><li>  The T1 stream runs on the P1 processor, calculates something, adds to the M1 array </li><li>  M1 is a stack; no outsider knows about it. </li><li>  Some part of M1 is located in the P1 cache. </li><li>  T1 is superseded by the scheduler and after some time it resumes at P2. </li><li>  P2 cache has no M1 content, it is read from memory - <b>data is not relevant</b> </li></ul><br>  It seems that it will not be possible to completely isolate local data from the coherence support protocol.  Let's see what can be done here. <br><br>  There are two ‚Äúorthogonal‚Äù variants - the evolution of the coherence support protocol and the local cache with ‚Äúrelocation‚Äù. <br><br><h4>  Local cache with ‚Äúrelocation‚Äù. </h4><br>  Appeals to local data take up to 95% [19].  Since they are not needed by anyone outside the kernel, it is logical to start their own cache for them without the support of coherence.  Problems begin when thread loses context because  at this moment we still do not know on which core our thread will resume. <br><br><ul><li>  because  It is likely that this will be a different core, it's time to reset all the untouched data, merge all changes into memory </li><li>  but if it‚Äôs the same core, you‚Äôll have to read it all over again. </li><li>  besides, transfer from kernel to kernel is faster than reading from memory </li><li>  if you postpone the ‚Äúmove‚Äù at the time of renewal, there will be an undesirable delay </li><li>  the time estimate for ‚Äúrelocation‚Äù is as follows: let the transfer occur at a clock frequency of 32 bytes per clock, this gives 100GB / s, 2MB transmission will take 20 microseconds, not so little. </li><li>  transferring thread to another kernel is a very frequent case and blocking the bus for tens of microseconds every time is very wasteful </li><li>  besides operating system support is required </li></ul><br>  The result is this - at least when transferring a thread inside a single processor, the option of ‚Äúmoving‚Äù the local cache is inappropriate. <br><br><h4>  <a href="https://en.wikipedia.org/wiki/Cache_memory">Coherence</a> Support Protocol. </h4><br>  The basis (for simplicity) is <a href="https://en.wikipedia.org/wiki/MESI_protocol">MESI</a> (Intel: Pentium, Core; PowerPc 604 [16]), which, although considered obsolete, is the basis for other protocols and is not as specific as <a href="https://en.wikipedia.org/wiki/MESIF_protocol">MESIF</a> (Intel Nehalem) or <a href="https://en.wikipedia.org/wiki/MOESI_protocol">MOESI</a> ( AMD Opteron). <br><br>  How it works physically is shown in the following illustrations. <br><div style="text-align:center;"><img src="https://habrastorage.org/web/9f7/ccc/8e5/9f7ccc8e5e684339921f36fa54c4345c.png"></div><br>  Example <a href="http://www.ixbt.com/cpu/sandy-bridge-3.shtml">SandyBridge</a> (MESIF).  There are 4 buses in each direction: requests, acknowledgments, <a href="http://www.ixbt.com/cpu/cpu-pedia.shtml">snoops</a> (to support coherence) and data itself (32 bytes wide) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/f90/18d/14f/f9018d14f09444368622a0e3e166b188.png"></div><br>  Elbrus ‚Äë 8C topological plan ( <a href="http://moglobi.ru/stati/a-s-kojin-e-s-kojin-v-o-kostenko-a-v-lavrov-zao-mcst-a-kozhin/main.html">MOSI for E-4C +</a> ): Core 0‚Äì7 - processor cores;  L3 B0‚Äì7 - banks of the third level cache;  SIC - system exchange controller;  DIR0,1 - global reference;  DDR3 PHY0‚Äì3 - blocks of the physical memory level;  IP PHY1, 2, 3 - blocks of the physical layer of interprocessor exchange channels;  IO PHY - block of the physical level of the I / O channel [13]. <br><br><h4>  So, MESI. </h4><br>  Requests from the kernel to the cache are: <br><br><ol><li>  <b>PrRd</b> : The kernel asks to read the cache line. </li><li>  <b>PrWr</b> : The kernel asks to write a cache line. </li></ol><br>  Requests from the bus are: <br><br><ol><li>  <b>BusRd</b> : Snoop occurs when there is a request to read this cache line from another kernel </li><li>  <b>BusRdX</b> : Snoop saying that there is a request to read this cache line from another kernel, and on the other side of this line is no longer </li><li>  <b>BusUpgr</b> : Some kernel asks for a (more recent) cache line despite the fact that its old version is there </li><li>  <b>Flush</b> : A signal that a cache line was unloaded by someone. </li><li>  <b>FlushOpt</b> : Passing a string from the cache to the cache. </li></ol><br>  Cache line states: <br><br><ul><li>  <b>Modified (M)</b> A string is only present in the cache of the kernel, but its contents are different from what is in memory.  One day this line will have to be written back into memory, with the state going to S. </li><li>  <b>Exclusive (E) The</b> unchanged line is present only in the cache of this kernel. </li><li>  <b>Shared (S)</b> This line can also be in the caches of other cores, while its contents are the same as in memory and can be safely reset at any time. </li><li>  <b>Invalid (I)</b> in the kernel cache is not a given line. </li></ul><br>  The machine for these states looks like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/1b4/611/6c3/1b46116c39de4ddeaec89e5bda4528fe.png"></div><br>  But all this concerns only those data that are marked in the tag as global.  What to do with local?  For them we will create new states: <br><br><ul><li>  <b>Local Modified (LM)</b> string is present only in the cache of the kernel, but its contents are different from what is in memory.  One day, this line will have to be written back into memory, with the state going to L <br><ol><li>  PrRd does not cause changes in the state </li><li>  PrWr does not cause state changes. </li><li>  BusRdX puts the string in state I, the content is sent to the requester </li><li>  BusRd, BusUpgr, Flush suggest that the requested copy has something that cannot be <br><br></li></ol></li><li>  <b>Local (L)</b> Unchanged local string. <br><ol><li>  PrRd does not cause changes in the state </li><li>  PrWr puts the string in the LM state. </li><li>  BusRdX puts the string in state I, the content is sent to the requester </li><li>  BusRd, BusUpgr, Flush suggest that the requested copy has something that cannot be </li></ol></li></ul><br>  Now, if we go back to the example, after T1 has resumed on the P2 core, an attempt to read M1 will lead to the migration of the cache contents from the P1 core to P2. <br><br>  In the cache of all cores, there is no more than one instance of any local data cache line.  At the same time, after the data migration has been completed, no read / write operations on the local data (except reading / writing the actual memory) do not lead to bus delays. <br><br>  It is worth noting the following nuances: <br><br><ul><li>  Initially, when requesting PrRd, it is not yet known what type the cache line will be.  Therefore, the decision about which state to assign to this line (L or E) is made after reading it. </li><li>  The cache line is larger than a word, for example, 64 bytes.  The globality flag is assigned to the entire string, the compiler must take care of proper alignment.  This is quite a serious problem.  If we want local and global data not to be mixed within the same cache line, the compiler must take this into account when aligning.  Moreover, the size of the cache line may depend on the processor model.  Although, on the other hand, if, along with global data, a piece of local will also come under the distribution, there should be no particular problems. <br></li><li>  Since caching is done in physical addresses, some effort must be made before forcing a page out of memory.  Namely, in all caches, the changed data of this page should be stored in memory, after which all of the lines on this page are invalid.  This also applies to local data. </li><li>  Any close coherence support algorithm is easily modified to work with local data in a similar way. </li><li>  Separate words deserve a dynamic change of the flag globality.  Suppose we allocate a memory block with malloc and make it global.  Since the memory block could be allocated from the CRT cache and was previously in circulation, it is likely that its residues are still in the kernel cache as local.  Therefore, before cocking globalization in the tag, it is necessary to clear this line from the caches of all cores.  The same applies to the reverse situation - the localization of global data. </li></ul><br>  An attentive reader can exclaim - let me, but in fact it happens in life.  States <b>E</b> and <b>M</b> are read and written without additional bus operations!  If the data is actually local and without any tags, working with them is quite local. <br><br>  There is a difference.  A BusRd request puts <b>M</b> and <b>E</b> into state <b>S</b> and the string continues to occupy the resource for a while.  In addition, we have considered only a relatively simple case - a single multicore processor. <br><br><h4>  SMP </h4><br>  Here we are dealing with multiple processors, the means of communication between them, and the total equally-accessible memory.  Such a scheme has a growth limit.  Either processors are connected to the <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BB%25D0%25B8%25D0%25BA%25D0%25B0_(%25D1%2582%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B8%25D1%258F_%25D0%25B3%25D1%2580%25D0%25B0%25D1%2584%25D0%25BE%25D0%25B2)">click</a> , or work through a common bus.  The first version is quadratic in complexity by the number of processors, the second one divides its bandwidth into all.  Combined methods collect not only advantages, but also disadvantages of both options. <br><br>  Up to a certain size can manage so-called.  directory based scheme.  In its purest form, a directory assumes a handle to each potential cache line.  Those.  if the line size is 64 bytes, then for every 64 bytes of main memory there should be a descriptor, telling which processors / cores / nodes have copies of this line. <br><br>  It is clear that this requires a large amount of additional memory, which because of this cannot be done quickly.  Therefore, this scheme greatly slows down the work with memory. <br><br>  On the other hand, directory is very sparse.  cache size is much smaller than main memory.  Therefore, attempts have been made (for example, [24]) to store information in a compressed form and still squeeze it into fast memory.  Here, the problem is quite simple - the amount of memory per directory is fixed, and the degree of compression is not constant.  Ensuring that it is possible to fit in a fixed volume is not so easy. <br><br>  There is a variant of the hybrid scheme, when the directory is used as a cache, and everything that does not fit into this cache is processed via the snoop protocol.  The disadvantage of this option is that you have to store along with the mask part of the cache line address, so that the efficiency of using fast and expensive memory decreases, but everything works on any directory volume. <br><br>  Example, Opteron from AMD: one of the banks of the L3 cache [28] is allocated to the directory - 1MB out of 6 on the processor, this covers 256K lines, 32 bits per line.  Those.  The directory serves approximately three times the cache size of its processor. <br><br>  Example, Nehalem from Intel.  <a href="https://en.wikipedia.org/wiki/Cache_inclusion_policy">Inclusive</a> L3 cache with a total volume of 2 MB per core.  There is no dedicated directory, but each L3 line contains a usage mask - one for each core of its processor and one for each foreign processor, for example 10 + 7 = 17 (10 - by the number of cores, needed for intra-processor MESIF, 7 - allowed up to 8 processors, this is just part of the directory) additional bits.  In fact, this is a L3-sized directory that is synchronous with L3. <br><br>  And we will tell about the already mentioned Elbrus-4C + [19] separately, it deserves it. <br><br><ul><li>  8 cores </li><li>  inclusive L3 cache with a total volume of 16MB </li><li>  SIC system exchange controller with four memory channels and input / output interfaces </li><li>  Four processors can be combined using three interprocessor links into a system consisting of 32 cores. </li><li>  each processor has two pipelines for memory access </li><li>  each pipeline is connected to two memory controllers </li><li>  each pipeline requires its own directory of 512Kb, which works taking into account interleaving to addresses (8th of the physical address) </li><li>  each directory entry (32 bits) is responsible for 2 cache lines (64 bytes each), i.e.  Totally 256K elements of each processor cover 32Mb L3 cache (out of 48 required - the processor's own cache is not taken into account, the directory is used only for searching in neighboring processors) </li><li>  The state of one cache line is described by 6 bits (valid, linkA, linkB, linkC, modified (2)), which is enough to implement MOESI protocol on 4 processors, MOSI actually works.  If we take into account that a directory item has 2 cache lines, there are 16 bits per line. </li></ul><br>  What can the proposed method of local data give in the case of SMP?  Recall, here we analyze the case of thread moving to another processor, since  we have already analyzed the movements inside one processor and decided that they do not require special actions. <br><br>  It is also worth noting that context switching is a rather expensive operation, just saving / restoring registers <a href="http://wiki.osdev.org/Context_Switching">takes</a> hundreds or even thousands of cycles.  The contribution of the losses associated with the loss of the cache is estimated [34] in a wide range from units to thousands of microseconds, depending on the task.  We also point out that the duration of the quantum of time allocated by the system to a thread before it is forcedly displaced is measured in <a href="https://www.ibm.com/support/knowledgecenter/ru/ssw_aix_71/com.ibm.aix.performance/mod_sched_time_schedo.htm">tens</a> or even <a href="http://intuit.valrkl.ru/course-1143/index.html">hundreds of</a> milliseconds. <br><br>  As we noted earlier, two ‚Äúorthogonal‚Äù variants are possible: <br><br><ol><li>  Cache Evolution  We do not make any additional efforts; cache lines with <b>L</b> and <b>LM</b> states are gradually reset at the old place and appear on the new one.  The difference with existing systems is that the local cache at the old place is deleted after reading, and does not remain in the <b>S</b> state. <br></li><li>  Cache with ‚Äúrelocation‚Äù.  Suppose we have some way to guarantee thread that its local data can only be in the memory or cache of its processor.  In this case, local data does not participate in interprocessor snoop distribution and is not represented in the directory, if it exists.  The efficiency of the directory (which now serves only global data) increases at the same size. <br><br>  How can you arrange it? <br><br><ol><li>        , ,       . </li><li>    thread'          <b>L</b>  <b>I</b> ,            . </li><li>    <b>LM</b>    ,    ,    .      . </li><li>           .            thread'   ,        thread'   . <br>         ,    ‚Äú‚Äù     ‚Äú‚Äù. </li><li>    ,    N-                 <b>FlushOpt</b> .         <b>L</b>     (  )  . <br><br>     ,   ,       .    ,      ,       thread.  thread,    ,      .         ,     ‚Äî  ,   ,  ,             - (     ).   ,  SMP,     . </li></ol></li></ol><br><h3> cc-NUMA </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Since centralized ownership of resources (eg, memory) has natural size limits, building larger systems requires fragmentation. The system consists of the so-called. nodes, each of which is usually arranged in the spirit of SMP. For communication between the nodes, there are special tools, the principal difference is the fact that memory access is now not uniform - the memory on its node is at least tens of percent faster [31.32]. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The shared cache has now become a three-level - on its processor - on its node - on a foreign node. Accordingly, the cost has increased. Snoop protocols for the entire system are no longer possible, in one form or another, a directory is required. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">From the point of view of the directory, this situation is fundamentally different from the SMP in that the directory itself has become two-level.</font></font> Those.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now the line can be shared not only with a different processor, but also with a different node. This division is necessary due to the fact that there can be a lot of processors - hundreds or thousands, it is impossible to reserve a discharge for each of them in the descriptor of each cache line. In addition, the cost of access in these situations is also different.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Therefore, it is necessary to clearly separate other people's nodes from their processors. For example, in the E-4C + directory there are three digits for specifying with which foreign processors we divide the line, i.e. the maximum number of cores in the system is 8x4. And if we reserve 6 bits - 3 for processors of our node and 3 for other nodes, we get 4x4x8 cores. Now, when receiving data from a remote node, there is no need to make a broadcast, you can refer to a specific node, and then it will figure out on which processor and in the cache of which kernel the line we need is located.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">From the point of view of the coherence protocol, the MESI variants are mainly used, taking into account the fact that line splitting can occur at three levels - (core-processor-node). </font><font style="vertical-align: inherit;">It should be said that there are not so many players in the NUMA-systems market and they are leading a positional war in the patent minefield. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What can be said about the proposed method of local data on the background of ccNUMA? </font><font style="vertical-align: inherit;">Those factors that spoke in favor of ‚Äúmoving‚Äù the cache became even more significant.</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Since it so happens that the OS has transferred a thread to a non-native node, we cannot do anything about it, it is worth trying to minimize losses. </font><font style="vertical-align: inherit;">You can, of course, try to set affiniti for the thread ( </font></font><a href="https://msdn.microsoft.com/en-us/library/windows/desktop/ms686247(v%3Dvs.85).aspx"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">1</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> or </font></font><a href="http://man7.org/linux/man-pages/man2/sched_getaffinity.2.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), but this is more a hint to the scheduler.</font></font></li><li>          ‚Äî <a href="https://linux.die.net/man/2/migrate_pages">migrate_pages</a> , ,           . </li><li>   ,         ,     .  ‚Äú‚Äù   ,    (..  )   ,         ,     , ,  .     ‚Äú‚Äù   ,      . </li><li>       ,    ,      .    ,     ‚Äú ,   ‚Äù. </li></ul><br><h3>  findings </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here is an unexpected application of the now-forgotten idea of ‚Äã‚Äãtagged memory. </font><font style="vertical-align: inherit;">As it turned out, it can be useful in SMP &amp; NUMA systems. </font><font style="vertical-align: inherit;">For this, however, synchronous actions are required in three areas:</font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is the responsibility of the programmer to select a class of data. </font><font style="vertical-align: inherit;">True, if he does not do anything, everything will just remain as it is now.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Processor developers should ‚Äústretch‚Äù tags through the entire system. </font><font style="vertical-align: inherit;">Nothing is impossible in this - the developers of the current ‚ÄúElbrus‚Äù in the 00s, the Soviet ‚ÄúElbrus‚Äù in the 70s, the non-Soviet ‚ÄúBarrows‚Äù in the 60s could do it ... Plus, changes in the coherence support protocol.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The operating system should make some effort when forcing / restarting threads. </font><font style="vertical-align: inherit;">And ‚Äústretch‚Äù the tags through the system, which was also successfully succeeded.</font></font></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Synergy from all this will reduce the exponential level and delay the communication storm. </font><font style="vertical-align: inherit;">Perhaps the game is worth the candle.</font></font><br><br><div class="spoiler">  <b class="spoiler_title">Sources</b> <div class="spoiler_text"> [1] <a href="http://www.cs.virginia.edu/brochure/images/manuals/b5000/descrip/descrip.html"> B5000</a> <br><br> [2] Wiki <a href="https://en.wikipedia.org/wiki/Burroughs_large_systems">Burroughs_large_systems</a> <br><br> [3] <a href="http://www.smecc.org/The%2520Architecture%2520%2520of%2520the%2520Burroughs%2520B-5000.htm">The Architecture of the Burroughs B5000; Alastair JW Mayer</a> <br><br> [4] <a href="http://www.mcst.ru/files/511cea/886487/1a8f40/000000/book_elbrus.pdf">     ¬´¬ª</a> <br><br> [5] <a href="http//mcst.ru/doc/volk_090610.doc">¬´     ‚Äû‚Äú    ;  ..,  ..,  ..,  ..,  ..,  ..,  ..</a> <br><br> [6] <a href="http://mcst.ru/doc/SecureLanguagesImplementation-req_rus-2008.doc">         ;  ..</a> <br><br> [7] <a href="https://www.ideals.illinois.edu/bitstream/handle/2142/34589/Crago_Neal.pdf%3Fsequence%3D1">ENERGY-EFFICIENT LATENCY TOLERANCE FOR 1000-CORE DATA <br> PARALLEL PROCESSORS WITH DECOUPLED STRANDS; NEAL CLAYTON CRAGO</a> <br><br> [8] <a href="http://lumetta.web.engr.illinois.edu/papers/pact2009.pdf">A Task-centric Memory Model for Scalable Accelerator Architectures <br> John H. Kelm, Daniel R. Johnson, Steven S. Lumetta, Matthew I. Frank‚àó <br> , and Sanjay J. Patel</a> <br><br> [9] <a href="https://www.quora.com/In-MIPS-what-are-load-linked-and-store-conditional-instructions">In MIPS, what are load linked and store conditional instructions?</a> <br><br> [10] <a href="http://hps.ece.utexas.edu/people/suleman/class_projects/pca_report.pdf">An Evaluation of Snoop-Based Cache Coherence Protocols; <br> Linda Bigelow Veynu Narasiman Aater Suleman</a> <br><br> [11] <a href="http://www.ixbt.com/cpu/sandy-bridge-3.shtml"> Intel Sandy Bridge ‚Äî  </a> <br><br> [12] <a href="https://www.scss.tcd.ie/Jeremy.Jones/vivio/caches/MESIHelp.htm">Interactive Reversible E-Learning Animations: MESI Cache Coherency Protocol</a> <br><br> [13] <a href="http://www.mcst.ru/files/58ca52/820cd8/507039/000000/kozhin_a._s._neyman-zade_m._i._tihorskiy_v._v._vliyanie_podsistemy_pamyati_vosmiyadernogo.pdf">     ¬´‚Äë8C¬ª   ; .. , .. -, .. </a> <br><br> [14] <a href="http://www.mcst.ru/files/58ca4b/070cd8/501a2f/000001/kozhin_a._s._nedbaylo_yu._a._metody_optimizatsii_vremeni_dostupa_v_obshchiy_kesh_mnogoyadernogo.pdf">        ; .. , .. </a> <br><br> [15] <a href="http://www.princeton.edu/~adam/R1/r1rpt.html">A Brief History of the Rice Computer</a> <br><br> [16] <a href="http://tibrewala.net/papers/mesi98/">Optimizing the MESI Cache Coherence Protocol for Multithreaded Applications on Small Symmetric Multiprocessor Systems; Robert Slater &amp; Neal Tibrewala</a> <br><br> [17] <a href="http://ctho.org/toread/forclass/18-742/3/p273-archibald.pdf">Cache Coherence Protocols: Evaluation Using a Multiprocessor Simulation Model; JAMES ARCHIBALD and JEAN-LOUP BAER</a> <br><br> [18] <a href="http://mcst.ru/doc/130429/kojin.doc">      <br>  ¬´-4+¬ª; .. , .. , .. , .. </a> <br><br> [19] <a href="http://gigabaza.ru/doc/2817.html">         ¬´-4+¬ª;</a>  <a href="http://gigabaza.ru/doc/2817.html">V.N.</a> <a href="http://gigabaza.ru/doc/2817.html">, .. , ..., . .. </a> <br><br> [20] <a href="http://scc.ustc.edu.cn/zlsc/lxwycj/200910/W020100308600770617815.pdf">MIPS64 Architecture For Programmers Volume III: The MIPS64 Privileged Resource <br> Architecture</a> <br><br> [21] <a href="http://irl.cs.ucla.edu/~yingdi/web/paperreading/whymb.2010.06.07c.pdf">Memory Barriers: a Hardware View for Software Hackers; Paul E. McKenney</a> <br><br> [22] <a href="http://panchul.com/dropbox/2015_10_26/presentations/tlb_mmu_in_mips_microaptiv_up_2015_10_25.pdf">     MIPS</a> <br><br> [23] <a href="https://habrahabr.ru/company/intel/blog/173001/">      ¬´ ¬ª     ?</a> <br><br> [24] <a href="http://research.cs.wisc.edu/multifacet/papers/tr1798_region_coherence.pdf">CMP Directory Coherence: One Granularity Does Not Fit All; Arkaprava Basu, Bradford M. Beckmann* Mark D. Hill, Steven K. Reinhardt <br></a> <br><br> [25] <a href="http://shikardos.ru/text/osnovnie-tendencii-v-arhitekture-visokoproizvoditelenih-mnogoy/">      ; Mikhail Isaev.</a> <br><br> [26] <a href="http://www.hotchips.org/wp-content/uploads/hc_archives/hc21/3_tues/HC21.25.800.ServerSystemsII-Epub/HC21.25.829.Kalla-IBM-POWER7NextGenerationServerProcessorv7display.pdf">POWER7: IBM's Next Generation Server Processor</a> <br><br> [27] <a href="http://www-vlsi.stanford.edu/people/alum/pdf/9210_Simoni____Cache_Coherence_Directori.pdf">CACHE COHERENCE DIRECTORIES FOR SCALABLE MULTIPROCESSORS; Richard Simoni</a> <br><br> [28] <a href="https://pdos.csail.mit.edu/6.828/2016/lec/AMD.pdf">Pat Conway, Nathan Kalyanasundharam, Gregg Donley, Kevin Lepak, Bill Hughes. Cache Hierarchy and Memory Subsystem of the AMD Opteron Processor.</a> <br><br> [29] <a href="http://spcl.inf.ethz.ch/Teaching/2016-dphpc/lecture/lecture2-cache-coherence.pdf">Design of Parallel and High-Performance Computing: Torsten Hoefler &amp; Markus P√ºschel</a> <br><br> [30] <a href="http://www.hotchips.org/wp-content/uploads/hc_archives/hc21/2_mon/HC21.24.100.ServerSystemsI-Epub/HC21.24.122-Kottapalli-Intel-NHM-EX.pdf">Nehalem-EX CPU Architecture Sailesh Kottapalli, Jeff Baxter</a> <br><br> [31] <a href="https://habrahabr.ru/company/intel/blog/171079/">     NUMA-    Linux</a> <br><br> [32] <a href="https://habrahabr.ru/company/intel/blog/165903/">NUM, NUM    NUMA</a> <br><br> [33] <a href="http://rus-linux.net/lib.php%3Fname%3D/MyLDP/hard/memory/memory.html">What Every Programmer Should Know About Memory</a> <br><br> [34] <a href="http://www.cs.rochester.edu/u/cli/research/switch.pdf">Quantifying The Cost of Context Switch; Chuanpeng Li,Chen Ding, Kai Shen</a> <br><br></div></div></div><p>Source: <a href="https://habr.com/ru/post/328542/">https://habr.com/ru/post/328542/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../328532/index.html">Heisenbag 2017 Piter: Call for Testing</a></li>
<li><a href="../328534/index.html">Video shooting training with a mentor</a></li>
<li><a href="../328536/index.html">Voting started for YiiConf reports</a></li>
<li><a href="../328538/index.html">Ransomware day: Wana Decrypt0r mass infection</a></li>
<li><a href="../328540/index.html">Security Week 19: Windows Defender launches someone else's code, a Trojan was in HandBrake, phishers attacked Gmail users</a></li>
<li><a href="../328544/index.html">Introducing 3CX V15.5 Beta</a></li>
<li><a href="../328546/index.html">Telegram of your business</a></li>
<li><a href="../328548/index.html">Get to know WannaCry</a></li>
<li><a href="../328550/index.html">Organizing a large project on the Zend Framework 2/3</a></li>
<li><a href="../328552/index.html">Dirty game tricks</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>