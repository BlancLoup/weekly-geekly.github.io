<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Apache Hadoop (Report by Vladimir Klimontovich on ADD-2010)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We bring to your attention the report by Vladimir Klimontovich , made by him at the Application Developer Days conference, in which he shared his expe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Apache Hadoop (Report by Vladimir Klimontovich on ADD-2010)</h1><div class="post__text post__text-html js-mediator-article">  We bring to your attention the report by <a href="http://klimontovich.moikrug.ru/">Vladimir Klimontovich</a> , made by him at the <a href="http://addconf.ru/">Application Developer Days</a> conference, in which he shared his experience in processing VERY BIG volumes of data, and using NOSQL approaches, in particular, <a href="http://hadoop.apache.org/">Apache Hadoop</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ccc/f2f/5b5/cccf2f5b5db179a8c7bdcec89e66a151.png"><br><br>  Below are the text version of the report + video + audio and presentation slides.  Thank you <a href="http://habrahabr.ru/users/belonesox/" class="user_link">belonesox</a> for working on the preparation of the report materials. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <a name="habracut"></a><br>  <b>Background</b> <br><ul><li>  Why the problem of processing more data becomes more and more urgent (an example of the growth in the amount of data in different areas). </li><li>  An article from Google about the MapReduce paradigm.  Brief description of the paradigm. </li><li>  Brief description of adjacent areas (distributed file system, bigtable-like storage). </li><li>  History and brief description of the Apache Hadoop platform. </li></ul><br><br>  <b>Examples of use</b> . <br><ul><li>  Using the hadoop platform in three separate areas: last.fm (building charts), online-advertising'e (building statistics), Yahoo (building a search index). </li><li>  Description of the traditional approach (SQL database) and the approach using Hadoop for each of the above problems.  Pros and cons of the SQL / Hadoop approach </li><li>  The general principle of the translation of a certain subtype of SQL queries in MapReduce job. </li></ul><br><br>  <b>Platforms built on top of Hadoop</b> . <br><ul><li>  Brief description of ETL-framework Hive and Pig, built on the basis of Hadoop. </li><li>  Examples of use (for example, facebook.com and Yahoo);  comparison with standard SQL approach </li></ul><br><br>  <b>Problems with real-time data access when using Apache Hadoop</b> . <br><ul><li>  Descriptions of cases when real-time is needed, and when not. </li><li>  Description of the solution to simple problems with realtime: in-memory caching (memcached), symbiosis with SQL </li><li>  Symbiosis with a bigtable-like database using the HBase example.  Brief description of HBase. </li></ul><br><br>  <b>Hadoop as a trend</b> . <br><ul><li>  A brief overview of the technical and business problems encountered when using Hadoop </li><li>  The hype around the Hadoop and NoSQL approach.  Describe when SQL is convenient. </li></ul><br><br>  <b>Video</b> <br><iframe src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/11910267&amp;xid=17259,15700022,15700186,15700191,15700253,15700256,15700259&amp;usg=ALkJrhg9M-x6KsPt2TiXhM9Wv0HkY98gfg" width="560" height="315" frameborder="0" title="Apache Hadoop (Vladimir Klimontovich at ADD-2010)" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe><br>  In case of problems with the display, you can use the <a href="http://vimeo.com/11910267">link</a> . <br><br>  <b>Audio</b> <br>  The audio version of the report is available <a href="http://belonesox.podfm.ru/addconf/21/">here</a> . <br><br>  <b>Presentation of the report</b> <br>  Presentation of the report is available <a href="http://lib.custis.ru/images/1/1c/Apache_Hadoop_%2528%25D0%2592%25D0%25BB%25D0%25B0%25D0%25B4%25D0%25B8%25D0%25BC%25D0%25B8%25D1%2580_%25D0%259A%25D0%25BB%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BD%25D1%2582%25D0%25BE%25D0%25B2%25D0%25B8%25D1%2587_%25D0%25BD%25D0%25B0_ADD-2010%2529.pdf">here</a> . <br><br><h5>  Data volumes </h5><br>  So, what are you talking about, what volume are you talking about?  For example, the <b>Facebook</b> company, which everyone knows, probably all have profiles there, this social network <b>has</b> <b>40 terabytes of</b> data <b>per day</b> - photos, posts, comments, again, the log file is simple - page impressions and so on. <br><br>  What we have next - again, the <b>New York Stock Exchange</b> - <b>one terabyte of</b> transactions per day, data on transactions, purchases and sales of shares, and so on. <br><br>  <b>Large Hadron Collider</b> : this is about <b>forty terabytes of</b> experimental data per day, information about the speed and position of particles, and so on. <br><br>  And for example, so that you can imagine what is happening in small companies - <b>ContextWeb</b> is a small American company where I essentially work, which is engaged in online advertising overwriting, a completely small company with a very small percentage of the market, however it is <b>115 gigabytes of logs</b> display contextual advertising per day.  And it‚Äôs not just text files, it‚Äôs 115 gigabytes of compressed data, i.e.  in fact, data is probably much more. <br><br><h5>  DFS / MapReduce </h5><br>  The question arises, what actually to do with them?  Because it is necessary to process them somehow.  By themselves, such amounts of data are not interesting to anyone and are rather useless. <br><br>  One way to handle this amount of data was invented by Google.  In 2003, Google released a fairly well-known article about distributed file systems, how they store data and indices, user data, and so on. <br><br>  In 2004, Google again released an article that describes the paradigm of processing such a volume of data called MapReduce. <br><br>  Actually, this is exactly what I'm going to tell you now about how it works in general, and how it is implemented in the Apache Hadoop platform. <br><br><h5>  Distributed FS </h5><br>  Distributed Filesystem - what is it all about?  What tasks are assigned to Distributed Filesystem? <br><br><ul><li>  First, it is storing large amounts of data ‚Äî files of any size ... </li><li>  Secondly, this is just transparency, we want to work with this file system as with a regular file system - we want to open files, write something there, close files, and not think about the fact that this is something distrubuted and big. </li><li>  We also want ... another requirement is scalability.  We want to store files on a cluster and rather easily scale it.  For example, our business has grown twice, it has doubled the amount of data, I want to, without fixed changes in the architecture, store more data twice, simply adding twice as many machines. </li><li>  And also reliability.  Those.  we have a cluster, let's say from a hundred machines, out of operation, let's say five, it‚Äôs necessary that it goes unnoticed for us - that the files are accessible, that we can read-write, yes, perhaps with a slightly lower performance, but that everything works until these five cars are noticed. </li></ul><br><br><h5>  DFS: architecture </h5><br>  In fact, how it is implemented.  There is a small graph here, probably it is not visible, but in general it is visible, yes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/de4/23a/23e/de423a23e7ede5ccc9933f5d204f5163.png"><br><br>  There is a cluster of machines, of which there are many, on which data is stored.  There is one machine called master node that coordinates everything. <br><br>  What is stored on the master node?  The master node simply stores a file table.  The file system structure, each file is divided into blocks, on which cluster machines, which blocks of files are stored. <br><br>  How do you write and read?  We want to read a file, we ask the master node where the blocks of such a file are stored, it tells us which machine the particular blocks are stored on, and we already read directly from cluster machines. <br><br>  The same with the record, we ask the master node where we need to write, in which specific blocks, in which particular machines, he tells us, and we write directly there. <br><br>  And to ensure reliability, each unit is stored in several copies, on several machines.  This ensures reliability, even if we lose, say, 10% of the machines in the cluster, most likely, we will not lose anything.  Those.  Yes, we will lose some blocks, but since these blocks are stored in several copies, we can again read and write <br><br><h5>  Configuration </h5><br>  A typical configuration, which, for example, is used in our company.  To store large amounts of data, for example, in our company it is 70 terabytes, which we regularly analyze, we are going to do something with them, this is about forty machines, each machine is a weak server, if you look at the industry, this is somewhere 16 gigabytes of RAM or 8 gigabytes, a terabyte disk, no RAID, just a regular disk. <br><br>  Any Intel Xeon, in general, some cheap server.  There are forty such servers too, and this allows you to store such amounts of data, say, hundreds of terabytes. <br><br><h5>  MapReduce </h5><br>  After all the files are stored on the distributed file system, the question arises how to process them.  For this, Google has invented a paradigm called MapReduce.  She looks pretty weird.  This is data processing, in three operations. <br><br>  The first operation ..., we have some input data, for example, a set of some input records. <br><br>  The first operation, which is called Map, which for each input record gives us a pair of "key ‚Üí value".  After that, inside, these ‚Äúkey ‚Üí value‚Äù pairs are grouped; for each key, when we process all the input records there can be several values.  Grouped, and issued to the procedure Reduce, which receives the key, and accordingly, a set of values, and gives already, the final final result. <br><br>  Thus, we already have a set of some input records, for example, these are lines in the log file, and we get some kind of output record records. <br><br>  All this looks rather strange, like some kind of highly specialized thing, like something from functional programming, it is not immediately clear how this can be applied in general practice. <br><br><h6>  Example </h6><br>  In fact, it can be applied very well in wide practice. <br><br>  The simplest example.  Suppose we are a Facebook company, we have a lot of data, ... well, just logs show Facebook pages.  And we must calculate what browser is used by anyone. <br><br>  This is done quite easily using the MapReduce paradigm. <br><br>  We define the Map operation, which, by the line in the access loge, defines the key value, where the key is the browser, and the value is just one. <br><br>  After that, it remains to do the Reduce operation, which simply does summation over a set of browsers and a multitude of ones, and at the output gives out for each browser the resulting amount. <br><br>  We run this MapReduce task on a cluster, at the beginning we have many, many log files, at the end we have such a small file, in which we have a browser, and accordingly, the number of hits.  This way we will know the statistics. <br><br><h5>  Parallelism </h5><br>  Why is this good, MapReduce? <br><br>  Such programs, when we set the Map and set the Reduce, they are very well parallel. <br><br>  Suppose we have some Input, this is a large file, or file sets, this file can be divided into many, many small pieces, for example, by the number of machines in a cluster, or more.  Accordingly, on each piece we will run our Map function, this can be done in parallel, it all runs on the cluster, is calculated in a quiet way, the result of each map is sorted inside, and sent to Reduce. <br><br>  The same thing, when we have the result of some Map, a lot of some data, we can again break this data into pieces, again run on a cluster, on many machines. <br><br>  Thanks to this scalability is achieved.  When we need to process the data twice as fast, we will simply add twice as many machines, the hardware is now relatively cheap, i.e.  without any changes to the architecture, we get twice the performance. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/136/b51/ba9/136b51ba92a0260658312c6ee7ea6e62.png"><br><br><h4>  Apache hadoop </h4><br>  Apache Hadoop - what is it?  After Google published these articles, everyone decided that this was a very convenient paradigm, in particular, the Apache Hadoop project was born.  They simply decided that what is written in these articles, about distributed file systems and the MapReduce paradigm, be implemented as an open-source Java project. <br><br>  It started back in 2004, when people wanted to write an open search engine Nutch, then, somewhere in 2005, from it Apache Hadoop stood out as a separate project, as the implementation of the distributed file system and the MapReduce paradigm.  At first it was a small project, not very stable, somewhere in 2006, Yahoo began trying to use Hadoop in its projects, and in 2008-2009, Yahoo launched its search, or rather not search, but indexing, which was completely arranged on the Apache Hadoop platform, and now Yahoo is indexing the Internet using the Apache Hadoop platform.  The index is stored in the distributed file system, and the index itself is made as a series of Map-Reduce tasks. <br><br>  Yes, again, Hadoop recently won a data sorting competition, there is such a ‚Äú1TB sort contest‚Äù when some people get together and try to sort one terabyte of data faster.  A system based on Apache Hadoop that runs on a Yahoo cluster wins on a regular basis. <br><br><h5>  Hadoop Moduli </h5><br>  Hadoop consists of two modules.  This is an implementation of the distributed file system paradigm called HDFS, and MapReduce, i.e.  implementation of the MapReduce framework. <br><br><h6>  Yahoo: web graph </h6><br>  Here are some more examples of how, let's say, Yahoo uses Hadoop.  Yahoo needs, for example, to build a graph of the entire Internet.  As vertices, we will have pages, if from one page there is a link to another, it will be an edge in the graph, and this edge is marked with the text of the link. <br><br>  For example, as does Yahoo.  This is also from the Map-Reduce task series.  First, Yahoo downloads all the pages that they are interested in indexing, and stores them, again, in HDFS.  To build such a graph, a map-reduce task is run. <br><br>  So, Map, we just take a page, and look where it refers, and just gives this as a key, Target URL, i.e.  where the page, value ‚Üí SourceURL, i.e.  where we link and link text. <br><br>  Reduce just gets all these pairs, ... i.e.  it receives the key, it is the TargetURL and the value set, i.e.  the set of SourceURLs and texts does some sort of filtering, for we obviously have some spam links that we don‚Äôt want to index, and returns all of this in a table ‚Äî TargetURL, SourceURL and text. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f93/3e3/777/f933e37777b52e5300bf436948079253.png"><br><br>  Such a table is a graph of the entire Internet. <br><br><h6>  Last.fm </h6><br><img src="https://habrastorage.org/getpro/habr/post_images/6f8/4c6/b30/6f84c6b300ecbbdda7f8c010133a3da9.png"><br><br>  Again, Last.fm, probably many people use it, who don't use it - I'll explain a little.  This is such a service, you install a plugin for your iTunes or WinAmp, it sends what you listen to last.fm in real time, then Last.fm does two things - for example, it builds such beautiful charts i.e.  for the last seven days or three months, which groups have you listened to, what compositions have you had, and still does some last.fm ???  radio based on the statistics of the songs you listened to, they recommend something different to you, something new and interesting for you, something that supposedly will be interesting to you.  If someone noticed, these charts, they are not updated in real-time, once a day, which, I don't remember, in general, is rare. <br><br>  Actually, these charts, they are built again, on the Apache Hadoop platform.  When you listen to a song, a line is simply written to a log file, ‚Äúa user with such an identifier listened to such and such a composition of such and such a group‚Äù.  After that, the Map-Reduce task is launched once a day.  How she looks like? <br><ul><li>  Input is the same log file of these auditions. </li><li>  Map looks like - we take a line from this log file, its parsim, and as a key, we give the pair ‚Äúuser and group‚Äù, and the value is one. </li><li>  Appropriately, then it all falls on Reduce in the form of a ‚Äúuser and group‚Äù pair and with a value in the form of a set of units, and is written at the end of the entire file as ‚Äúuser-group and number of auditions‚Äù. </li></ul><br><br>  After that, when you go to your page, this file is parsed, there is a record relating to you, and this is the chart that was on the last slide. <br><br><h5>  SQL </h5><br>  In fact, a large number of SQL queries easily parallel in the form of ..., can be easily expressed in the form of Map-Reduce tasks.  For example, standard SQL, many people write, many use it like this, - a set of fields, f1, f2, sum, where, some condition and group by. <br><br>  Those.  This is a standard SQL query that is used in many places, for building reports and some statistics. <br><br>  So, such a request is easily parallelized as a map-reduce.  Instead of a table, let's say we have a text file, as data storage.  Instead of SQL engine, we have map-reduce.  Instead of query results, we have a text file. <br><br>  In fact, how it works. <br><br>  Map process.  As input, we have lines in the log file, as an output, we parse this line and output as a key, fields that interest us as a group by, and value, a field that we aggregate, in this case we count the amount of a. <br><br>  Reduce gets as a key these fields, as values, a set of fields that we aggregate, i.e.  some A1, .... An, and just do the summation. <br><br>  As a matter of fact, that's all, we defined such map-reduce procedures on a cluster and got the results for this query. <br><br><h5>  SQL: Principle </h5><br>  In fact, many SQL queries can be parallelized ..., can be expressed in terms of map-reduce job. <br><br>  If we have GROUP BY, the fields by which we make GROUP BY are defined as the key in the Map process. <br><br>  WHERE is just a filtering in the Map process. <br><br>  Again, we consider all sums, AVG, and other aggregation functions at the Reduce stage. <br><br>  It is very easy to implement the condition of HAVING, JOIN and so on. <br><br><h5>  SQL: partitioning </h5><br>  A bit about partitioning.  When we process data in this way, let's say, in the same last.fm, we build these statistics, we have data stored in a file, if we run map-reduce work every time on all the files that exist, it will be very long and wrong. <br><br>  Usually partitioning is used for data, for example, by date.  Those.  we store everything not in one single log file, but break it by the hour or by day.  Then, when we map our SQL query on MapReduce-jobs, we first limit the set of input data to ..., strictly speaking, by files.  Suppose if we are interested in the data for the last day, we only take the data for the last day, and only then we start map-reduce-job. <br><br><h5>  Apache heve </h5><br>  Strictly speaking, this principle is implemented in the <a href="http://hive.apache.org/">Apache Hive</a> project, this is such a framework built on the basis of Hadoop. <br><br>  How does all this look from the user's point of view?  We set some SQL query, define where we have data, after that, this framework expresses this SQL query as map-reduce tasks, in the form of one or a whole sequence, runs them ... everything looks quite transparent to the user. <br><br>  Those.  we defined a set of input data, defined a SQL query, and at the output we also received some kind of table. <br><br><h5>  Apache pig </h5><br>  The second framework, <a href="http://pig.apache.org/">Apache Pig</a> , is the same, approximately, solves the same problem, i.e.  transparent for the user creation of map-reduce job-s, without having to write any code. <br><br>  We set in such an ETL language, the sequence that we want, where we want to load something from, how we will filter this data, which columns we are interested in, and all of this is translated into map-reduce jobs. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/664/5e8/8b7/6645e88b742f8eb8139ff8dc9e5c56a8.png"><br><br><h5>  Areas of use </h5><br>  As a matter of fact, the fields of application, all this Hadoop and others. <br><br>  Hadoop is very well used for building statistical models, and in general for data analysis. <br><br>  If we have a lot of log files, we want to find some correlations, how the user behaves, depending on what, such tasks are very well solved with the help of Hadoop, respectively, building reports, again, the same Last.fm.  When we have a lot of data, and we need to build some reports, we don‚Äôt need them real-time, we are ready to update them once a day, or in a few hours, everything is also very convenient. <br><br><h5>  Virtues </h5><br><br>  The advantages of this approach, this platform.  Very good and smooth scalability.  Those.  if we need to process twice as much data, or store twice as much data, we just need to add twice as many machines to the cluster.  Those.  not exactly exactly twice, but almost twice. <br><br>  Zero cost software.  There you have a lot of data, you can go to Oracle, buy a clustered Oracle for a couple of million dollars, and as many consultants, this is not suitable for all companies, especially startup companies.  I do not know, well, any new social network, they just can not afford to spend a few million on Oracle.  They can afford Hadoop, take a cluster and use open-source Hadoop as a data analysis and storage system. <br><br>  Another Hadoop is convenient for research tasks.  For example, you are researcher, and you want to investigate the correlation of user behavior with anything on Facebook, anywhere else on your social network, you have a lot of files, hadoop is available, as an on-demand service on Amazon. <br><br>  Those.  you wrote something at yourself, locally debugged, say, OK, now I need a cluster of one hundred machines for two hours, Amazon immediately represents a cluster of one hundred machines, you run your task there, get some results, and everything is would. <br><br>  A hundred cars for an hour at Amazon are relatively cheap, cheaper than storing a cluster at home.  For research, this is quite convenient, in the sense that you need all this once a week, you do not need to keep a cluster, you can order it from Amazon. <br><br>  <i>From the audience: How much is the order of numbers?</i> <br><br>  For an hour ... well, that's about a hundred dollars.  I honestly do not remember the price of an Amazon, but it is quite cheap, it is quite affordable. <br><br>  <i>From the audience: Fifty cents per hour ...</i> <br><br>  Yes, but for Hadoop, we need more instances, and perhaps a bigger cluster, well, yes, in general, hundreds of dollars.  This is such an order, it is clear that if a big task, then it‚Äôs not an hour, but ten hours, but still, it‚Äôs about hundreds of dollars, i.e.  about something not very big. <br><br><h5>  disadvantages </h5><br>  And what are the flaws in Hadoop? <br><br>  First, it is a rather high cost of support.  If you have a Hadoop cluster, from many machines, you need to find a smart system administrator who will understand the Hadoop architecture, how it works, and will support all this.  Those.  it really is not easy, it really takes a lot of time. <br><br>  This, unlike any industrial and expensive storage, the cost of new data processing is quite high.  Those.  if you buy some kind of Oracle, or something similar in this style, it‚Äôs enough for you in principle to hire some business analysts who will write just SQL queries and get some results.  In this case, this will not work with Hadoop, you will need people who will invent the business part, what kind of data they need, and you will need a team of Java developers who will write these map-reduce jobs. <br><br>  The team is not very big, but nevertheless, it still costs money, the developers are quite expensive. <br><br>  And again, the problem with real-time.  Hadoop is not a real-time system.  If you want to receive any data, you will not be able to run map-reduce jobs when the user visits the site.  You need to update the data, at least once an hour, in the background, and the user to show the already calculated data. <br><br><h6>  Real-Time? </h6><br>  With real-tim, the problem is relatively solvable.  For example, how this is solved in our company.  We do not need to provide real-time access to the entire amount of data that we have, we run map-reduce jobs, get some rather valuable, but reasonable-sized results that we store in the SQL database, in MemCache, in memory, this however, will not work when you have a lot of this data. <br><br>  So now ... is there any time left? <br><br>  Ten minutes left, so I will talk about column-oriented databases, also an approach to storing large amounts of data that need realtime access. <br><br><h5>  Column oriented databases </h5><br>  How does it work?  What are some problems with SQL?  In SQL, in MySQL, you cannot store volumes, say, in several terabytes, such a table simply will not work, you cannot receive data from it. <br><br>  Other problems with SQL, if you change the schema, say, some ALTER TABLE, it is long and problematic to add some columns on a large table.  This is quite problematic, and when you don‚Äôt need it, when you don‚Äôt need to store structured data, when you remove restrictions ... you don‚Äôt use SQL capabilities, like structured data storages, you don‚Äôt use relationality, you can store data in a slightly more efficient structure, and for that get great performance. <br><br>  This is just a slightly different approach called the column-oriented database. <br><br><h5>  Bigtable </h5><br>  It was submitted by Google, and they still use it, if my memory serves me, this is the article ‚ÄúBigTable‚Äù, which was published in 2004. <br><br>  Actually, what is BigTable? <br><br>  It is built on several principles. <br><br>  The first principle is that we abandon relationality, indexation by fields, in our table there is exactly one field by which you can perform a search, what is called rowkey, the analog is the primary key in the table. <br><br>  We do not index all other fields, do not look for them, do not structure them, and the second principle is that the table is wide, i.e.  we can add columns at any time, with any type of data, should be cheap and good. <br><br><h6>  BigTable example </h6><br>  Let me give you an example when it is used.  We want to store data about users of our site.  I went to the site, did some actions, in the case of Google - made some requests, looked at something, looked at some advertisements, this is an anonymous user ... and we want to remember what he did.  How is this problem solved?  Many people store information about users in Cookies, what he did, what pages he looked at, what advertisements he looked at, what he clicked on.  There is a big problem with this approach - the size of the cookie is very limited, it‚Äôs impossible to write there, let's say, the history of the user‚Äôs actions for the last month, it doesn‚Äôt work, there‚Äôs no space.  How can this problem be solved with BigTable? <br><br>  If we have a repository such as BigTable, we can store a single parameter in the cookie ‚Äî a unique user ID.  In BigTable, we will store the UserUID as the main key in the table, and many-many fields that interest us, for example, history of visits, clicks on advertising, clicks on links, and so on. <br><br>  What is good?  The fact that we obviously do not need to look for anything in the other fields.  If we want to know some information about the user, in order to show him the appropriate advertising, we need to search only for information on the UserID. <br><br>  And it‚Äôs also good that since business can change, we can add many different fields, and it will be cheap and good.  Actually for this it is very good to use BigTable, userID, as rowkey, and all other data, as the table. <br><br><h6>  BigTable: design </h6><br>  How does all this work?  ,   BigTable,        .  Those.    rowkey,      rowkey, ,   ,      ,         ,    rang-     .  Those.     ¬´     -¬ª?  -  ,  range-    ,   ,  -,     ,      ,    . <br><br>     ‚Äî      ‚Üí      ,   ,    ,  ,    ,  ,        . <br><br><h5> HBase </h5><br>   BigTable,    Apache Hadoop,  ,  <a href="http://hbase.apache.org/">HBase</a> . <br><br>    Hadoop Distributed File System,    ,     Hadoop-,   , - map-reduce job,   - ,    , ..  Reduce      .  Reduce  ,         ,  HBase,   ,    Hadoop,    . <br><br><h6> HBase:  </h6><br> ,  . ,      ,  ‚Äî  16  8   ,   , 10 RPM,    -  Intel Xeon,     . <br><br>     ,   ,    3-5 ‚Ä¶      300        ,    - 18 ,  ‚Äî‚Äî , - 10 .    - MySQL  .  <a href="http://hbase.apache.org/">HBase</a>  . <br><br><h6> HBase:  </h6><br>    HBase,  ,   BigTable ? <b></b> , .. .       ,     ,    join-,      WHERE,  ,    ,    ,        . <br><br>     ,   <a href="http://hbase.apache.org/">HBase</a> ,   ,    ,      ,    ,   ,   ,   . <br><br><h5> Hadoop:   </h5><br>      ,  , map-reduce  ,  research,   ,    ,   HBase  BigTable‚Ä¶, ,      HBase-,    ,  ,  ,  -    ,  , ,   , , , ,  . <br><br> HBase     ,       ,      ,   ,     .      ,      ,   .‚Ä¶ ‚Ä¶ ( ). <br><br><h5>  Questions </h5><br>     , ‚Äî    ,    (. )    <a href="http://lib.custis.ru/Apache_Hadoop_%2528%25D0%2592%25D0%25BB%25D0%25B0%25D0%25B4%25D0%25B8%25D0%25BC%25D0%25B8%25D1%2580_%25D0%259A%25D0%25BB%25D0%25B8%25D0%25BC%25D0%25BE%25D0%25BD%25D1%2582%25D0%25BE%25D0%25B2%25D0%25B8%25D1%2587_%25D0%25BD%25D0%25B0_ADD-2010%2529"></a> . <br><br></div><p>Source: <a href="https://habr.com/ru/post/117359/">https://habr.com/ru/post/117359/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../117353/index.html">Pong Screensaver in Ubuntu Clock Mode</a></li>
<li><a href="../117354/index.html">Video broadcast of the conference "The State in the XXI Century"</a></li>
<li><a href="../117355/index.html">A new selection of indie games "The Humble Frozenbyte Bundle"</a></li>
<li><a href="../117356/index.html">We decorate the horizontal menu of sites on WordPress</a></li>
<li><a href="../117357/index.html">The App Store will appear in Windows 8</a></li>
<li><a href="../117360/index.html">Google City Hash Family</a></li>
<li><a href="../117361/index.html">Clodo API Open</a></li>
<li><a href="../117362/index.html">Game boy music</a></li>
<li><a href="../117364/index.html">Alternative build environment for the N900</a></li>
<li><a href="../117365/index.html">These countless paradigms, concepts, tools and frameworks</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>