<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Search under the hood Chapter 1. Network spider</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The ability to search for information on the Internet is vital. When we click on the ‚Äúsearch‚Äù button in our favorite search engine, we receive an answ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Search under the hood Chapter 1. Network spider</h1><div class="post__text post__text-html js-mediator-article"><p>  The ability to search for information on the Internet is vital.  When we click on the ‚Äúsearch‚Äù button in our favorite search engine, we receive an answer in a split second. </p><br><p>  Most do not think at all about what is happening ‚Äúunder the hood‚Äù, and meanwhile the search engine is not only a useful tool, but also a complex technological product.  Modern search engine for its work uses almost all the advanced achievements of the computer industry: big data, graph and network theory, analysis of natural language texts, machine learning, personalization and ranking.  Understanding how the search engine works gives an idea of ‚Äã‚Äãthe level of technology development, and therefore any engineer will be able to understand this. </p><br><p><img src="https://habrastorage.org/webt/b4/ol/em/b4olemfslwldnbs_pzjit92teka.png"></p><br><p>  In several articles, I will talk step by step about how the search engine works, and, in addition, for the purposes of illustration, I will build my own small search engine so as not to be unfounded.  This search engine will, of course, be ‚Äúeducational‚Äù, with a very strong simplification of what is happening inside Google or Yandex, but, on the other hand, I will not simplify it too much. </p><br><p>  The first step is to collect data (or, as it is also called, crawling). </p><a name="habracut"></a><br><h2 id="veb--eto-graf">  Web is a graph </h2><br><p>  That part of the Internet that we are interested in consists of web pages.  In order for the search engine to be able to find a particular web page at the user's request, it must know in advance that such a page exists and contains information relevant to the request.  The user usually finds out about the existence of a web page from a search engine.  How does the search engine itself know about the existence of a web page?  After all, no one is obliged to tell her explicitly about this. </p><br><p>  Fortunately, web pages do not exist by themselves, they contain links to each other.  The search robot can follow these links and discover all new web pages. </p><br><p>  In fact, the structure of the pages and the links between them describes the data structure called "graph".  A graph, by definition, consists of vertices (web pages in our case) and edges (links between vertices, in our case, hyperlinks). </p><br><p>  Other examples of graphs are social network (people - peaks, edges - friendship), a road map (cities - peaks, edges - roads between cities) and even all possible combinations in chess (a chess combination - top, edge between peaks exists, if one position to another can move in one move). </p><br><p>  Graphs are oriented and unoriented - depending on whether the direction is indicated on the edges.  The Internet is a directed graph, since it is possible to go only in one direction by hyperlinks. </p><br><p>  For further description, we will assume that the Internet is a strongly connected graph, that is, starting at any point on the Internet, you can get to any other point.  This assumption is obviously wrong (I can easily create a new web page that will not be available from anywhere and therefore cannot be reached), but for the task of designing a search engine it can be accepted: as a rule, web pages that are not links are not of great interest to search. </p><br><p>  A small part of the web graph: </p><br><p><img src="https://habrastorage.org/webt/8e/hq/xx/8ehqxxndctdff-ejozlhk9hhbai.jpeg"></p><br><h2 id="algoritmy-obhoda-grafa-poisk-v-shirinu-i-v-glubinu">  Graph traversal algorithms: search in width and depth </h2><br><h3 id="poisk-v-glubinu">  Depth search </h3><br><p>  There are two classic graph traversal algorithms.  The first one, simple and powerful, is called Depth-first Search (DFS).  It is based on recursion, and it represents the following sequence of actions: </p><br><ol><li>  Mark the current vertex processed. </li><li>  We process the current vertex (in the case of a search robot, processing will simply save a copy). </li><li>  For all vertices to which you can go from the current one: if the vertex has not yet been processed, we recursively process it too. </li></ol><br><p>  Python code that implements this approach literally: </p><br><pre><code class="python hljs">seen_links = set() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dfs</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(url)</span></span></span><span class="hljs-function">:</span></span> seen_links.add(url) print(<span class="hljs-string"><span class="hljs-string">'processing url '</span></span> + url) html = get(url) save_html(url, html) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> link <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> get_filtered_links(url, html): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> link <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> seen_links: dfs(link) dfs(START_URL)</code> </pre> <br><p>  <a href="https://gist.github.com/anonymous/c67a51d7b42f266bb8457a128ace62db">Full github code</a> </p><br><p>  For example, the standard Linux utility wget works in the same way with the -r parameter, which indicates that the site needs to be pumped out recursively: </p><br><pre> <code class="bash hljs">wget -r habrahabr.ru</code> </pre><br><p>  It is advisable to use the depth-first search method to bypass web pages on a small site, however, it is not very convenient to crawl the entire Internet: </p><br><ol><li>  The recursive call it contains does not parallel well. </li><li>  With this implementation, the algorithm will get deeper and deeper through the links, and in the end it will most likely not have enough space in the recursive call stack and we will get the error stack overflow. </li></ol><br><p>  In general, both of these problems can be solved, but we will use another classical algorithm instead - a search in width. </p><br><h3 id="poisk-v-shirinu">  Search wide </h3><br><p>  <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25BE%25D0%25B8%25D1%2581%25D0%25BA_%25D0%25B2_%25D1%2588%25D0%25B8%25D1%2580%25D0%25B8%25D0%25BD%25D1%2583">A wide search</a> (breadth-first search, BFS) works in a manner similar to depth-first search, but it goes around the top of the graph in order of distance from the start page.  For this, the algorithm uses the ‚Äúqueue‚Äù data structure ‚Äî you can add elements to the end of the queue and pick it up from the beginning. </p><br><ol><li>  The algorithm can be described as follows: </li><li>  We add to the queue the first vertex and to the set of "seen". </li><li>  If the queue is not empty, retrieve the next vertex from the queue for processing. </li><li>  We process top. </li><li>  For all edges emanating from the vertex being processed that are not included in the "seen": <br><ol><li>  Add to "seen"; </li><li>  Add to queue. </li></ol></li><li>  Go to step 2. </li></ol><br><p>  Python code: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">bfs</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(start_url)</span></span></span><span class="hljs-function">:</span></span> queue = Queue() queue.put(start_url) seen_links = {start_url} <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> (queue.empty()): url = queue.get() print(<span class="hljs-string"><span class="hljs-string">'processing url '</span></span> + url) html = get(url) save_html(url, html) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> link <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> get_filtered_links(url, html): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> link <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> seen_links: queue.put(link) seen_links.add(link) bfs(START_URL)</code> </pre> <br><p>  <a href="https://gist.github.com/anonymous/100304ca217363c4a8e5e5b2918f48fb">Full github code</a> </p><br><p><img src="https://habrastorage.org/webt/fg/bp/5f/fgbp5fvkvnfrosujor1mnakvzs4.gif"></p><br><p>  It is clear that in the queue there will first be vertices located at a distance of one link from the initial one, then two links, then three links, etc., that is, the search algorithm will always <strong>reach the vertex by the shortest path.</strong> </p><br><p>  Another important point: the queue and the set of ‚Äúseen‚Äù vertices in this case use only simple interfaces (add, take, check for entries) and can easily be transferred to a separate server that communicates with the client through these interfaces.  This feature allows us to implement a <strong>multi-threaded graph traversal</strong> - we can run several simultaneous handlers using the same queue. </p><br><h3 id="robotstxt">  Robots.txt </h3><br><p>  Before describing the actual implementation, I would like to note that a well-behaved crawler takes into account the restrictions imposed by the owner of the website in the robots.txt file.  Here, for example, the contents of robots.txt for the site lenta.ru: </p><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">User</span></span>-agent: YandexBot Allow: /rss/yandexfull/turbo <span class="hljs-keyword"><span class="hljs-keyword">User</span></span>-agent: Yandex Disallow: /<span class="hljs-keyword"><span class="hljs-keyword">search</span></span> Disallow: /check_ed Disallow: /auth Disallow: /my Host: https://lenta.ru <span class="hljs-keyword"><span class="hljs-keyword">User</span></span>-agent: GoogleBot Disallow: /<span class="hljs-keyword"><span class="hljs-keyword">search</span></span> Disallow: /check_ed Disallow: /auth Disallow: /my <span class="hljs-keyword"><span class="hljs-keyword">User</span></span>-agent: * Disallow: /<span class="hljs-keyword"><span class="hljs-keyword">search</span></span> Disallow: /check_ed Disallow: /auth Disallow: /my Sitemap: https://lenta.ru/sitemap.xml.gz</code> </pre> <br><p>  It can be seen that there are certain sections of the site that are forbidden to visit the robots of Yandex, Google and everyone else.  In order to take into account the contents of robots.txt in python, you can use the filter implementation included in the standard library: </p><br><pre> <code class="python hljs">In [<span class="hljs-number"><span class="hljs-number">1</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> urllib.robotparser <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RobotFileParser ...: rp = RobotFileParser() ...: rp.set_url(<span class="hljs-string"><span class="hljs-string">'https://lenta.ru/robots.txt'</span></span>) ...: rp.read() ...: In [<span class="hljs-number"><span class="hljs-number">3</span></span>]: rp.can_fetch(<span class="hljs-string"><span class="hljs-string">'*'</span></span>, <span class="hljs-string"><span class="hljs-string">'https://lenta.ru/news/2017/12/17/vivalarevolucion/'</span></span>) Out[<span class="hljs-number"><span class="hljs-number">3</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> In [<span class="hljs-number"><span class="hljs-number">4</span></span>]: rp.can_fetch(<span class="hljs-string"><span class="hljs-string">'*'</span></span>, <span class="hljs-string"><span class="hljs-string">'https://lenta.ru/search?query=big%20data#size=10|sort=2|domain=1 ...: |modified,format=yyyy-MM-dd'</span></span>) Out[<span class="hljs-number"><span class="hljs-number">4</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">False</span></span></code> </pre> <br><h2 id="implementaciya">  Implementation </h2><br><p>  So, we want to bypass the Internet and save it for further processing. </p><br><p>  Of course, for demonstration purposes, it will not be possible to bypass and save the entire Internet - it would be VERY expensive, but we will develop the code with an eye to the fact that it could potentially be scaled to the size of the entire Internet. </p><br><p>  This means that we must work on a large number of servers at the same time and save the result to some kind of storage from which it can be easily processed. <br>  I chose <a href="https://aws.amazon.com/ru/">Amazon Web Services</a> as the basis for my solution, because there you can easily pick up a certain number of machines, process the result and save it to the distributed storage of <a href="https://aws.amazon.com/ru/s3/">Amazon S3</a> .  Similar solutions have, for example, <a href="https://cloud.google.com/">google</a> , <a href="https://azure.microsoft.com/en-us/">microsoft</a> and <a href="https://cloud.yandex.ru/">Yandex</a> . </p><br><p><img src="https://habrastorage.org/webt/1x/lf/tb/1xlftbg824x2hb96hbbgwn0akkg.jpeg"></p><br><h3 id="arhitektura-razrabotannogo-resheniya">  Architecture of the developed solution </h3><br><p>  The central element in my data collection scheme is the queue server, which holds the queue URLs to be downloaded and processed, as well as many URLs that our handlers have already ‚Äúseen‚Äù.  In my implementation, they are based on the simplest data structures Queue and set of the python language. </p><br><p>  In a real production system, most likely, instead of them, it would be worthwhile to use some existing solution for queues (for example, <a href="http://kafka.apache.org/">kafka</a> ) and for distributed storage of sets (for example, solutions of the in-memory key-value class of <a href="http://aerospike.com/">aerospike</a> databases would be <a href="http://aerospike.com/">appropriate</a> ).  This would allow to achieve full horizontal scalability, but in general the load on the queue server is not very large, so there is no sense in such a scale in my small demo project. </p><br><p>  Working servers periodically pick up a new group of URLs for downloading (we take away immediately a lot so as not to create an extra load on the queue), download the web page, save it on s3 and add new found URLs to the download queue. </p><br><p>  In order to reduce the load on adding URLs, adding is also done in groups (I immediately add all new URLs found on the web page).  I also periodically synchronize many ‚Äúseen‚Äù URLs with working servers in order to pre-filter already added pages on the side of the working node. </p><br><p>  I save downloaded web pages to a distributed cloud storage (S3) - it will be convenient later for distributed processing. </p><br><p>  The queue periodically sends statistics on the number of added and processed requests to the statistics server.  Statistics are sent in total and separately for each work node - this is necessary in order to make it clear that the download is in normal mode.  It is impossible to read the logs of each individual machine, so we will monitor the behavior on the charts.  As a solution for monitoring downloads, I chose <a href="https://graphiteapp.org/">graphite</a> . </p><br><h2 id="zapusk-kraulera">  Crawler launch </h2><br><p>  As I already wrote, in order to download the entire Internet, you need a huge amount of resources, so I limited myself to only a small part of it - namely, the sites habrahabr.ru and geektimes.ru.  However, the restriction is rather conditional, and its extension to other sites is simply a matter of the amount of available iron.  To launch, I implemented simple scripts that raise a new cluster in the amazon cloud, copy the source code there and start the corresponding service: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#deploy_queue.py from deploy import * def main(): master_node = run_master_node() deploy_code(master_node) configure_python(master_node) setup_graphite(master_node) start_urlqueue(master_node) if __name__ == main(): main()</span></span></code> </pre> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#deploy_workers.py #run as: http://&lt;queue_ip&gt;:88889 from deploy import * def main(): master_node = run_master_node() deploy_code(master_node) configure_python(master_node) setup_graphite(master_node) start_urlqueue(master_node) if __name__ == main(): main()</span></span></code> </pre> <br><p>  <a href="https://github.com/asash/minisearch/blob/master/crawler/deploy.py">The code of the</a> deploy.py <a href="https://github.com/asash/minisearch/blob/master/crawler/deploy.py">script</a> containing all called functions </p><br><p>  Using graphite statistics as a tool allows you to draw beautiful graphs: <br><img src="https://habrastorage.org/webt/nu/uj/1h/nuuj1hl--p7j1dchqm3dkkuwbv4.png"></p><br><p>  Red graph - found URLs, green - downloaded, blue - URL in the queue.  All time downloaded 5.5 million pages. </p><br><p><img src="https://habrastorage.org/webt/v-/es/nx/v-esnxzoxojrdd-i_dx88xmsrcu.png"><br>  The number of pages scored per minute, broken down by working nodes.  Charts are not interrupted, cracking is in normal mode. </p><br><h2 id="rezultaty">  results </h2><br><p>  Downloading habrahabr and geektimes took me three days. <br>  It would be possible to download much faster, increasing the number of copies of workers on each working machine, as well as increasing the number of workers, but then the load on the Habr itself would be very large - why create problems for your favorite site? <br>  In the process, I added a couple of filters to the crawler, starting to filter some obviously garbage pages that are irrelevant to the development of the search engine. </p><br><p>  The developed crawler, although it is a demonstration, is generally scalable and can be used to collect large amounts of data from a large number of sites at the same time (although it is possible that production should focus on existing crawling solutions, such as <a href="https://github.com/internetarchive/heritrix3">heritrix</a> . Real production crawler also should be run periodically, not once, and implement a lot of additional functionality, which I have so far neglected. </p><br><p>  During the crawler, I spent about $ 60 on the amazon cloud.  Total downloaded 5.5 million pages, a total of 668 gigabytes. </p><br><p>  In the next article of the cycle, I will build an index on the downloaded web pages using big data technology and design the simplest search engine for the downloaded pages. </p><br><p>  <a href="https://github.com/asash/minisearch">Project code is available on github</a> </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Qq3Qh426R5k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/345672/">https://habr.com/ru/post/345672/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../345662/index.html">How to replace HR-a with a robot?</a></li>
<li><a href="../345664/index.html">Data mining in R</a></li>
<li><a href="../345666/index.html">My first job</a></li>
<li><a href="../345668/index.html">A bit about IT in New Zealand</a></li>
<li><a href="../345670/index.html">JetBrains CLion for microcontrollers</a></li>
<li><a href="../345674/index.html">Product Design Digest, December 2017</a></li>
<li><a href="../345688/index.html">How to capture / protect an open-source project</a></li>
<li><a href="../345690/index.html">Harmful spells in programming</a></li>
<li><a href="../345692/index.html">Installing 1C Fresh from scratch using Linux and PostgreSQL</a></li>
<li><a href="../345694/index.html">My experience is to get into IT in comparison with a tambovka taxi driver</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>