<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Analysis of the statistical language model from Google - part 1: vector representation of characters</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This year, Google Brain researchers published an article called Exploring the Limits of Language Modeling , which described a language model that sign...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Analysis of the statistical language model from Google - part 1: vector representation of characters</h1><div class="post__text post__text-html js-mediator-article">  This year, Google Brain researchers published an article called <a href="http://arxiv.org/abs/1602.02410">Exploring the Limits of Language Modeling</a> , which described a language model that significantly reduced perplexion (from about 50 to 30) in the <a href="">One Billion Word Benchmark</a> dictionary. <br><br>  In this post we will tell about the lowest level of this model - the representation of characters. <br><br><img src="https://habrastorage.org/files/b69/453/00a/b6945300aef84a09ab2363823e8c3d1c.png"><br><a name="habracut"></a><br><h2>  <font color="#c75733">Introduction: Language Models</font> </h2><br>  To begin with, we define the very concept of a <b>language model</b> .  The language model is a probability distribution on a set of vocabulary sequences.  For a sentence like ‚ÄúHello world‚Äù or ‚ÄúBuffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo‚Äù, the language model gives us the likelihood that we will meet this sentence. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The quality model of the language model is <a href="https://en.wikipedia.org/wiki/Perplexity">perplexity</a> ‚Äî a measure of how well a model predicts the details of a test collection (the less perplexion, the better the model). <br><br>  The <code>lm_1b</code> language model takes one word from a sentence and calculates the probability distribution of the next word.  Thus, it can calculate the probability of a sentence such as ‚ÄúHello, world‚Äù, as follows: <br><br><pre> <code class="python hljs">P(<span class="hljs-string"><span class="hljs-string">"&lt;S&gt; Hello world . &lt;/S&gt;"</span></span>) = product(P(<span class="hljs-string"><span class="hljs-string">"&lt;S&gt;"</span></span>), P(<span class="hljs-string"><span class="hljs-string">"Hello"</span></span> | <span class="hljs-string"><span class="hljs-string">"&lt;S&gt;"</span></span>), P(<span class="hljs-string"><span class="hljs-string">"world"</span></span> | <span class="hljs-string"><span class="hljs-string">"&lt;S&gt; Hello"</span></span>), P(<span class="hljs-string"><span class="hljs-string">"."</span></span> | <span class="hljs-string"><span class="hljs-string">"&lt;S&gt; Hello world"</span></span>), P(<span class="hljs-string"><span class="hljs-string">"&lt;/S&gt;"</span></span> | <span class="hljs-string"><span class="hljs-string">"&lt;S&gt; Hello world ."</span></span>))</code> </pre> <br>  ("&lt;S&gt;" and "&lt;/ S&gt;" denote the beginning and end of a sentence). <br><br><h2>  <font color="#c75733">Lm_1b architecture</font> </h2><br>  <code>lm_1b</code> consists of three main components (see figure): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/af6/562/049/af6562049fa346e8bd0f70b7b29c7a99.png"></div><br><ol><li>  A CNN char (blue rectangle) receives the characters that make up a word as input, displays a word vector representation (word embedding). <br><br></li><li>  <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a> (Long short-term memory) (yellow) gets a representation of the word, as well as a state vector (for example, words that have already been encountered in this sentence) and computes the representation of the next word. <br><br></li><li>  The last layer, softmax (green), taking into account the information received from the LSTM, calculates the distribution for all vocabulary words. </li></ol><br><h2>  <font color="#c75733">Char CNN</font> </h2><br>  This is the abbreviation of the character-level convolutional neural network.  If you do not know what it is, forget what I just said, since in this post we will focus on what happens when the network starts performing any convolutions, namely, on <b>character embedding</b> . <br><br><h2>  <font color="#c75733">Character Embedding</font> </h2><br>  The most obvious way to represent a symbol as an input value for our neural network is direct coding ( <a href="https://en.wikipedia.org/wiki/One-hot">one-hot encoding</a> ).  For example, an alphabet consisting of lowercase Latin letters would be represented as follows: <br><br><pre> <code class="python hljs">onehot(<span class="hljs-string"><span class="hljs-string">'a'</span></span>) = [<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>] onehot(<span class="hljs-string"><span class="hljs-string">'c'</span></span>) = [<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>]</code> </pre> <br>  And so on.  Instead, we learn the ‚Äúdense‚Äù representation of each character.  If you have already used a vector representation of words like <a href="https://en.wikipedia.org/wiki/Word2vec">word2vec</a> , then this method will seem familiar to you. <br><br>  The first CNN Char layer is responsible for translating the raw characters of the input word to a vector representation, which is passed to the input to convolution filters. <br><br>  In <code>lm_1b</code> alphabet has a dimension of 256 (non-ASCII characters are expanded to several bytes, each of which is encoded separately) and is mapped to a dimension space of 16. For example, the character 'a' is represented by the following vector: <br><br><pre> <code class="python hljs">array([ <span class="hljs-number"><span class="hljs-number">1.10141766</span></span>, <span class="hljs-number"><span class="hljs-number">-0.67602301</span></span>, <span class="hljs-number"><span class="hljs-number">0.69620615</span></span>, <span class="hljs-number"><span class="hljs-number">1.96468627</span></span>, <span class="hljs-number"><span class="hljs-number">0.84881932</span></span>, <span class="hljs-number"><span class="hljs-number">0.88931531</span></span>, <span class="hljs-number"><span class="hljs-number">-1.02173674</span></span>, <span class="hljs-number"><span class="hljs-number">0.72357982</span></span>, <span class="hljs-number"><span class="hljs-number">-0.56537604</span></span>, <span class="hljs-number"><span class="hljs-number">0.09024946</span></span>, <span class="hljs-number"><span class="hljs-number">-1.30529296</span></span>, <span class="hljs-number"><span class="hljs-number">-0.76146501</span></span>, <span class="hljs-number"><span class="hljs-number">-0.30620322</span></span>, <span class="hljs-number"><span class="hljs-number">0.54770935</span></span>, <span class="hljs-number"><span class="hljs-number">-0.74167275</span></span>, <span class="hljs-number"><span class="hljs-number">1.02123129</span></span>], dtype=float32)</code> </pre> <br>  It is quite difficult to comprehend.  Let's reduce the dimensionality of the representation of characters to two using the <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a> algorithm to represent where they will be located relative to each other.  t-SNE will position our view so that the pairs with the smallest distance in the 16-dimensional vector space will also be closer to each other in a two-dimensional projection. <br><br><img src="https://habrastorage.org/files/b69/453/00a/b6945300aef84a09ab2363823e8c3d1c.png"><br><br>  t-SNE representation of the most frequency symbols.  Pink marks correspond to special meta-characters.  &lt;S&gt; and &lt;/ S&gt; mark the beginning and end of a sentence.  &lt;W&gt; and &lt;/ W&gt; mark the beginning and end of a word.  &lt;PAD&gt; uses 50-character word length restrictions.  Yellow marks are punctuation marks, blue marks are numbers, and light and dark green marks are upper and lower case letter symbols. <br><br>  Here are some interesting patterns: <br><br><ul><li>  The numbers are not only grouped together, but they are arranged in order and ‚Äúsnake‚Äù. </li><li>  In most cases, the same upper and lower case letters are located side by side, but some (for example, k / K) are significantly distant from each other. </li><li>  In the upper right corner there are punctuation marks that can complete a sentence ( <code>.?!</code> ). </li><li>  Metacharacters (pink) form a so-called loose cluster, and the other special characters make it even more loose (with '%' and ')' as drop-down values). </li></ul><br>  Also worth noting is the <i>lack of pattern</i> .  In addition to not too regular matching pairs of lowercase / uppercase, otherwise the arrangement of letters seems random.  They are quite distant from each other and spread over the entire projection plane.  Not observed, for example, islands of vowels or sonorous consonants.  There is no universal separation of lowercase letters from capital letters. <br><br>  Perhaps this information is reflected in the view, but t-SNE simply does not have enough degrees of freedom to reflect these differences in a two-dimensional projection.  Maybe by examining each dimension in turn, we could get more information? <br><br><img src="https://habrastorage.org/files/d3f/582/98f/d3f58298f64c477086db933d7ab885e1.png"><br><br>  Or maybe not.  You can look at the <a href="http://colinmorris.github.io/lm1b/char_emb_dimens/">graphs of all 16 measurements here</a> - I did not manage to find any patterns in them. <br><br><h2>  <font color="#c75733">Vector computing</font> </h2><br>  Perhaps the most famous feature of the vector representations of words is the ability to add and subtract them and (sometimes) get semantically meaningful results.  For example, <br><br><pre> <code class="python hljs">vec(<span class="hljs-string"><span class="hljs-string">'woman'</span></span>) + (vec(<span class="hljs-string"><span class="hljs-string">'king'</span></span>) - vec(<span class="hljs-string"><span class="hljs-string">'man'</span></span>)) ~= vec(<span class="hljs-string"><span class="hljs-string">'queen'</span></span>)</code> </pre> <br>  I wonder if we can do the same with the representation of characters.  There are not too many obvious analogies here, but what about adding and subtracting ‚Äútitle‚Äù? <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">analogy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(a, b, c)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""a is to b, as c is to ___, Return the three nearest neighbors of c + (ba) and their distances. """</span></span> <span class="hljs-comment"><span class="hljs-comment"># ...</span></span></code> </pre> <br>  'a' refers to 'A' just as 'b' refers to ... <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>analogy(<span class="hljs-string"><span class="hljs-string">'a'</span></span>, <span class="hljs-string"><span class="hljs-string">'A'</span></span>, <span class="hljs-string"><span class="hljs-string">'b'</span></span>) b: <span class="hljs-number"><span class="hljs-number">4.2</span></span> V: <span class="hljs-number"><span class="hljs-number">4.2</span></span> Y: <span class="hljs-number"><span class="hljs-number">5.1</span></span></code> </pre> <br>  Okay, not a good start.  Let's try again: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>analogy(<span class="hljs-string"><span class="hljs-string">'b'</span></span>, <span class="hljs-string"><span class="hljs-string">'B'</span></span>, <span class="hljs-string"><span class="hljs-string">'c'</span></span>) c: <span class="hljs-number"><span class="hljs-number">4.2</span></span> C: <span class="hljs-number"><span class="hljs-number">5.2</span></span> +: <span class="hljs-number"><span class="hljs-number">5.9</span></span> &gt;&gt;&gt; analogy(<span class="hljs-string"><span class="hljs-string">'b'</span></span>, <span class="hljs-string"><span class="hljs-string">'B'</span></span>, <span class="hljs-string"><span class="hljs-string">'d'</span></span>) D: <span class="hljs-number"><span class="hljs-number">4.2</span></span> ,: <span class="hljs-number"><span class="hljs-number">4.9</span></span> d: <span class="hljs-number"><span class="hljs-number">5.0</span></span> &gt;&gt;&gt; analogy(<span class="hljs-string"><span class="hljs-string">'b'</span></span>, <span class="hljs-string"><span class="hljs-string">'B'</span></span>, <span class="hljs-string"><span class="hljs-string">'e'</span></span>) N: <span class="hljs-number"><span class="hljs-number">4.7</span></span> ,: <span class="hljs-number"><span class="hljs-number">4.7</span></span> e: <span class="hljs-number"><span class="hljs-number">5.0</span></span></code> </pre> <br>  Partial success? <br><br>  Having made many attempts, we now and then received the correct answer, but how is this method better than random?  Do not forget that half of the lowercase letters are located in close proximity to the corresponding capital letters.  Already therefore, if we move from a letter in a random direction, we are very likely to stumble upon its pair. <br><br><h2>  <font color="#c75733">Vector computing (now for real)</font> </h2><br>  I think it remains to try only one thing: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>analogy(<span class="hljs-string"><span class="hljs-string">'1'</span></span>, <span class="hljs-string"><span class="hljs-string">'2'</span></span>, <span class="hljs-string"><span class="hljs-string">'2'</span></span>) <span class="hljs-number"><span class="hljs-number">2</span></span>: <span class="hljs-number"><span class="hljs-number">2.4</span></span> E: <span class="hljs-number"><span class="hljs-number">3.6</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span>: <span class="hljs-number"><span class="hljs-number">3.6</span></span> &gt;&gt;&gt; analogy(<span class="hljs-string"><span class="hljs-string">'3'</span></span>, <span class="hljs-string"><span class="hljs-string">'4'</span></span>, <span class="hljs-string"><span class="hljs-string">'8'</span></span>) <span class="hljs-number"><span class="hljs-number">8</span></span>: <span class="hljs-number"><span class="hljs-number">1.8</span></span> <span class="hljs-number"><span class="hljs-number">7</span></span>: <span class="hljs-number"><span class="hljs-number">2.2</span></span> <span class="hljs-number"><span class="hljs-number">6</span></span>: <span class="hljs-number"><span class="hljs-number">2.3</span></span> &gt;&gt;&gt; analogy(<span class="hljs-string"><span class="hljs-string">'2'</span></span>, <span class="hljs-string"><span class="hljs-string">'5'</span></span>, <span class="hljs-string"><span class="hljs-string">'5'</span></span>) <span class="hljs-number"><span class="hljs-number">5</span></span>: <span class="hljs-number"><span class="hljs-number">2.7</span></span> <span class="hljs-number"><span class="hljs-number">6</span></span>: <span class="hljs-number"><span class="hljs-number">4.0</span></span> <span class="hljs-number"><span class="hljs-number">7</span></span>: <span class="hljs-number"><span class="hljs-number">4.0</span></span> <span class="hljs-comment"><span class="hljs-comment"># It'd be really surprising if this worked... &gt;&gt;&gt; nearest_neighbors(vec('2') + vec('2') + vec('2')) 2: 6.0 1: 6.9 3: 7.1</span></span></code> </pre> <br>  Note for the future: never use character embedding to calculate tips. <br><br>  It seems useful to place figures close in size close to each other due to interchangeability.  A ‚Äú36-year-old‚Äù can easily be replaced with a ‚Äú37-year-old‚Äù (or even a ‚Äú26-year-old‚Äù), 800 bucks are more like 900 or 700 bucks than 100. Looking at our t-SNE projection, we can say that This model works.  But this does not mean that the numbers are lined up (let's start at least with the fact that models need to learn some of the subtleties associated with the numbers, for example, take into account that the year most often begins with ‚Äú19‚Äù or ‚Äú20‚Äù. <br><br><h2>  <font color="#c75733">Why all this?</font> </h2><br>  Before guessing why a certain character is presented in one way or another, it is worth asking: <b>why use character embedding at all?</b> <br><br>  One of the reasons may be a decrease in the complexity of the model.  For each character, it will be enough for the selection filter of attributes in Char CNN to remember 16 weights instead of 256. If we remove the vector representation layer, the number of weights at the feature selection stage will increase by 16 times, that is, from about 460K (4096 filters * maximum width 7 * 16- dimensional representation) to 7.3M.  It seems like a lot, but the total number of parameters for the entire network (CNN + LSTM + Softmax) is 1.04 billion!  So a couple extra millions will not play a big role. <br><br>  In fact, <code>lm_1b</code> includes char embedding, because their CNN char is developed based on the <a href="https://arxiv.org/abs/1508.06615">article by Kim et.</a>  <a href="https://arxiv.org/abs/1508.06615">al</a> 2015, where char embedding was also used.  The footnote in this article explains: <br><br><blockquote>  Since | C |  usually a little, some authors use direct coding for the vector representation of characters.  However, we found that using the representation of symbols of smaller dimensions shows slightly better performance. </blockquote><br>  Apparently, the best performance meant a lower perplexion, and not a model learning speed, for example. <br><br>  Why does character representation improve performance?  Well, why does the word presentation improve the performance of tasks in the field of natural languages?  They improve the <b>generalization</b> .  There are many words in a language, and <a href="https://en.wikipedia.org/wiki/Zipf%2527s_law">many of them are rare</a> .  If you meet the words ‚Äúraspberry‚Äù, ‚Äústrawberry‚Äù and ‚Äúgooseberry‚Äù in the same context, we assign them close vectors.  And if we have repeatedly met the phrase "raspberry jam" and "strawberry jam", we can assume that the combination of "gooseberry jam" is quite likely, even if we have never met him in our case. <br><br><h2>  <font color="#c75733">Generalization of characters?</font> </h2><br>  To begin with, the analogy with the word vectors is not quite appropriate here.  The Billion Word package consists of 800,000 individual words, while we are dealing with only 256 characters.  Is it worth thinking about generalization?  And how to generalize, for example, 'g' to another character? <br><br>  It seems the answer will be ‚Äúno way‚Äù.  Sometimes we can draw conclusions on the basis of generalization for upper and lower case versions of a single letter, but in general, alphabetic characters are isolated, and they are all found so often that we should not be bothered by generalization. <br><br>  But are there any characters that are rarely enough for generalization to play an important role?  Let's get a look. <br><img src="https://habrastorage.org/files/9fc/055/fac/9fc055fac8c84c55a690970f6663b122.png"><br>  The frequency of the n-th most popular symbol.  Calculated on the basis of the Billion Word Benchmark training set (about 770 thousand words).  More than 50 characters are completely absent in the package (for example, ASCII control characters). <br><br>  Yes, this is not quite Zipf‚Äôs law (we are approaching a straight line using only a logarithmic scale on the y axis instead of a double logarithmic scale), but it‚Äôs still clear that there are a large number of rarely used symbols (mostly non-ASCII characters and rare punctuation marks ). <br><br>  Perhaps our vector representation helps us use generalization to infer such symbols.  On the t-SNE chart above, I depicted only those symbols that are quite common (I took the least frequency letter symbol 'x' for the lower border).  What if we depict a representation for characters appearing at least 50 times in the body? <br><br><img src="https://habrastorage.org/files/818/86a/65e/81886a65e54b4fd79b74c1162426dbc4.png"><br><br>  Green = letters, blue = numbers, yellow = punctuation, red-brown = metacharacters.  The pink marks are bytes up to 127 and everything else that is not included in the previous groups. <br><br>  This seems to confirm our hypothesis!  Letters, as before, are antisocial and rarely touch each other.  But in several areas, our rare ‚Äúpink‚Äù characters form dense clusters or lines. <br><br>  The most likely assumption: the alphabetic characters stand apart, while rare symbols, as well as symbols with a high degree of interchangeability (numbers, punctuation marks at the end of a sentence) tend to be located close to each other. <br><br>  That's all for now.  Thanks to the Google Brain team for the release of <code>lm_1b</code> .  If you want to conduct your experiments with this model, do not forget to read the instructions <a href="https://github.com/tensorflow/models/tree/master/lm_1b">here</a> .  I laid out the scripts with which I did the visualization for this post <a href="https://github.com/colinmorris/lm1b-notebook">here</a> - feel free to reuse or change them, although they look awful. <br><br>  Next time we will look at the second stage of the CNN Char - convolutional filters. <br><br><blockquote><div class="spoiler">  <b class="spoiler_title">Oh, and come to work with us?</b>  <b class="spoiler_title">:)</b> <div class="spoiler_text">  <a href="http://wunderfund.io/"><b>wunderfund.io</b></a> is a young foundation that deals with <a href="https://en.wikipedia.org/wiki/High-frequency_trading">high-frequency algorithmic trading</a> .  High-frequency trading is a continuous competition of the best programmers and mathematicians of the whole world.  By joining us, you will become part of this fascinating fight. <br><br>  We offer interesting and challenging data analysis and low latency tasks for enthusiastic researchers and programmers.  Flexible schedule and no bureaucracy, decisions are quickly made and implemented. <br><br>  Join our team: <a href="http://wunderfund.io/">wunderfund.io</a> </div></div></blockquote></div><p>Source: <a href="https://habr.com/ru/post/318454/">https://habr.com/ru/post/318454/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../318444/index.html">How to become the first in sports programming: ITMO University shares experience. Part 1</a></li>
<li><a href="../318446/index.html">Seattle's voice: talking to Sergey Teplyakov</a></li>
<li><a href="../318448/index.html">Hakintosh: we deploy macOS Sierra on Intel-PC in detail and clearly</a></li>
<li><a href="../318450/index.html">The most popular programming languages ‚Äã‚Äã2016</a></li>
<li><a href="../318452/index.html">We deliver a cup of hot coffee to our office with one team of console using TestCafe</a></li>
<li><a href="../318456/index.html">The history of one bug: the alignment of data on x86</a></li>
<li><a href="../318458/index.html">On a combinatorial problem</a></li>
<li><a href="../318460/index.html">How long does it take to learn a new technology?</a></li>
<li><a href="../318464/index.html">Data Storage Architecture in Facetz.DCA</a></li>
<li><a href="../318466/index.html">Comparison of monitoring systems: Shinken vs Sensu vs Icinga 2 vs Zabbix</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>