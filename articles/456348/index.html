<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>AERODISK Engine: Disaster. Part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello, readers Habra! The topic of this article will be the implementation of means of disaster recovery in storage systems AERODISK Engine. Initially...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>AERODISK Engine: Disaster. Part 1</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/2b/54/ub/2b54ub9jta3knw6ff5eseo1buyu.jpeg"></p><br><p>  Hello, readers Habra!  The topic of this article will be the implementation of means of disaster recovery in storage systems AERODISK Engine.  Initially, we wanted to write in one article about both means: replication and the metrocluster, but, unfortunately, the article turned out to be too large, so we divided the article into two parts.  Let's go from simple to complex.  In this article we will set up and test synchronous replication - drop one data center, and also break the communication channel between data centers and see what happens. </p><a name="habracut"></a><br><p>  Our customers often ask us different questions about replication, so before proceeding to setting up and testing the replica implementation, we will tell you a little about replication in the storage system. </p><br><h2 id="nemnogo-teorii">  A bit of theory </h2><br><p>  Storage replication is an ongoing process for ensuring the identity of data simultaneously across multiple storage systems.  Technically, replication is performed by two methods. </p><br><p>  <strong>Synchronous replication</strong> is the copying of data from the main storage system to the backup one, followed by the mandatory confirmation of both storage systems that the data has been recorded and confirmed.  It is after confirmation from both sides (from both storage systems) that the data is considered recorded, and it is possible to work with them.  This ensures guaranteed data identity across all storage systems participating in the replica. </p><br><p>  Advantages of this method: </p><br><ul><li>  Data is always identical on all storage systems. </li></ul><br><p>  Minuses: </p><br><ul><li>  High cost of the solution (fast communication channels, expensive optical fiber, long-wave transceivers, etc.) </li><li>  Distance limits (within a few tens of kilometers) </li><li>  There is no protection against logical data corruption (if data is spoiled (consciously or accidentally) on the main storage system, then they automatically and immediately become corrupted on the backup, since the data are always identical (this is the paradox) </li></ul><br><p>  <strong>Asynchronous replication</strong> is also copying data from the main storage system to the backup one, but with a certain delay and without the need to confirm the record on the other side.  You can work with the data immediately after writing to the main storage system, and on the backup storage the data will be available after some time.  The identity of the data in this case, of course, is not ensured at all.  Data on the backup storage is always a bit "in the past." </p><br><p>  Advantages of asynchronous replication: </p><br><ul><li>  Low cost of the solution (any communication channels, optics optional) </li><li>  No distance limits </li><li>  On the backup storage, the data does not deteriorate if they are damaged on the primary (at least for a while), if the data become corrupted, you can always stop the replica to prevent data corruption on the backup storage </li></ul><br><p>  Minuses: </p><br><ul><li>  Data in different data centers is always non-identical. </li></ul><br><p>  Thus, the choice of replication mode depends on the business objectives.  If it is critical for you to have absolutely the same data in the backup data center as in the main (i.e. business requirement for RPO = 0), then you will have to fork out and put up with the limitations of the synchronous replica.  And if the delay in the state of the data is permissible or there is simply no money, then, clearly, an asynchronous method should be used. </p><br><p>  Separately, we single out such a mode (more precisely, already a topology) as a metrocluster.  In the metrocluster mode, synchronous replication is used, but, unlike the usual replica, the metrocluster allows both storage systems to work in the active mode.  Those.  you do not have division into active-backup data centers.  Applications work simultaneously with two storage systems that are physically located in different data centers.  Downtime in case of accidents in this topology is very small (RTO, usually minutes).  In this article we will not consider our implementation of the metrocluster, since this is a very large and capacious topic, so we will devote a separate, next article to it, in continuation of this. </p><br><p>  Also, very often, when we talk about replication by means of storage, many have a reasonable question:&gt; ‚ÄúMany applications have their own replication tools, why use replication on storage?  Is it better or worse? </p><br><p>  There is no unequivocal answer, so we give the arguments FOR and AGAINST: </p><br><p>  Arguments for replication storage: </p><br><ul><li>  Ease of solution.  With one tool, you can replicate an entire array of data, regardless of the type of load and applications.  If you use a replica from the application, you will have to configure each application separately.  If there are more than 2 of them, then it is extremely time consuming and expensive (application replication requires, as a rule, a separate and not free license for each application. But more on this below). </li><li>  You can replicate anything ‚Äî any applications, any data ‚Äî and they will always be consistent.  Many (most) applications do not have replication tools, and replicas from storage systems are the only means to provide protection against disasters. </li><li>  No need to overpay for application replication functionality.  As a rule, it is not cheap, as well as licenses for the storage replica.  But you need to pay once for the license for storage replication, and you need to buy a license for the application replica for each application separately.  If there are many such applications, then it costs a lot of money and the cost of licenses for replicating the storage system becomes a drop in the ocean. </li></ul><br><p>  Arguments VS storage replication: </p><br><ul><li>  Replica means of applications has more functionality in terms of the applications themselves, the application knows its data better (which is obvious), so the options for working with them are more. </li><li>  Manufacturers of some applications do not guarantee the consistency of their data, if replication is done by third-party tools.  * </li></ul><br><p>  * - controversial thesis.  For example, a well-known manufacturer of a DBMS, for a long time officially stated that their DBMS can normally be replicated only by their means, and the rest of replication (including SHD-shnaya) is ‚Äúnot true‚Äù.  But life has shown that it is not.  Most likely, (but this is not certain) it is simply not the most honest attempt to sell more licenses to customers. </p><br><p>  As a result, in most cases, replication by the storage system is better, because  This is a simpler and less expensive option, but there are complex cases when specific functionality of applications is needed, and it is necessary to work with application-level replication. </p><br><h2 id="s-teoriey-zakonchili-teper-praktika">  With theory finished, now practice </h2><br><p>  We will set up a cue in our lab.  In the laboratory, we emulated two data centers (in fact, two stands next to each other, which seem to be in different buildings).  The stand consists of two SHD Engine N2, which are interconnected by optical cables.  Both storage systems are connected to a physical server running Windows Server 2016 using 10Gb Ethernet.  The stand is quite simple, but essentially it does not change. </p><br><p>  Schematically, it looks like this: </p><br><p><img src="https://habrastorage.org/webt/wj/u4/rc/wju4rcak9ilbms68pnffyvsb6ly.png"></p><br><p>  Logically, replication is organized as follows: </p><br><p><img src="https://habrastorage.org/webt/yf/yh/dy/yfyhdy19cbj3sc0gpzp8jx6liz0.jpeg"></p><br><p>  Now let's look at the replication functionality that we have now. <br>  Two modes are supported: asynchronous and synchronous.  It is logical that the synchronous mode is limited by distance and communication channel.  In particular, for synchronous mode, you need to use fiber as physics and 10 Gigabit Ethernet (or higher). </p><br><p>  The supported distance for synchronous replication is 40 kilometers, the delay value of the optics channel between data centers is up to 2 milliseconds.  In general, it will work with long delays, but then there will be strong brakes when writing (which is also logical), so if you conceived synchronous replication between data centers, you should check the quality of the optics and the delays. </p><br><p>  Asynchronous replication requirements are not as serious.  More precisely, they are not at all.  Any working Ethernet connection will work. </p><br><p>  Currently, AERODISK ENGINE storage systems support replication for block devices (LUNs) over Ethernet (copper or optics).  For projects where replication through the SAN factory via Fiber Channel is required, we are now adding the appropriate solution, but for the time being it is not ready, therefore in our case it is only Ethernet. </p><br><p>  Replication can work between any ENGINE series storage systems (N1, N2, N4) from lower systems to older systems and vice versa. </p><br><p>  The functionality of both replication modes is completely identical.  Below is more about what is: </p><br><ul><li>  Replication "one to one" or "one to one", that is, the classic version with two data centers, main and backup </li><li>  Replication "one to many" or "one to many", i.e.  one LUN can be replicated to several storage systems at once </li><li>  Activation, deactivation and reversal of replication, respectively, to enable, disable or change the direction of replication </li><li>  Replication is available for both RDG (Raid Distributed Group) pools and DDP (Dynamic Disk Pool).  However, the LUNs of the RDG pool can only be replicated to another RDG.  C DDP is similar. </li></ul><br><p>  There are many more minor features, but there is not much point in listing them, we will mention them in the course of customization. </p><br><h2 id="nastroyka-replikacii">  Replication setup </h2><br><p>  The setup process is quite simple and consists of three stages. </p><br><ol><li>  Network configuration </li><li>  Storage Configuration </li><li>  Setting rules (relationships) and mapping </li></ol><br><p> An important point in setting up replication is that the first two stages should be repeated on the remote storage system, the third stage only on the main one. </p><br><h3 id="nastroyka-setevyh-resursov">  Setting up network resources </h3><br><p>  The first step is to configure the network ports on which replication traffic will be transmitted.  To do this, the ports must be enabled and IP addresses on them are specified in the Front-end adapters section. </p><br><p>  After that we need to create a pool (in our case RDG) and virtual IP for replication (VIP).  The VIP is a floating IP address that is tied to the two ‚Äúphysical‚Äù addresses of the storage controllers (the ports that we just configured).  It will be the main replication interface.  You can also operate not with VIPs, but with VLANs if you need to work with tagged traffic. </p><br><p><img src="https://habrastorage.org/webt/di/ye/5f/diye5fvbzya3cvtebg7memcs5jo.jpeg"></p><br><p>  The process of creating a VIP for replica is not much different from creating a VIP for input / output (NFS, SMB, iSCSI).  VIP in this case, we create a normal (no VLAN), but be sure to indicate that it is for replication (without this pointer, we can not add a VIP to the rule in the next step). </p><br><p><img src="https://habrastorage.org/webt/nd/i9/2d/ndi92dbmjvxuqjidju802r-vl7s.png"></p><br><p>  VIP must be on the same subnet as the IP ports between which it ‚Äúfloats‚Äù. </p><br><p><img src="https://habrastorage.org/webt/vm/no/9m/vmno9ms_uas_guk28o1etun7kg4.png"></p><br><p>  We repeat these settings on a remote storage system, with a different IP-schnick, by itself. <br>  VIPs from different storage systems can be on different subnets, as long as there is routing between them.  In our case, this example is shown (192.168.3.XX and 192.168.2.XX) </p><br><p><img src="https://habrastorage.org/webt/w5/r6/re/w5r6rexidxqry4rnfdrvgp5gzcq.jpeg"></p><br><p>  This completes the preparation of the network part. </p><br><h3 id="nastraivaem-hranilischa">  Configuring Storage </h3><br><p>  Configuring storage for a replica differs from the usual only in that we do the mapping through the special menu "Mapping Replication".  Otherwise, everything is the same as with the usual setting.  Now in order. </p><br><p>  In the previously created pool R02, you need to create a LUN.  Create, call it LUN1. </p><br><p><img src="https://habrastorage.org/webt/tg/l8/vk/tgl8vkdsqf-4oljacfssnh2_zus.jpeg"></p><br><p>  We also need to create the same LUN on the remote storage system of the same volume.  We create.  To avoid confusion, the remote LUN is called LUN1R. </p><br><p><img src="https://habrastorage.org/webt/xm/kl/v4/xmklv4deigknjz1vadluit9pdds.jpeg"></p><br><p>  If we needed to take a LUN that already exists, then at the time of setting up the replica this productive LUN would need to be unmounted from the host, and on a remote storage system simply create an empty LUN of identical size. </p><br><p>  Configuration of the storage is completed, proceed to the creation of the replication rule. </p><br><h3 id="nastroyka-pravil-replikacii-ili-replikacionnyh-svyazey">  Configuring Replication Rules or Replication Links </h3><br><p>  After creating LUNs on the storage system, which is currently Primary, configure the LUN1 replication rule on SHD1 to LUN1R on SHD2. </p><br><p>  Setup is made in the "Remote Replication" menu </p><br><p>  Create a rule.  To do this, specify the recipient of the replica.  In the same place we set the name of the connection and the type of replication (synchronous or asynchronous). </p><br><p><img src="https://habrastorage.org/webt/xk/yu/8f/xkyu8ftgcubcwu4-ul_ux3vc7lq.jpeg"></p><br><p>  In the field "remote systems" add our SHD2.  To add, you need to use the control IP storage (MGR) and the name of the remote LUN to which we will perform replication (in our case, LUN1R).  Managing IPs are needed only at the stage of adding a connection, replication traffic will not be transmitted through them, for this, the previously configured VIP will be used. </p><br><p>  Already at this stage, we can add more than one remote system for the ‚Äúone to many‚Äù topology: click on the ‚Äúadd node‚Äù button, as in the figure below. </p><br><p><img src="https://habrastorage.org/webt/rv/xb/bh/rvxbbh4umovgds3gduoaxg3tfc8.jpeg"></p><br><p>  In our case, the remote system is one, so we limit ourselves to this. </p><br><p>  The rule is ready.  Please note that it is added automatically on all replication members (in our case there are two of them).  You can create as many rules as you like, for any number of LUNs and in any direction.  For example, for load balancing we can replicate part of LUNs from SHD1 to SHD2, and the other part, on the contrary, from SHD2 to SHD1. </p><br><p>  SHD1.  Immediately after the creation of synchronization began. </p><br><p><img src="https://habrastorage.org/webt/y7/v8/gg/y7v8gg7bboqpit0zrow87pgvi5y.jpeg"></p><br><p>  SHD2.  We see the same rule, but synchronization has already ended. </p><br><p><img src="https://habrastorage.org/webt/tb/dl/0k/tbdl0k_anxtcwmecg31bk0s7fmo.jpeg"></p><br><p>  LUN1 on SHD1 is in the role of Primary, that is, it is active.  LUN1R on SHD2 is in the role of Secondary, that is, it is in the pipeline, in case of failure of SHD1. <br>  Now we can connect our LUN to the host. </p><br><p>  We will do an iSCSI connection, although you can also do it by FC.  Setting up an iSCSI LUN mapping in a replica is almost the same as a regular script, so we will not discuss it in detail here.  If anything, this process is described in the article " <a href="https://habr.com/ru/company/tssolution/blog/432876/">Quick Setup</a> ". </p><br><p>  The only difference is that we create mapping in the menu ‚ÄúMapping Replication‚Äù </p><br><p><img src="https://habrastorage.org/webt/xn/uy/p9/xnuyp9dccwbpg93ahvefmhifmdq.jpeg"></p><br><p>  Configured a mapping, gave LUN to a host.  The host saw LUN. </p><br><p><img src="https://habrastorage.org/webt/qg/y3/vm/qgy3vmfark_2-pvl8liqtozb2iu.jpeg"></p><br><p>  Format it to the local file system. </p><br><p><img src="https://habrastorage.org/webt/zd/8l/qg/zd8lqglmv194u9-zsctatxrsuzk.jpeg"></p><br><p>  That's it, the setup is complete.  Further tests will go. </p><br><h2 id="testirovanie">  Testing </h2><br><p>  We will test three main scenarios. </p><br><ol><li>  Regular role switching Secondary&gt; Primary.  A regular role switch is necessary in case, for example, in the main data center, we need to perform any preventive operations and for this time, so that the data are available, we transfer the load to the backup data center. </li><li>  Emergency role switching Secondary&gt; Primary (data center failure).  This is the main scenario for which there is replication, which can help to survive a complete failure of the data center, without stopping the company for a long time. </li><li>  Break of communication channels between data centers.  Verification of the correct behavior of two storage systems in conditions where, for whatever reason, the communication channel between data centers is not available (for example, the excavator did not dig there and broke dark optics). </li></ol><br><p>  To begin, we will start writing data to our LUN (we write files with random data).  At once we look that the communication channel between SHD is utilized.  This is easy to understand if you open monitoring the load of ports that are responsible for replication. </p><br><p><img src="https://habrastorage.org/webt/s7/99/bt/s799bttjt3v6q24uhvxoywfrwne.jpeg"></p><br><p>  Both storage systems now have ‚Äúuseful‚Äù data; we can begin the test. </p><br><p><img src="https://habrastorage.org/webt/r3/vs/dv/r3vsdvpsp9avchablad41pxrfdu.jpeg"></p><br><p>  Just in case, we will look at the hash sums of one of the files and write them down. </p><br><p><img src="https://habrastorage.org/webt/e1/zi/st/e1zistvzwlkimqbupxjtgnltc9o.jpeg"></p><br><h3 id="shtatnoe-pereklyuchenie-roley">  Regular role switching </h3><br><p>  The operation of switching roles (changing the direction of replication) can be done with any storage system, but you still need to go to both, since you need to disable mapping on the Primary and enable it on the Secondary (which will become Primary). </p><br><p>  Perhaps now a reasonable question arises: why not automate it?  The answer is: everything is simple, replication is a simple means of disaster recovery, based solely on manual operations.  To automate these operations, there is a metrocluster mode, it is fully automated, but its configuration is much more complicated.  We will write about the metro cluster setting in the next article. </p><br><p>  On the main storage, we disable mapping to ensure that the recording is stopped. </p><br><p><img src="https://habrastorage.org/webt/jk/j4/1l/jkj41ltsncz2hmqoclkrecqvguy.jpeg"></p><br><p>  Then on one of the data storage systems (not important, on the main or backup) in the ‚ÄúRemote Replication‚Äù menu, select our REPL1 link and click ‚ÄúChange role‚Äù. </p><br><p><img src="https://habrastorage.org/webt/dc/bb/8t/dcbb8tv24xxhofmg_avtodfelas.jpeg"></p><br><p>  After a few seconds, LUN1R (backup storage) becomes Primary. </p><br><p><img src="https://habrastorage.org/webt/-v/hf/l2/-vhfl2g0v20bnfnomxwupf0_9xk.jpeg"></p><br><p>  We do LUN1R mapping with SHD2. </p><br><p><img src="https://habrastorage.org/webt/lh/uw/cy/lhuwcyscu0quljitysu2pkk35hg.jpeg"></p><br><p>  After that, on the host, our E: drive automatically clings, only this time it ‚Äúflew in‚Äù from LUN1R. </p><br><p>  Just in case, we compare hash sums. </p><br><p><img src="https://habrastorage.org/webt/g6/st/qh/g6stqhn-xr0yqlw84t7y4_y5sqm.png"></p><br><p>  Identically.  Test passed. </p><br><h3 id="avariynoe-pereklyuchenie-otkaz-cod-a">  Emergency switching.  Data Center Failure </h3><br><p>  At the moment, the main storage after standard switching is SHD2 and LUN1R, respectively.  To emulate an accident, we turn off the power on both the storage controllers 2. <br>  No more access to it. </p><br><p>  We look that occurs on SHD 1 (reserve at the moment). </p><br><p><img src="https://habrastorage.org/webt/ai/oy/jt/aioyjtl8xqmmgngtidigkvoesai.jpeg"></p><br><p>  See that Primary LUN (LUN1R) is unavailable.  There was an error message in the logs, in the information panel, as well as in the replication rule itself.  Accordingly, data from the host is currently unavailable. </p><br><p>  Change the role of LUN1 to Primary. </p><br><p><img src="https://habrastorage.org/webt/ef/vr/wv/efvrwvemqzysnrtebprwvmqnxw8.jpeg"></p><br><p>  Cause mapping to host. </p><br><p><img src="https://habrastorage.org/webt/o9/es/oj/o9esojg6xcl-uv_wbbj6afkrz18.jpeg"></p><br><p>  Make sure that drive E appears on the host. </p><br><p><img src="https://habrastorage.org/webt/rg/kj/0s/rgkj0s-0rgoumtmnzk98bd-bgl4.jpeg"></p><br><p>  Checking the hash. </p><br><p><img src="https://habrastorage.org/webt/hn/yb/yq/hnybyqjm7w_g1il4bowg1-xgq5y.jpeg"></p><br><p>  Everything is good.  The fall of the data center, which was active, survived successfully.  The approximate time we spent on connecting the ‚Äúreversal‚Äù of replication and connecting the LUN from the backup data center was about 3 minutes.  It is clear that in real production everything is much more complicated, and besides actions with storage systems, many more operations need to be performed on the network, on hosts, in applications.  And in life, this period of time will be much longer. </p><br><p>  Here I want to write that everything, the test has been successfully completed, but we will not hurry.  The main storage system is ‚Äúlying‚Äù, we know that when it ‚Äúfell‚Äù, it was in the role of Primary.  What happens if it suddenly turns on?  There are two roles Primary, which is equal to data corruption?  Now check. <br>  We are going to suddenly turn on the underlying storage system. </p><br><p>  It is loaded for several minutes and after that it returns to service after a short synchronization, but already in the role of Secondary. </p><br><p><img src="https://habrastorage.org/webt/27/hu/q6/27huq6b6guby7o-g7_xkz7quugy.jpeg"></p><br><p>  All OK.  Split-brain did not happen.  We thought about this, and always after the fall of the storage system rises in the role of Secondary, regardless of the role in which it was "during life."  Now we can definitely say that the data center failure test was successful. </p><br><h3 id="otkaz-kanalov-svyazi-mezhdu-cod-ami">  Failure of communication channels between data centers </h3><br><p>  The main task of this test is to make sure that the storage system does not begin to wonder if it temporarily lost the communication channels between the two storage systems and then reappeared. <br>  So.  Disconnect the wires between the storage systems (imagine that they dug an excavator). </p><br><p>  At Primary, we see that there is no connection with Secondary. </p><br><p><img src="https://habrastorage.org/webt/yh/nf/ar/yhnfarhppjrnbaxotu4ds4szz5c.jpeg"></p><br><p>  On Secondary we see that there is no connection with Primary. </p><br><p><img src="https://habrastorage.org/webt/f4/k9/7h/f4k97hzr11uh3cytxpsjlq2anly.jpeg"></p><br><p>  Everything is working fine, and we continue to write data to the main storage system, that is, they are guaranteed to differ from the backup, that is, they have ‚Äúparted‚Äù. </p><br><p>  In a few minutes we are ‚Äúrepairing‚Äù the communication channel.  As soon as they saw each other's storage systems, data synchronization is automatically turned on.  Here from the administrator nothing is required. </p><br><p><img src="https://habrastorage.org/webt/wo/os/yy/woosyydo-vvbauzsd7lgu4qwfos.jpeg"></p><br><p>  After a while, the synchronization ends. </p><br><p><img src="https://habrastorage.org/webt/up/ne/es/upneeslicidwf8manqfmlvcaohu.jpeg"></p><br><p>  The connection is restored, no abnormal situations have caused a break in communication channels, and after switching on, synchronization has automatically passed. </p><br><h2 id="vyvody">  findings </h2><br><p>  We have analyzed the theory of what is needed and why, where are the pluses, and where are the minuses.  Then configured synchronous replication between two SHD. </p><br><p>  Next, the main tests were carried out on the regular switching, data center failure, and the interruption of communication channels.  In all cases, the storage system worked well.  There are no data losses, administrative operations are minimized for a manual scenario. </p><br><p>  Next time we will complicate the situation and show how all this logic works in an automated metro cluster in active-active mode, that is, when both storage systems are basic, and the behavior in case of storage failure is fully automated. </p><br><p>  Please write comments, we will be happy for sensible criticism and practical advice. </p><br><p>  Until new meetings. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/456348/">https://habr.com/ru/post/456348/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456338/index.html">13 helpful JavaScript liners</a></li>
<li><a href="../456340/index.html">A story about how a team of freelancers writes full JavaScript applications in JavaScript</a></li>
<li><a href="../456342/index.html">One language to rule everyone</a></li>
<li><a href="../456344/index.html">Why ['1', '7', '11']. Map (parseInt) returns [1, NaN, 3] in Javascript?</a></li>
<li><a href="../456346/index.html">Interactive Roadmap for web development learners</a></li>
<li><a href="../45635/index.html">Sendercallouts (Sender Address Verify) - What is Bad</a></li>
<li><a href="../456350/index.html">Digital events in Moscow from June 17 to 23</a></li>
<li><a href="../456352/index.html">Wireless communication module with a WISE-4000 object</a></li>
<li><a href="../456354/index.html">How we collect set-top boxes</a></li>
<li><a href="../456358/index.html">13 most mined articles last year</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>