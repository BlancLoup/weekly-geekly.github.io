<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>An example of express analysis of the performance of storage using the free service Mitrend</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The study of performance problems and the search for solutions are familiar to many firsthand. There are a large number of tools for visualizing and p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>An example of express analysis of the performance of storage using the free service Mitrend</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/597/1db/d00/5971dbd0088544d283485cd07a8b8c10.JPG"><br><br>  The study of performance problems and the search for solutions are familiar to many firsthand.  There are a large number of tools for visualizing and parsing I / O statistics.  At present, automation of intellectual analysis based on Internet services is gaining momentum. <br><br>  In this post I want to share an example of analyzing the performance problem of storage systems based on one of these services (Mitrend) and offer ways to solve it.  In my opinion, this example is an interesting study that I think can be useful for a wide range of IT readers. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      So, the customer asked EMC to see the performance of the VNX5500 hybrid storage system deployed in its SAN.  VMware servers are connected to the storage system, on which "everything is altogether": from infrastructure tasks to file balls and database servers.  The reason for carrying out this rapid assessment was complaints about the suspension of applications deployed on servers connected to VNX. <br><a name="habracut"></a><br>  For preprocessing, I used the freely available <a href="http://app.mitrend.com/">Mitrend</a> service. <br>  A detailed description of this service is not included in the objectives of this post, so I invite everyone to learn more about it - go to <a href="http://app.mitrend.com/">its website</a> and see for yourself. <br><br>  Mitrend receives input files with input / output statistics from the system under study and prepares graphs for the most frequently requested parameters, as well as makes preliminary analytics, the results of which will be used later. <br><br><img src="https://habrastorage.org/files/c68/f10/cf2/c68f10cf27d34a079a6ba646c0e5372a.png"><br><br>  One example of such analytics is a heat map showing how much the various components of the system are loaded at different points in time.  In fact, this is a schematic depiction of the system and its components, within each of which a schedule of its loading is built.  A general view of it allows you to see potentially problematic places.  In this case, it is clear that the cache is a write cache.  Here is the chart: <br><br><img src="https://habrastorage.org/files/98b/279/fcc/98b279fcce6c45cd86782ea42fef208e.png"><br><br>  Record cache utilization is at a high level, from which regular ‚Äúshooting‚Äù into the red zone (above 90%) occurs. <br><br>  This is a typical symptom of performance problems.  A kind of "high temperature".  In this case, we have to study what exactly leads to such a situation and outline ways to solve it. <br><br>  Disks, processors, I / O ports, disk bus are not loaded.  And this is a bit strange, amid the fact that the write cache is ‚Äúcrammed‚Äù. <br><br><img src="https://habrastorage.org/files/6b8/b74/8b6/6b8b748b6d354183aafaa9338e33f391.png"><br><br><img src="https://habrastorage.org/files/0f0/f6f/416/0f0f6f4169ac439cbe8eaf0d22a01cf6.png"><br><br><img src="https://habrastorage.org/files/59f/079/52d/59f07952defc45d08c59f466e3866d88.png"><br><br>  Let's now take a look at the disks in more detail.  For clarity, I circled various-colored discs with different lines and signed a legend from below.  In the file with the analysis it can be seen without a legend. <br><br><img src="https://habrastorage.org/files/d85/2d7/686/d852d76868364704ba621b59f065b6cb.png"><br><br>  Let's take a closer look at what the disk system in question is all about: three 200GB flash drives, two of which are configured in FAST Cache with a usable volume of 183GB, and the third is in hot spare.  Those.  very robust, mirrored hot-flash flash memory.  The effectiveness of its work can be seen in the graph below: <br><br><img src="https://habrastorage.org/files/a59/faa/5ce/a59faa5ceb1646b29971e92e71cf8047.png"><br><br>  The system has 5 900 GB disks that are not used at all.  Since these are system disks, they try not to touch them out of habit, because there is an opinion that this causes performance problems.  My opinion on this is that <a href="http://denserov.com/2012/09/24/vnx-vault-use/">they can be used</a> if it is done in a meaningful way.  Performance problems are usually for completely different reasons. <br><br>  Usually, disks of different types are combined into hybrid pools, so that the system itself determines where it is better to place the data (using FAST VP).  But in this case, the implementation specialists did not trust her with this responsible business and rigidly divided the data by disk type.  Therefore, the disks are divided into 2 separate groups - Pool 0 and Pool 1. They did this in order to isolate them from the point of view of performance, so that non-critical applications would not affect those that need speed. <br><br>  Pool 0 (RAID5) is designed for critical application servers and consists of SAS 10k disks. <br><br>  Pool 1 (RAID6) - this is a custom "balls" and any kind of performance-friendly environment.  It consists of NL SAS 7.2k disks. <br><br>  Examining the disk group summary shows that FAST Cache is disabled on the Pool 1 group. <br><br><img src="https://habrastorage.org/files/dae/a1f/e04/daea1fe04a564a3ba173bad20f902c9a.jpg"><br><br>  The conversation with the customer made it clear that this was done in order to increase the priority of resources for the performance-critical Pool 0. <br><br>  It is interesting to note that despite this, complaints come precisely from applications using Pool 0, whose disks are almost not loaded.  Moreover, 80% of all read operations and 91% of all write operations of this pool are serviced by FAST Cache. <br><br><img src="https://habrastorage.org/files/728/97e/96d/72897e96d62e43dc8e8cebfd28dfc5bf.jpg"><br><br><img src="https://habrastorage.org/files/82d/689/580/82d689580ec14788acff3b5e607f3bf8.jpg"><br><br>  That is, despite the amazing efficiency of FAST Cache, applications are experiencing problems.  Why?  To go further, let's look at the LUNs and load distribution. <br><br><img src="https://habrastorage.org/files/32c/3f0/61d/32c3f061d50d49c999eef5cbb25a2b5b.png"><br><br>  It turns out that the three most heavily loaded LUNs are located exactly on slow NL-SAS disks in RAID6.  There are no complaints about them.  A conversation with users showed that they were extremely pleased with how quickly their file servers began to work after switching to VNX. <br><br>  There are complaints about LUNs on Pool 0 (green on the chart above).  Specifically, this is LUN with numbers 0 through 8, which are listed in the table below. <br><br><img src="https://habrastorage.org/files/c33/b68/f41/c33b68f41bd9475a8941d2126c78426f.png"><br><br>  If you now look at the degree of utilization of LUNs, then it can be seen that the LUNs from Pool 0 are recycled rather poorly.  The chart below shows the LUN numbers horizontally, so it's easy to identify which LUNs are ‚Äúours‚Äù.  The most "loaded" of them busy working only 40%. <br><br><img src="https://habrastorage.org/files/5de/c63/fa5/5dec63fa53bc4154aabb999951681901.png"><br><br>  The system works "on average well."  The average response time of volumes within 10 ms.  This is the average temperature of the hospital. <br><br>  Against the background that the load on the problematic LUNs is low, it can be concluded that their competition for some common resource causes problems. <br><br><img src="https://habrastorage.org/files/6d5/e50/fce/6d5e50fcebf0470d8776a56f2ec3a4ef.png"><br><br>  Let's see how the system cache works.  Reading from the cache is very efficient. <br><br><img src="https://habrastorage.org/files/567/2aa/b3c/5672aab3c0c241c9a486d4e824e0abaf.jpg"><br><br>  Analysis of the write cache shows that its load is kept within the specified framework of 60-80% with periodic bursts of up to 90% or more.  It's not very good. <br><br><img src="https://habrastorage.org/files/43a/38e/234/43a38e2342764d4e921e9eb7e308ff45.png"><br><br>  Let's see how often the system has to resort to extreme measures in order to clear the cache to an acceptable level. <br><br><img src="https://habrastorage.org/files/326/393/7f1/3263937f1db84ce599bcb8b03c6864bc.jpg"><br><br>  This means that the system does not have time to work out recording bursts.  But the system settings can be changed by moving the upper and lower limits to more comfortable levels.  30-50%, for example.  But this is the same as bringing down the temperature of the patient.  To do this, you must first establish the diagnosis and the root cause.  Now let's look at the pools and try to understand what exactly causes forced cache flushes. <br><br><img src="https://habrastorage.org/files/270/b94/b1f/270b94b1fb2b444b94cceeae2bb5790a.jpg"><br><br>  We see that regular forced drops occur on both disk pools.  And if at Pool 0 this happens extremely rarely (isolated cases), then at Pool 1 this situation is very difficult (tens and hundreds of events per hour).  But Pool 0 interests us. Everything is fine there, isn't it? <br><br>  We have come close to the solution.  But to move on is a lyrical digression, since we need to clarify the logic for managing the write cache fullness in VNX.  It is shown below. <br><br><img src="https://habrastorage.org/files/0fb/212/ffb/0fb212ffb22b459d9ae2903bc00d8b5c.png"><br>  <i>In normal mode, the system maintains a cache between two boundaries ‚Äî High and Low watermarks.</i> <i><br><br></i>  <i>The lower limit is the threshold below which the write cache is not reset, since the data it contains may be needed for reading, or be overwritten.</i>  <i>In addition, the VNX write cache, by its nature, keeps some amount of data blocks, in the hope that they can be combined to write with other blocks nearby, for writing to physical disks.</i>  <i>This reduces the load on the back-end.</i> <i><br><br></i>  <i>The upper limit is the threshold for enabling disc write cache.</i>  <i>When High Watermark Flushing mode is turned on, data is flushed from the cache to disk to the lower level, and then goes back to standby mode.</i> <i><br><br></i>  <i>We do not want the cache to fill up to 100%, because then we will not be able to provide space for new entries.</i>  <i>Therefore, try to keep the upper limit at a safe distance of 100%.</i>  <i>Usually 80% is normal.</i>  <i>But maybe lower.</i>  <i>It all depends on the nature of the load.</i> <i><br><br></i>  <i>If the cache fills up to 100%, then from High Watermark flush mode the system includes forced cache flush, or Forced Flush.</i> <i><br><br></i>  <i>Forced Flush mode has a major impact on all write operations on the storage system.</i>  <i>New data is written to the storage system with an additional delay.</i>  <i>Those.</i>  <i>in order to write a data block in the storage system, you must first free up space from the old data using the LRU algorithm (Least Recently Used), etc.</i> <br><br>  Let's return to our situation.  Obviously, the slower Pool 1 is the weak link in terms of write cache.  Data that arrives on slow disks in RAID6 lingers in the cache longer than necessary, and when it comes to Forced Flush, it takes too long to go to physical disks. <br><br>  It is necessary to pay attention to the fact that Pool 0 uses FAST Cache, and most of the requests are served from flash disks.  Until Forced Flush comes on, and the response time of the flash begins to depend on how quickly the data on the NL-SAS is cleared.  It seems that the weak link was found.  How true this conclusion is is that the hypothesis test should show in practice. <br><br>  How then can the ‚Äúsuspect‚Äù alibi be explained - low loading of NL-SAS disks?  Since the load is average over the time interval, and in this case, the statistics collection interval was 10 minutes, it is possible that during this time there was a short burst of data recording, causing a short ‚Äúfreeze‚Äù of applications, and on average 10 minutes the load was not so big .  Since we have found where the highest value of Forced Flush occurs, there can be no doubt about the ‚Äúguilt‚Äù of this disk pool. <br><br><h3>  What can you do about it? </h3><br>  The implementation itself, in itself, contains planning errors, since the old approach to configuration in a system with a new generation architecture has been used.  Communication with the customer helped to clarify that the matter is in previously accepted standards, which were not revised at the time of planning.  But since the system is already combat, and it is impossible to rebuild it, it remains to look for solutions in the field of online reconfigurations in order not to interrupt the operation of applications. <br><br>  I found at least three measures that can be taken either individually or together, complementing each other.  I list the degree of complexity of implementation. <br><br><ol><li>  In order for the storage system to work out periodic bursts of load, it is necessary to lower Low / High watermarks to the level of 30/50 and see how well these bursts will be worked out.  Ideally, filling the write cache during bursts should not be up to 90%. </li><li>  Enable FAST Cache on Pool 1. The most frequently updated data will be transferred from slow disks to SSD.  Resetting the write cache on the SSD is much faster.  This will reduce the likelihood of Forced Flush. </li><li>  Create a RAID10 RAID group on free SAS 900GB 10k disks (4 pieces) and transfer the most frequently updated LUNs from Pool 1 to them. Disable the write cache in the created RAID group. </li></ol><br><br>  There are other ways to optimize, however, I specifically tried not to complicate this example in order to more compactly demonstrate one of the possible approaches. <br><br>  You can start with these measures, since all of the listed changes are reversible and can be applied or reversed in any order. <br><br>  In the process of further researching the behavior of the system, other useful conclusions can be made. <br><br><h3>  Afterword </h3><br>  Intelligent storage systems have rich built-in functionality for both analysis and performance tuning.  However, detailed manual analysis and adjustment are quite time-consuming tasks that we have touched on in this post only superficially.  Usually, administrators have very little time to fully study the work of the storage system and optimize it.  With dynamic workloads and increasingly complex IT infrastructures, a new level of development and automation is required. <br><br>  To solve these problems, a whole complex of technologies has been developed at all levels. <br><br>  From more convenient and fast performance analysis to new intelligent and self-optimizing systems. <br>  Here are just some examples: <br><ul><li>  1. <a href="http://app.mitrend.com/">Mitrend</a> - freely available for all automated analysis of the IT infrastructure of different manufacturers <br>  2. Automated multi-level storage and cache on SSD: <a href="https://www.emc.com/collateral/white-papers/h12102-vnx-fast-vp-wp.pdf">FAST VP</a> and <a href="https://www.emc.com/collateral/white-papers/h12208-vnx-multicore-fast-cache-wp.pdf">FAST Cache</a> <br>  3. The next-generation systems have implemented an adaptive VNX2 cache with intelligent auto-tuning of the data reset rate per LUN ( <a href="https://www.emc.com/collateral/white-papers/h12090-emc-vnx-mcx.pdf">see whitepaper page 13</a> ). <br></li></ul></div><p>Source: <a href="https://habr.com/ru/post/271799/">https://habr.com/ru/post/271799/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../271789/index.html">Go Mutex Dancing</a></li>
<li><a href="../271791/index.html">Black Friday 2015</a></li>
<li><a href="../271793/index.html">Black Friday at hosters</a></li>
<li><a href="../271795/index.html">Use TSQL to play Baldu</a></li>
<li><a href="../271797/index.html">COUNT (*)</a></li>
<li><a href="../271803/index.html">Google spam confession</a></li>
<li><a href="../271805/index.html">Jira Automation on Groovy</a></li>
<li><a href="../271809/index.html">Investigation of the installation error Visual Studio 2015</a></li>
<li><a href="../271811/index.html">Development of fast mobile applications on Android. Part two</a></li>
<li><a href="../271813/index.html">New Acquisition Options and Free Visual Studio 2015 Offers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>