<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>8 myths about deduplication</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="It's time to look at all the myths and find out where the truth is in deduplication issues for data arrays. 



 Despite the fact that the deduplicati...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>8 myths about deduplication</h1><div class="post__text post__text-html js-mediator-article">  It's time to look at all the myths and find out where the truth is in deduplication issues for data arrays. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/21e/07b/2f0/21e07b2f0be84f2787d440dac66c2c01.jpg"></div><br><br>  Despite the fact that the deduplication technology has been known for quite some time, but only now the technologies used in modern data arrays have allowed her to experience a second birth.  In all modern data arrays, deduplication is currently used, but the presence of this function in the array does not mean that it will give significant advantages for your data. <br>  Unfortunately, a large number of administrators are taken "on faith" and believe that deduplication has unlimited possibilities. <br><a name="habracut"></a><br>  It doesn't matter if you are a <a href="https://www.hpe.com/us/en/storage/tier-1.html">tier-1</a> storage system administrator, <a href="https://www.hpe.com/us/en/storage/protection-retention.html">archive storage,</a> or <a href="https://www.hpe.com/us/en/storage/flash-hybrid.html">all-flash hybrid storage systems</a> , you will be interested in exploring myths and legends of deduplication to avoid annoying mistakes when designing or working with your storage systems. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  <font color="#00b882">Data reduction ratio: there are no miracles</font> </h2><br>  While deduplication has become available both for arrays that store your productive data, and for arrays that store backup data, the deduplication ratio on these arrays can be completely different.  Architects very often believe that the coefficient achieved on an archived array can be applied to productive storage. <br><br>  Deduplication is an automatic process that exists on many arrays of well-known manufacturers, but the potential coefficient you can get is different for arrays of different types.  As a result, for example, if you need an array of 100TB, and you consider a factor of 10: 1, then you will acquire storage under 10TB, or, say, if you evaluate the ratio as 2: 1, therefore, you will acquire storage of 50TB - in the end, these completely different approaches lead to a completely different purchase price!  You must, in practice, understand what coefficient you can get on your productive data before making a choice in favor of a particular model with a certain volume. <br><br>  When building configurations of data sets for various operational storage and backup storage tasks, one often faces difficulties in correctly determining the deduplication factor.  If you are interested in the subtleties of architectural design of arrays for deduplication, this discussion is for you. <br><br>  At a minimum, understanding at the basic level the 8 myths below will allow you to consciously understand deduplication and evaluate its coefficient for your data. <br><br><h2>  <font color="#00b882">Myth1.</font>  <font color="#00b882">Greater deduplication ratio provides more advantages for data storage.</font> </h2><br>  Is it true that if one vendor offers a deduplication ratio of 50: 1, this is five times better than the alternative 10: 1 offer?  You need to check and compare the total cost of ownership!  Deduplication can reduce resource requirements, but what are the potential savings in volume?  10: 1 reduces the size of the stored data (reduction ratio) by 90%, while a 50: 1 ratio increases this figure by 8% and gives a 98% reduction ratio (see chart below).  But this is only 8% of the difference ... <br><br>  In general, the higher the deduplication rate, the less advantages there are for reducing the amount of data, according to the <a href="https://en.wikipedia.org/wiki/Diminishing_returns">law of diminishing returns</a> .  The explanation of the law of diminishing returns may be as follows: the additionally applied costs of one factor (for example, the deduplication coefficient) are combined with a constant amount of another factor (for example, the amount of data).  Consequently, new additional costs for the current volume provide less and less resource savings. <br><br>  For example, you have an office where clerks work.  Over time, if you increase the number of clerks without increasing the size of the room, they will get in the way of each other‚Äôs feet, and perhaps the costs will exceed revenues. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2f0/8ba/0b1/2f08ba0b1471451da41e43b6c98f11c1.png"></div><br>  <i><font color="#999999">Fig.</font></i>  <i><font color="#999999">1 Increased deduplication ratio and reduced storage</font></i> <br><br><h2>  <font color="#00b882">Myth2.</font>  <font color="#00b882">There is a clear definition of the term "deduplication"</font> </h2><br>  Deduplication reduces the amount of stored data by removing duplicate data sequences from the pool.  Deduplication can be at the file level, block level, or at the application or content level.  Most products combine deduplication with compression to further reduce the amount of stored data.  While some manufacturers do not share these terms, some separate them and introduce terms such as ‚Äúcompaction‚Äù, which, in essence, is simply another name for ‚Äúdeduplication plus compression‚Äù.  Unfortunately, there is no unique definition of deduplication.  At the philistine level, it will be important to you how you can save on disk resources of your storage and backup system using deduplication.  Below we will cover this topic. <br><br>  Speaking about the line of HPE storage and backup systems, it is important to note that both storage systems and backup systems have interesting functionality that allows customers to save on disk resources. <br><br>  For the storage systems of operational data in the 3PAR array, a whole complex of utilities and mechanisms has been developed, which allows reducing the amount of data on the productive array. <br>  This complex is called HPE 3PAR Thin Technologies and consists of several mechanisms: <br><br><ul><li>  Thin Provisioning is the most efficiently implemented in 3PAR storage systems, since  disk space virtualization is applied and the array uses its internal map of the stored blocks, when the array is freed up, the array does not need to be audited (garbage collection), the released blocks are immediately ready for further use ... Allows you to select exactly as much volume as it needs, but take on the array only as much as this volume is physically recorded. <br><br></li><li>  Thin Conversion is a technology that allows real-time conversion of volumes from old HPE data sets (3PAR, EVA), EMC, Hitachi, and other manufacturers to thin volumes (which use Thin Provisioning) on ‚Äã‚Äãa 3PAR array with a reduction in volume on the target device. <br><br></li><li>  Thin Persistence and Thin Copy Reclamation is a technology that allows the 3PAR array at a very low granular level to understand the operation of all popular file systems and hypervisors and, in the case of deleting files (freeing up physical volume), transfer the corresponding blocks into a pool of free resources. <br><br></li><li>  Thin Deduplication is a technology that allows you to use deduplication on a productive array in real time, without a significant performance drop. </li></ul><br>  All three technologies are available free of charge and without time or volume restrictions for any 3PAR storage system, including those installed with our customers, <a href="https://www.hpe.com/h20195/v2/GetPDF.aspx/4AA3-8987ENW.pdf">for more information about these technologies</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/1d1/157/efe/1d1157efe3af49c7bd29a24f2c496352.png"></div><br>  <i><font color="#999999">Fig.</font></i>  <i><font color="#999999">2 Thin technologies in 3PAR arrays</font></i> <br><br><h2>  <font color="#00b882">Myth3.</font>  <font color="#00b882">The deduplication coefficients on the main array are the same as on the backup array.</font> </h2><br>  Storage designers use various deduplication algorithms.  Some of them require a lot of CPU resources and are more complicated than others, therefore, it shouldn‚Äôt be surprising that the deduplication ratio varies quite a lot. <br><br>  However, the biggest factor affecting what deduplication rate you get is how much duplicate data you have.  For this reason, backup systems containing multiple copies of the same data (daily, weekly, monthly, quarterly, annual) have such a high deduplication ratio.  While operational storage systems have an almost unique set of data, which almost always results in a low deduplication ratio.  In case you keep several copies of operational data on a productive array (for example, in the form of clones), this increases the deduplication factor, since  apply mechanisms to reduce storage space. <br><br>  Therefore, for operational storage arrays, having a ratio of 5: 1 is also wonderful, as having a ratio of 30: 1 or 40: 1 for backup systems, since this ratio depends on how many copies of productive data are stored on such arrays. <br><br>  If we consider the products of HPE, then in the HPE 3PAR online storage arrays the search for duplicate sequences (for example, when virtual machines are initialized or snapshots are created) is performed on the fly on a special ASIC chip installed in each array controller.  This approach allows to unload the central processors of the array for other, more important, tasks and makes it possible to enable deduplication for all data types, without fear that the array will ‚Äúslip‚Äù under load.  You <a href="https://www.hpe.com/h20195/v2/getpdf.aspx/4AA4-9573ENW.pdf">can read</a> more about deduplication on the 3PAR array. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2f1/530/dff/2f1530dffb164fa797c84a906da63c1a.png"></div><br>  <i><font color="#999999">Fig.3 Deduplication in 3PAR arrays is performed on a dedicated ASIC chip</font></i> <br><br>  The HPE portfolio also has hardware systems for backing up data with online variable-level deduplication at the block level - HPE StoreOnce.  System variants cover a full range of customers from the initial to the corporate level: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/510/30d/827/51030d827434459aa3ec37d6adb0599b.png"></div><br>  <i><font color="#999999">Fig.</font></i>  <i><font color="#999999">4 HPE StoreOnce Backup System Portfolio</font></i> <br><br>  You can read about the advantages of StoreOnce backup systems in <a href="https://habrahabr.ru/company/hpe/blog/276499/">other articles</a> . <br>  It may be interesting for customers that the HPE 3PAR and StoreOnce bundle allows you to simplify and speed up the process of transferring data from the production array to the backup system without using backup software or a dedicated backup server.  This bundle is called <a href="http://www8.hp.com/ru/ru/products/data-storage/storeonce-rmc.html">HPE StoreOnce RMC,</a> and more about it can also be found in <a href="https://habrahabr.ru/company/hpe/blog/276499/">our article</a> . <br><br><h2>  <font color="#00b882">Myth4.</font>  <font color="#00b882">All data is the same.</font> </h2><br>  There should be no doubt, all data is different.  Even data from the same application under different conditions will have different deduplication factors on the same array.  The deduplication ratio for specific data depends on various factors: <br><br><ul><li>  Data type ‚Äî data that has undergone software compression, metadata, media streams, and encrypted data always have a very low deduplication factor or are not compressed at all. <br><br></li><li>  The degree of data variability - the higher the amount of daily data changes at the block or file level, the lower the deduplication factor.  This is especially true for backup systems. <br><br></li><li>  Shelf life - the more copies you have, the higher the deduplication ratio. <br><br></li><li>  Backup policy - a policy of creating full day copies, as opposed to a policy with incremental or differential backups, will give a greater deduplication ratio (see the study below). </li></ul><br>  The table below gives a superficial estimate of the deduplication ratio, depending on the type of data.  It must be remembered that the deduplication coefficient on the main dataset will always be lower than the deduplication coefficient on the backup array. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f17/3f2/103/f173f2103eeb441e9d8e930c1dcaa18e.png"></div><br>  <i><font color="#999999">Fig.</font></i>  <i><font color="#999999">5 Estimation of deduplication ratio depending on data types and backup policies</font></i> <br><br><h2>  <font color="#00b882">Myth5.</font>  <font color="#00b882">Grouping disconnected data types increases deduplication</font> </h2><br>  In theory, you can mix completely different types of data in a shared storage pool for deduplication.  You may feel that you have a very large set of unique data and, therefore, the probability of finding previously already recorded blocks or objects in this pool will be great.  In practice, this approach does not work between unrelated data types, for example, between a database and Exchange, because the data formats are different, even if the same data set is stored.  Such a growing pool all the time becomes more complex and takes more time to search for repeating sequences.  The best practice is to separate pools by data type. <br><br>  For example, if you perform deduplication of a single virtual machine, you will get some coefficient, if you create multiple copies of this virtual machine and perform deduplication on this pool, your deduplication ratio will increase, and if you group several virtual machines by application type and create multiple copies of these virtual machines - the ratio will increase even more. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/e04/e75/f35/e04e75f355274ba3bec8c14e37280863.png"></div><br>  <i><font color="#999999">Fig. 6. The dependence of the deduplication ratio on the number of virtual machines in the pool and the size of the data block.</font></i> <br><br><h2>  <font color="#00b882">Myth6.</font>  <font color="#00b882">Your first backup will show you the expected deduplication ratio.</font> </h2><br>  This erroneous opinion appears when comparing the coefficients on the main array and the backup system.  If you are storing only one copy of the data, you may see some deduplication factor greater than one.  This ratio can increase if you increase the number of copies of very similar data, such as backup copies of the current database. <br><br>  The graph below shows a very typical deduplication ratio curve.  The application in this graph is the SAP HANA DB, but most applications show a similar curve.  Your first backup shows a certain deduplication, but the big savings come from data compression.  As soon as you start to keep more copies of the data in the pool, the deduplication ratio of the pool begins to grow (blue line).  The coefficient of an individual backup soars up after the creation of the second copy (orange line), since  at the block level, the first and second backups are very similar. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d46/528/f6f/d46528f6f94a458c998cea3649bfc6ed.png"></div><br>  <i><font color="#999999">Fig.</font></i>  <i><font color="#999999">7 Graph of the growth of the deduplication ratio with the increase in the number of backups ( <a href="https://www.hpe.com/h20195/v2/GetPDF.aspx/4AA6-4641ENW.pdf">more in the document</a> ).</font></i> <br><br><h2>  <font color="#00b882">Myth 7.</font>  <font color="#00b882">You cannot increase the level of deduplication</font> </h2><br>  It would be naive to argue that there is no possibility of artificially increasing the level of deduplication.  Another question - why?  If you show marketing numbers, this is one thing, if you need to create an effective backup scheme, this is another.  If the goal is to have the highest synthetic deduplication ratio, then you just need to store as many copies of the same data as possible.  Of course, this will increase the amount of stored data, but your deduplication coefficient will soar to the skies. <br><br>  Changing the backup policy also definitely affects the deduplication ratio, as can be seen in the example below for the actual data type, which compares the full backup policies and the combination of full copies with incremental and differential backups.  In the example below, the best rate is obtained when using only daily full backups.  However, on the same data, the storage capacity is quite different for all three approaches.  Therefore, it is necessary to understand that a change in your approach to backups can quite strongly affect the deduplication ratio and the physical amount of stored data. <br><br><h2>  <font color="#00b882">Myth8.</font>  <font color="#00b882">There is no way to predict the deduplication rate</font> </h2><br>  Every environment is unique and it is very difficult to accurately predict the actual deduplication ratio.  But nevertheless, manufacturers of backup systems produce sets of small utilities for basic storage systems and backup systems, which give an idea of ‚Äã‚Äãthe type of data, the backup policy, the retention period.  These utilities provide some insight into the expected deduplication ratio. <br><br>  Manufacturers also have an idea of ‚Äã‚Äãthe coefficients obtained from other customers in a similar environment and industry segment and can use this information to build a forecast.  While this does not guarantee that you will receive a similar coefficient on your data, at least you should look at these figures. <br><br>  But the most accurate prediction of the deduplication ratio is obtained in the course of testing on real data. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2c3/eba/86d/2c3eba86d791401b96f58a7515880437.png"></div><br>  <i><font color="#999999">Fig.</font></i>  <i><font color="#999999">8 Changes in the deduplication ratio and the amount of data occupied depending on the backup policy on the data of a specific customer</font></i> <br><br>  HPE has a set of utilities and sizers that allows you to predict (with some assumption) the amount of storage systems that customers need. <br><br><ol><li>  For online data storage there is a free program for assessing the current utilization of the array and estimating space savings, in case of <a href="http://www8.hp.com/ru/ru/products/data-storage/data-storage-products.html%3FcompURI%3D1284392">switching to 3PAR</a> . <br><br></li><li>  To assess resource utilization in the operational array and build a forecast for data growth on already installed systems, subject to permission to send an array of information about its status to HPE technical support: <a href="http://www.storefrontremote.com/">www.storefrontremote.com</a> <br><br></li><li>  A similar <a href="http://www8.hp.com/in/en/products/data-storage/storeonce-protected.html">program</a> to assess the utilization of backup systems. </li></ol><br>  It is also possible to estimate the estimated volume that we will receive after enabling deduplication on the 3PAR system in simulator mode. To do this, you need to run the evaluation command on the 3PAR online, transparently for the host: <br><br><pre><code class="hljs">checkvv -dedup_dryrun {  }</code> </pre> <br>  And get a preliminary assessment: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/5c9/f49/eb1/5c9f49eb185640ff96687f4205721fff.png"></div><br><br>  So, there is no magic behind the concept of deduplication, and dispelling the myths cited above will allow you to better understand what your data is capable of and allow you to predict the utilization of your arrays. <br><br>  It should be noted that the modern growth of SSD and reducing storage costs by 1GB on flash drives (and the cost is already equivalent to <a href="http://www8.hp.com/us/en/products/disk-storage/product-detail.html%3Foid%3D8737813">$ 1.5 per GB</a> ) pushes issues related to the effectiveness of deduplication to the second place for operational storage, but are becoming increasingly relevant for backup systems. <br><br>  By the way, there is an alternative vision of the future (without deduplication): Wikibon believes that eliminating copies of the same data is more effective than increasing the coefficient of deduplication and compression (see the <a href="http://wikibon.org/wiki/v/Evolution_of_All-Flash_Array_Architectures">link in the middle of the report</a> ), but this approach requires a radical introduction of a whole range of technical measures, changes in the entire infrastructure, rules for simultaneous operation of applications (processing, analytics) with data so that they do not reduce performance (when implementing good tools for working with SLA) and reliability. <br><br>  And, most importantly, if all this is implemented in the entire ecosystem - both software developers, and vendors, and CIO, then in a few years the savings from this will be greater than from deduplication. <br><br>  What school of thought will win - time will tell. <br><br><blockquote>  Materials <a href="https://community.hpe.com/t5/Around-the-Storage-Block/Myth-Busting-8-Common-Data-Deduplication-Misconceptions/ba-p/6901652%3Fdysig_tid%3D1360ea31461c41288850c325abab37cf">used</a> </blockquote></div><p>Source: <a href="https://habr.com/ru/post/311296/">https://habr.com/ru/post/311296/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../311286/index.html">.NET and CLR: An inside look</a></li>
<li><a href="../311288/index.html">History of programming languages: SQL life-long standardization</a></li>
<li><a href="../311290/index.html">‚ÄúSmart‚Äù weapons - the child of the blockchain, clouds and IoT</a></li>
<li><a href="../311292/index.html">RMM-systems: remote monitoring and corporate network management (with an example of quick implementation)</a></li>
<li><a href="../311294/index.html">Prototyping iOS animations with Framer</a></li>
<li><a href="../311298/index.html">The book "Graphic Design. Basic concepts "</a></li>
<li><a href="../311300/index.html">Do you remember a wonderful moment?</a></li>
<li><a href="../311302/index.html">How Yahoo moved from Flash to HTML5 to video</a></li>
<li><a href="../311304/index.html">CSN-Ajax programming template</a></li>
<li><a href="../311306/index.html">We work in the cloud based on Hyper-V, Part 2: Deploying Exchange Server</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>