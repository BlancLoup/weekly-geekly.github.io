<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Classifying Text with Java Neural Network</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="‚ÄúOur Lena is going on maternity leave,‚Äù the boss said, ‚Äúso we need to look for a replacement for the time of her absence. We will distribute part of t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Classifying Text with Java Neural Network</h1><div class="post__text post__text-html js-mediator-article">  ‚ÄúOur Lena is going on maternity leave,‚Äù the boss said, ‚Äúso we need to look for a replacement for the time of her absence.  We will distribute part of the tasks, but what about the task of redirecting user requests? <br><br>  <i><font color="grey">Lena is our technical support officer.</font></i>  <i><font color="grey">One of her responsibilities is the distribution of requests received by e-mail among specialists.</font></i>  <i><font color="grey">It analyzes the circulation and determines a number of characteristics.</font></i>  <i><font color="grey">For example, "Type of treatment": a system error, the user just needs a consultation, the user wants some new functionality.</font></i>  <i><font color="grey">Defines the "Functional module of the system": the accounting module, the module certification of equipment, etc.</font></i>  <i><font color="grey">Putting all these characteristics, she redirects the appeal to the appropriate specialist.</font></i> <br><br>  - Let me write a program that will do this automatically!  - I replied. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      At this fascinating novel we end and turn to the technical part. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/47f/ba6/f77/47fba6f779b44c6485eefd49af708f44.png"></div><a name="habracut"></a><br><h1>  We formalize the problem </h1><br><ul><li>  text of arbitrary length and content arrives at the input; </li><li>  the text is written by man, so it may contain typos, errors, abbreviations and generally be obscure </li><li>  This text needs to be somehow analyzed and classified according to several <u>unrelated</u> characteristics.  In our case, it was necessary to determine the essence of the appeal: an error message, a request for new functionality or consultation is needed, and also to define a functional module: salary calculation, accounting, warehouse management, etc .; </li><li>  only one value can be assigned for each characteristic; </li><li>  the set of possible values ‚Äã‚Äãfor each characteristic is known in advance; </li><li>  There are several thousand already classified complaints. </li></ul><br>  Having formalized the task and started developing for our specific needs, I realized that it would be better to develop a universal tool right away that would not be rigidly tied to any specific characteristics and the number of these characteristics.  As a result, a tool was born that can classify text according to arbitrary characteristics. <br><br>  I did not begin to look for ready-made solutions, because there was time and interest to make on my own, immersing in parallel the study of neural networks. <br><br><h1>  Selection of tools </h1><br>  I decided to develop in Java. <br><br>  I used <a href="https://ru.wikipedia.org/wiki/SQLite">SQLite</a> and <a href="https://ru.wikipedia.org/wiki/H2">H2</a> as a DBMS.  Also useful <a href="https://ru.wikipedia.org/wiki/Hibernate_(%25D0%25B1%25D0%25B8%25D0%25B1%25D0%25BB%25D0%25B8%25D0%25BE%25D1%2582%25D0%25B5%25D0%25BA%25D0%25B0)">Hibernate</a> . <br><br>  <a href="https://github.com/encog">From here I</a> took a ready implementation of the neural network (Encog Machine Learning Framework).  I decided to use the neural network as a classifier, and not, for example, the <a href="https://ru.wikipedia.org/wiki/%25D0%259D%25D0%25B0%25D0%25B8%25D0%25B2%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25B1%25D0%25B0%25D0%25B9%25D0%25B5%25D1%2581%25D0%25BE%25D0%25B2%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9_%25D0%25BA%25D0%25BB%25D0%25B0%25D1%2581%25D1%2581%25D0%25B8%25D1%2584%25D0%25B8%25D0%25BA%25D0%25B0%25D1%2582%25D0%25BE%25D1%2580">naive Bayes classifier</a> , because, firstly, theoretically, the neural network should be more precise.  Secondly, I just wanted to play around with neural networks. <br><br>  To read the Excel data file, some <a href="https://poi.apache.org/">Apache POI</a> libraries were needed for training. <br><br>  Well, for tests, I traditionally used <a href="https://ru.wikipedia.org/wiki/JUnit">JUnit 4</a> + <a href="https://en.wikipedia.org/wiki/Mockito">Mockito</a> . <br><br><h1>  A little bit of theory </h1><br>  I will not describe the theory in detail on neural networks, as there is plenty of it ( <a href="https://habrahabr.ru/post/312450/">there is a</a> good introduction material here).  In short and in a simple way: the network has an input layer, hidden layers and an output layer.  The number of neurons in each layer is determined by the developer in advance and, after learning the network, cannot be changed: if at least one neuron is added / subtracted, then the network must be retrained.  For each neuron of the input layer, a normalized number (usually from 0 to 1) is fed.  Depending on the set of these numbers on the input layer, after certain calculations, on each output neuron, we also get a number from 0 to 1. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/b6f/0e1/838/b6f0e18382784586a497684739824152.png"></div><br>  The essence of network training is that the network adjusts its link weights involved in the calculations, so that when a set of numbers on the input layer is known in advance, a set of numbers on the output layer will be known in advance.  Adjusting the weights is an iterative process and occurs until the network reaches the specified accuracy on the training set or until it reaches a certain number of iterations.  After training, it is assumed that the network will produce a <i>close</i> to the reference set of numbers on the output layer, if the input set file numbers, <i>similar</i> to the one that was in the training set. <br><br><h1>  Go to practice </h1><br>  The first task was to figure out how to convert the text to a form that can be transmitted to the input of a neural network.  But first it was necessary to determine the size of the input layer of the network, since it must be specified in advance.  Obviously, the input layer must be of such a size that any text can be ‚Äúfit‚Äù into this layer.  The first thing that comes to mind is that the <b>size of the input layer should be equal to the size of the dictionary</b> containing the words / phrases of which the texts consist. <br><br>  There are many ways to build a dictionary.  You can, for example, stupidly take all the words of the Russian language and this will be our dictionary.  But this approach is not suitable, because the size of the input layer will be so huge that there will not be enough resources for a simple workstation to create a neural network.  For example, imagine that our dictionary consists of 100,000 words, then we have 100,000 neurons in the input layer;  for example, 80,000 in a hidden layer (the method for determining the dimension of a hidden layer is described below) and 25 in the output layer.  Then, just to store the link weights, you will need ~ 60 GB of RAM: ((100,000 * 80,000) + (80,000 * 25)) * 64 bits (type double in JAVA).  Secondly, such an approach is not suitable, because the texts can use specific terminology, which is absent in dictionaries. <br><br>  This suggests the conclusion that the dictionary should be built only from those words / phrases that our analyzed texts consist of.  <b>It is important to understand</b> that in this case there must be a sufficiently large amount of training data to build the dictionary. <br><br>  One of the ways of ‚Äúpulling out‚Äù words / phrases (even more precisely, fragments) from the text is called building an <a href="https://ru.wikipedia.org/wiki/N-%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B0">N-gram</a> .  The most popular are unigrams and bigrams.  There are also character N-grams - this is when the text is divided not into separate words, but into segments of characters of a certain length.  It is difficult to say in advance which of the N-grams will be more effective in a specific task, so you need to experiment. <br><table><tbody><tr><th>  <b>Text</b> </th><th>  <b>Unigram</b> </th><th>  <b>Bigram</b> </th><th>  <b>3-character N-grams</b> </th></tr><tr><td>  This text should be broken apart. </td><td>  ["This", "text", "must", "be", "broken", "on", "parts"] </td><td>  ["This text", "the text should", "should be", "be broken", "broken into", "into parts"] </td><td>  [‚ÄúThis‚Äù, ‚Äút‚Äù, ‚Äúex‚Äù, ‚Äút d‚Äù, ‚ÄúVolzh‚Äù, ‚Äúen‚Äù, ‚Äúlife‚Äù, ‚Äúl p‚Äù, ‚Äúaz‚Äù, ‚Äúit‚Äù, ‚Äúon‚Äù, ‚Äúhour "," Ty "] </td></tr></tbody></table><br>  I decided to move from simple to complex and first developed the Unigram class. <br><br><div class="spoiler">  <b class="spoiler_title">Unigram class</b> <div class="spoiler_text"><pre><code class="java hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Unigram</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">implements</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">NGramStrategy</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> Set&lt;String&gt; </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getNGram</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(String text)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (text == <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>) { text = <span class="hljs-string"><span class="hljs-string">""</span></span>; } <span class="hljs-comment"><span class="hljs-comment">// get all words and digits String[] words = text.toLowerCase().split("[ \\pP\n\t\r$+&lt;&gt;‚Ññ=]"); Set&lt;String&gt; uniqueValues = new LinkedHashSet&lt;&gt;(Arrays.asList(words)); uniqueValues.removeIf(s -&gt; s.equals("")); return uniqueValues; } }</span></span></code> </pre> <br></div></div><br>  As a result, after processing ~ 10,000 texts, I received a dictionary of ~ 32,000 elements.  Having analyzed the resulting dictionary diagonally, I realized that there is a lot of superfluous in it, which we should get rid of.  To do this, do the following: <br><br><ol><li>  Removed all non-alphabetic characters (numbers, punctuation marks, arithmetic operations, etc.), since they, as a rule, do not carry any meaning. </li><li>  I drove the words through the procedure of <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2582%25D0%25B5%25D0%25BC%25D0%25BC%25D0%25B8%25D0%25BD%25D0%25B3">stemming</a> (I used <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2582%25D0%25B5%25D0%25BC%25D0%25BC%25D0%25B5%25D1%2580_%25D0%259F%25D0%25BE%25D1%2580%25D1%2582%25D0%25B5%25D1%2580%25D0%25B0">Stemmer Porter</a> for the Russian language).  By the way, a useful side effect of this procedure is the ‚Äúunisexation‚Äù of the texts, that is, ‚Äúmade‚Äù and ‚Äúmade‚Äù will be transformed into ‚Äúmade‚Äù. </li><li>  First, I wanted to identify and correct typos and grammatical errors.  I read about the Oliver Algorithm ( <a href="http://php.net/manual/ru/function.similar-text.php">similar_text</a> function in PHP) and <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D1%2581%25D1%2582%25D0%25BE%25D1%258F%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%259B%25D0%25B5%25D0%25B2%25D0%25B5%25D0%25BD%25D1%2588%25D1%2582%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25B0">Levenshtein Distance</a> .  But the task was solved much easier, albeit with an error: I decided that if an element of the N-gram is found less than in 4 texts from the training set, then we do not include this element in the dictionary as useless in the future.  In this way, I got rid of most typos, grammatical words, ‚Äúcoarse words‚Äù and just very rare words.  But it should be understood that if in future texts typos and grammatical errors are often encountered, the classification accuracy of such texts will be lower and then you will still have to implement a mechanism for correcting typos and grammatical errors.  <b>Important</b> : such a "trick" with throwing out rarely occurring words is permissible with a large amount of data for training and building a dictionary. </li></ol><br>  All this is implemented in the classes FilteredUnigram and VocabularyBuilder. <br><br><div class="spoiler">  <b class="spoiler_title">Class FilteredUnigram</b> <div class="spoiler_text"><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">FilteredUnigram</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">implements</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">NGramStrategy</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> Set&lt;String&gt; </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getNGram</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(String text)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// get all significant words String[] words = clean(text).split("[ \n\t\r$+&lt;&gt;‚Ññ=]"); // remove endings of words for (int i = 0; i &lt; words.length; i++) { words[i] = PorterStemmer.doStem(words[i]); } Set&lt;String&gt; uniqueValues = new LinkedHashSet&lt;&gt;(Arrays.asList(words)); uniqueValues.removeIf(s -&gt; s.equals("")); return uniqueValues; } private String clean(String text) { // remove all digits and punctuation marks if (text != null) { return text.toLowerCase().replaceAll("[\\pP\\d]", " "); } else { return ""; } } }</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">VocabularyBuilder class</b> <div class="spoiler_text"><pre> <code class="java hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">VocabularyBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> NGramStrategy nGramStrategy; VocabularyBuilder(NGramStrategy nGramStrategy) { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (nGramStrategy == <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> IllegalArgumentException(); } <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.nGramStrategy = nGramStrategy; } <span class="hljs-function"><span class="hljs-function">List&lt;VocabularyWord&gt; </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getVocabulary</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(List&lt;ClassifiableText&gt; classifiableTexts)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (classifiableTexts == <span class="hljs-keyword"><span class="hljs-keyword">null</span></span> || classifiableTexts.size() == <span class="hljs-number"><span class="hljs-number">0</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> IllegalArgumentException(); } Map&lt;String, Integer&gt; uniqueValues = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> HashMap&lt;&gt;(); List&lt;VocabularyWord&gt; vocabulary = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ArrayList&lt;&gt;(); <span class="hljs-comment"><span class="hljs-comment">// count frequency of use each word (converted to n-gram) from all Classifiable Texts // for (ClassifiableText classifiableText : classifiableTexts) { for (String word : nGramStrategy.getNGram(classifiableText.getText())) { if (uniqueValues.containsKey(word)) { // increase counter uniqueValues.put(word, uniqueValues.get(word) + 1); } else { // add new word uniqueValues.put(word, 1); } } } // convert uniqueValues to Vocabulary, excluding infrequent // for (Map.Entry&lt;String, Integer&gt; entry : uniqueValues.entrySet()) { if (entry.getValue() &gt; 3) { vocabulary.add(new VocabularyWord(entry.getKey())); } } return vocabulary; } }</span></span></code> </pre><br></div></div><br>  Example of compiling a dictionary: <br><table><tbody><tr><th>  <b>Text</b> </th><th>  <b>Filtered unigram</b> </th><th>  <b>Vocabulary</b> <br></th></tr><tr><td>  Need to find a sequence of 12 tasks. </td><td>  necessary, consistent, tasks </td><td rowspan="3">  necessary, knight, consistently, tasks, for, arbitrarily, add, transpose </td></tr><tr><td>  Task for arbitrary </td><td>  tasks for arbitrary </td></tr><tr><td>  Add arbitrary transposition </td><td>  add, arbitrary, transpose </td></tr></tbody></table><br>  To build Bigrams, I also wrote a class right away, so that after the experiments, to stop the choice on the variant that gives the best result in terms of the dictionary size and classification accuracy. <br><br><div class="spoiler">  <b class="spoiler_title">Class bigram</b> <div class="spoiler_text"><pre> <code class="java hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Bigram</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">implements</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">NGramStrategy</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> NGramStrategy nGramStrategy; Bigram(NGramStrategy nGramStrategy) { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (nGramStrategy == <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> IllegalArgumentException(); } <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.nGramStrategy = nGramStrategy; } <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> Set&lt;String&gt; </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getNGram</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(String text)</span></span></span><span class="hljs-function"> </span></span>{ List&lt;String&gt; unigram = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ArrayList&lt;&gt;(nGramStrategy.getNGram(text)); <span class="hljs-comment"><span class="hljs-comment">// concatenate words to bigrams // example: "How are you doing?" =&gt; {"how are", "are you", "you doing"} Set&lt;String&gt; uniqueValues = new LinkedHashSet&lt;&gt;(); for (int i = 0; i &lt; unigram.size() - 1; i++) { uniqueValues.add(unigram.get(i) + " " + unigram.get(i + 1)); } return uniqueValues; } }</span></span></code> </pre><br></div></div><br>  I have decided to stop at this for now, but further processing of words for compiling a dictionary is also possible.  For example, you can define synonyms and bring them to a single form, you can analyze the similarity of words, you can even invent something of your own, etc.  But, as a rule, this does not give a significant increase in the accuracy of the classification. <br><br>  Okay, go ahead.  The size of the input layer of the neural network, which will be equal to the number of elements in the dictionary, we calculated. <br><br>  The size of the output layer for our task will be equal to the number of possible values ‚Äã‚Äãfor the characteristic.  For example, we have 3 possible values ‚Äã‚Äãfor the "Type of address" characteristic: system error, user assistance, new functionality.  Then the number of neurons in the output layer will be three.  When training the network, for each value of the characteristic it is necessary to determine in advance the reference unique set of numbers that we expect to receive on the output layer: 1 0 0 for the first value, 0 1 0 for the second, 0 0 1 for the third ... <br><br>  As for the number and dimension of hidden layers, then there is no specific recommendation.  The sources say that the optimal size for each specific task can be calculated only experimentally, but for a narrowing network it is recommended to start with one hidden layer, the size of which varies between the size of the input layer and the output layer.  To begin with, I created one hidden layer of 2/3 in size from the input layer, and then I experimented with the number of hidden layers and their sizes.  Here you can read a little theory and recommendations on this issue.  It also describes how much data there should be for training. <br><br>  So, we have created a network.  Now we need to decide on how we will convert the text into figures suitable for ‚Äúfeeding‚Äù the neural network.  To do this, we need to convert the text into <i>a text vector</i> .  First you need to assign each word in the dictionary a unique <i>word vector</i> , the size of which should be equal to the size of the dictionary.  After learning the network to change the vector for words can not.  Here‚Äôs how it looks for a 4-word dictionary: <br><table><tbody><tr><th>  <b>Word in dictionary</b> </th><th>  <b>Word vector</b> </th></tr><tr><td>  Hello </td><td>  1 0 0 0 </td></tr><tr><td>  as </td><td>  0 1 0 0 </td></tr><tr><td>  affairs </td><td>  0 0 1 0 </td></tr><tr><td>  you </td><td>  0 0 0 1 </td></tr></tbody></table><br>  The procedure of converting text into a vector of text implies the addition of vectors of words used in the text: the text ‚Äúhow do you say hello?‚Äù Will be converted into a vector ‚Äú1 1 0 1‚Äù.  We can already feed this vector to the input of the neural network: each individual number for each individual neuron of the input layer (the number of neurons is just equal to the size of the text vector). <br><br><div class="spoiler">  <b class="spoiler_title">Method that calculates the text vector</b> <div class="spoiler_text"><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] getTextAsVectorOfWords(ClassifiableText classifiableText) { <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] vector = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[inputLayerSize]; <span class="hljs-comment"><span class="hljs-comment">// convert text to nGram Set&lt;String&gt; uniqueValues = nGramStrategy.getNGram(classifiableText.getText()); // create vector // for (String word : uniqueValues) { VocabularyWord vw = findWordInVocabulary(word); if (vw != null) { // word found in vocabulary vector[vw.getId() - 1] = 1; } } return vector; }</span></span></code> </pre><br></div></div><br>  <a href="https://habrahabr.ru/post/149605/">Here</a> and <a href="https://habrahabr.ru/company/meanotek/blog/256593/">here</a> you can additionally read about the preparation of the text for analysis. <br><br><h1>  Classification accuracy </h1><br>  Having experimented with different dictionary formation algorithms and with different number and dimension of hidden layers, I stopped at this option: use FilteredUnigram with cutting off rarely used words to form a dictionary;  we make 2 hidden layers with the dimension 1/6 of the dictionary size - the first layer and 1/4 of the size of the first layer - the second layer. <br><br>  After training on ~ 20,000 texts (and this is very small for a network of this size) and on the network run of 2,000 reference texts, we have: <br><table><tbody><tr><th>  <b>N-gram</b> </th><th>  <b>Accuracy</b> </th><th>  <b>Dictionary size</b> <br>  (without rarely used words) <br></th></tr><tr><td>  Unigram </td><td>  58% </td><td>  ~ 25 000 </td></tr><tr><td>  Filtered unigram </td><td>  73% </td><td>  ~ 1,200 </td></tr><tr><td>  Bigram </td><td>  63% </td><td>  ~ 8 000 </td></tr><tr><td>  Filtered bigram </td><td>  69% </td><td>  ~ 3,000 </td></tr></tbody></table><br>  This is accuracy for one feature.  If accuracy is needed for several characteristics, the calculation formulas are as follows: <br><br><ol><li>  The probability of guessing <b>all the</b> characteristics at <b>once</b> is equal to the product of the probabilities of guessing each characteristic. </li><li>  The probability of guessing <b>at least one</b> characteristic is equal to the difference between the unit and the product of the probabilities of incorrectly determining each characteristic. </li></ol><br>  Example: <br><br>  Assume that the accuracy of determining one characteristic is 65%, the second characteristic is 73%.  Then the accuracy of determining <b>both at once</b> is equal to 0.65 * 0.73 = 0.4745 = <b>47.45%</b> , and the accuracy of determining <b>at least one</b> characteristic is 1- (1-0.65) * (1-0.73) = 0 , 9055 = <b>90.55%</b> . <br><br>  It is quite a good result for a tool that does not require preliminary manual processing of incoming data. <br><br>  There is one ‚Äúbut‚Äù: the accuracy strongly depends on the similarity of texts that should be assigned to different categories: the less similar the texts from the ‚ÄúSystem Errors‚Äù category are to the texts from the ‚ÄúNeed Help‚Äù category, the more accurate the classification will be.  Therefore, with the same network and dictionary settings in different tasks and on different texts there can be significant differences in accuracy. <br><br><h1>  Final program </h1><br>  As already said, I decided to write a universal program that is not tied to the number of characteristics by which the text should be classified.  I will not describe in detail all the details of the development here, I will describe only the algorithm of the program, and the link to the sources will be at the end of the article. <br><br>  The general algorithm of the program: <br><br><ol><li>  When you first start the program asks for the XLSX file with the training data.  A file can consist of one or two sheets.  On the first sheet, the data for training, on the second - the data for subsequent testing of the accuracy of the network.  The sheet structure is the same.  The first column should contain the analyzed text.  Subsequent columns (there may be any number of them) should contain the values ‚Äã‚Äãof the characteristics of the text.  The first line should contain the names of these characteristics. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/4c6/7d2/967/4c67d2967a6e4425b1bcb0b0d8f2b3da.png"></div><br></li><li>  Based on this file, a dictionary is built, a list of characteristics and unique values ‚Äã‚Äãvalid for each characteristic are determined.  All this is stored in the repository. </li><li>  A separate neural network is created for each characteristic. </li><li>  All created neural networks are trained and stored. </li><li>  Upon subsequent launch, all saved trained neural networks are loaded.  The program is ready for text analysis. </li><li>  The resulting text is processed independently by each neural network, and the total result is output as a value for each characteristic. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/e90/82c/889/e9082c88938f4158921e8c40f5cef6ea.png"></div><br></li></ol><br>  In the plans: <br><br><ul><li>  Add other types of classifiers (for example, a <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">convolutional neural network</a> and a <a href="https://ru.wikipedia.org/wiki/%25D0%259D%25D0%25B0%25D0%25B8%25D0%25B2%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25B1%25D0%25B0%25D0%25B9%25D0%25B5%25D1%2581%25D0%25BE%25D0%25B2%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9_%25D0%25BA%25D0%25BB%25D0%25B0%25D1%2581%25D1%2581%25D0%25B8%25D1%2584%25D0%25B8%25D0%25BA%25D0%25B0%25D1%2582%25D0%25BE%25D1%2580">naive Bayes classifier</a> ); </li><li>  Try using a dictionary consisting of a combination of unigrams and bigrams. </li><li>  Add a mechanism for removing typos and grammatical errors. </li><li>  Add a dictionary of synonyms. </li><li>  Eliminate too frequent words, as they are usually informational noise. </li><li>  Add weights to individual significant words so that their influence in the analyzed text is greater. </li><li>  A little to alter the architecture, simplifying the API, so that you can bring the main functionality into a separate library. </li></ul><br><h1>  Little about source </h1><br>  Sources can be useful as an example of using some design patterns.  There is no sense in putting it into a separate article, but I don‚Äôt want to be silent about them, since I share my experience, so let it be in this article. <br><br><ul><li>  Pattern <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2582%25D1%2580%25D0%25B0%25D1%2582%25D0%25B5%25D0%25B3%25D0%25B8%25D1%258F_(%25D1%2588%25D0%25B0%25D0%25B1%25D0%25BB%25D0%25BE%25D0%25BD_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B5%25D0%25BA%25D1%2582%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D1%258F)">"Strategy"</a> .  The NGramStrategy interface and the classes that implement it. </li><li>  Pattern <a href="https://ru.wikipedia.org/wiki/%25D0%259D%25D0%25B0%25D0%25B1%25D0%25BB%25D1%258E%25D0%25B4%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BB%25D1%258C_(%25D1%2588%25D0%25B0%25D0%25B1%25D0%25BB%25D0%25BE%25D0%25BD_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B5%25D0%25BA%25D1%2582%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D1%258F)">"Observer"</a> .  The LogWindow and Classifier classes implement the Observer and Observable interfaces. </li><li>  Pattern <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25B5%25D0%25BA%25D0%25BE%25D1%2580%25D0%25B0%25D1%2582%25D0%25BE%25D1%2580_(%25D1%2588%25D0%25B0%25D0%25B1%25D0%25BB%25D0%25BE%25D0%25BD_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B5%25D0%25BA%25D1%2582%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D1%258F)">"Decorator"</a> .  Bigram class. </li><li>  Pattern "Simple Factory".  The NGramStrategy interface's getNGramStrategy () method. </li><li>  Pattern <a href="https://ru.wikipedia.org/wiki/%25D0%25A4%25D0%25B0%25D0%25B1%25D1%2580%25D0%25B8%25D1%2587%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BC%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_(%25D1%2588%25D0%25B0%25D0%25B1%25D0%25BB%25D0%25BE%25D0%25BD_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B5%25D0%25BA%25D1%2582%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D1%258F)">"Factory Method"</a> .  The getNgramStrategy () method of the NGramStrategyTest class. </li><li>  Pattern <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25B1%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BA%25D1%2582%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2584%25D0%25B0%25D0%25B1%25D1%2580%25D0%25B8%25D0%25BA%25D0%25B0_(%25D1%2588%25D0%25B0%25D0%25B1%25D0%25BB%25D0%25BE%25D0%25BD_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B5%25D0%25BA%25D1%2582%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D1%258F)">"Abstract Factory"</a> .  JDBCDAOFactory class. </li><li>  Pattern (anti-pattern?) <a href="https://ru.wikipedia.org/wiki/%25D0%259E%25D0%25B4%25D0%25B8%25D0%25BD%25D0%25BE%25D1%2587%25D0%25BA%25D0%25B0_(%25D1%2588%25D0%25B0%25D0%25B1%25D0%25BB%25D0%25BE%25D0%25BD_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B5%25D0%25BA%25D1%2582%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D1%258F)">"Loner"</a> .  EMFProvider class. </li><li>  Pattern <a href="https://ru.wikipedia.org/wiki/%25D0%25A8%25D0%25B0%25D0%25B1%25D0%25BB%25D0%25BE%25D0%25BD%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BC%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_(%25D1%2588%25D0%25B0%25D0%25B1%25D0%25BB%25D0%25BE%25D0%25BD_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B5%25D0%25BA%25D1%2582%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D1%258F)">"Template method"</a> .  The initializeIdeal () method of the NGramStrategyTest class.  True, there is not quite a classic of its application. </li><li>  Pattern <a href="https://ru.wikipedia.org/wiki/Data_Access_Object">"DAO"</a> .  Interfaces CharacteristicDAO, ClassifiableTextDAO, etc. </li></ul><br>  Full source code: <a href="https://github.com/RusZ/TextClassifier">https://github.com/RusZ/TextClassifier</a> <br><br>  Constructive suggestions and criticism are welcome. <br><br>  PS: Do not worry about Lena - this is a fictional character. </div><p>Source: <a href="https://habr.com/ru/post/332078/">https://habr.com/ru/post/332078/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../332068/index.html">Automation blocking Petya / NonPetya</a></li>
<li><a href="../332070/index.html">We answer readers' questions: what is the IBM Watson cognitive system, and how does it work?</a></li>
<li><a href="../332072/index.html">‚ÄúYou, thunderstorm, threaten, and we hold on to each other!‚Äù - tale about how I saved the ADSL modem</a></li>
<li><a href="../332074/index.html">Autoencoders in Keras, Part 6: VAE + GAN</a></li>
<li><a href="../332076/index.html">Dynamic instrumentation is not easy, but trivial *: we write yet another instrumentation for American Fuzzy Lop</a></li>
<li><a href="../332080/index.html">Hackers and exchanges: how to attack the sphere of finance</a></li>
<li><a href="../332082/index.html">Integration of 1C with DLL using Python</a></li>
<li><a href="../332084/index.html">Work with heterogeneous containers with C ++ 17</a></li>
<li><a href="../332086/index.html">Automatic application deployment with Maven and Wildfly</a></li>
<li><a href="../332088/index.html">Tasks with interviews. Three adequate tasks to "think"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>