<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>What artificial intelligence researchers think about the possible risks associated with it</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The risks associated with AI, I became interested in 2007. At that time, most people‚Äôs reaction to this topic was something like this: ‚ÄúVery funny, co...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>What artificial intelligence researchers think about the possible risks associated with it</h1><div class="post__text post__text-html js-mediator-article">  The risks associated with AI, I became interested in 2007.  At that time, most people‚Äôs reaction to this topic was something like this: ‚ÄúVery funny, come back when someone other than Internet idiots will believe it.‚Äù <br><br>  In the years that followed, several extremely intelligent and influential figures, including <a href="http://www.cnet.com/news/bill-gates-is-worried-about-artificial-intelligence-too/">Bill Gates</a> , <a href="http://www.bbc.com/news/technology-30290540">Stephen Hawking</a> and <a href="http://www.forbes.com/sites/ericmack/2015/01/15/elon-musk-puts-down-10-million-to-fight-skynet/">Ilon Mask</a> , publicly shared their concerns about the risks of AI, followed by hundreds of other intellectuals, from Oxford philosophers to cosmologists from MIT and investors from Silicon Valley .  And we are back. <br><br>  Then the reaction changed to: "Well, a couple of some scientists and businessmen can believe it, but it is unlikely that they will be real experts in this field, really versed in the situation." 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      From here came statements like the article in Popular Science ‚Äú <a href="http://www.popsci.com/bill-gates-fears-ai-ai-researchers-know-better">Bill Gates is afraid of AI, but AI researchers know better</a> ‚Äù: <br><blockquote>  After talking with AI researchers ‚Äî real researchers who hardly make such systems work at all, let alone work well, it becomes clear that they are not afraid that superintelligence will suddenly sneak up on them, either now or in the future. .  Despite all the frightening stories told by Mask, researchers are in no hurry to build protective rooms and countdown self-destruction. </blockquote><a name="habracut"></a><br>  Or, as they wrote on Fusion.net in the article ‚Äú <a href="http://fusion.net/story/54583/the-case-against-killer-robots-from-a-guy-actually-building-ai/">Objection about killer robots from a person who actually develops AI</a> ‚Äù: <br><blockquote>  Andrew Angie professionally develops AI systems.  He led the course on AI at Stanford, developed AI at Google, and then moved to Baidu, a Chinese search engine, to continue his work in the front ranks of applying AI to real-world problems.  So, when he hears about how Ilon Musk or Stephen Hawking - people who are not familiar with modern technology - are talking about AI, potentially capable of destroying humanity, you can almost hear him cover his face. </blockquote><br>  Ramez Naam [Ramez Naam] from Marginal Revolution repeats about the same thing in the article ‚Äú <a href="http://marginalrevolution.com/marginalrevolution/2015/05/what-do-ai-researchers-think-of-the-risks-of-ai.html">What do researchers think about the risks of AI?</a> ‚Äù: <br><blockquote>  Ilon Musk, Stephen Hawking and Bill Gates have recently expressed fears that the development of AI can implement the "killer AI" scenario, and potentially lead to the extinction of humanity.  They do not belong to AI researchers and, as far as I know, did not work directly with AI.  What do real AI researchers think about the risks of AI? </blockquote><br>  He quotes the words of specially selected AI researchers, as well as the authors of other stories - and then he stops, without mentioning any opinions different from this. <br><br>  But they exist.  AI researchers, including leaders in this field, actively expressed concerns about the risks of AI and beyond intelligence, from the very beginning.  I will begin by listing these people, in peak to the list of Naam, and then move on to why I do not consider this a ‚Äúdiscussion‚Äù in the classical sense expected from listing the list of luminaries. <br><br>  The criteria for my list are the following: I mention only the most prestigious researchers, or professors of science in good institutions with many quotations of scientific works, or very respected scientists from the industry who work for large companies and have a good track record.  They do AI and machine learning.  They have several strong statements in support of some point of view about the onset of a singularity or a serious risk from the AI ‚Äã‚Äãin the near future.  Some of them wrote about this work or book.  Some simply expressed their thoughts, believing that this was an important topic worth exploring. <br><br>  If someone disagrees with the inclusion of a person in this list or thinks that I forgot something important, let me know. <br><br>  * * * * * * * * * * <br><br>  <a href="http://en.wikipedia.org/wiki/Stuart_J._Russell">Stuart Russell</a> is a professor of computer science at Berkeley, a winner of the IJCAI Computers And Thought Award, a researcher at the computer mechanization association, a researcher at the American Academy of Advanced Scientific Research, the director of the Center for Intelligent Systems, a Blaze Pascal award winner, etc.  etc.  Co-author of the book " <a href="http://smile.amazon.com/gp/product/0136042597/ref%3Das_li_tl%3Fie%3DUTF8%26camp%3D1789%26creative%3D390957%26creativeASIN%3D0136042597%26linkCode%3Das2%26tag%3Dslastacod-20%26linkId%3DKI5CQ6KYVQTMTRB6">AI: a modern approach,</a> " a classic textbook used in 1,200 universities around the world.  On <a href="https://www.cs.berkeley.edu/~russell/research/future/">his website</a> he writes: <br><blockquote>  In the field of AI, 50 years have been developing under the banner of the assumption that the smarter the better.  With this you need to combine concern for the benefit of humanity.  The argument is simple: <br><br>  1. The AI ‚Äã‚Äãis likely to be successfully created. <br>  2. Unlimited success leads to great risks and great benefits. <br>  3. What can we do to increase the chances of getting benefits and avoiding risks? <br><br>  Some organizations are already working on these issues, including the Institute for the Future of Humanity at Oxford, the Center for Existential Risk Studies at Cambridge (CSER), the Institute for the Study of Machine Intelligence at Berkeley and the Institute for Future Life at Harvard / MIT (FLI).  I am on the advisory boards for CSER and FLI. <br><br>  In the same way as nuclear fusion researchers considered the problem of restricting nuclear reactions as one of the main problems in their field, the development of the AI ‚Äã‚Äãfield will inevitably raise control and safety issues.  Researchers are already beginning to raise questions, from purely technical (basic problems of rationality and utility, etc.) to broadly philosophical. </blockquote><br>  On <a href="http://edge.org/response-detail/26157">edge.org,</a> he describes a similar point of view: <br><blockquote>  As explained by Steve Omohandro, Nick Bostrom and others, the discrepancy in values ‚Äã‚Äãwith decision-making systems, the possibilities of which are constantly increasing, can lead to problems - perhaps even the problems of the scale of extinction of the species, if the machines prove to be more capable than people.  Some believe that in the coming centuries, imaginable risks for humanity are not foreseen, perhaps forgetting that the time difference between Rutherford‚Äôs confident statement that nuclear energy can never be recovered, and the invention of Silard initiated by the neutrons of the nuclear chain reaction took less than 24 hours . </blockquote><br>  He also tried to represent these ideas in an academic environment, <a href="https://intelligence.org/2014/05/13/christof-koch-stuart-russell-machine-superintelligence/">pointing out</a> : <br><blockquote>  I find that the main people in this industry, who have never previously expressed fears, think to themselves that this problem should be taken very seriously, and the sooner we take its seriousness, the better it will be. </blockquote><br>  <a href="http://en.wikipedia.org/wiki/David_A._McAllester">David McAllister</a> is a professor and senior fellow at Toyota Institute of Technology, affiliated with the University of Chicago, who previously worked in the faculties of MIT and Cornell Institute.  He is an employee of the American Association of AI, published more than a hundred papers, conducted research in the areas of machine learning, programming theory, automatic decision making, AI planning, computational linguistics, and had a serious impact on the algorithms of the famous Deep Blue chess computer.  According to an <a href="http://technews.acm.org/archives.cfm%3Ffo%3D2009-11-nov/nov-06-2009.html">article</a> in the Pittsburgh Tribune Review: <br><blockquote>  Chicago professor David McAllister considers the inevitable emergence of the ability of fully automatic intelligent machines to develop and create smarter versions of themselves, that is, the onset of an event known as [technological] singularity.  The singularity will allow the machines to become infinitely intelligent, leading to an ‚Äúincredibly dangerous scenario,‚Äù he says. </blockquote><br>  In his blog " <a href="https://machinethoughts.wordpress.com/2014/08/10/friendly-ai-and-the-servant-mission/">Thoughts on cars</a> ", he writes: <br><blockquote> Most computer scientists refuse to talk about real progress in the field of AI.  I think that it would be more reasonable to say that no one is able to predict when an AI will be obtained that is comparable with the human mind.  John McCarthy once told me that when asked about how soon human-level AI will be created, he replies that it is five to five hundred years old.  Makarty was smart.  Given the uncertainties in this area, it is reasonable to consider the problem of a friendly AI ... <br><br>  In the early stages, a generalized AI will be safe.  However, the early stages of the AIS will be an excellent test ground for AI in the role of servant or other options friendly AI.  An experimental approach is also advertised by Ben G√∂rzel in a good post on his blog.  If we are waiting for an era of safe and not very smart OII, then we will have time to think about more dangerous times. </blockquote><br>  He was a member of the AAAI Panel On Long-Term AI Futures expert group dedicated to the long-term outlook for AI, chaired the long-term monitoring committee and is <a href="http://blog.computationalcomplexity.org/2009/07/singularity.html">described as follows</a> : <br><blockquote>  McAllister spoke with me about the approach of "singularity", an event when computers become smarter than people.  He did not name the exact date of her offensive, but said that this could happen in the next couple of decades, and in the end it will definitely happen.  Here are his views on singularity.  Two significant events will occur: operational rationality, in which we can easily talk to computers, and an AI chain reaction, in which the computer can improve itself without help, and then repeat it again.  The first event we notice in the systems of automatic assistance, which really will help us.  Later it will be really interesting to communicate with computers.  And in order for computers to be able to do everything that people can do, it is necessary to wait for the onset of the second event. </blockquote><br>  <a href="http://en.wikipedia.org/wiki/Hans_Moravec">Hans Moravec</a> is a former professor at the Institute of Robotics at Carnegie Mellon University, named after him is <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B0%25D1%2580%25D0%25B0%25D0%25B4%25D0%25BE%25D0%25BA%25D1%2581_%25D0%259C%25D0%25BE%25D1%2580%25D0%25B0%25D0%25B2%25D0%25B5%25D0%25BA%25D0%25B0">the</a> <a href="http://seegrid.com/product/vision-guided-vehicles/">Moravek</a> <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B0%25D1%2580%25D0%25B0%25D0%25B4%25D0%25BE%25D0%25BA%25D1%2581_%25D0%259C%25D0%25BE%25D1%2580%25D0%25B0%25D0%25B2%25D0%25B5%25D0%25BA%25D0%25B0">paradox</a> , the founder of <a href="http://seegrid.com/product/vision-guided-vehicles/">SeeGrid Corporation</a> , a company engaged in computer vision systems for industrial applications.  His work " <a href="http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/676">sensor synthesis in the lattice of certainty of mobile robots</a> " was quoted over a thousand times, and he was invited to write an article for the British Encyclopedia on Robotics, at a time when articles in encyclopedias were written by world experts in this field, rather than hundreds of anonymous Internet commentators. <br><br>  He is also the author of the book " <a href="http://smile.amazon.com/gp/product/0195136306/ref%3Das_li_tl%3Fie%3DUTF8%26camp%3D1789%26creative%3D390957%26creativeASIN%3D0195136306%26linkCode%3Das2%26tag%3Dslastacod-20%26linkId%3DYSNWMXT4POFD425Q">Robot: From a Simple Machine to a Transcendental Mind</a> ", which Amazon describes as follows: <br><blockquote>  In this exciting book, Hans Moravek predicts that by 2040 cars will approach the intellectual level of people, and by 2050 they will surpass us.  But although Moravec predicts the end of an era of human domination, his vision of this event is not so gloomy.  He does not isolate himself from the future, in which machines rule the world, but accepts him, and describes an amazing point of view, according to which intelligent robots will become our evolutionary descendants.  Moravec believes that at the end of this process, ‚Äúimmense cyberspace will unite with inhuman supermind and deal with matters as far from people as human affairs are from bacteria.‚Äù </blockquote><br>  Shane Leg is co-founder of <a href="http://en.wikipedia.org/wiki/Google_DeepMind">DeepMind Technologies</a> , an AI startup bought in 2014 for $ 500 million by Google.  He received his doctorate at the Institute of AI them.  Daile Moule in Switzerland, and also worked in the Division of Computational Neurobiology named after  Gatsby in London.  At the end of the thesis ‚Äúmachine superintelect‚Äù he <a href="http://www.vetta.org/documents/Machine_Super_Intelligence.pdf">writes</a> : <br><blockquote>  If ever there is something that can come close to absolute power, it will be a super-intelligent machine.  By definition, she will be able to achieve a large number of goals in a wide variety of environments.  If we prepare carefully for such an opportunity in advance, we will be able not only to avoid a catastrophe, but also to begin an era of prosperity, unlike any other that existed before. </blockquote><br>  In a subsequent interview, he <a href="http://future.wikia.com/wiki/Scenario:_Shane_Legg">says</a> : <br><blockquote>  AI is now in the same place where the Internet was in 1988.  Requirements for machine learning are required in special applications (search engines like Google, hedge funds and bioinformatics), and their number is growing every year.  I think that around the middle of the next decade this process will become widespread and noticeable.  The AI ‚Äã‚Äãboom should occur in the 2020 area, followed by a decade of rapid progress, possibly after market correction.  A person‚Äôs AI level will be created around mid-2020, although many people will not accept this event.  After this, the risks associated with a developed AI will receive practical implementation.  I will not say about the "singularity", but they expect that at some point after the creation of the AIS, crazy things will start to happen.  This is somewhere between 2025 and 2040. </blockquote><br>  He and his co-founders <a href="http://en.wikipedia.org/wiki/Demis_Hassabis">Demis Khasabis</a> and <a href="http://en.wikipedia.org/wiki/Mustafa_Suleyman">Mustafa Suleiman</a> signed a petition to the Institute for the Future Life regarding the risks of AI, and one of their conditions for joining Google was that the company agrees to organize an <a href="http://www.forbes.com/sites/privacynotice/2014/02/03/inside-googles-mysterious-ethics-board/">AI ethics council</a> to research these issues. <br><br>  Steve Omohundro is a former computer science professor at the University of Illinois, the founder of the computer vision and training group at the Center for the Study of Complex Systems, and the inventor of various important developments in machine learning and computer vision.  He worked on lip-reading robots, StarLisp, a parallel programming language, and geometric learning algorithms.  He now heads <a href="http://selfawaresystems.com/">Self-Aware Systems</a> , "a team of scientists working to ensure that intelligent technology benefits humanity."  His work, "the <a href="https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf">basics of AI motivation,</a> " helped bring about the field of machine ethics, since he noted that super-intelligent systems would be directed towards potentially dangerous goals.  He's writing: <br><blockquote>  We have shown that all advanced AI systems are likely to have a set of basic motivations.  It is extremely important to understand these motivations in order to create technologies that ensure a positive future for humanity.  Yudkovsky called for the creation of a "friendly AI".  To do this, we need to develop a scientific approach for ‚Äúutilitarian development‚Äù, which will allow us to develop socially useful functions that will lead to the desired sequences.  The rapid pace of technological progress suggests that these problems may soon become critical. </blockquote><br>  Under the <a href="http://steveomohundro.com/scientific-contributions/">link</a> you can find his articles on the topic "rational AI for the common good." <br><br>  <a href="http://www.doc.ic.ac.uk/~mpsha/">Murray Shanahan</a> received his Ph.D. in computer science from Cambridge, and now he is a professor of cognitive robotics at Imperial College London.  He published works in such areas as robotics, logic, dynamical systems, computational neuroscience, philosophy of mind.  He is currently working on the book " <a href="http://smile.amazon.com/gp/product/0262527804/ref%3Das_li_tl%3Fie%3DUTF8%26camp%3D1789%26creative%3D390957%26creativeASIN%3D0262527804%26linkCode%3Das2%26tag%3Dslastacod-20%26linkId%3D7NK3HKZJTIAT2WU5">Technological Singularity</a> ", which will be published in August.  Amazon's advertising summary is as follows: <br><blockquote>  Shanakhan describes technological advances in the field of AI, both made under the influence of knowledge from biology and developed from scratch.  He explains that when a human-level AI is created ‚Äî a theoretically possible, but difficult task ‚Äî the transition to a super-intelligent AI will be very fast.  Shanahan reflects on what the existence of supramental machines for such areas as personality, responsibility, rights and individuality can lead to.  Some representatives of the supramental AI can be created for the benefit of the person, some can get out of control.  (That is, Siri or HAL?) The singularity represents for humanity both an existential threat and an existential possibility to overcome its limitations.  Shanahan makes it clear that if we want to achieve a better result, we need to imagine both possibilities. </blockquote><br>  <a href="http://en.wikipedia.org/wiki/Marcus_Hutter">Markus Hatter</a> is a professor of computer science at the National Australian University.  Prior to that, he worked at the Institute of AI them.  Dale Moule in Switzerland and the National Institute of Informatics and Communications in Australia, also worked on stimulated learning, Bayesian conclusions, the theory of computational complexity, Solomon's theory of inductive predictions, computer vision and genetic profiles.  He also wrote a lot about singularity.  In the article ‚Äú <a href="http://www.hutter1.net/publ/singularity.pdf">Can Intellect Explode?</a> ‚Äù He writes: <br><blockquote>  This century can witness a technological explosion, the scale of which deserves the name of a singularity.  The default scenario is a community of interacting rational individuals in the virtual world, simulated on computers with hyperbolically increasing computing resources.  This is inevitably accompanied by an explosion of speed, measured by physical time, but not necessarily an explosion of intelligence.  If the virtual world is populated with free and interacting individuals, evolutionary pressure will lead to the emergence of individuals with increasing intelligence, who will compete for computing resources.  The end point of this evolutionary acceleration of the intellect can be a community of the most intelligent individuals.  Some aspects of this singular community can theoretically be studied using modern scientific tools.  Long before the appearance of this singularity, even placing this virtual community in our imagination, one can imagine the emergence of differences, such as, for example, a sharp drop in the value of an individual, which can lead to drastic consequences. </blockquote><br>  <a href="http://en.wikipedia.org/wiki/J%25C3%25BCrgen_Schmidhuber">J√ºrgen Schmidhuber</a> is a professor of AI at the University of Lugano and a former professor of cognitive robotics at the Munich University of Technology.  He develops some of the most advanced neural networks in the world, works on evolutionary robotics and the theory of computational complexity, and serves as a research assistant at the European Academy of Arts and Sciences.  In the book " <a href="https://books.google.com/books%3Fid%3DgVxGAAAAQBAJ%26pg%3DPA8%26lpg%3DPA8%26dq%3D%2522juergen%2Bschmidhuber%2522%2Bsingularity%26source%3Dbl%26ots%3DvpfD_69J8B%26sig%3DHOhUv_JABWg0kXpsMfuCcX7zB8I%26hl%3Den%26sa%3DX%26ei%3D801dVY6LK5CQoQSCoIOoAQ%26ved%3D0CCQQ6AEwATgK">Hypotheses of Singularities,</a> " he argues that "with the continuation of the existing trends, we will face an intellectual explosion in the next few decades."  When he was directly asked about the risks associated with AI at Reddit AMA, he <a href="https://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%25C3%25BCrgen_schmidhuber_ama/cp65ico">replied</a> : <br><blockquote>  Stewart Russell‚Äôs anxiety about AI seems reasonable.  Can we do something to control the influence of AI?  In response, hidden in a nearby thread, I pointed out: at first glance, the recursive self-improvement <a href="https://en.wikipedia.org/wiki/G%25C3%25B6del_machine">of G√∂del‚Äôs machines</a> offers us a way to create a future superintelligence.  The self-repair of the G√∂del machine is in some sense optimal.  It will only make changes in its code that are proven to improve the situation, according to the original utility function.  That is, in the beginning you have the opportunity to send it on the right path.  But other people can equip their G√∂del cars with other utility functions.  They will compete.  And the resulting ecology of individuals, some utility functions will be better suited to our physical universe than others, and they <a href="http://people.idsia.ch/~juergen/2012futurists.pdf">will find a niche for survival</a> . " </blockquote><br>  <a href="http://en.wikipedia.org/wiki/Richard_S._Sutton">Richard Saton</a> is a professor and member of the iCORE committee at the University of Alberta.  He serves as a research fellow at the Association for the Development of AI, co-author of the <a href="http://smile.amazon.com/gp/product/0262193981/ref%3Das_li_tl%3Fie%3DUTF8%26camp%3D1789%26creative%3D390957%26creativeASIN%3D0262193981%26linkCode%3Das2%26tag%3Dslastacod-20%26linkId%3DHCZ4TIUPMZNBFWEC">most popular textbook on stimulated learning</a> , the discoverer of the method of temporal differences, one of the most important in this area. <br><br>  In his <a href="http://futureoflife.org/misc/ai_conference">report</a> at the conference on AI, organized by the Institute for the Future, Suton argued that "there is a real chance that even during our life" an AI will be created, comparable in intelligence to a person, and added that this AI "will not obey us" , ‚ÄúWill compete and cooperate with us‚Äù, and that ‚Äúif we create super-intelligent slaves, we will get super-intelligent opponents‚Äù.  In conclusion, he said that ‚Äúwe need to think over the mechanisms (social, legal, political, cultural) to provide the desired outcome‚Äù, but that ‚Äúinevitably ordinary people will become less important‚Äù.  He also mentioned similar problems at <a href="http://www.vetta.org/2011/05/sutton-on-human-level-ai/">the</a> Gadsby Institute <a href="http://www.vetta.org/2011/05/sutton-on-human-level-ai/">presentation</a> .  In addition, there are such lines in <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D0%25B5%25D0%25BA,_%25D0%2593%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25BD">Glenn Beck‚Äôs</a> book: ‚ÄúRichard Suton, one of the greatest specialists in AI, predicts an explosion of intelligence somewhere by the middle of the century.‚Äù <br><br>  <a href="http://www.doc.ic.ac.uk/~ajd/">Andrew Davison</a> is a professor of machine vision at Imperial College London, leading the robotic vision group and robotics laboratory Dyson, and the inventor of the computerized localization and markup system MonoSLAM.  On his website he <a href="http://www.doc.ic.ac.uk/~ajd/singularity.html">writes</a> : <br><blockquote>  At the risk of being in an unpleasant position in certain academic circles, to which, I hope, I belong, since 2006 I began to take the idea of ‚Äã‚Äãtechnological singularity with full seriousness: the exponential growth of technology can lead to the emergence of superhuman AI and other developments that are extremely strong will change the world in a surprisingly near future (perhaps in the next 20-30 years).  I was influenced by both the reading of Kurzweil's book ‚ÄúThe Singularity is Close‚Äù (I found it sensational, but generally intriguing), and my own observations of the incredible progress in science and technology that has been happening recently, especially in the field of computer vision and robotics, with whom I am personally connected.  Modern decision-making systems, training, methods based on Bayesian probability theory, coupled with the exponentially growing capabilities of inexpensive computer processors, are becoming capable of demonstrating amazing properties similar to people's skills, particularly in the field of computer vision. <br><br>  It is hard to imagine all the possible consequences of what is happening, positive or negative, and I will try to enumerate only the facts without hitting the opinions (although I myself am not in the camp of super-optimists).  I seriously believe that it is worth talking to scholars and the public about this.  I will make a list of "signs of singularity" and will update it.  It will be little news about technology or news that confirms my feelings that technology is surprisingly evolving faster and faster, and very few people are now thinking about the consequences of this. </blockquote><br>  <a href="http://en.wikipedia.org/wiki/Alan_Turing">Alan Turing</a> and <a href="http://en.wikipedia.org/wiki/I._J._Good">Irving John Good</a> need no introduction.  Turing invented the mathematical foundations of computational science and named after him the Turing machine, Turing completeness and Turing test.  Hood worked with Turing at Bletchley Park, helped create one of the first computers and invented many well-known algorithms, for example, the fast algorithm for the discrete Fourier transform, known as the FFT algorithm.  In his work "Can digital machines think?" Turing writes: <br><blockquote>  Let's assume that such machines can be created, and consider the consequences of creating them.  Such an act will undoubtedly be met with hostility, if only we have not advanced in religious tolerance since the times of Galileo.  The opposition will consist of intellectuals who are afraid of losing their jobs.  But it is likely that intellectuals will be wrong.  It will be possible to tackle many things in trying to keep our intellect at the level of standards set by the machines, since after running the machine method it does not have to take a long time until the machines exceed our insignificant possibilities.  At some point, one should expect the machines to take over control. </blockquote><br>  While working in the Atlas computer lab in the 60s, Goode developed this idea in <a href="http://webdocs.cs.ualberta.ca/~sutton/Good65ultraintelligent.pdf">Reasonings Concerning the First Ultra-Intellectual Machine</a> : <br><blockquote>  We define an ultra-intelligent machine as a machine capable of surpassing a person in any intellectual work.  Since the development of such machines is one of the examples of intellectual work, an ultra-intelligent machine can develop better quality machines.  As a result, no doubt, one should expect an ‚Äúexplosion of intellect‚Äù, and the human intellect will remain far behind.  Therefore, the invention of the ultra-intelligent machine is the last of the inventions that need to be made by man. </blockquote><br>  * * * * * * * * * * <br><br>  It bothers me that this list may create the impression that there is some kind of dispute between ‚Äúbelievers‚Äù and ‚Äúskeptics‚Äù in a given area, during which they tear each other apart.  But I did not think so. <br><br>  When I read articles about skeptics, I always meet two arguments.  First, we are still very far from human-level AI, not to mention superintelligence, and there is no obvious way to reach such heights.  Secondly, if you demand bans on AI research, you are an idiot. <br><br>  I completely agree with both points.  Like the leaders of the movement of risk AI. <br><br>  A survey among AI researchers ( <a href="http://www.nickbostrom.com/papers/survey.pdf">Muller &amp; Bostrom, 2014</a> ) showed that, on average, they give 50% because human-level AIs will appear by 2040, and 90% that they will appear by 2075. On average, 75% of them believe that superintelligence (‚Äúmachine intelligence, seriously surpassing the capabilities of every person in most professions‚Äù) will appear within 30 years after the advent of human-level AI.  And although the technique of this survey raises some doubts, if we accept its results, it turns out that most AI researchers agree that something that should be worried will appear in one or two generations. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But the director of the Institute of Machine Intelligence Luc Muelhauser and the director of the Institute for the Future of Humanity Nick Bostrom, stated that their predictions for the development of AI are much later than the predictions of scientists participating in the survey. If we study the </font></font><a href="http://lesswrong.com/lw/e79/ai_timeline_prediction_data/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">data on the predictions of AI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> from Stuart Armstrong, it is clear that, in general, the estimates for the time of the appearance of AI, made by the supporters of AI, do not differ from the estimates made by AI skeptics. Moreover, the most long-term prediction in this table belongs to Armstrong himself. However, Armstrong is now working at the Institute for the Future of Humanity, </font></font><a href="http://smile.amazon.com/gp/product/B00IB4N4KU/ref%3Das_li_tl%3Fie%3DUTF8%26camp%3D1789%26creative%3D390957%26creativeASIN%3DB00IB4N4KU%26linkCode%3Das2%26tag%3Dslastacod-20%26linkId%3DL3KTZD5JGHSRYCSQ"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">drawing attention to the risks of AI</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and the need to research the goals of superintelligence.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The difference between supporters and skeptics is not in their assessments of when we should expect the appearance of human-level AI, but when we need to start preparing for it. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Which brings us to the second point. The position of skeptics, it seems, is that although we probably should send a couple of smart people to work on a preliminary assessment of the problem, there is no need to panic or ban AI research. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fans of AI, however, insist that although we absolutely do not need to panic or ban research of AI, it is probably worth sending a couple of smart people to work on a preliminary assessment of the problem. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yang Lekun is perhaps the most ardent skeptic of the risks of AI. He was abundantly quoted in an </font></font><a href="http://www.popsci.com/bill-gates-fears-ai-ai-researchers-know-better"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article on Popular Science</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , in a </font></font><a href="http://marginalrevolution.com/marginalrevolution/2015/05/what-do-ai-researchers-think-of-the-risks-of-ai.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">post on Marginal Revolution.</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, and he also spoke with </font></font><a href="http://www.kdnuggets.com/2014/02/exclusive-yann-lecun-deep-learning-facebook-ai-lab-part2.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">KDNuggets</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href="http://spectrum.ieee.org/automaton/robotics/artificial-intelligence/facebook-ai-director-yann-lecun-on-deep-learning"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IEEE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> on the ‚Äúinevitable questions of singularity,‚Äù which he himself describes as ‚Äúso far away that science fiction can be written about them.‚Äù </font><font style="vertical-align: inherit;">But when he was asked to clarify his position, he stated:</font></font><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ilon Musk is very worried about the existential threats to mankind (so he builds rockets to send people to colonize other planets). </font><font style="vertical-align: inherit;">And although the risk of an AI uprising is very small and very distant in the future, we need to think about it, develop precautions and rules. </font><font style="vertical-align: inherit;">Just as the bioethics committee appeared in the 1970s and 1980s, before the extensive use of genetics, we need committees on AI ethics. </font><font style="vertical-align: inherit;">But, as Joshua Benjio wrote, we still have plenty of time.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horwitz is another expert, often referred to as the main voice of skepticism and constraints. His point of view was described in articles such as ‚ÄúThe </font></font><a href="http://www.bbc.com/news/technology-31023741"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">director of Microsoft‚Äôs research department thinks that an AI that got out of control won't kill us,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Äù and ‚Äú </font></font><a href="http://www.stuff.co.nz/technology/digital-living/65551109/Nothing-to-fear-from-artificial-intelligence-Microsofts-Eric-Horvitz"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Eric Horvitz of Microsoft believes that AI should not be afraid</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .‚Äù But what he said in a longer interview with NPR:</font></font><br><blockquote> :  ,    -   ,   .  ,    ,       747.   ,    ? <br><br> : .  ,     ,       . <br><br> :   -  ,  ,             ,         .      .  ,       ,    ? <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Horwitz: I truly believe that the stakes are high enough to spend time and energy on actively seeking solutions, even if the likelihood of such events is low. </font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This, in general, coincides with the position of many of the most zealous AI risk agitators. </font><font style="vertical-align: inherit;">With such friends and enemies are not needed. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The article in Slate, " </font></font><a href="http://www.slate.com/articles/technology/future_tense/2014/10/elon_musk_artificial_intelligence_why_you_shouldn_t_be_afraid_of_ai.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Do not be afraid of AI,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> " also surprisingly puts a lot in the right light:</font></font><br><blockquote>    ,              .    ,    ¬´¬ª      .     . <br><br> -,    Skynet   .       ,    ¬´  ¬ª,  -    ,          - ,       . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yang Lekun, head of the AI ‚Äã‚Äãlaboratory on Facebook, briefly summed up this idea in a post on Google+ in 2013: Hype hurts AI. </font><font style="vertical-align: inherit;">Over the past five decades, the hype has killed the AI ‚Äã‚Äãfour times. </font><font style="vertical-align: inherit;">It needs to be stopped. "Lekun and others are rightly afraid of hype. The inability to meet the high expectations imposed by science fiction leads to serious cuts in budgets for AI research.</font></font></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Scientists working on AI are smart people. </font><font style="vertical-align: inherit;">They are not interested in falling into the classic political traps, in which they would be divided into camps and would accuse each other of panicking or ostrichism. </font><font style="vertical-align: inherit;">Apparently, they are trying to find a balance between the need to begin preliminary work related to the danger looming somewhere far away, and the risk of causing such a strong hype that will hit them.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I do not want to say that there is no difference of opinion about how soon you need to start addressing this issue. Basically, it all comes down to whether it is possible to say that ‚Äúwe will solve the problem when we encounter it,‚Äù or expect such an unexpected take-off, due to which everything gets out of control, and for which, therefore, you need to prepare in advance. I see less and less than I would like evidence that most AI researchers who have their own opinion understand the second possibility. What can I say, if even in the article on Marginal Revolution </font></font><a href="https://medium.com/backchannel/ai-wont-exterminate-us-it-will-empower-us-5b7224735bf3"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">quotes an expert</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> who says that superintelligence does not pose a big threat, because "smart computers will not be able to set goals for themselves", although anyone who read </font></font><a href="http://smile.amazon.com/gp/product/0199678111/ref%3Das_li_tl%3Fie%3DUTF8%26camp%3D1789%26creative%3D390957%26creativeASIN%3D0199678111%26linkCode%3Das2%26tag%3Dslastacod-20%26linkId%3DKXW3Z5OQNRXKMZNC"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bostrom</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">knows that this is the whole problem. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is still a lot of work to be done. </font><font style="vertical-align: inherit;">But not to specifically select articles in which "real experts on AI are not worried about superintelligence."</font></font></div><p>Source: <a href="https://habr.com/ru/post/402379/">https://habr.com/ru/post/402379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../402369/index.html">How to measure the speed of a 3D printer - its hotand. And not only speed</a></li>
<li><a href="../402371/index.html">The State Duma is considering amendments to legislation simplifying the blocking of "mirrors" of prohibited sites</a></li>
<li><a href="../402373/index.html">What gives the "genetics of microbiota"</a></li>
<li><a href="../402375/index.html">8-kilowatt 4-channel AC switch with consumption measurement. Part 1</a></li>
<li><a href="../402377/index.html">What do your smartphones about car USB charging think</a></li>
<li><a href="../402381/index.html">How to recruit astronauts</a></li>
<li><a href="../402383/index.html">Saw, Shura: how we created the design of the mobile application tracker for dogs Mishiko</a></li>
<li><a href="../402385/index.html">Why should we expect a boom in the creation of robots for commercial premises</a></li>
<li><a href="../402387/index.html">3D pen for 3D printer, fixed jogging</a></li>
<li><a href="../402389/index.html">MPAA and RIAA plan to recover data from failed Megaupload file sharing hard drives</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>