<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Hierarchical classification of sites in Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! As mentioned in the last article , an important part of our work is user segmentation. How do we do it? Our system sees users as unique cook...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Hierarchical classification of sites in Python</h1><div class="post__text post__text-html js-mediator-article">  <b>Hi, Habr!</b>  As mentioned in the <a href="http://habrahabr.ru/company/dca/blog/260845/">last article</a> , an important part of our work is user segmentation.  How do we do it?  Our system sees users as unique cookies identifiers that we or our data providers assign to them.  This id looks like this: <br><br><blockquote><code>42bcfae8-2ecc-438f-9e0b-841575de7479</code> </blockquote> <br>  These numbers are keys in different tables, but the initial value is, first of all, the URL of the pages on which this cookie was downloaded, search queries, and sometimes some additional information that the provider gives - IP address, timestamp, customer information And so on.  This data is rather heterogeneous, so the URL is the most valuable for segmentation.  When creating a new segment, the analyst indicates a certain list of addresses, and if some cookie lights up on one of these pages, then it falls into the corresponding segment.  It turns out that almost 90% of the working time of such analysts is spent on finding the right set of URLs - as a result of hard work with search engines, Yandex.Wordstat and other tools. <br> <a href="http://habrahabr.ru/company/dca/blog/261677/"><img src="https://habrastorage.org/files/936/3bb/701/9363bb7011d94abb8020b51a18ba76a9.png" alt="logo" width="800"></a> <br>  Having thus received more than a thousand segments, we realized that this process should be automated and simplified as much as possible, while being able to monitor the quality of the algorithms and provide analysts with a convenient interface for working with the new tool.  Under the cut, I will tell you how we solve these problems. <br><a name="habracut"></a><br>  So, as it was possible to understand from the introduction, in fact, it is not the users that need to be segmented, but the pages of Internet sites, - our analytic engine will automatically distribute users to the received segments. <br><br>  It is worth saying a few words about how the segments are represented in our DMP.  The main feature of a set of segments is that their structure is hierarchical, that is, is a tree.  We do not impose any restrictions on the depth of the hierarchy, since each next level allows us to more accurately describe the portrait of the user's interests.  Here are examples of several branches of the hierarchy: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/files/610/5b1/039/6105b103990d4c38801ad9b918400e5a.png" alt="Taxonomy Example 1"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/49c/1b8/e3c/49c1b8e3ce32407988696e7ee04d4467.png" alt="Taxonomy Example 2"></div><br>  If a user has visited a site that tells how to feed puppies or train a cat to the tray, it is likely that he is the owner of this animal, and it makes sense to show him the corresponding advertisements - about the veterinary clinic or a new line of feed.  And if, before that, he chose premium brands in his online store, then he could have relatively high incomes, and he could advertise more expensive services - a feline psychologist or a canine barber. <br><br>  In general, having some kind of manual taxonomy of the subject of Internet pages, it was necessary to create a service that, given an input URL, would output a list of topics suitable for it.  We solve the problem of determining the subject of a web page as a multi-class classification according to the ‚Äúone against all‚Äù scheme, that is, each classifier of its taxonomy has its own classifier.  Classifiers are recursively bypassed, starting from the root of the topic tree and further down along those branches that are identified as suitable at each current level. <br><br><h2>  Device classifier </h2><br>  The frontend classifier is a <a href="http://flask.pocoo.org/">Flask</a> application, which holds an object in memory.  In essence, it deals only with data preparation, deserialization of objects of trained classifiers of the class <a href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html">sklearn.ensemble.RandomForestClassifier</a> stored in mongoDB, execution of their predict_proba () methods and processing of results in accordance with the existing taxonomy.  Taxonomy with queries and test samples, by the way, is also stored in mongoDB. <br><br>  The application waits for POST requests by the URI of the form: <br><br><ul><li>  localhost / text / </li><li>  localhost / url / </li><li>  localhost / tokens / </li></ul><br><pre> <code class="python hljs">classifier = RecursiveClassifier() app = Flask(__name__) @app.route(<span class="hljs-string"><span class="hljs-string">"/text/"</span></span>, methods=[<span class="hljs-string"><span class="hljs-string">'POST'</span></span>]) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_text_topics</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span>  data = json.loads(request.get_data().decode())  text = data[<span class="hljs-string"><span class="hljs-string">'text'</span></span>]  <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Response(json.dumps(classifier.get_text_topics(text), indent=<span class="hljs-number"><span class="hljs-number">4</span></span>), mimetype=<span class="hljs-string"><span class="hljs-string">'application/json'</span></span>) @app.route(<span class="hljs-string"><span class="hljs-string">"/url/"</span></span>, methods=[<span class="hljs-string"><span class="hljs-string">'POST'</span></span>]) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_url_topics</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span>  data = json.loads(request.get_data().decode())  url = data[<span class="hljs-string"><span class="hljs-string">'url'</span></span>]  html = html_get(url)  text = clean_html(html)  <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Response(json.dumps(classifier.get_text_topics(text,url), indent=<span class="hljs-number"><span class="hljs-number">4</span></span>), mimetype=<span class="hljs-string"><span class="hljs-string">'application/json'</span></span>) @app.route(<span class="hljs-string"><span class="hljs-string">"/tokens/"</span></span>, methods=[<span class="hljs-string"><span class="hljs-string">'POST'</span></span>]) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_tokens_topics</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span>  data = json.loads(request.get_data().decode())  <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Response(json.dumps(classifier.get_tokens_topics(data), indent=<span class="hljs-number"><span class="hljs-number">4</span></span>), mimetype=<span class="hljs-string"><span class="hljs-string">'application/json'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">"__main__"</span></span>:  app.run(host=<span class="hljs-string"><span class="hljs-string">"0.0.0.0"</span></span>, port=config.server_port)</code> </pre> <br>  Obtaining, for example, a certain URL, the application downloads its body, extracts the text directly from there, and initiates a recursive taxonomy traversal from the root to the children.  Recursion occurs only for those nodes of the tree for which at the current step the probability of a page being owned by this node exceeds the threshold specified in the config. <br><br>  Data preparation includes one-time tokenization of the text, calculation of the frequency characteristics of words and a feature conversion for each classifier in accordance with the weights for the tokens selected at the stage of the selection selection (more on this later).  This uses the ‚Äú <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag of words</a> ‚Äù model, that is, the relative position of words in the text is ignored. <br><br><h2>  Training classifier </h2><br>  The learning process performs the backend.  When changes are made to the taxonomy or to the list of requests for a node, the texts of new pages are downloaded and tokenized, then the learning algorithm is launched for all topics at the same level as the changed one.  All the ‚Äúbrothers‚Äù of the classifier are retrained along with the changed, because the training sample for the whole level is the same - the texts of the sites from the TOP-50 Bing search results, found from all nodes of the brothers and all their children.  For each topic, positive examples are sites that match their needs and the needs of their children, all other pages are negative examples.  The result is stored in the <a href="http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.html">pandas.DataFrame</a> object. <br><br>  The resulting sets of tokens with labels are then randomly distributed to the training sample (70%), the sample for the feature selection (15%) and the test sample (15%) - it is stored in mongoDB. <br><br><h2>  Feature selection </h2><br>  The selection of the most informative tokens is carried out in the learning process using the dg metric, and this is how it is implemented: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dg</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(arr)</span></span></span><span class="hljs-function">:</span></span>  avg = scipy.average(arr)  summ = <span class="hljs-number"><span class="hljs-number">0.0</span></span>  <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> arr:      summ += (s - avg) ** <span class="hljs-number"><span class="hljs-number">2</span></span>  summ /= len(arr)  <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> math.sqrt(summ) / avg</code> </pre> <br>  And this is how it is called for a set of tokens: <br><br><pre> <code class="python hljs">token_cnt = Counter() topic_cnt = Counter() topic_token_cnt = defaultdict(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span>: Counter()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> row <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> dataset.index:  topic = dataset[<span class="hljs-string"><span class="hljs-string">'topic'</span></span>][row]  topic_cnt[topic] += <span class="hljs-number"><span class="hljs-number">1</span></span>  <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> token <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> set(dataset[<span class="hljs-string"><span class="hljs-string">'tokens'</span></span>][row]):      token_cnt[token] += <span class="hljs-number"><span class="hljs-number">1</span></span>      topic_token_cnt[topic][token] += <span class="hljs-number"><span class="hljs-number">1</span></span> topics = list(topic_cnt.keys()) token_distr = {} <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> token <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> token_cnt:  distr = []  <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> topic <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> topics:      distr.append(topic_token_cnt[topic][token] / topic_cnt[topic])  token_distr[token] = distr token_dg = {} <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> token <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> token_distr:  token_dg[token] = dg(token_distr[token]) * math.log(token_cnt[token])</code> </pre> <br>  Thus, the significance of words is assessed from the point of view of all texts of the training set.  Since the frequency characteristics of the occurrence of words in the collection are used, it is interesting to look at <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">the Zipf distribution</a> .  Here it is (green color shows linear interpolation of data): <br><br><img src="https://habrastorage.org/files/6d6/3e2/7a7/6d63e27a79474c3796163d15f71d5177.png"><br><br>  Further, for vectorization of classified texts, including in the learning process, the weight obtained in this way is multiplied by the frequency of the word in this text, and 5 words are selected with the highest value of this value for each topic at a given taxonomy level.  These vectors are further concatenated, resulting in a vector of length 5 * m, where m is the number of nodes in the level.  Now the data is ready for classification. <br><br><h2>  Classifier quality assessment </h2><br>  We wanted to be able to get one number to evaluate the work of the entire classifier as a whole.  It is clear that it is easy to calculate the accuracy, completeness and F-measure for each individual taxonomy node, but when more than one hundred classes come into it, there is little sense from this.  Since the classifier is hierarchical, the quality of work of individual classifiers in the lower nodes depends on the quality of the previous ones - this is a key feature of our algorithm.  Precision and Recall are calculated using the following formulas: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/dff/fc8/152/dfffc8152edb406997e73e51fade03fa.png" alt="Formulas for calculating the completeness and accuracy" height="130"></div><br>  <i>(where TP is the number of true positive results, FN is the number false negative, etc.)</i> <br><br>  The F-measure is the harmonic average between completeness and accuracy, and you can set the ratio with which these values ‚Äã‚Äãare included in the result using the parameter √ü: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/7ae/2a6/945/7ae2a694523a4b21ac63b326d63e5547.png" alt="Formula for calculating F-measures" height="70"></div><br>  When √ü&gt; 1, the metric gets skewed towards completeness, with 0 &lt;√ü &lt;1 - towards accuracy.  We choose this parameter in accordance with the proportion of positive test sample examples for each subject at the current level, because the more vectors the classifier misses further, the more chances to make a mistake with the next one, and so on. <br><br>  The next step is the calculation of the averaged F-measure for each independent branch of the tree, that is, for all the child nodes of the parent from the first level.  Since for each node the F-measure was already calculated taking into account the composition of the training sample, it is sufficient to simply calculate the average F-measure of all the classifiers in the branch without additional weighting. <br><br>  The single metric for the entire classifier is calculated as the weighted average of the branch metrics; weight is the proportion of positive branch examples in the sample.  Here, a simple average is not enough;  The number of nodes and search queries in different branches can vary greatly.  We can boast a value of ~ 0.8 for the F-measure of the whole classifier, calculated in this way. <br>  It is important to note that when testing a classifier, we remove the words of the corresponding search queries from the list of tokens in order to avoid feedback. <br><br>  To visualize the test results, <a href="https://developers.google.com/chart/interactive/docs/gallery/orgchart">Google OrgChart</a> is used - it first of all visually depicts the tree structure of the taxonomy, and also allows you to specify the values ‚Äã‚Äãof metrics in each node and even hang the color indicators right inside the sheets.  This is what one of the branches looks like: <br><br> <a href=""><img src="https://habrastorage.org/files/29b/a7f/63c/29ba7f63c1dc40e0b8c18aa0e0025067.png" alt="A piece of taxonomy built in Google OrgChart (clickable)"></a> <br><br>  The tester is implemented as a separate Flask application that loads pre-calculated metrics values ‚Äã‚Äãfrom mongoDB on demand, completes what is missing due to possible changes in the classifier since the last calculation, and draws an orgchart.  As a nice addition, a simple interface is available from there, allowing you to insert a list of URLs or plain text into the text field and look at the result of the classification. <br><br><h2>  DMP Integration </h2><br>  Now, when there is a similar service, you need to actively use it.  Every day, one million of the most visited sites of the last day are selected from our DMP and run through the classifier.  These sites are marked with the id of the segments in which they fall, and users who have visited these pages in the last month fall into these segments.  Now the classification of one page takes about 0.2 - 0.3 seconds (minus the latency of the hosting sites). <br><br>  This approach allows you to automatically assign segments to thousands of URLs per day, whereas manually the analyst usually added no more than a hundred pages.  Now the work of the analyst will be reduced only to the selection of suitable topics of search queries, DMP will do the rest for him and even will orient how well this has happened. <br><br><h2>  Future plans </h2><br>  First of all, we were faced with the task of implementing a working classifier prototype, and at this stage we did not particularly bother with the choice of optimal parameters for estimators and other settings.  However, we did not expect that such a rather simple mathematical model would show a quite decent quality of work.  Of course, all the sounded algorithm constants can be flexibly adjusted, a separate article may be devoted to the selection of optimal settings.  Now the plans include the following works: <br><br><ul><li>  We will transfer the classifier to the non-blocking <a href="http://www.tornadoweb.org/en/stable/">Tornado</a> server so that it can be accessed asynchronously; </li><li>  In addition to the dg metric, we consider various variations of <a href="https://ru.wikipedia.org/wiki/TF-IDF">tf-idf</a> ; </li><li>  We will try to separately take into account the words that are contained in the title of the pages and meta tags; </li><li>  Twist the numerous estimator settings, try replacing random forest with SVM ensembles and increase the number of words selected for classification. </li></ul><br>  We hope that the story was interesting.  In the comments we will answer your questions and will be happy to discuss suggestions for improving our classifier. </div><p>Source: <a href="https://habr.com/ru/post/261677/">https://habr.com/ru/post/261677/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../261667/index.html">Transfer Android to HDD in Mele A100 (and similar, Allwinner A10)</a></li>
<li><a href="../261669/index.html">The game is familiar from childhood in the implementation of JavaScript</a></li>
<li><a href="../261671/index.html">We send and visualize data from the Intel Galileo / Edison board in the Azure cloud</a></li>
<li><a href="../261673/index.html">How to implement conversion from raster to black and white vector on the site</a></li>
<li><a href="../261675/index.html">Z-monitor - update service</a></li>
<li><a href="../261679/index.html">Grid for responsive design</a></li>
<li><a href="../261681/index.html">The tale of how "tsifir" did not agree</a></li>
<li><a href="../261683/index.html">Dino - a new state-sponsored spyware with French roots</a></li>
<li><a href="../261687/index.html">Microsoft is preparing a major update for Windows RT</a></li>
<li><a href="../261689/index.html">Compromise microservices</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>