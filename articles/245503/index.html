<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Concurrent programming with CUDA. Part 1: Introduction</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Another article about CUDA - why? 
 On Habr√© there were already a lot of good articles on CUDA - one , two and others. However, the search for the com...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Concurrent programming with CUDA. Part 1: Introduction</h1><div class="post__text post__text-html js-mediator-article"><h4>  Another article about CUDA - why? </h4><br>  On Habr√© there were already a lot of good articles on CUDA - <a href="http://habrahabr.ru/post/119435/">one</a> , <a href="http://habrahabr.ru/post/54707/">two</a> and others.  However, the search for the combination ‚ÄúCUDA <a href="http.developer.nvidia.com/GPUGems3/gpugems3_ch39.html">scan</a> ‚Äù produced only 2 articles in no way connected with, actually, the scan on GPU algorithm - and this is one of the most basic algorithms.  Therefore, inspired by the recently viewed course on Udacity - <a href="https://www.udacity.com/course/cs344">Intro to Parallel Programming</a> , I decided to write a more complete series of articles on CUDA.  I will say right away that the series will be based on this course, and if you have time, it will be much more useful to go through it. <br><a name="habracut"></a><br><h4>  Content </h4><br>  At the moment, the following articles are planned: <br>  <b>Part 1: Introduction.</b> <br>  <a href="http://habrahabr.ru/company/epam_systems/blog/245523/">Part 2: GPU hardware and parallel communication patterns.</a> <br>  <a href="http://habrahabr.ru/company/epam_systems/blog/247805/">Part 3: GPU fundamental algorithms: convolution (reduce), scan (scan) and histogram (histogram).</a> <br>  Part 4: Fundamental GPU algorithms: compaction (compact), segmented scan (segmented scan), sorting.  Practical application of some algorithms. <br>  Part 5: Optimization of GPU programs. <br>  Part 6: Examples of parallelization of sequential algorithms. <br>  Part 7: Additional Parallel Programming Topics, Dynamic Concurrency. <br><habracut><br><h4>  Delay vs bandwidth </h4><br><img src="https://habrastorage.org/files/cb2/94c/38c/cb294c38cf0b4597acc43f4e31e32dae.jpg"><br>  The first question that everyone should ask before using a GPU to solve their problems - and for what purposes is the GPU good, when should it be used?  To answer, you need to define 2 concepts: <br>  <b>Delay</b> (latency) - the time spent on the execution of a single instruction / operation. <br>  <b>Bandwidth</b> is the number of instructions / operations performed per unit of time. <br>  A simple example: we have a passenger car at a speed of 90 km / h and a capacity of 4 people, and a bus at a speed of 60 km / h and a capacity of 20 people.  If you take 1 person per kilometer for the operation, then the delay of the car - 3600/90 = 40s - for so many seconds, 1 person will overcome the distance of 1 kilometer, the capacity of the car - 4/40 = 0.1 operations / second;  bus delay - 3600/60 = 60s, bus capacity - 20/60 = 0.3 (3) operations / second. <br>  So, the CPU is a car, the GPU is a bus: it has a big delay but also a large bandwidth.  If for your task the delay of each specific operation is not as important as the number of these operations per second - it is worth considering the use of the GPU. <br><br><h4>  Basic CUDA Concepts and Terms </h4><br>  So, let's deal with the terminology of CUDA: <br><img src="https://habrastorage.org/files/9d5/29e/19f/9d529e19f57c4232bf9713eb8c2e96ab.jpg"><br><ul><li>  <b>Device (device)</b> - GPU.  It performs the role of ‚Äúsubordinate‚Äù - it does only what the CPU tells it. </li><li>  <b>Host (host)</b> - CPU.  Performs the control role - runs tasks on the device, allocates memory on the device, moves the memory to / from the device.  And yes, using CUDA assumes that both the device and the host have their own separate memory. </li><li>  <b>The kernel</b> is a task that is started by a host on a device. </li></ul><br>  When using CUDA, you simply write code in your favorite programming language ( <a href="http://en.wikipedia.org/wiki/CUDA">list of</a> supported languages, not considering C and C ++), after which the CUDA compiler will generate code separately for the host and separately for the device.  A small caveat: the code for the device should be written only in C with some 'CUDA extensions'. <br><br><h4>  The main stages of the CUDA program </h4><br><ol><li>  The host allocates the required amount of memory on the device. </li><li>  The host copies data from its memory to the device‚Äôs memory. </li><li>  The host starts the execution of certain cores on the device. </li><li>  The device performs the kernel. </li><li>  The host copies the results from the device‚Äôs memory to its memory. </li></ol><br>  Naturally, for the most efficient use of the GPU, it is necessary that the ratio of time spent on cores to the time spent on memory allocation and data movement be as large as possible. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Kernels </h4><br>  Let us consider in more detail the process of writing kernel code and launching it.  An important principle is that the <b>kernels are written as (practically) ordinary sequential programs</b> ‚Äî that is, you will not see the creation and launch of threads in the code of the cores themselves.  Instead, for the organization of parallel computing, the <b>GPU will launch a large number of copies of the same kernel in different threads</b> - or rather, you yourself tell how many threads to launch.  And yes, returning to the question of the efficiency of using the GPU - the more threads you run (provided they all do useful work) - the better. <br>  Kernel code is different from regular sequential code at such times: <br><ol><li>  Inside the cores, you have the opportunity to find out the ‚Äúidentifier‚Äù or, more simply, the position of the stream that is currently running - using this position, we achieve that the same core will work with different data depending on the thread in which it is running.  By the way, such an organization of parallel computing is called <a href="http://en.wikipedia.org/wiki/SIMD">SIMD</a> (Single Instruction Multiple Data) - when several processors simultaneously perform the same operation but on different data. </li><li>  In some cases, it is necessary to use different synchronization methods in the kernel code. </li></ol><br>  How do we set the number of threads in which the kernel will run?  Since the GPU is still the <b>Graphics</b> Processing Unit, this naturally affected the CUDA model, namely, the method of defining the number of threads: <br><ul><li>  First, the dimensions of the so-called grid (grid), in 3D coordinates: <i>grid_x, grid_y, grid_z</i> .  As a result, the grid will consist of <i>grid_x * grid_y * grid_z</i> blocks. </li><li>  Then the block sizes in 3D coordinates are <i>specified</i> : <i>block_x, block_y, block_z</i> .  As a result, the block will consist of <i>block_x * block_y * block_z</i> streams.  Total, we have <i>grid_x * grid_y * grid_z * block_x * block_y * block_z</i> flows.  Important note - the maximum number of threads in one block is limited and depends on the GPU model - typical values ‚Äã‚Äãare 512 (older models) and 1024 (newer models). </li><li>  Inside the kernel, the variables <b>threadIdx</b> and <b>blockIdx are available</b> with the fields <b>x, y, z</b> - they contain the 3D coordinates of the flow in the block and the block in the grid, respectively.  Also available are <b>blockDim</b> and <b>gridDim variables</b> with the same fields ‚Äî block and grid sizes, respectively. </li></ul><br>  As you can see, this method of starting streams is really suitable for processing 2D and 3D images: for example, if you need to process each pixel of 2D or 3D images in a certain way, then after selecting block sizes (depending on image size, processing method and GPU model), grid sizes are chosen such that the entire image is covered, possibly in excess - if the image sizes are not completely divided by the block sizes. <br><br><h4>  Writing the first program on CUDA </h4><br>  Enough theory, time to write code.  Instructions for installing and configuring CUDA for different operating systems - <a href="http://docs.nvidia.com/cuda/index.html">docs.nvidia.com/cuda/index.html</a> .  Also, for ease of working with image files, we will use <a href="http://opencv.org/">OpenCV</a> , and for comparing CPU and GPU performance - <a href="http://openmp.org/wp/">OpenMP</a> . <br>  The task is pretty simple: converting a color image to <a href="https://ru.wikipedia.org/wiki/%25D0%259E%25D1%2582%25D1%2582%25D0%25B5%25D0%25BD%25D0%25BA%25D0%25B8_%25D1%2581%25D0%25B5%25D1%2580%25D0%25BE%25D0%25B3%25D0%25BE">shades of gray</a> .  For this, the pixel brightness <i>pix</i> in the gray scale is calculated by the formula: <i>Y</i> = <i>0.299 * pix.R + 0.587 * pix.G + 0.114 * pix.B.</i> <br>  First, write the skeleton of the program: <br><div class="spoiler">  <b class="spoiler_title">main.cpp</b> <div class="spoiler_text"><pre><code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;chrono&gt; #include &lt;iostream&gt; #include &lt;cstring&gt; #include &lt;string&gt; #include &lt;opencv2/core/core.hpp&gt; #include &lt;opencv2/highgui/highgui.hpp&gt; #include &lt;opencv2/opencv.hpp&gt; #include &lt;vector_types.h&gt; #include "openMP.hpp" #include "CUDA_wrappers.hpp" #include "common/image_helpers.hpp" using namespace cv; using namespace std; int main( int argc, char** argv ) { using namespace std::chrono; if( argc != 2) { cout &lt;&lt;" Usage: convert_to_grayscale imagefile" &lt;&lt; endl; return -1; } Mat image, imageGray; uchar4 *imageArray; unsigned char *imageGrayArray; prepareImagePointers(argv[1], image, &amp;imageArray, imageGray, &amp;imageGrayArray, CV_8UC1); int numRows = image.rows, numCols = image.cols; auto start = system_clock::now(); RGBtoGrayscaleOpenMP(imageArray, imageGrayArray, numRows, numCols); auto duration = duration_cast&lt;milliseconds&gt;(system_clock::now() - start); cout&lt;&lt;"OpenMP time (ms):" &lt;&lt; duration.count() &lt;&lt; endl; memset(imageGrayArray, 0, sizeof(unsigned char)*numRows*numCols); RGBtoGrayscaleCUDA(imageArray, imageGrayArray, numRows, numCols); return 0; }</span></span></span></span></code> </pre> <br></div></div><br>  It's all pretty obvious - read the image file, prepare pointers to color and grayscale image, run the option <br>  with OpenMP and CUDA version, we measure time.  The <i>prepareImagePointers</i> function has the following form: <br><div class="spoiler">  <b class="spoiler_title">prepareImagePointers</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">template</span></span> &lt;<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">T1</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">T2</span></span></span><span class="hljs-class">&gt; </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">void</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">prepareImagePointers</span></span></span><span class="hljs-class">(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">const</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">char</span></span></span><span class="hljs-class"> * </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">const</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">inputImageFileName</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">cv</span></span></span><span class="hljs-class">:</span></span>:Mat&amp; inputImage, T1** inputImageArray, cv::Mat&amp; outputImage, T2** outputImageArray, <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> outputImageType) { <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> <span class="hljs-keyword"><span class="hljs-keyword">namespace</span></span> <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> <span class="hljs-keyword"><span class="hljs-keyword">namespace</span></span> cv; inputImage = imread(inputImageFileName, IMREAD_COLOR); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (inputImage.empty()) { <span class="hljs-built_in"><span class="hljs-built_in">cerr</span></span> &lt;&lt; <span class="hljs-string"><span class="hljs-string">"Couldn't open input file."</span></span> &lt;&lt; <span class="hljs-built_in"><span class="hljs-built_in">endl</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">exit</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>); } <span class="hljs-comment"><span class="hljs-comment">//allocate memory for the output outputImage.create(inputImage.rows, inputImage.cols, outputImageType); cvtColor(inputImage, inputImage, cv::COLOR_BGR2BGRA); *inputImageArray = (T1*)inputImage.ptr&lt;char&gt;(0); *outputImageArray = (T2*)outputImage.ptr&lt;char&gt;(0); }</span></span></code> </pre><br></div></div><br>  I went for a little trick: the fact is that we do very little work on every pixel of the image ‚Äî that is, with the CUDA version, the above mentioned problem arises between the time needed to perform useful operations and the time to allocate memory and copy data, and as a result The CUDA option will be larger than the OpenMP option, but we want to show that CUDA is faster :) Therefore, for CUDA, only the time spent on performing the actual image conversion will be measured - without taking into account memory operations.  In my defense, I‚Äôll say that for a large class of tasks, the useful time will still dominate, and CUDA will be faster even with regard to memory operations. <br>  Next, write the code for the OpenMP version: <br><div class="spoiler">  <b class="spoiler_title">openMP.hpp</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;stdio.h&gt; #include &lt;omp.h&gt; #include &lt;vector_types.h&gt; void RGBtoGrayscaleOpenMP(uchar4 *imageArray, unsigned char *imageGrayArray, int numRows, int numCols) { #pragma omp parallel for collapse(2) for (int i = 0; i &lt; numRows; ++i) { for (int j = 0; j &lt; numCols; ++j) { const uchar4 pixel = imageArray[i*numCols+j]; imageGrayArray[i*numCols+j] = 0.299f*pixel.x + 0.587f*pixel.y+0.114f*pixel.z; } } }</span></span></span></span></code> </pre><br></div></div><br>  Everything is pretty straightforward - we just added the <a href="http://supercomputingblog.com/openmp/tutorial-parallel-for-loops-with-openmp/"><i>omp parallel for</i></a> directive to the single-threaded code - this is all the beauty and power of OpenMP.  I tried to play with the <a href="https://software.intel.com/en-us/articles/openmp-loop-scheduling"><i>schedule</i></a> parameter, but it turned out only worse than without it. <br>  Finally, go to CUDA.  Here we will write in more detail.  First you need to allocate memory for the input data, move it from the CPU to the GPU and allocate memory for the output: <br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">RGBtoGrayscaleCUDA</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">const</span></span></span></span><span class="hljs-function"><span class="hljs-params"> uchar4 * </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">const</span></span></span></span><span class="hljs-function"><span class="hljs-params"> h_imageRGBA, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">unsigned</span></span></span></span><span class="hljs-function"><span class="hljs-params"> </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">char</span></span></span></span><span class="hljs-function"><span class="hljs-params">* </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">const</span></span></span></span><span class="hljs-function"><span class="hljs-params"> h_imageGray, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">size_t</span></span></span></span><span class="hljs-function"><span class="hljs-params"> numRows, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">size_t</span></span></span></span><span class="hljs-function"><span class="hljs-params"> numCols)</span></span></span><span class="hljs-function"> </span></span>{ uchar4 *d_imageRGBA; <span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">char</span></span> *d_imageGray; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> <span class="hljs-keyword"><span class="hljs-keyword">size_t</span></span> numPixels = numRows * numCols; cudaSetDevice(<span class="hljs-number"><span class="hljs-number">0</span></span>); checkCudaErrors(cudaGetLastError()); <span class="hljs-comment"><span class="hljs-comment">//allocate memory on the device for both input and output checkCudaErrors(cudaMalloc(&amp;d_imageRGBA, sizeof(uchar4) * numPixels)); checkCudaErrors(cudaMalloc(&amp;d_imageGray, sizeof(unsigned char) * numPixels)); //copy input array to the GPU checkCudaErrors(cudaMemcpy(d_imageRGBA, h_imageRGBA, sizeof(uchar4) * numPixels, cudaMemcpyHostToDevice));</span></span></code> </pre><br></div></div><br>  It is worth paying attention to the naming standard of variables in CUDA - the data on the CPU starts with <i>h_</i> ( <b>h</b> ost), the data and the GPU - with <i>d_</i> ( <b>d</b> evice).  <i>checkCudaErrors</i> - macro, taken from the <a href="">gdubub repository of the</a> Udacity course.  It has the following form: <br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;cuda.h&gt; #define checkCudaErrors(val) check( (val), #val, __FILE__, __LINE__) template&lt;typename T&gt; void check(T err, const char* const func, const char* const file, const int line) { if (err != cudaSuccess) { std::cerr &lt;&lt; "CUDA error at: " &lt;&lt; file &lt;&lt; ":" &lt;&lt; line &lt;&lt; std::endl; std::cerr &lt;&lt; cudaGetErrorString(err) &lt;&lt; " " &lt;&lt; func &lt;&lt; std::endl; exit(1); } }</span></span></span></span></code> </pre><br></div></div><br>  <i>cudaMalloc</i> is an analog <i>malloc</i> for a GPU, <i>cudaMemcpy</i> is an analog of <i>memcpy</i> , it has an additional parameter in the form of an enum that indicates the type of copying: cudaMemcpyHostToDevice, cudaMemcpyDeviceToHost, cudaMemcpyDeviceToDevice. <br>  Next, you need to set the dimensions of the grid and the block and call the core, do not forget to measure the time: <br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><pre> <code class="cpp hljs"> dim3 blockSize; dim3 gridSize; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> threadNum; cudaEvent_t start, stop; cudaEventCreate(&amp;start); cudaEventCreate(&amp;stop); threadNum = <span class="hljs-number"><span class="hljs-number">1024</span></span>; blockSize = dim3(threadNum, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>); gridSize = dim3(numCols/threadNum+<span class="hljs-number"><span class="hljs-number">1</span></span>, numRows, <span class="hljs-number"><span class="hljs-number">1</span></span>); cudaEventRecord(start); rgba_to_grayscale_simple&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(d_imageRGBA, d_imageGray, numRows, numCols); cudaEventRecord(stop); cudaEventSynchronize(stop); cudaDeviceSynchronize(); checkCudaErrors(cudaGetLastError()); <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> milliseconds = <span class="hljs-number"><span class="hljs-number">0</span></span>; cudaEventElapsedTime(&amp;milliseconds, start, stop); <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">cout</span></span> &lt;&lt; <span class="hljs-string"><span class="hljs-string">"CUDA time simple (ms): "</span></span> &lt;&lt; milliseconds &lt;&lt; <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">endl</span></span>;</code> </pre><br></div></div><br>  Pay attention to the format of the kernel call - <i>kernel_name &lt;&lt;&lt; gridSize, blockSize &gt;&gt;&gt;</i> .  The code for the kernel itself is also not very complicated: <br><div class="spoiler">  <b class="spoiler_title">rgba_to_grayscale_simple</b> <div class="spoiler_text"><pre> <code class="cpp hljs">__<span class="hljs-function"><span class="hljs-function">global__ </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">rgba_to_grayscale_simple</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">const</span></span></span></span><span class="hljs-function"><span class="hljs-params"> uchar4* </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">const</span></span></span></span><span class="hljs-function"><span class="hljs-params"> d_imageRGBA, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">unsigned</span></span></span></span><span class="hljs-function"><span class="hljs-params"> </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">char</span></span></span></span><span class="hljs-function"><span class="hljs-params">* </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">const</span></span></span></span><span class="hljs-function"><span class="hljs-params"> d_imageGray, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> numRows, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> numCols)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> y = blockDim.y*blockIdx.y + threadIdx.y; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> x = blockDim.x*blockIdx.x + threadIdx.x; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x&gt;=numCols || y&gt;=numRows) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> offset = y*numCols+x; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> uchar4 pixel = d_imageRGBA[offset]; d_imageGray[offset] = <span class="hljs-number"><span class="hljs-number">0.299f</span></span>*pixel.x + <span class="hljs-number"><span class="hljs-number">0.587f</span></span>*pixel.y+<span class="hljs-number"><span class="hljs-number">0.114f</span></span>*pixel.z; }</code> </pre><br></div></div><br>  Here we calculate the coordinates <i>y</i> and <i>x of</i> the pixel being processed using the previously described variables <b>threadIdx</b> , <b>blockIdx</b> and <b>blockDim</b> , and we also do the conversion.  Pay attention to the <i>if</i> check <i>(x&gt; = numCols || y&gt; = numRows)</i> - since the dimensions of the image will not necessarily be completely divided into the sizes of blocks, some blocks may ‚Äúgo beyond‚Äù the image - therefore this check is necessary.  Also, the kernel function must be marked with the <i>__global__ specifier</i> . <br>  The last step is to copy the result back from the GPU to the CPU and free the allocated memory: <br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><pre> <code class="cpp hljs"> checkCudaErrors(cudaMemcpy(h_imageGray, d_imageGray, <span class="hljs-keyword"><span class="hljs-keyword">sizeof</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">char</span></span>) * numPixels, cudaMemcpyDeviceToHost)); cudaFree(d_imageGray); cudaFree(d_imageRGBA);</code> </pre><br></div></div><br>  By the way, CUDA allows you to use the C ++ compiler for the host code - so you can easily write wrappers to automatically free memory. <br>  So, we start, we measure (the size of the input image is <a href="">10,109 √ó 4,542</a> ): <br><pre> <code class="bash hljs">OpenMP time (ms):45 CUDA time simple (ms): 43.1941</code> </pre><br>  Configuration of the machine on which the tests were conducted: <br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text">  Processor: Intel¬Æ Core (TM) i7-3615QM CPU @ 2.30GHz. <br>  GPU: NVIDIA GeForce GT 650M, 1024 MB, 900 MHz. <br>  RAM: DD3,2x4GB, 1600 MHz. <br>  OS: OS X 10.9.5. <br>  Compiler: g ++ (GCC) 4.9.2 20141029. <br>  CUDA compiler: Cuda compilation tools, release 6.0, V6.0.1. <br>  Supported OpenMP Version: OpenMP 4.0. <br></div></div><br>  It turned out somehow not very impressive :) And the problem is still the same - too little work is done on each pixel - we run thousands of threads, each of which works almost instantly.  In the case of the CPU, such a problem does not occur - OpenMP will launch a relatively small number of threads (8 in my case) and divide the work between them equally - so the processors will be occupied by almost 100%, while with the GPU we are, in fact, do not use all his power.  The solution is quite obvious - to process several pixels in the core.  A new, optimized kernel will look like this: <br><div class="spoiler">  <b class="spoiler_title">rgba_to_grayscale_optimized</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">define</span></span></span><span class="hljs-meta"> WARP_SIZE 32 __global__ void rgba_to_grayscale_optimized(const uchar4* const d_imageRGBA, unsigned char* const d_imageGray, int numRows, int numCols, int elemsPerThread) { int y = blockDim.y*blockIdx.y + threadIdx.y; int x = blockDim.x*blockIdx.x + threadIdx.x; const int loop_start = (x/WARP_SIZE * WARP_SIZE)*(elemsPerThread-1)+x; for (int i=loop_start, j=0; j&lt;elemsPerThread &amp;&amp; i&lt;numCols; i+=WARP_SIZE, ++j) { const int offset = y*numCols+i; const uchar4 pixel = d_imageRGBA[offset]; d_imageGray[offset] = 0.299f*pixel.x + 0.587f*pixel.y+0.114f*pixel.z; } }</span></span></code> </pre><br></div></div><br>  It is not so simple as with the previous core.  If to understand, now each stream will process <i>elemsPerThread</i> pixels, and not in a row, but with a distance in WARP_SIZE between them.  What is WARP_SIZE, why it is equal to 32, and why to process pixels in a free way, will be explained in more detail in the following parts, now I‚Äôll just say that we are achieving more efficient work with memory.  Each stream now processes <i>elemsPerThread</i> pixels with a distance in WARP_SIZE between them, so the x-coordinate of the first pixel for this stream based on its position in the block is now calculated using a slightly more complex formula than before. <br>  This kernel is launched as follows: <br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><pre> <code class="cpp hljs"> threadNum=<span class="hljs-number"><span class="hljs-number">128</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> elemsPerThread = <span class="hljs-number"><span class="hljs-number">16</span></span>; blockSize = dim3(threadNum, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>); gridSize = dim3(numCols / (threadNum*elemsPerThread) + <span class="hljs-number"><span class="hljs-number">1</span></span>, numRows, <span class="hljs-number"><span class="hljs-number">1</span></span>); cudaEventRecord(start); rgba_to_grayscale_optimized&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(d_imageRGBA, d_imageGray, numRows, numCols, elemsPerThread); cudaEventRecord(stop); cudaEventSynchronize(stop); cudaDeviceSynchronize(); checkCudaErrors(cudaGetLastError()); milliseconds = <span class="hljs-number"><span class="hljs-number">0</span></span>; cudaEventElapsedTime(&amp;milliseconds, start, stop); <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">cout</span></span> &lt;&lt; <span class="hljs-string"><span class="hljs-string">"CUDA time optimized (ms): "</span></span> &lt;&lt; milliseconds &lt;&lt; <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">endl</span></span>;</code> </pre><br></div></div><br>  The number of blocks on the x-coordinate is now calculated as <i>numCols / (threadNum * elemsPerThread) + 1</i> instead of <i>numCols / threadNum + 1</i> .  Otherwise, everything remains the same. <br>  Run: <br><pre> <code class="bash hljs">OpenMP time (ms):44 CUDA time simple (ms): 53.1625 CUDA time optimized (ms): 15.9273</code> </pre><br>  We received a speed increase of 2.76 times (again, without taking into account the time for a memory operation) - for such a simple problem, this is pretty good.  Yes, yes, this task is too simple - the CPU copes well with it.  As can be seen from the second test, a simple implementation on a GPU can even play according to the speed of implementation on the CPU. <br>  Today, everything, in the next part, will look at the GPU hardware and the main patterns of parallel communication. <br>  All source code is available on <a href="https://bitbucket.org/VladyslavGorbatiuk/cuda-parallel-programming-code">bitbucket</a> . </habracut></div><p>Source: <a href="https://habr.com/ru/post/245503/">https://habr.com/ru/post/245503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../245487/index.html">Simple event system - non-standard approach</a></li>
<li><a href="../245489/index.html">Occlusion Culling and LOD for Unity Indie</a></li>
<li><a href="../245491/index.html">Python Memory and Numbers</a></li>
<li><a href="../245493/index.html">SED: There are contraindications</a></li>
<li><a href="../245497/index.html">Algorithm TILT or non-standard use of the rank of the matrix</a></li>
<li><a href="../245505/index.html">How to independently register Ltd.</a></li>
<li><a href="../245507/index.html">ASH Viewer</a></li>
<li><a href="../245509/index.html">Onto engineer: work on concepts</a></li>
<li><a href="../245511/index.html">Analog video capture with STM32F4-DISCOVERY</a></li>
<li><a href="../245513/index.html">Read SVG in C ++</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>