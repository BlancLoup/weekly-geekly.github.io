<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Command line utilities can be 235 times faster than your Hadoop cluster</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tsafin notes : 

 Before publishing my series of articles on MapReduce in Cach√©, it seemed to me important to voice this last year‚Äôs point of view fro...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Command line utilities can be 235 times faster than your Hadoop cluster</h1><div class="post__text post__text-html js-mediator-article">  <em><a href="https://habrahabr.ru/users/tsafin/" class="user_link">Tsafin notes</a> :</em> <em><br><br></em>  <em>Before publishing my series of articles on MapReduce in Cach√©, it seemed to me important to voice this last year‚Äôs point of view from <a href="http://aadrake.com/author/adam-drake.html">Adam Drake</a> <a href="http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html">‚Äôs</a> article <a href="http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html">‚ÄúCommand-line tools can be 235x faster than your Hadoop cluster‚Äù</a> .</em>  <em>Unfortunately, the original article by <a href="http://tomhayden3.com/2013/12/27/chess-mr-job/">Tom Heiden</a> , which he refers to, has become unavailable on Tom‚Äôs site, but it can still be found <a href="http:/tomhayden3.com/2013/12/27/chess-mr-job/%3F">in the archives</a> .</em>  <em>For completeness, I suggest to get acquainted with it too.</em> <br><h2>  Introduction </h2><br>  Visiting my favorite sites once again, I found a cool <a href="http://tomhayden3.com/2013/12/27/chess-mr-job/">Tom Hayden</a> article about using <a href="https://aws.amazon.com/elasticmapreduce/">Amazon Elastic Map Reduce</a> (EMR) and <a href="https://github.com/Yelp/mrjob">mrjob</a> to calculate win / lose statistics for a dataset with chess match statistics that he downloaded from <a href="http://www.top-5000.nl/pgn.htm">millionbase archive</a> , and where he started playing using EMR.  Since the data volume was only 1.75GB, describing 2 million chess games, I was skeptical about using Hadoop for this task, although its intentions were simple to play around and learn more closely, using a real example, the mrjob utility and the EMR infrastructure. <br><a name="habracut"></a><br><br>  Initially, the statement of the problem (to find the result string in the file and calculate the alignment) was more suitable for threading through shell commands.  I tried this approach with a similar amount of data on my laptop, and I got the result in about 12 seconds (i.e., the effective data processing speed was about 270MB / s), while processing in Hadoop took <i>26 minutes</i> (i.e., the effective data processing 1.14MB / s). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      When telling about the processing of such a data volume on a cluster of 7 c1.medium machines, Tom said that it took 26 minutes in a cluster and ‚Äúperhaps this time is better than if I did it consistently on my machine, but it‚Äôs still slower than handled this through a tricky, multi-threaded application locally. " <br><br>  This is absolutely true, although we note that even local sequential processing can easily beat these 26 stated minutes.  Yes, Tom did this project just to have fun and have fun, but often many others use <em>Big Data (tm)</em> tools for processing and analyzing such a ‚Äúnot very large‚Äù amount of data with which you can get results using tools and techniques and simpler and quickly. <br><br>  One, the most underrated approach to data processing is the use of good old tools and shell constructions.  The advantages of this approach are enormous, because  creating a pipeline of data from shell commands, you run all the processing steps in parallel with minimal overhead.  It‚Äôs as if you had your local <a href="http://storm-project.net/">Storm</a> cluster.  You can even try to translate the concepts of Spouts, Bolts and Sinks into simple commands and pipelines in the shell.  You can easily construct a pipeline for processing data from simple commands, and they will work faster than most tools from the <em>Big Data (tm)</em> arsenal. <br><br>  Also let's talk about the differences in streaming and batch approaches in data processing.  Tom mentioned at the beginning of the article that if he downloaded more than 10,000 game results locally, then his [Python] program would crash out of memory very quickly.  This was due to the fact that all game data was loaded first into memory for later analysis.  However, with a more in-depth study of the problem, you can create a pipeline for streamlined data processing that will not actually consume memory. <br><br>  <em>The resulting pipeline that we created was more than 236 times faster than the Hadoop implementation, and almost did not eat up the memory</em> . <br><br><h2>  Research data </h2><br>  The first step of the exercise is to download these PGN files.  Since  I had no idea what this format looks like, then I looked into <a href="https://en.wikipedia.org/wiki/Portable_Game_Notation">Wikipedia</a> . <br><br><pre><code class="dos hljs">[Event "F/S Return Match"] [Site "Belgrade, Serbia Yugoslavia|JUG"] [<span class="hljs-built_in"><span class="hljs-built_in">Date</span></span> "<span class="hljs-number"><span class="hljs-number">1992</span></span>.<span class="hljs-number"><span class="hljs-number">11</span></span>.<span class="hljs-number"><span class="hljs-number">04</span></span>"] [Round "<span class="hljs-number"><span class="hljs-number">29</span></span>"] [White "Fischer, Robert J."] [Black "Spassky, Boris V."] [Result "<span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">2</span></span>-<span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">2</span></span>"] (moves from the game follow...)</code> </pre> <br>  We are only interested in the results of the games, with 3 possible options.  1-0 will mean that White won, 0-1 - Black won and 1 / 2-1 / 2 means a draw in the game.  There is also a result <em>-</em> which means that the game is going on or was interrupted, but we ignore this case in our exercise. <br><br>  <strong>Getting a dataset</strong> <br><br>  First of all we will try to download data about the games.  This was not so easy, but after a short search in the network I found the <a href="https://github.com/rozim/ChessData">rozim</a> Git repository in which there were many different sources of data about chess games for various periods of time.  I used the code from this repository to compile a 3.46GB dataset, which is almost twice the amount of data used by Tom in his test.  The next step is to put all this data into our pipeline. <br><br><h2>  Build a data processing pipeline </h2><br>  <em>If you follow the narration and measure each step, then do not forget to clear the cache of the pages of your operating system in order to get more accurate timekeys when measuring data processing.</em> <br><br>  Using commands from the shell is convenient for data processing, since  You can get concurrency in the execution of commands for nothing.  Check, for example, how this command will be executed in the terminal. <br><br><pre> <code class="bash hljs">sleep 3 | <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">", "</span></span></code> </pre><br>  Intuitively, it may seem that here we first linger for 3 seconds on the first team and then issue it to the Hello World terminal.  But, in fact, both commands are executed at the same time.  This is one of the main factors why we can get such acceleration on one machine for simple commands that do not load IO. <br><br>  Before we start constructing the data processing pipeline, let's define the top performance stream by simply copying the data into / dev / null. <br><br>  In this case, the operation takes 13 seconds, and for a 3.46GB array, this means 272MB / s.  This will be our upper limit to the speed of reading from the disk. <br><br>  Now we can start building the pipeline, and in the first step we will use cat to generate the data flow: <br><br><pre> <code class="bash hljs">cat *.pgn</code> </pre><br>  Since we are only interested in the lines with the results, we can scan the files and select only the lines containing the 'Result' using the grep utility: <br><br><pre> <code class="bash hljs">cat *.pgn | grep <span class="hljs-string"><span class="hljs-string">"Result"</span></span></code> </pre><br>  [ <em>It is not clear why he didn‚Äôt immediately launch it as `grep Result * .pgn`?</em>  ] <br><br>  This will give us only strings containing Result.  Now, if you want, you can use the sort and uniq commands to get a list of unique elements and to count their number. <br><br><pre> <code class="bash hljs">cat *.pgn | grep <span class="hljs-string"><span class="hljs-string">"Result"</span></span> | sort | uniq -c</code> </pre><br>  This is a very simple approach to analyze, and it gives us the result in 70 seconds.  Of course, we can speed it up, but even now we note that, given the linear scaling, this would take about 52 minutes for processing on the Hadoop cluster. <br><br>  To speed up, we can remove the sort |  uniq out of the pipeline and replace them with an AWK call, which is a great tool for handling event data. <br><br><pre> <code class="bash hljs">cat *.pgn | grep <span class="hljs-string"><span class="hljs-string">"Result"</span></span> | awk <span class="hljs-string"><span class="hljs-string">'{ split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print white+black+draw, white, black, draw }'</span></span></code> </pre><br>  Here we take the entire record, divide the hyphen by the sign, and take the character immediately preceding from the left, if there is 0, then Black won, 1 is white, and 2 is a draw.  Note that here $ 0 is a built-in variable representing the entire entry. <br><br>  This reduces the execution time to about 65 seconds, and  we process about 2 times more data, then this acceleration is about 47 times. <br><br>  Even at the current step, we have acceleration 47 times with local execution.  Moreover, the size of the memory used is almost zero, since  only the current record data is stored, and incrementing 3 integer type variables is an insignificant operation in terms of memory consumption.  However, htop shows that grep in this sequence is a bottleneck, and fully utilizes the processor time of one core. <br><br><h2>  Bottleneck parallelization </h2><br>  The problem of under-using additional kernels can be solved with the help of the remarkable xargs command, which can run grep in parallel.  Since  xargs expects input in a specific format, then we will use find with the argument -print0, so that the name of the file passed to xargs ends with zero.  Accordingly, we pass -0 so that xargs on its side will wait for zero-terminated strings.  The -n option controls the number of lines passed in one call, and -P means the number of parallel commands to be executed.  It is important to note that such a parallel pipeline does not guarantee the order of delivery, but this is not a problem if you were counting on distributed processing initially.  The option -F in grep means that we are looking for a simple string match, and do not ask for any confused regular expressions, which in theory can give an additional gain in speed, which was not noticed in the experiments <br>  [ <em>this statement is not quite true for our example, gnu grep builds a deterministic finite automaton, which in this simple expression will be equivalent to a simple string match.</em>  <em>The only thing we gain is the compile time of the regular expression, which can be neglected in this case</em> ]. <br><br><pre> <code class="bash hljs">find . -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -name <span class="hljs-string"><span class="hljs-string">'*.pgn'</span></span> -print0 | xargs -0 -n1 -P4 grep -F <span class="hljs-string"><span class="hljs-string">"Result"</span></span> | gawk <span class="hljs-string"><span class="hljs-string">'{ split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print NR, white, black, draw }'</span></span></code> </pre><br>  As a result, we get a time of 38 seconds, which gave a ‚Äúweld‚Äù of 40% only due to the parallelization of the launch of grep in our pipeline.  Now we are about 77 times faster implementation on Hadoop. <br><br>  Although we have improved performance "dramatically" by parallelizing the step with grep, we can generally get rid of it by making awk itself look for the necessary records and work only with those that contain the string "Result". <br><br><pre> <code class="bash hljs">find . -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -name <span class="hljs-string"><span class="hljs-string">'*.pgn'</span></span> -print0 | xargs -0 -n1 -P4 awk <span class="hljs-string"><span class="hljs-string">'/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++;} END { print white+black+draw, white, black, draw }'</span></span></code> </pre><br>  This may seem like the right decision, but it gives the results for <strong>each</strong> file separately.  Whereas we need a common, aggregated result.  The correct implementation of aggregation is conceptually very similar to MapReduce: <br><br><pre> <code class="bash hljs">find . -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -name <span class="hljs-string"><span class="hljs-string">'*.pgn'</span></span> -print0 | xargs -0 -n4 -P4 awk <span class="hljs-string"><span class="hljs-string">'/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++ } END { print white+black+draw, white, black, draw }'</span></span> | awk <span class="hljs-string"><span class="hljs-string">'{games += $1; white += $2; black += $3; draw += $4; } END { print games, white, black, draw }'</span></span></code> </pre><br>  By adding a second awk call, we end up with the desired aggregate information about the games. <br><br>  This further increases the final speed, reducing the launch time to 18 seconds, which is 174 times faster than the Hadoop implementation. <br><br>  However, we can speed it up a bit more by using <a href="http://invisible-island.net/mawk/mawk.html">mawk</a> , which completely replaces gawk, while maintaining the same launch options, but providing better performance. <br><br><pre> <code class="bash hljs">find . -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -name <span class="hljs-string"><span class="hljs-string">'*.pgn'</span></span> -print0 | xargs -0 -n4 -P4 mawk <span class="hljs-string"><span class="hljs-string">'/Result/ { split($0, a, "-"); res = substr(a[1], length(a[1]), 1); if (res == 1) white++; if (res == 0) black++; if (res == 2) draw++ } END { print white+black+draw, white, black, draw }'</span></span> | mawk <span class="hljs-string"><span class="hljs-string">'{games += $1; white += $2; black += $3; draw += $4; } END { print games, white, black, draw }'</span></span></code> </pre><br>  Thus, the pipeline using output redirection between <b>find |</b>  <b>xargs mawk |</b>  <b>mawk</b> allowed processing in 12 seconds, which is about 270 MB / s, and that is more than 235 times faster than the implementation on Hadoop. <br><br><h2>  Conclusion </h2><br>  Hopefully, we were able to argue several of our ideas regarding the vicious practice of using Hadoop tools to process a not very large set of data, instead of processing them on the same computer using ordinary shell commands and tools.  If you have a really gigantic dataset or really need distributed computing, then of course you may need to use [heavy] Hadoop tools, but more often we see a situation where Hadoop is used instead of traditional relational databases, or where other solutions are both faster and cheaper and easier to maintain. </div><p>Source: <a href="https://habr.com/ru/post/267697/">https://habr.com/ru/post/267697/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../267681/index.html">Why did Apple come up with and introduced in iOS a new font San Francisco?</a></li>
<li><a href="../267683/index.html">Security Week 39: XcodeGhost, D-Link certificate leak, million for a bug on iOS9</a></li>
<li><a href="../267687/index.html">How to improve application localization using glossaries</a></li>
<li><a href="../267689/index.html">Activity Control with Google reCAPTCHA</a></li>
<li><a href="../267691/index.html">Live broadcast Droidcon Moscow 2015</a></li>
<li><a href="../267699/index.html">How yoga kodit and live helps: personal experience</a></li>
<li><a href="../267701/index.html">Playing on bare HTML / CSS</a></li>
<li><a href="../267703/index.html">Putting gnome-screenshot from source to change the format of the screenshot file name</a></li>
<li><a href="../267705/index.html">Violet M-125</a></li>
<li><a href="../267707/index.html">Game client download tracking system</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>