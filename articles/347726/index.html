<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>What is Tokio and Async I / O and why is it needed?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The Rust community has recently concentrated a lot of its efforts on asynchronous I / O, implemented as a Tokio library. And that's great. 


 For man...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>What is Tokio and Async I / O and why is it needed?</h1><div class="post__text post__text-html js-mediator-article"><p> The Rust community has recently concentrated a lot of its efforts on asynchronous I / O, implemented as a <a href="https://github.com/tokio-rs/"><code>Tokio</code></a> library.  And that's great. </p><br><p>  For many of the community members, those who have not worked with web servers and related things, it is not clear what we want to achieve.  When these things were discussed at the time of version 1.0, I also had a vague idea about this, having never worked with it before. </p><br><ul><li>  What is it - <strong>Async I / O</strong> ? </li><li>  What are <strong><em>coroutines</em></strong> ? </li><li>  What are lightweight threads? </li><li>  What are futures?  ( <strong><em>futures</em></strong> )? </li><li>  <strong>How do they fit together?</strong> </li></ul><br><p>  Consider the models of multithreading on the example of Rust and Go. </p><a name="habracut"></a><br><hr><br><h2 id="kakuyu-problemu-my-pytaemsya-reshit">  What problem are we trying to solve? </h2><br><p>  One of the key features of Rust is "fearless concurrency" ( <strong><em>fearless concurrency</em></strong> ).  However, the kind of competition that is needed to handle a large number of tasks, depending on the input / output ( <strong><em>I / O</em></strong> ) performance, and which is available in Go, Elixir, Erlang - is absent in Rust. </p><br><p>  Let's assume that you want to build something like a web server.  It will process thousands of requests at a time (problem <a href="https://en.wikipedia.org/wiki/C10k_problem">c10k</a> ).  Generally speaking, the problem we are considering consists of many tasks that perform mainly I / O operations (especially those related to network interaction). </p><br><p>  "Simultaneous processing of N problems" - this problem is best solved using threads.  However ... <em>Thousands of</em> threads?  This is probably too much.  Working with threads can be quite resource-intensive: each thread must allocate a large stack ( <strong><em>stack</em></strong> ), adjust the thread using a set of system calls.  On top of that, context switching is also expensive. </p><br><p>  Of course, there will be no thousands of threads working at the same time: you have a limited number of cores ( <strong><em>core</em></strong> ), and <strong>at any given time only one thread will be executed on this core</strong> . </p><br><p>  But in examples like this web server, most of these threads will not do any work.  They will wait for either a request or a response. </p><br><p>  With normal threads, when you perform an I / O blocking operation, the system call returns control to the kernel, which the control will not return, because probably the I / O operation has not yet completed.  Instead, the kernel will use this moment as an opportunity to ‚Äúload‚Äù ( <strong><em>swap in</em></strong> ) another thread and continue executing the original thread (starting the I / O operation) when the I / O operation is completed, that is, when the original thread is ‚Äúunblocked‚Äù ( <strong><em>unblocked</em></strong> ) .  That's how you solve such problems in <br>  Rust, when you don‚Äôt use <code>Tokio</code> and similar libraries, run a million threads and let the OS independently schedule ( <strong><em>schedule</em></strong> ) the start and end of threads depending on the I / O. </p><br><p>  But, as we have already found out, the use of threads does not scale well for tasks like this.  (one) </p><br><p>  We need more "lighter" threads. </p><br><hr><br><h2 id="model-mnogopotochnosti-osnovannaya-na-legkovesnyh-nityah-_lightweight-threads_-dumayu-dlya-togo-chtoby-luchshe-ponyat-dannuyu-model-nuzhno-na-nekotoroe-vremya-otvlechsya-ot-rust-i-posmotret-na-yazyk-kotoryy-spravlyaetsya-s-etim-horosho-go">  A threading model based on lightweight threads.  I think in order to better understand this model, you need to take a break from Rust for a while and look at a language that copes with this well, Go. </h2><br><p>  So, <strong>Go</strong> has lightweight threads called goroutines.  You run them with the <code>go</code> keyword.  The web server can execute code similar to the following: </p><br><pre> <code class="rust hljs">listener, err = net.Listen(...) <span class="hljs-comment"><span class="hljs-comment">//   for { conn, err := listener.Accept() //   //   go handler(conn) }</span></span></code> </pre> <br><p>  This is a cycle that waits for new TCP requests and launches a new gorutin and the <code>handler</code> function in it.  Each network connection will be processed by a separate gorutina, which will be destroyed when the <code>handler</code> function is completed.  At the same time, the main loop will be executed, because it works in a separate gorutin. </p><br><p>  If these are not real (OS supported) threads, then what happens? </p><br><p>  Gorutina is an example of a lightweight thread.  The OS does not know anything about them; it sees N threads that are at the disposal of the <strong><em>Go runtime</em></strong> system ( <strong><em>Go runtime</em></strong> , hereinafter CB Go). </p><br><p>  CB Go displays M Gorutin (2) on them, loading and unloading Gorutins like an OS <strong><em>scheduler</em></strong> .  CB Go can do this because the Go code allows interruptible <em>interconnect</em> , allowing the garbage collector ( <strong><em>GC</em></strong> ) to do its work.  All this makes it possible for the scheduler to intentionally stop the work of the gorutina. </p><br><p>  The scheduler is aware of the I / O system, so when Gorutin is waiting for the completion of the I / O operation, she returns the right to execution to the scheduler back. </p><br><p>  In essence, the compiled Go function will have a set of scattered places around it, where it says to the scheduler and the GC: "Take control of yourself if you want" (and also "I expect this and that, please take control of yourself) . </p><br><p>  When the gorutin is loaded into the OS thread, some of the registers will be saved and the pointer to the current instruction ( <em>program counter</em> , <strong><em>PC</em></strong> ) will be transferred to the new gorutin. </p><br><p>  But what happens to the stack?  OS threads have a large stack with them; it is needed so that the functions can work. </p><br><p>  Go uses segmented stacks.  The reason why the thread needs a large stack is that most programming languages ‚Äã‚Äã(PL), including C, expect the stack to be continuous, and the stacks cannot be relocated ( <em>reallocated</em> ), as we do with growing memory buffers, because we expect the data on the stack to remain in the same place, allowing the pointers to the data on the stack to continue to work.  So we prudently reserve for ourselves the entire stack, which in our opinion, we might need (approximately 8 MB).  In this case, we expect that this is enough for us. </p><br><p>  But the expectation that stacks will be continuous, strictly speaking, is not necessary.  In Go, stacks are made up of small pieces.  When the function is called, it checks if there is enough space on the stack for its execution, and if not, it allocates a new piece of stack and runs on it.  So if you want to have thousands of threads doing a small amount of work, they will all receive thousands of small stacks, and everything will be fine. </p><br><p>  In fact, Go does a little different today: it copies stacks.  I mentioned that stacks cannot just be re-allocated, it is expected that the data on the stack will remain in the same place.  But this is not always the case, because Go has a GC, so in any case, it knows which pointers to which it points, and can rewrite the pointers pointing to the stack if necessary. </p><br><p>  In any case, the Go runtime system works well with such things.  Gorutin are very cheap, and you can run them thousands, without any problems. </p><br><p>  Rust <em>supported</em> lightweight / green threads (I believe he used segmented stacks).  However, Rust carefully ensures that you do not pay for those things that you do not use, and this (the use of lightweight threads) imposes restrictions on all your code, even if you do not use these lightweight threads. </p><br><p>  <strong>Lightweight threads were removed from Rust before version 1.0 was released</strong> . </p><br><hr><br><h2 id="asinhronnyy-vvodvyvod">  Asynchronous I / O </h2><br><p>  The main building block of this is asynchronous I / O.  As mentioned in the previous section, with the usual <strong>blocking I / O</strong> , when you start an I / O operation, your thread will not be allowed to continue execution (it has become blocked) until the operation is completed.  This is good when you work with OS threads (the OS scheduler does all the work for you!), But if you have lightweight threads, you will want to replace the lightweight thread that runs on the OS thread with another thread. </p><br><p>  Instead, you use <strong>non-blocking I / O</strong> , where the thread queues the I / O request to the OS and continues execution.  An I / O request will be executed by the kernel after some time.  After that, the thread will have to ask the OS: ‚ÄúHas my I / O operation completed?‚Äù Before using the I / O result. </p><br><p>  Of course, frequently asking the OS whether it has completed an I / O requires resources.  That is why there are such system calls as <code>epoll</code> .  Here you can put together a set of incomplete I / O operations and then ask the OS to wake up your thread when any of these operations is complete.  So you can have a scheduler thread (real thread) that unloads lightweight threads that are waiting for the I / O to complete.  When nothing happens, the scheduler thread may go to sleep ( <em>sleep</em> ), causing an <code>epoll</code> , waiting until the OS wakes it up (when one of the I / O operations is completed). </p><br><p>  The internal mechanism involved here is probably more complicated. </p><br><p>  Let's go back to Rust.  Rust has a <a href="https://github.com/carllerche/mio"><code>mio</code></a> library, which is a platform-independent wrapper over non-blocking I / O and tools like <code>epoll</code> (GNU / Linux), <code>kqueue</code> (FreeBSD), etc. This is a building block, and although those that are used to use <code>epoll</code> in C directly, it may seem convenient, it does not provide wonderful <br>  models like go.  However, we can achieve this. </p><br><hr><br><h2 id="futury-_futures_">  Futures ( <em>futures</em> ) </h2><br><p>  They are another building block.  <code>Future</code> - the promise that sooner or later the value will be obtained (in JavaScript, they are called <code>Promise</code> ). </p><br><p>  For example, you can make a request for the arrival of a request on a network socket and get <code>Future</code> back (actually <code>Stream</code> , which is similar to futur, but is used to obtain a sequence of values).  This <code>Future</code> will not contain the answer, but will know when it comes.  You can call <code>wait()</code> on <code>Future</code> , which will be blocked until the resulting value is obtained.  You can also call <code>poll()</code> on the <code>Future</code> , <br>  asking if the <code>Future</code> response (it will give you the result, if any). </p><br><p>  Futures can be chained, so you can write code like <code>future.then(|result| process(result))</code> .  A passed closure ( <em>closure</em> ) itself can produce another futur, so that you can chain several entities in a chain, for example, an I / O operation.  With futures on a chain, <code>poll()</code> is a way to gradually execute a program;  each time you call it, it will go to the next futur, provided that the current future is ready (contains the result). </p><br><p>  This is a good explanation of how non-blocking I / O works. </p><br><p>  Linking futures to a chain works like chained iterators.  Each <code>and_then</code> call (or any other combinator) returns a structure that contains an internal future, which can optionally contain a closure.  The closures themselves carry their own links and data with them, so all this is very similar to small stacks! </p><br><hr><br><h2 id="tokio">  Tokio </h2><br><p>  <code>Tokio</code> is essentially a great <code>mio</code> wrapper that uses futures.  Tokio has a main <strong><em>event loop,</em></strong> and you pass it closures that return futures.  This cycle will perform all the closures that you pass to it, using <code>mio</code> to find out which futures can progress and push them further (causing <code>poll()</code> ). </p><br><p>  At the ideological level, this is quite similar to what Go does.  You must manually configure the main event loop (scheduler), and as soon as you do this, you can pass on to the loop tasks that I / O do periodically.  Every time one of the current tasks is blocked on an I / O operation, the cycle will load a new task.  The key difference is that <code>Tokio</code> works in the same thread, while the Go scheduler can use several threads of the OS for execution.  However, you can move tasks that are highly processor dependent to other threads of the OS and use channels ( <em>channels</em> ) to coordinate them, which is not difficult. </p><br><p>  Although at the ideological level, this is similar to what we have in Go, the code does not look very nice.  The following code on Go: </p><br><pre> <code class="rust hljs"><span class="hljs-comment"><span class="hljs-comment">//      func foo(...) ReturnType { data := doIo() result := compute(data) moreData = doMoreIo(result) moreResult := moreCompute(data) // ... return someFinalResult }</span></span></code> </pre> <br><p>  on Rust looks like this: </p><br><pre> <code class="rust hljs"><span class="hljs-comment"><span class="hljs-comment">//      fn foo(...) -&gt; Future&lt;ReturnType, ErrorType&gt; { do_io().and_then(|data| do_more_io(compute(data))) .and_then(|more_data| do_even_more_io(more_compute(more_data))) // ...... }</span></span></code> </pre> <br><p>  Not beautiful.  <a href="http://alexcrichton.com/futures-rs/futures/future/fn.loop_fn.html">The code gets worse if you enter branching and loops</a> .  The problem is that in Go we got the <em>interruption points</em> for free, but in Rust we have to manually encode this by linking the combinators into a chain, getting some sort of state machine (automaton). </p><br><hr><br><h2 id="generatory-i-asyncawait">  Generators and <code>async/await</code> </h2><br><p>  This is where the generators appear (they are also called corintins). <br>  <a href="https://doc.rust-lang.org/nightly/unstable-book/language-features/generators.html">Generators</a> are an experimental feature in Rust.  Here is an example: </p><br><pre> <code class="rust hljs"><span class="hljs-keyword"><span class="hljs-keyword">let</span></span> <span class="hljs-keyword"><span class="hljs-keyword">mut</span></span> generator = || { <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">loop</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> i; i += <span class="hljs-number"><span class="hljs-number">1</span></span>; } }; <span class="hljs-built_in"><span class="hljs-built_in">assert_eq!</span></span>(generator.resume(), GeneratorState::Yielded(<span class="hljs-number"><span class="hljs-number">0</span></span>)); <span class="hljs-built_in"><span class="hljs-built_in">assert_eq!</span></span>(generator.resume(), GeneratorState::Yielded(<span class="hljs-number"><span class="hljs-number">1</span></span>)); <span class="hljs-built_in"><span class="hljs-built_in">assert_eq!</span></span>(generator.resume(), GeneratorState::Yielded(<span class="hljs-number"><span class="hljs-number">2</span></span>));</code> </pre> <br><p>  Functions are entities that perform the task and return the result to the calling code once.  And generators can return ( <code>yield</code> ) several times: they stop execution in order to return some data, and can be continued, at the same time they will be executed until the next <code>yield</code> .  Although my example does not show this, generators can complete execution in the same way as regular functions. </p><br><p>  Closures in Rust are <a href="http://huonw.github.io/blog/2015/05/finding-closure-in-rust/"><code>   ,    ( <em>captured</em> )  +    Fn</code> types of traits, in order to make the structure callable</a> . </p><br><p>  Generators are similar to them, among other things, they implement a type of <code>Generator</code> and usually contain an <code>enum</code> , which represents different states. </p><br><p>  <a href="https://doc.rust-lang.org/nightly/unstable-book/language-features/generators.html">The "unstable" book</a> has some examples of how this state machine looks. </p><br><p>  This is much closer to what we were looking for!  Now our code looks like this: </p><br><pre> <code class="rust hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fn</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">foo</span></span></span></span>(...) -&gt; Future&lt;ReturnType, ErrorType&gt; { <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> generator = || { <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> <span class="hljs-keyword"><span class="hljs-keyword">mut</span></span> future = do_io(); <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> data; <span class="hljs-keyword"><span class="hljs-keyword">loop</span></span> { <span class="hljs-comment"><span class="hljs-comment">//  ,     //   ,     match future.poll() { Ok(Async::Ready(d)) =&gt; { data = d; break }, Ok(Async::NotReady(d)) =&gt; (), Err(..) =&gt; ... }; yield future.polling_info(); } let result = compute(data); //      `doMoreIo()`,  . . } futurify(generator) }</span></span></code> </pre> <br><p>  <code>futurify</code> is a function that accepts a generator and returns a footer, which on each call of the <code>poll</code> will do the generator's <code>resume()</code> and return <code>NotReady</code> until the generator completes its execution. </p><br><p>  But wait, it's even more ugly!  What is the point of transforming our relatively clean chain of callbacks into this messivo? </p><br><p>  If you look carefully, you will see that the code is consistent.  We converted our callback code to the same linear flow as the Go code, however it contains a strange <code>yield</code> code in a loop, <code>futurify</code> also does not look <code>futurify</code> . </p><br><p>  Here <a href="https://github.com/alexcrichton/futures-await">futures-await</a> comes to the rescue.  <code>futures-await</code> is a procedural macro that removes this redundant code.  In essence, it allows the function to be rewritten as </p><br><pre> <code class="rust hljs"><span class="hljs-meta"><span class="hljs-meta">#[async]</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fn</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">foo</span></span></span></span>(...) -&gt; <span class="hljs-built_in"><span class="hljs-built_in">Result</span></span>&lt;ReturnType, ErrorType&gt; { <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> data = await!(do_io()); <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> result = compute(data); <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> more_data = await!(do_more_io()); <span class="hljs-comment"><span class="hljs-comment">// ....</span></span></code> </pre> <br><p>  Neat and clean.  Almost as clean as Go code, we also directly call <code>await!()</code> .  The <code>await</code> call data provides the same functionality as the automatically set breakpoints in Go. </p><br><p>  And, of course, since the code uses generators, you can use a loop, enter a branch and do whatever you want, and the code will still remain clean. </p><br><hr><br><h2 id="podvodya-itogi">  Summing up </h2><br><p>  Futures in Rust can be chained together to provide a lightweight, stack-like system.  With <code>async/await</code> you can beautifully write a chain of futures, and <code>await</code> provides explicit breakpoints for each I / O operation.  <code>Tokio</code> provides a ‚Äúscheduler‚Äù ‚Äîthe main event loop to which you can pass asynchronous functions, while under the hood <code>mio</code> used to abstract from low-level non-blocking I / O primitives. </p><br><p>  These are components that can be used separately - you can use <code>Tokio</code> with futures, without <code>async/await</code> .  You can use <code>async/await</code> without using <code>Tokio</code> .  For example, this might be suitable for a Servo network stack.  It does not need to do a lot of parallel I / O operations (at least not on the order of a thousand threads), so that it can use multiplexed OS threads.  However, we still want to have a pool of threads and <br>  consistently (pipeline) to process data, and async / await is a good help here. </p><br><p>  To summarize: all these components, despite the small amount of redundant ( <em>boilerplate</em> ) code, give something almost as clean as the Gorutins in Go.  And since the generators (and, therefore, <code>async/await</code> ) coexist well with the borrowing analyzer ( <strong><em>borrow checker</em></strong> , borrowck), the security invariants supported by Rust are still valid, and we get "fearless competitiveness" for programs that perform a large amount of I / O tasks. </p><br><p>  Many thanks to everyone from the Rustycrate community who participated in the translation, proofreading and editing of this article.  Namely: vitvakatu, ozkriff. </p><br><hr><br><p>  [1]: This is not necessarily the case for all server applications. <br>  For example, <code>Apache HTTP Server</code> uses OS threads.  Often OS threads are the best. <br>  tool for solving the problem. </p><br><p>  [2]: Lightweight threads are said to implement the M: N model - ( <em>"green" threading</em> ). </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/347726/">https://habr.com/ru/post/347726/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../347712/index.html">A little about cheating counters site visits</a></li>
<li><a href="../347716/index.html">Experience algorithmic composition in the language of ChucK</a></li>
<li><a href="../347718/index.html">Making a log system for Minecraft</a></li>
<li><a href="../347720/index.html">"... They want to know what will happen" or write a fortunetech ball in C # NanoCAD CAD software (MultiCAD .NET API)</a></li>
<li><a href="../347722/index.html">PHP Digest number 124 (January 14 - 28, 2018)</a></li>
<li><a href="../347728/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ299 (January 22 - 28, 2018)</a></li>
<li><a href="../347730/index.html">Elm Developer Tools</a></li>
<li><a href="../347734/index.html">Automatic monitoring of newly installed software in ZABBIX</a></li>
<li><a href="../347736/index.html">CC1101 running a PIC controller or building a peer-to-peer network for a radio engineer (part 2)</a></li>
<li><a href="../347738/index.html">Mobile devices from the inside. What is GPT?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>