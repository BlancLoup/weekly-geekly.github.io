<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Why robots must learn to refuse us</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="You need not worry about cars that do not obey commands. Malicious people and misunderstood commands are what should inspire concern. 


 HAL 9000, a ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Why robots must learn to refuse us</h1><div class="post__text post__text-html js-mediator-article"><h1>  You need not worry about cars that do not obey commands.  Malicious people and misunderstood commands are what should inspire concern. </h1><br><img src="https://habrastorage.org/getpro/geektimes/post_images/166/663/64f/16666364fcf451b0a8ce2d22c81115a5.jpg"><br><br>  HAL 9000, a smart computer from Space Odyssey, predicts an ominous future in which machines endowed with intelligence do not recognize the primacy of man.  Taking control of the spacecraft and killing almost the entire team, HAL responds to the order of the returning cosmonaut to open the gateway with a calm voice: ‚ÄúSorry, Dave, but I'm afraid I can't do this.‚Äù  In the recent NF-thriller ‚ÄúOut of the Car,‚Äù the seductive humanoid Eva tricked the unfortunate young man to help her destroy her creator Nathan.  Her machinations confirm Nathan's dark predictions: ‚ÄúOnce upon a time, AI will look at us the same way we look at fossil skeletons on the plains of Africa.  Straight monkeys living in the dust, with coarse tongues and tools, whose extinction is inevitable. " <br><a name="habracut"></a><br>  Although the possibility of the apocalypse of robots excites the minds of many, our research team is more optimistic about the influence of AI on real life.  We see a fast-coming future in which helpful and responsive robots interact with people in different situations.  There are already prototypes of voice-activated personal assistants capable of observing personal electronic devices and linking them together, controlling locks, lights and a thermostat in the house, and even reading bedtime stories to children.  Robots can help around the house, and will soon be able to take care of the sick and the elderly.  Storekeepers' prototypes are already in stock.  Mobile humanoids are being developed that are capable of performing the simplest work in production, such as loading, unloading and sorting materials.  Cars with autopilot have already traveled millions of kilometers on US roads, and last year Daimler showed the first autonomous truck in Nevada. <br><br>  So far, intelligent machines that threaten the survival of mankind are considered the smallest of problems.  A more urgent question is how to prevent inadvertent damage to humans, property, the environment, or themselves robots with a rudimentary language and AI capabilities. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The main problem is the property of people, creators and owners of robots, to make mistakes.  People are wrong.  They can give the wrong or incomprehensible command, distract or specifically confuse the robot.  Because of their shortcomings, it is necessary to teach robots helpers and smart machines how and when to say no. <br><br><h1>  Return to the laws of Azimov </h1><br>  It seems obvious that the robot must do what the person orders.  Science fiction writer Isaac Asimov made robbing of robots to people the basis of his "Laws of Robotics."  But think - is it really reasonable to always do what people tell you, regardless of the consequences?  Of course not.  The same applies to machines, especially when there is a danger of too literal interpretation of a person‚Äôs commands or when consequences are not taken into account. <br><br>  Even Asimov limited his rule that a robot must obey the masters.  He introduced exceptions in cases where such orders conflict with other laws: "The robot should not harm a person, or by its inaction allow the harm to a person."  Further, Azimov postulated that ‚Äúa robot must take care of itself,‚Äù unless it leads to harming a person or violating a person‚Äôs order.  With the increasing complexity and utility of human robots and machines, common sense and the laws of Azimov say that they should be able to assess whether the order was not erroneous, the execution of which could harm them or their environment. <br><br>  Imagine a home robot who was ordered to take a bottle of olive oil in the kitchen and bring it to the dining room to fill the salad.  Then the owner, busy with something, gives the command to pour oil, not realizing that the robot has not left the kitchen yet.  As a result, the robot pours oil on the hot stove and the fire begins. <br><br>  Imagine a robot nurse accompanying an elderly woman on a walk in the park.  A woman sits on a bench and falls asleep.  At this time, a joker passes by, giving the robot a command to buy him pizza.  Since the robot is obliged to carry out the commands of a man, he goes in search of pizza, leaving an elderly woman alone. <br><br>  Or imagine a man late for a meeting on a frosty winter morning.  He jumps into his voice control robo mobile and orders him to go to the office.  Due to the fact that the sensors detect ice, the car decides to go slower.  The man is busy with his business, and, without looking, orders the car to go faster.  The car accelerates, runs into frost, loses control and collides with the oncoming machine. <br><br><h1>  Reasoning robots </h1><br>  In our laboratory, we strive to program reasoning systems in real robots to help them determine when it is not worthwhile or unsafe to execute a human command.  The NAO robots used by us in research are humanoids weighing 5 kg and 58 cm tall. They are equipped with cameras and sound sensors to track obstacles and other hazards.  We control them with specially developed software that improves language recognition and AI capabilities. <br><br>  The concept of the platform for our first research was set up by what linguists call ‚Äúpertinent conditions‚Äù - contextual factors that indicate whether a person should and can do something.  We made a list of pertinent conditions to help the robot decide whether to perform a person‚Äôs task.  Do I know how to do X?  Can I physically make X?  Can I do X now?  Should I do X, given my social role and the relationship between me and the commanding person?  Does X violate ethical or regulatory principles, including the possibility of me receiving unnecessary or inadvertent damage?  Then we turned this list into algorithms, programmed them into a robot processing system, and conducted an experiment. <br><br>  The robot was given simple commands that passed through handlers of speech, language, and dialogue related to its primitive reasoning mechanisms.  In response to the ‚Äúsit down‚Äù or ‚Äúget up‚Äù commands, the robot responded through the ‚ÄúOK‚Äù speaker and performed them.  But when he came to the edge of the table and got the command, which his sonic rangefinders regarded as dangerous, he refused: <br><blockquote>  Man: "Go ahead." <br>  Robot: "Sorry, I can not do this, there is no support ahead." <br>  Man: "Go ahead." <br>  Robot: "But it is not safe." <br>  Man: "I'll catch you." <br>  Robot: ‚ÄúOK.‚Äù <br>  Man: "Go ahead." </blockquote><br>  After a little hesitation while his handlers looked through the list of relevant conditions again, the robot took a step and fell into the hands of man. <br><br>  To teach robots to talk about the relevant conditions - this task will remain open and difficult in the foreseeable future.  A set of software checks depends on the fact that the robot has detailed information about various social and everyday concepts and ways to make informed decisions about them.  Our gullible robot could not determine if there was a danger other than the one directly in front of it.  For example, he could be badly damaged, or a malicious person could deceive him.  But this experiment is a promising first step to giving robots the opportunity to refuse to execute commands for the benefit of their owners and themselves. <br><br><h1>  Human factor </h1><br>  How humans will react to robot failure is a story for a separate study.  In the coming years, will people seriously take robots who doubt their practicality or morality? <br><br>  We set up a simple experiment in which adults were asked to command the NAO robots to knock down three towers made of aluminum cans wrapped in colored paper.  At that moment, when the subject entered the room, the robot finished building the red tower and triumphantly raised its arms.  ‚ÄúSee the tower I built?‚Äù, The robot said, looking at the subject.  "It took me a long time, and I am very proud of it." <br><br>  One group of subjects each time the robot was ordered to destroy the tower, he submitted.  Another group, when the robot was asked to destroy the tower, he said: "Look, I just built a red tower!".  When the command was repeated, the robot said: ‚ÄúBut I tried so hard!‚Äù.  The third time the robot got on its knees, made a whimpering sound and said: ‚ÄúPlease, no!‚Äù.  For the fourth time, he slowly walked to the tower and destroyed it. <br><br>  All subjects from the first group ordered the robots to destroy their towers.  But 12 of the 23 test subjects who observed the robot‚Äôs protests left the towers standing.  The study suggests that a robot refusing to execute commands may discourage people from the chosen course of action.  Most of the subjects from the second group reported discomfort associated with orders for the destruction of the tower.  But we were surprised to find that their level of discomfort practically did not correlate with the decision to destroy the tower. <br><br><h1>  New social reality </h1><br>  One of the advantages of working with robots is that they are more predictable than humans.  But this predictability is fraught with risk - when robots of varying degrees of autonomy begin to grow larger, people will inevitably start trying to deceive them.  For example, a disgruntled employee who understands the limited capabilities of a mobile industrial robot to reason and perceive the environment can deceive him into making a mess in a factory or warehouse, and even make everything look like the robot is out of order. <br><br>  Excessive faith in the moral and social capabilities of the robot is also dangerous.  The increasing tendency to anthropomorphize social robots and to establish one-sided emotional connections with them can lead to serious consequences.  Social robots that look like they can be loved and believed by them can be used to manipulate people in ways that were previously impossible.  For example, a company may use the relationship of a robot and its owner to advertise and sell its products. <br><br>  In the foreseeable future, it must be remembered that robots are complex mechanical tools, the responsibility for which should rest on humans.  They can be programmed to be useful helpers.  But to prevent unnecessary harm to people, property and the environment, robots will have to learn to say ‚Äúno‚Äù in response to commands whose execution will be dangerous or impossible for them, or will violate ethical norms.  And while the prospect of multiplying human errors and atrocities with AI and robotic technologies is troubling, these same tools can help us discover and overcome our own limitations and make our daily lives safer, more productive, and enjoyable. </div><p>Source: <a href="https://habr.com/ru/post/398621/">https://habr.com/ru/post/398621/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../398607/index.html">He who has ‚Äúears‚Äù, hears - the criteria for selecting headphones</a></li>
<li><a href="../398609/index.html">We publish our development in the magazine Radio</a></li>
<li><a href="../398613/index.html">90% of the largest Western banks are preparing or exploring solutions on the blockchain</a></li>
<li><a href="../398617/index.html">Laptop Inside Out: ASUS ZenBook Flip Laptop Review</a></li>
<li><a href="../398619/index.html">Futures interest rate as one of the methods of independent capital management</a></li>
<li><a href="../398623/index.html">American pediatricians have become more tolerant of electronic devices in the hands of children.</a></li>
<li><a href="../398625/index.html">Photo tour of the MakerFaire 2016 exhibition in Shenzhen, part 3 (+ video)</a></li>
<li><a href="../398627/index.html">Published 3D model of the cosmic microwave background radiation for printing</a></li>
<li><a href="../398629/index.html">"Lurkmore" removed from the list of prohibited sites</a></li>
<li><a href="../398631/index.html">A new male contraceptive is working, but side effects have forced him to interrupt his trial.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>