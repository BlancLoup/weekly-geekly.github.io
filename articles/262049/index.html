<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>OpenStack - deployed "hands" Kilo</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello to all Habraluds! 

 In the last article , I talked about how you can quickly deploy a test environment using DevStack. In this post, I‚Äôll tell ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>OpenStack - deployed "hands" Kilo</h1><div class="post__text post__text-html js-mediator-article">  Hello to all Habraluds! <br><br>  In the last <a href="http://habrahabr.ru/post/261715/">article</a> , I talked about how you can quickly deploy a test environment using DevStack.  In this post, I‚Äôll tell you how to deploy your OpenStack ‚Äúcloud‚Äù on two machines (Controller, Compute) in the configuration: <br><ul><li>  Keystone </li><li>  Glance </li><li>  Nova </li><li>  Neutron </li><li>  Cinder </li><li>  Horizon </li></ul><br><br>  In general, this system will allow us to run multiple virtual machines (how much memory and CPU will allow for <i>compute</i> ), create virtual networks, create virtual disks and connect them to a VM, and of course, manage all of this through a convenient dashboard. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Caution!  A lot of "tails" with a list of commands and configs! <br><a name="habracut"></a><br><br>  Immediately I will say: <br><ul><li>  Theoretically, I could forget to add something or not correct in configs, forget that you need to restart some service or something else. </li><li>  The publication does not clone, but takes its roots from the official documentation that is <a href="http://docs.openstack.org/kilo/install-guide/install/apt/content/">here</a> ( <i>eng.</i> ) </li><li>  The purpose of writing all this is to have some more readable OpenStack mana in Russian.  I do not pretend that it turned out for me, but I hope this will encourage someone to do better.  Also, I am ready to take into account comments on the necessary edits of the publication, so that it becomes more readable. </li></ul><br><br>  Do not mindlessly "copy-paste".  This, of course, will help establish the OpenStack environment for this guide, but will not teach how to use this knowledge in the field. <br><br><h5>  <b>What will we use?</b> </h5><br><ul><li>  Controller <i>i3-540 / 8Gb / 2x120Gb + 2x500Gb / 2NIC</i> </li><li>  Compute <i>i7-2600 / 32Gb / 2x500Gb / 1NIC</i> </li></ul><br>  OS: <i>Ubuntu 14.04</i> (You can use CentOS, but the guide will be based on Ubuntu). <br>  OpenStack Edition: <i>Kilo</i> <br><br><h5>  <b>Training</b> </h5><br><h6>  Network </h6><br><br>  The original manual uses 4 networks: <br>  Management - 10.0.0.0/24 - VLAN 10 <br>  Tunnel - 10.0.1.0/24 - VLAN 11 <br>  Storage - 10.0.2.0/24 - VLAN 12 <br>  External - 192.168.1.0/24 <br><br>  In our case, the external network looks somewhere into the home network, but by and large this interface can also look into the ‚Äúworld wide web‚Äù - everything depends on what you are deploying the cloud for. <br><br>  It would be very nice to have a workable dns-server.  I used dnsmasq. <br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/hosts 10.0.0.11 controller 10.0.0.31 compute1</span></span></code> </pre> <br><br>  Configuring interfaces <br><div class="spoiler">  <b class="spoiler_title">on the controller:</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/network/interfaces auto p2p1.10 iface p2p1.10 inet static address 10.0.0.11 netmask 255.255.255.0 gateway 10.0.0.1 dns-nameservers 10.0.0.1 auto p2p1.11 iface p2p1.11 inet static address 10.0.1.11 netmask 255.255.255.0 auto p2p1.12 iface p2p1.12 inet static address 10.0.2.11 netmask 255.255.255.0 #     auto p3p1 iface p3p1 inet manual up ip link set dev $IFACE up down ip link set dev $IFACE down</span></span></code> </pre><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">on the compute node:</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/network/interfaces auto p2p1.10 iface p2p1.10 inet static address 10.0.0.31 netmask 255.255.255.0 gateway 10.0.0.1 dns-nameservers 10.0.0.1 auto p2p1.11 iface p2p1.11 inet static address 10.0.1.31 netmask 255.255.255.0 auto p2p1.12 iface p2p1.12 inet static address 10.0.2.31 netmask 255.255.255.0</span></span></code> </pre><br></div></div><br><br>  We check that both cars see each other and go to the network. <br><br><h6>  NTP </h6><br>  On the controller: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install ntp -y # cat /etc/ntp.conf server ntp.oceantelecom.ru iburst restrict -4 default kod notrap nomodify restrict -6 default kod notrap nomodify # service ntp stop # ntpdate ntp.oceantelecom.ru # service ntp start</span></span></code> </pre><br><br>  On compute node: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install ntp -y # cat /etc/ntp.conf server controller iburst # service ntp stop # ntpdate controller # service ntp start</span></span></code> </pre><br><br><h6>  Kilo repository </h6><br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install ubuntu-cloud-keyring # echo "deb http://ubuntu-cloud.archive.canonical.com/ubuntu" "trusty-updates/kilo main" &gt; /etc/apt/sources.list.d/cloudarchive-kilo.list</span></span></code> </pre><br><br>  Kilo is a pretty young release - April 2015.  Most of all in this release I liked the Russian language in the Horizon interface. <br>  More details can be read <a href="https://wiki.openstack.org/wiki/ReleaseNotes/Kilo">here</a> . <br><br>  Updating: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get update &amp;&amp; apt-get dist-upgrade -y</span></span></code> </pre><br><br><h6>  SQL + RabbitMQ </h6><br>  As a SQL server, it can be MySQL, PostgreSQL, Oracle, or any other that is supported by SQLAlchemy.  We will install MariaDB as in the official manual. <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install mariadb-server python-mysqldb -y # cat /etc/mysql/conf.d/mysqld_openstack.cnf [mysqld] bind-address = 10.0.0.11 default-storage-engine = innodb innodb_file_per_table collation-server = utf8_general_ci init-connect = 'SET NAMES utf8' character-set-server = utf8 # service mysql restart # mysql_secure_installation</span></span></code> </pre><br>  If there are unnecessary HDDs with good performance, then the database files can be put on it and it will not be superfluous if you plan to develop the booth by computational nodes. <br><br>  And of course RabbitMQ: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install rabbitmq-server # rabbitmq-plugins enable rabbitmq_management # service rabbitmq-server restart</span></span></code> </pre><br>  We install raebita and run the administrative WebGUI, for the convenience of tracking the queues. <br><br>  Create a user and set rights to him: <br><pre> <code class="bash hljs">rabbitmqctl add_user openstack RABBIT_PASS rabbitmqctl set_permissions openstack <span class="hljs-string"><span class="hljs-string">".*"</span></span> <span class="hljs-string"><span class="hljs-string">".*"</span></span> <span class="hljs-string"><span class="hljs-string">".*"</span></span></code> </pre><br><br><h5>  <b>Keystone</b> </h5><br>  Keystone is the authorization center for OpenStack.  All authorizations go through it.  Keystone stores data in a SQL database, but also uses memcache. <br><br>  Prepare the database: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># mysql -u root -p CREATE DATABASE keystone; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY 'KEYSTONE_DBPASS'; GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY 'KEYSTONE_DBPASS';</span></span></code> </pre><br>  Naturally, do not forget to substitute your password, as elsewhere. <br><br>  Disable autoloading keystone service and install all the necessary components: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># echo "manual" &gt; /etc/init/keystone.override # apt-get install keystone python-openstackclient apache2 libapache2-mod-wsgi memcached python-memcache</span></span></code> </pre><br><br>  In the <i>/etc/keystone/keystone.conf</i> config <i>we</i> write the following lines: <br><pre> <code class="bash hljs">[DEFAULT] admin_token = ADMIN_TOKEN [database] connection = mysql://keystone:KEYSTONE_DBPASS@controller/keystone [memcache] servers = localhost:11211 [token] provider = keystone.token.providers.uuid.Provider driver = keystone.token.persistence.backends.memcache.Token [revoke] driver = keystone.contrib.revoke.backends.sql.Revoke</code> </pre><br><br>  <i>ADMIN_TOKEN</i> generic with " <i>openssl rand -hex 16</i> ". <br>  Synchronize local database with SQL server <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># su -s /bin/sh -c "keystone-manage db_sync" keystone</span></span></code> </pre><br><br>  Configuring Apache: <br><div class="spoiler">  <b class="spoiler_title">footcloth</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/apache2/apache2.conf ... ServerName controller ... # cat /etc/apache2/sites-available/wsgi-keystone.conf Listen 5000 Listen 35357 &lt;VirtualHost *:5000&gt; WSGIDaemonProcess keystone-public processes=5 threads=1 user=keystone display-name=%{GROUP} WSGIProcessGroup keystone-public WSGIScriptAlias / /var/www/cgi-bin/keystone/main WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On &lt;IfVersion &gt;= 2.4&gt; ErrorLogFormat "%{cu}t %M" &lt;/IfVersion&gt; LogLevel info ErrorLog /var/log/apache2/keystone-error.log CustomLog /var/log/apache2/keystone-access.log combined &lt;/VirtualHost&gt; &lt;VirtualHost *:35357&gt; WSGIDaemonProcess keystone-admin processes=5 threads=1 user=keystone display-name=%{GROUP} WSGIProcessGroup keystone-admin WSGIScriptAlias / /var/www/cgi-bin/keystone/admin WSGIApplicationGroup %{GLOBAL} WSGIPassAuthorization On &lt;IfVersion &gt;= 2.4&gt; ErrorLogFormat "%{cu}t %M" &lt;/IfVersion&gt; LogLevel info ErrorLog /var/log/apache2/keystone-error.log CustomLog /var/log/apache2/keystone-access.log combined &lt;/VirtualHost&gt; # ln -s /etc/apache2/sites-available/wsgi-keystone.conf /etc/apache2/sites-enabled # mkdir -p /var/www/cgi-bin/keystone # curl http://git.openstack.org/cgit/openstack/keystone/plain/httpd/keystone.py?h=stable/kilo | tee /var/www/cgi-bin/keystone/main /var/www/cgi-bin/keystone/admin # chown -R keystone:keystone /var/www/cgi-bin/keystone # chmod 755 /var/www/cgi-bin/keystone/* # service apache2 restart # rm -f /var/lib/keystone/keystone.db</span></span></code> </pre><br></div></div><br>  We change the <i>ServerName</i> to the name of our controller. <br>  We take working scripts from the openstack repository. <br><br>  Set up endpoints.  In general, it is thanks to endpoint `s that openstack will know where and which service is running. <br><br>  Add environment variables in order not to specify them each time in the keystone parameters: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># export OS_TOKEN=ADMIN_TOKEN # export OS_URL=http://controller:35357/v2.0</span></span></code> </pre><br><br>  Now we create the service: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># openstack service create --name keystone --description "OpenStack Identity" identity</span></span></code> </pre><br>  Well, create endpoint API: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># openstack endpoint create --publicurl http://controller:5000/v2.0 --internalurl http://controller:5000/v2.0 --adminurl http://controller:35357/v2.0 --region RegionOne identity</span></span></code> </pre><br>  <i>RegionOne</i> can be changed to any readable name.  I will use it to not "bother". <br><br>  Create projects, users and roles. <br><br>  We will continue to do the official mana, so that everything is the same: admin and demo <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># openstack project create --description "Admin Project" admin # openstack user create --password-prompt admin # openstack role create admin # openstack role add --project admin --user admin admin</span></span></code> </pre><br>  Password for admin come up with yourself.  In order: created the project ‚ÄúAdmin Project‚Äù, the user and the admin role, and connect the project and the user with the role. <br><br>  Now create the <i>service</i> project: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># openstack project create --description "Service Project" service</span></span></code> </pre><br><br>  By analogy with admin, we create a demo: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># openstack project create --description "Demo Project" demo # openstack user create --password-prompt demo # openstack role create user # openstack role add --project demo --user demo user</span></span></code> </pre><br><br>  Create environment scripts: <br><div class="spoiler">  <b class="spoiler_title">scripts</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat admin-openrc.sh export OS_PROJECT_DOMAIN_ID=default export OS_USER_DOMAIN_ID=default export OS_PROJECT_NAME=admin export OS_TENANT_NAME=admin export OS_USERNAME=admin export OS_PASSWORD=ADMIN_PASS export OS_AUTH_URL=http://controller:35357/v3 export OS_IMAGE_API_VERSION=2 export OS_VOLUME_API_VERSION=2 # cat demo-openrc.sh export OS_PROJECT_DOMAIN_ID=default export OS_USER_DOMAIN_ID=default export OS_PROJECT_NAME=demo export OS_TENANT_NAME=demo export OS_USERNAME=demo export OS_PASSWORD=DEMO_PASS export OS_AUTH_URL=http://controller:5000/v3 export OS_IMAGE_API_VERSION=2 export OS_VOLUME_API_VERSION=2</span></span></code> </pre><br></div></div><br><br>  Actually: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># source admin-openrc.sh</span></span></code> </pre><br>  This completes the <i>keystone</i> service <i>setup</i> . <br><br><h5>  <b>Glance</b> </h5><br>  Glance is an OpenStack tool for storing templates (images) of virtual machines.  Images can be stored in Swift, in Glance`s own repository, but somewhere else - the main thing is that this image can be obtained via http. <br><br>  Let's start as always with <i>mysql</i> : <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># mysql -u root -p CREATE DATABASE glance; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' IDENTIFIED BY 'GLANCE_DBPASS'; GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' IDENTIFIED BY 'GLANCE_DBPASS';</span></span></code> </pre><br><br>  Create in the <i>keystone</i> information about the future service: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># openstack user create --password-prompt glance # openstack role add --project service --user glance admin # openstack service create --name glance --description "OpenStack Image service" image # openstack endpoint create --publicurl http://controller:9292 --internalurl http://controller:9292 --adminurl http://controller:9292 --region RegionOne image</span></span></code> </pre><br>  We create the user <i>glance</i> and connect it to the <i>admin</i> role, since  all services will work from this role, we create the <i>glance</i> service, set the endpoint. <br><br>  Now we proceed to the installation: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install glance python-glanceclient</span></span></code> </pre><br>  and setup: <br><div class="spoiler">  <b class="spoiler_title">tuning</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/glance/glance-api.conf [DEFAULT] ... notification_driver = noop [database] connection = mysql://glance:GLANCE_DBPASS@controller/glance [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = glance password = GLANCE_PASS [paste_deploy] flavor = keystone [glance_store] default_store = file filesystem_store_datadir = /var/lib/glance/images/</span></span></code> </pre><br></div></div><br><br>  Whatever is in the <i>[keystone_authtoken]</i> section, it needs to be deleted.  GLANCE_PASS is the password from the glance user in keystone.  <i>filesystem_store_datadir</i> is the path to the repository where our images will be located.  I recommend that you mount either a raid array or reliable network storage to this directory in order not to accidentally lose all our images due to a disk failure. <br><br>  In <i>/etc/glance/glance-registry.conf we</i> duplicate the same information from the <i>database, keystone_authtoken, paste_deploy, DEFAULT</i> sections. <br><br>  Synchronize DB: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># su -s /bin/sh -c "glance-manage db_sync" glance</span></span></code> </pre><br><br>  Restart the services and delete the local database: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># service glance-registry restart # service glance-api restart # rm -f /var/lib/glance/glance.sqlite</span></span></code> </pre><br><br>  The official manual loads <i>cirros</i> , which, in general, we do not need, so we will upload the Ubuntu image: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># mkdir /tmp/images # wget -P /tmp/images http://cloud-images.ubuntu.com/releases/14.04.2/release/ubuntu-14.04-server-cloudimg-amd64-disk1.img # glance image-create --name "Ubuntu-Server-14.04.02-x86_64" --file /tmp/images/ubuntu-14.04-server-cloudimg-amd64-disk1.img --disk-format qcow2 --container-format bare --visibility public --progress # rm -r /tmp/images</span></span></code> </pre><br>  You can immediately fill in all the images we need, but I‚Äôm waiting for the moment when we get the Dashboard. <br>  Overall - our Glance service is ready. <br><br><h5>  <b>Nova</b> </h5><br>  Nova - the main part of IaaS in OpenStack.  Actually, thanks to Nova, virtual machines are created automatically.  Nova can interact with KVM, Xen, Hyper-V, VMware and Ironic (I honestly do not quite understand how it works).  We will use KVM, for other hypervisors configs will be different. <br><br><h6>  Controller </h6><br><br>  Again we start with the database: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># mysql -u root -p CREATE DATABASE nova; GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' IDENTIFIED BY 'NOVA_DBPASS'; GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' IDENTIFIED BY 'NOVA_DBPASS';</span></span></code> </pre><br><br>  Add information to the keystone: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># openstack user create --password-prompt nova # openstack role add --project service --user nova admin # openstack service create --name nova --description "OpenStack Compute" compute # openstack endpoint create --publicurl http://controller:8774/v2/%\(tenant_id\)s --internalurl http://controller:8774/v2/%\(tenant_id\)s --adminurl http://controller:8774/v2/%\(tenant_id\)s --region RegionOne compute</span></span></code> </pre><br><br>  Install the necessary packages: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install nova-api nova-cert nova-conductor nova-consoleauth nova-novncproxy nova-scheduler python-novaclient</span></span></code> </pre><br><br><div class="spoiler">  <b class="spoiler_title">/etc/nova/nova.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">[DEFAULT] ... rpc_backend = rabbit auth_strategy = keystone my_ip = 10.0.0.11 vncserver_listen = 10.0.0.11 vncserver_proxyclient_address = 10.0.0.11 [database] connection = mysql://nova:NOVA_DBPASS@controller/nova [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = openstack rabbit_password = RABBIT_PASS [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = nova password = NOVA_PASS [glance] host = controller [oslo_concurrency] lock_path = /var/lib/nova/tmp</code> </pre><br></div></div><br><br>  Synchronize the database, restart the services and delete the local database. <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># su -s /bin/sh -c "nova-manage db sync" nova # service nova-api restart # service nova-cert restart # service nova-consoleauth restart # service nova-scheduler restart # service nova-conductor restart # service nova-novncproxy restart # rm -f /var/lib/nova/nova.sqlite</span></span></code> </pre><br><br><h6>  Compute node </h6><br>  Now we finally start working with the compute node.  All described actions are valid for each computing node in our system. <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install nova-compute sysfsutils</span></span></code> </pre><br><br><div class="spoiler">  <b class="spoiler_title">/etc/nova/nova.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">[DEFAULT] ... verbose = True rpc_backend = rabbit auth_strategy = keystone my_ip = 10.0.0.31 <span class="hljs-comment"><span class="hljs-comment">#MANAGEMENT_INTERFACE_IP_ADDRESS vnc_enabled = True vncserver_listen = 0.0.0.0 vncserver_proxyclient_address = 10.0.0.31 #MANAGEMENT_INTERFACE_IP_ADDRESS novncproxy_base_url = http://controller:6080/vnc_auto.html [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = openstack rabbit_password = RABBIT_PASS [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = nova password = NOVA_PASS [glance] host = controller [oslo_concurrency] lock_path = /var/lib/nova/tmp [libvirt] virt_type = kvm</span></span></code> </pre><br></div></div><br>  <i>MANAGEMENT_INTERFACE_IP_ADDRESS</i> is the address of the compute node from VLAN 10. <br>  In <i>novncproxy_base_url</i> controller must correspond to the address through which it will be possible to access through the Web-browser.  Otherwise, you will not be able to use the vnc console from Horizon. <br><br>  Restart the service and delete the local copy of the database: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># service nova-compute restart # rm -f /var/lib/nova/nova.sqlite</span></span></code> </pre><br><br>  Check whether everything works correctly: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nova service-list +----+------------------+------------+----------+---------+-------+----------------------------+-----------------+ | Id | Binary | Host | Zone | Status | State | Updated_at | Disabled Reason | +----+------------------+------------+----------+---------+-------+----------------------------+-----------------+ | 1 | nova-conductor | controller | internal | enabled | up | 2014-09-16T23:54:02.000000 | - | | 2 | nova-consoleauth | controller | internal | enabled | up | 2014-09-16T23:54:04.000000 | - | | 3 | nova-scheduler | controller | internal | enabled | up | 2014-09-16T23:54:07.000000 | - | | 4 | nova-cert | controller | internal | enabled | up | 2014-09-16T23:54:00.000000 | - | | 5 | nova-compute | compute1 | nova | enabled | up | 2014-09-16T23:54:06.000000 | - | +----+------------------+------------+----------+---------+-------+----------------------------+-----------------+</span></span></code> </pre><br>  The fifth line says that we did everything right. <br><br>  We have done the most important thing - now we have IaaS. <br><br><h5>  <b>Neutron</b> </h5><br>  Neutron is a network-as-a-service (NaaS) service.  In general, official documentation gives a slightly different definition, but I think it will be clearer.  Nova-networking has been declared obsolete in new versions of OpenStack, so we will not use it.  Yes, and neutron functionality is much broader. <br><br><h6>  Controller </h6><br>  We install the network core on the controller, although the manual uses the 3rd node.  If there are a lot of computation nodes (&gt; 10) and / or the network load is high enough, then it is better to move the Network server to a separate node. <br><br>  As always, let's start with the database <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># mysql -u root -p CREATE DATABASE neutron; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' IDENTIFIED BY 'NEUTRON_DBPASS'; GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' IDENTIFIED BY 'NEUTRON_DBPASS';</span></span></code> </pre><br><br>  Keystone: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># openstack user create --password-prompt neutron # openstack role add --project service --user neutron admin # openstack service create --name neutron --description "OpenStack Networking" network # openstack endpoint create --publicurl http://controller:9696 --adminurl http://controller:9696 --internalurl http://controller:9696 --region RegionOne network</span></span></code> </pre><br><br>  Install the necessary components: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install neutron-server neutron-plugin-ml2 python-neutronclient neutron-plugin-openvswitch-agent neutron-l3-agent neutron-dhcp-agent neutron-metadata-agent</span></span></code> </pre><br><br>  It is also necessary to correct <i>/etc/sysctl.conf</i> <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/sysctl.conf net.ipv4.ip_forward=1 net.ipv4.conf.all.rp_filter=0 net.ipv4.conf.default.rp_filter=0 # sysctl -p</span></span></code> </pre><br><br><div class="spoiler">  <b class="spoiler_title">/etc/neutron/neutron.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">[DEFAULT] ... rpc_backend = rabbit auth_strategy = keystone core_plugin = ml2 service_plugins = router allow_overlapping_ips = True notify_nova_on_port_status_changes = True notify_nova_on_port_data_changes = True nova_url = http://controller:8774/v2 [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = openstack rabbit_password = RABBIT_PASS [database] connection = mysql://neutron:NEUTRON_DBPASS@controller/neutron [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = neutron password = NEUTRON_PASS [nova] auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default region_name = RegionOne project_name = service username = nova password = NOVA_PASS</code> </pre><br></div></div><br>  Editing the config should not delete anything from there, except for commented lines. <br><div class="spoiler">  <b class="spoiler_title">/etc/neutron/plugins/ml2/ml2_conf.ini</b> <div class="spoiler_text"><pre> <code class="bash hljs">[ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = gre mechanism_drivers = openvswitch [ml2_type_gre] tunnel_id_ranges = 1000:2000 [ml2_type_flat] flat_networks = external [securitygroup] enable_security_group = True enable_ipset = True firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver [ovs] local_ip = 10.0.1.11 <span class="hljs-comment"><span class="hljs-comment">#INSTANCE_TUNNELS_INTERFACE_IP_ADDRESS bridge_mappings = external:br-ex [agent] tunnel_types = gre</span></span></code> </pre><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">/etc/neutron/l3_agent.ini</b> <div class="spoiler_text"><pre> <code class="bash hljs">[DEFAULT] interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver external_network_bridge = router_delete_namespaces = True</code> </pre><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">/etc/neutron/dhcp_agent.ini</b> <div class="spoiler_text"><pre> <code class="bash hljs">[DEFAULT] interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq dhcp_delete_namespaces = True dnsmasq_config_file = /etc/neutron/dnsmasq-neutron.conf</code> </pre><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">/etc/neutron/dnsmasq-neutron.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">dhcp-option-force=26,1454</code> </pre><br>  In the official documentation, this setting was used for network devices without jumbo frames, but in general, almost any settings for dnsmasq can be written there. <br></div></div><br><br>  We kill all processes dnsmasq <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># pkill dnsmasq</span></span></code> </pre><br><br><div class="spoiler">  <b class="spoiler_title">/etc/neutron/metadata_agent.ini</b> <div class="spoiler_text"><pre> <code class="bash hljs">[DEFAULT] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_region = RegionOne auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = neutron password = NEUTRON_PASS nova_metadata_ip = controller metadata_proxy_shared_secret = METADATA_SECRET</code> </pre><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">/etc/nova/nova.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">[DEFAULT] ... network_api_class = nova.network.neutronv2.api.API security_group_api = neutron linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver [neutron] url = http://controller:9696 auth_strategy = keystone admin_auth_url = http://controller:35357/v2.0 admin_tenant_name = service admin_username = neutron admin_password = NEUTRON_PASS service_metadata_proxy = True metadata_proxy_shared_secret = METADATA_SECRET</code> </pre><br>  <i>METADATA_SECRET</i> is also a set of characters from 10 to 16 characters <br></div></div><br><br>  <i>We</i> do not delete anything from <i>nova.conf</i> , we just add it. <br><br>  Synchronize the database and restart the services: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron # service nova-api restart # service neutron-server restart # service openvswitch-switch restart</span></span></code> </pre><br><br>  Create a bridge and link it with an external interface. <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ovs-vsctl add-br br-ex # ovs-vsctl add-port br-ex p3p1</span></span></code> </pre><br><br>  Restart interfaces <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># service neutron-plugin-openvswitch-agent restart # service neutron-l3-agent restart # service neutron-dhcp-agent restart # service neutron-metadata-agent restart</span></span></code> </pre><br><br><h6>  Compute node </h6><br><br>  No comments. <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /etc/sysctl.conf net.ipv4.conf.all.rp_filter=0 net.ipv4.conf.default.rp_filter=0 net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 # sysctl -p</span></span></code> </pre><br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install neutron-plugin-ml2 neutron-plugin-openvswitch-agent</span></span></code> </pre><br><br><div class="spoiler">  <b class="spoiler_title">/etc/neutron/neutron.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">[DEFAULT] ... rpc_backend = rabbit auth_strategy = keystone core_plugin = ml2 service_plugins = router allow_overlapping_ips = True [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = openstack rabbit_password = RABBIT_PASS [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = neutron password = NEUTRON_PASS</code> </pre><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">/etc/neutron/plugins/ml2/ml2_conf.ini</b> <div class="spoiler_text"><pre> <code class="bash hljs">[ml2] type_drivers = flat,vlan,gre,vxlan tenant_network_types = gre mechanism_drivers = openvswitch [ml2_type_gre] tunnel_id_ranges = 1000:2000 [ml2_type_flat] flat_networks = external [securitygroup] enable_security_group = True enable_ipset = True firewall_driver = neutron.agent.linux.iptables_firewall.OVSHybridIptablesFirewallDriver [ovs] local_ip = 10.0.1.31 <span class="hljs-comment"><span class="hljs-comment">#INSTANCE_TUNNELS_INTERFACE_IP_ADDRESS bridge_mappings = external:br-ex [agent] tunnel_types = gre</span></span></code> </pre><br></div></div><br><br>  Restart openvswitch <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># service openvswitch-switch restart</span></span></code> </pre><br><br>  Add lines to <i>/etc/nova/nova.conf</i> <br><pre> <code class="bash hljs">[DEFAULT] ... network_api_class = nova.network.neutronv2.api.API security_group_api = neutron linuxnet_interface_driver = nova.network.linux_net.LinuxOVSInterfaceDriver firewall_driver = nova.virt.firewall.NoopFirewallDriver [neutron] url = http://controller:9696 auth_strategy = keystone admin_auth_url = http://controller:35357/v2.0 admin_tenant_name = service admin_username = neutron admin_password = NEUTRON_PASS</code> </pre><br><br>  Restart services: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># service nova-compute restart # service neutron-plugin-openvswitch-agent restart</span></span></code> </pre><br><br>  If I did not forget to mention anything, it should turn out like this: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># neutron agent-list +--------------------------------------+--------------------+----------+-------+----------------+---------------------------+ | id | agent_type | host | alive | admin_state_up | binary | +--------------------------------------+--------------------+----------+-------+----------------+---------------------------+ | 30275801-e17a-41e4-8f53-9db63544f689 | Metadata agent | network | :-) | True | neutron-metadata-agent | | 4bd8c50e-7bad-4f3b-955d-67658a491a15 | Open vSwitch agent | network | :-) | True | neutron-openvswitch-agent | | 756e5bba-b70f-4715-b80e-e37f59803d20 | L3 agent | network | :-) | True | neutron-l3-agent | | 9c45473c-6d6d-4f94-8df1-ebd0b6838d5f | DHCP agent | network | :-) | True | neutron-dhcp-agent | | a5a49051-05eb-4b4f-bfc7-d36235fe9131 | Open vSwitch agent | compute1 | :-) | True | neutron-openvswitch-agent | +--------------------------------------+--------------------+----------+-------+----------------+---------------------------+</span></span></code> </pre><br><br><h6>  Network </h6><br>  Now we will do the initial procurement of our networks.  We will create one external network and one internal. <br><br>  Create a virtual network: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># neutron net-create ext-net --router:external --provider:physical_network external --provider:network_type flat</span></span></code> </pre><br><br>  We configure our external subnet: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># neutron subnet-create ext-net 192.168.1.0/24 --name ext-subnet \ --allocation-pool start=192.168.1.100,end=192.168.1.200 \ --disable-dhcp --gateway 192.168.1.1</span></span></code> </pre><br>  Our external network is 192.168.1.0/24 and the router is releasing to the Internet 192.168.1.1.  External addresses for our cloud will be issued from the range 192.168.1.101-200. <br><br>  Next, we will create an internal network for the <i>demo</i> project, so you should load variables for the demo user: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># source demo-openrc.sh</span></span></code> </pre><br><br>  Now we create a virtual internal network: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># neutron net-create demo-net # neutron subnet-create demo-net 172.16.1.0/24 --name demo-subnet --gateway 172.16.1.1</span></span></code> </pre><br>  It is clear that our virtual network will be 172.16.1.0/24 and all instances from it will receive 172.16.1.1 as a router. <br>  Question: what is this router? <br>  Answer: this is a virtual router. <br><br>  The "trick" is that in Neutron you can build virtual networks with a sufficiently large number of subnets, which means they need a virtual router.  Each virtual router can add ports to any of the available virtual and external networks.  And this is really "strong"!  We only assign access to networks to routers, and we manage all firewall rules from security groups.  Moreover!  We can create a virtual machine with a software router, configure interfaces to all necessary networks and control access through it (I tried using Mikrotik). <br>  In general, Neutron gives plenty of imagination. <br><br>  Create a virtual router, assign an interface to it in the demo-subnet and connect it to the external network: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># neutron router-create demo-router # neutron router-interface-add demo-router demo-subnet # neutron router-gateway-set demo-router ext-net</span></span></code> </pre><br><br>  Now our virtual router should be pinging from the external network: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ping 192.168.1.100 PING 192.168.1.100 (192.168.1.100) 56(84) bytes of data. 64 bytes from 192.168.1.100: icmp_req=1 ttl=64 time=0.619 ms 64 bytes from 192.168.1.100: icmp_req=2 ttl=64 time=0.189 ms 64 bytes from 192.168.1.100: icmp_req=3 ttl=64 time=0.165 ms 64 bytes from 192.168.1.100: icmp_req=4 ttl=64 time=0.216 ms ...</span></span></code> </pre><br><br>  In general, we already have a workable cloud with the network. <br><br><h5>  <b>Cinder</b> (Block Storage) </h5><br>  Cinder is a service that provides the ability to manage block devices (virtual disks) and attach them to virtual instances.  Virtual disks can be bootable.  This can be very convenient for transferring a VM to another compute instance. <br><br>  DB: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># mysql -u root -p CREATE DATABASE cinder; GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' IDENTIFIED BY 'CINDER_DBPASS'; GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' IDENTIFIED BY 'CINDER_DBPASS';</span></span></code> </pre><br><br>  Keystone: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># openstack user create --password-prompt cinder # openstack role add --project service --user cinder admin # openstack service create --name cinder --description "OpenStack Block Storage" volume # openstack service create --name cinderv2 --description "OpenStack Block Storage" volumev2 # openstack endpoint create --publicurl http://controller:8776/v2/%\(tenant_id\)s --internalurl http://controller:8776/v2/%\(tenant_id\)s --adminurl http://controller:8776/v2/%\(tenant_id\)s --region RegionOne volume # openstack endpoint create --publicurl http://controller:8776/v2/%\(tenant_id\)s --internalurl http://controller:8776/v2/%\(tenant_id\)s --adminurl http://controller:8776/v2/%\(tenant_id\)s --region RegionOne volumev2</span></span></code> </pre><br><br>  Install the necessary packages: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install cinder-api cinder-scheduler python-cinderclient</span></span></code> </pre><br><br>  Let's fix the config: <br><div class="spoiler">  <b class="spoiler_title">/etc/cinder/cinder.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">[DEFAULT] ... rpc_backend = rabbit auth_strategy = keystone my_ip = 10.0.0.11 [oslo_messaging_rabbit] rabbit_host = controller rabbit_userid = openstack rabbit_password = RABBIT_PASS [database] connection = mysql://cinder:CINDER_DBPASS@controller/cinder [keystone_authtoken] auth_uri = http://controller:5000 auth_url = http://controller:35357 auth_plugin = password project_domain_id = default user_domain_id = default project_name = service username = cinder password = CINDER_PASS [oslo_concurrency] lock_path = /var/lock/cinder</code> </pre><br></div></div><br><br>  Next, synchronize the database and restart the services: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># su -s /bin/sh -c "cinder-manage db sync" cinder # service cinder-scheduler restart # service cinder-api restart</span></span></code> </pre><br><br>  Since  Since our controller is also a repository, the following actions are also carried out on it. <br>  Install the necessary packages: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install qemu lvm2</span></span></code> </pre><br>  Remember I mentioned in the configuration about two 500GB disks?  We will make RAID 1 of them (I‚Äôm not going to describe it).  Technically, we could just create an lvm partition from two physical disks, but this option is bad because we don‚Äôt have a HA project and the crash of one of the disks can be critical.  I will not understand how to create a RAID-array, it is easily googled.  We assume that our raid is called <i>/ dev / md1</i> : <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># pvcreate /dev/md1 # vgcreate cinder-volumes /dev/md1</span></span></code> </pre><br>  We created a physical LVM device and created lvm-group <i>cinder-volumes</i> . <br>  Next, edit <i>/etc/lvm/lvm.conf</i> . <br>  We find (or add) the following line: <br><pre> <code class="bash hljs">devices { ... filter = [ <span class="hljs-string"><span class="hljs-string">"a/md1/"</span></span>, <span class="hljs-string"><span class="hljs-string">"r/.*/"</span></span>]</code> </pre><br>  We assume that except in the raid section we have nothing connected with lvm.  If the working partition is also deployed to lvm, then it should be added.  For example, if our system is deployed on <i>/ dev / md0</i> and lvm is deployed on top of it, then our config will look like this: <br><pre> <code class="bash hljs">devices { ... filter = [ <span class="hljs-string"><span class="hljs-string">"a/md0/"</span></span>, <span class="hljs-string"><span class="hljs-string">"a/md1/"</span></span>, <span class="hljs-string"><span class="hljs-string">"r/.*/"</span></span>]</code> </pre><br>  In general, I think for those who are faced with lvm it should not be difficult. <br><br>  Install the necessary packages: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install cinder-volume python-mysqldb</span></span></code> </pre><br><br>  add to config: <br><div class="spoiler">  <b class="spoiler_title">/etc/cinder/cinder.conf</b> <div class="spoiler_text"><pre> <code class="bash hljs">[DEFAULT] ... enabled_backends = lvm glance_host = controller [lvm] volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver volume_group = cinder-volumes iscsi_protocol = iscsi iscsi_helper = tgtadm</code> </pre><br></div></div><br><br>  And restart the services: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># service tgt restart # service cinder-scheduler restart # service cinder-api restart # service cinder-volume restart</span></span></code> </pre><br><br><h5>  <b>Horizon</b> (dashboard) </h5><br>  Horizon - dashboard for OpenStack, written in Python 2.7, the engine is Django.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> From here, the entire OpenStack environment is fully managed: managing users / projects / roles, managing images, virtual disks, instances, networks, etc. </font></font><br><br><h6>  Installation </h6><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># apt-get install openstack-dashboard</span></span></code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Installation can be done on a separate server with access to the Controller node, but we will install it on the controller. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Straightened configuration </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">/etc/openstack-dashboard/local_settings.py</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> :</font></font><br><pre> <code class="bash hljs">... OPENSTACK_HOST = <span class="hljs-string"><span class="hljs-string">"controller"</span></span> ... ALLOWED_HOSTS = <span class="hljs-string"><span class="hljs-string">'*'</span></span> ... CACHES = { <span class="hljs-string"><span class="hljs-string">'default'</span></span>: { <span class="hljs-string"><span class="hljs-string">'BACKEND'</span></span>: <span class="hljs-string"><span class="hljs-string">'django.core.cache.backends.memcached.MemcachedCache'</span></span>, <span class="hljs-string"><span class="hljs-string">'LOCATION'</span></span>: <span class="hljs-string"><span class="hljs-string">'127.0.0.1:11211'</span></span>, } } ... OPENSTACK_KEYSTONE_DEFAULT_ROLE = <span class="hljs-string"><span class="hljs-string">"user"</span></span> ... TIME_ZONE = <span class="hljs-string"><span class="hljs-string">"Asia/Vladivostok"</span></span> ...</code> </pre><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">TIME_ZONE</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - your time zone may be (and most likely will be) different. </font></font><a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> you will find your own. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Restart Apache:</font></font><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># service apache2 reload</span></span></code> </pre><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now you can go to the </font></font><i><a href="http://controller/horizon"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">controller / horizon</font></font></a></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">In my previous publication, you can see screenshots of dashboards. </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Ubuntu</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> additionally installs the </font><i><font style="vertical-align: inherit;">openstack-dashboard-ubuntu-theme</font></i><font style="vertical-align: inherit;"> package </font><font style="vertical-align: inherit;">, which adds some links with a hint of Juju. </font><font style="vertical-align: inherit;">If you want to return the original version, you can simply remove the package. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can also choose the interface language </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Russian</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in the user profile </font><font style="vertical-align: inherit;">, then it will considerably facilitate the work of the Developer.</font></font><br><br>  Done! <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The publication was very cumbersome, but did not want to share. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I hope the article will help anyone. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the next publication (if my karma is not cast as ‚Äútomatoes‚Äù) I will describe the primitive installation of a Chef server and a simple recipe.</font></font></div><p>Source: <a href="https://habr.com/ru/post/262049/">https://habr.com/ru/post/262049/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../262037/index.html">Books for the system administrator. My bookshelf</a></li>
<li><a href="../262039/index.html">Baidu anti-virus software removal procedure</a></li>
<li><a href="../262043/index.html">Permanent loss of EC2 instance, EBS volumes and all snapshots</a></li>
<li><a href="../262045/index.html">Processing 1 million requests per minute with Go</a></li>
<li><a href="../262047/index.html">How to steal a control plane or cooking EIGRP for Juniper</a></li>
<li><a href="../262051/index.html">Development of trainings in the grocery company</a></li>
<li><a href="../262053/index.html">We collect people base from open data WhatsApp and VK</a></li>
<li><a href="../262055/index.html">The word ‚ÄúM‚Äù, or Monads, is already here.</a></li>
<li><a href="../262057/index.html">Twelve simple initial steps to developing a module for Node.js</a></li>
<li><a href="../262061/index.html">Malefactors actively use 0day vulnerability of Flash Player for cyber attacks</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>