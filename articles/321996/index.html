<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neural Network Based Interview Robot</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Good day to all! I am a student, for my thesis I chose the topic ‚ÄúInformation Neural Networks‚Äù (INS). Tasks where it is necessary to work with numbers...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neural Network Based Interview Robot</h1><div class="post__text post__text-html js-mediator-article"><p>  Good day to all!  I am a student, for my thesis I chose the topic ‚ÄúInformation Neural Networks‚Äù (INS).  Tasks where it is necessary to work with numbers, were solved rather easily.  And I decided to complicate the system by adding word processing.  Thus, I set myself the task of developing a ‚Äúrobot interlocutor‚Äù who could communicate on any particular topic. </p><br><p>  Since the topic of communication with the robot is quite extensive, I do not appreciate the dialogue as a whole (hello to Comrade Turing), only the adequacy of the ‚Äúinterlocutor ‚Äù‚Äôs response to the person‚Äôs comment is considered. </p><a name="habracut"></a><br><p>  Further, we will call the <i>question a</i> sentence coming in at the INS input, and the <i>answer the</i> sentence received at its output. </p><br><h3>  Architecture 1. A two-layer direct distribution neural network with one hidden layer </h3><br><p>  Since neural networks only work with numbers, you need to encode words.  For simplicity, punctuation marks are excluded from consideration, only proper names are written with a capital letter. </p><br><p>  Each word is encoded with two integers, starting from one (zero is responsible for the absence of a word) - the category number and the word number in this category.  It was assumed in the "category" to store words that are similar in meaning or type (colors, names, for example). </p><br><p>  <u>Table 1</u> <br></p><table><tbody><tr><td><br></td><td>  <b>Category 1</b> <br></td><td>  <b>Category 2</b> <br></td><td>  <b>Category 3</b> <br></td><td>  <b>Category 4</b> <br></td></tr><tr><td>  <b>one</b> <br>  <b>2</b> <br>  <b>3</b> <br>  <b>four</b> <br>  <b>five</b> <br>  <b>6</b> <br>  <b>7</b> <br></td><td>  you <br>  your <br>  you <br>  yours <br>  you <br>  you <br>  I <br></td><td>  OK <br>  perfectly <br>  wonderful <br>  excellent <br>  good <br>  good <br>  good ones <br></td><td>  badly <br>  horrible <br>  disgusting <br>  bad <br>  the bad <br></td><td>  Hello <br>  Hello <br>  welcome <br>  hello <br>  healthy <br></td></tr></tbody></table><p></p><br><p>  For a neural network, data is normalized, reduced to a range <math> </math> $ inline $ [0, 1] $ inline $   .  Category number and words - to the maximum value. <math> </math> $ inline $ M $ inline $   category numbers or words in all categories.  The sentence is translated into a real vector of fixed length, the missing elements are filled with zeros. </p><br><p>  Each sentence (question and answer) can consist of a maximum of ten words.  Thus, a network with 20 inputs and 20 outputs is obtained. </p><br><p>  The required number of links in the network to remember <i>N</i> examples was calculated by the formula <br></p><p><math> </math> $$ display $$ L_W = (m + n + 1) (m + N) + m, $$ display $$ </p><br>  where <i>m</i> is the number of inputs, <i>n</i> is the number of outputs, <i>N</i> is the number of examples. <p></p><p>  The number of connections in the network with one hidden layer consisting of <i>H</i> neurons <br></p><p><math> </math> $$ display $$ L_W = 20H + 20H = 40H, $$ display $$ </p><br>  where does the required number of hidden neurons come from <br><p><math> </math> $$ display $$ H = L_W / 40 $$ display $$ </p><br>  For <math> </math> $ inline $ n = 20 $ inline $   , <math> </math> $ inline $ m = 20 $ inline $   it turns out a match <br><p><math> </math> $$ display $$ L_W (N) = 41N + 480 $$ display $$ </p><br>  As a result, we obtain the dependence of the number of hidden neurons on the number of examples: <br><p><math> </math> $$ display $$ H (N) = \ frac {41} {40} H + 21 $$ display $$ </p><br><p>  The structure of the learning network is shown in Figure 1. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/076/a20/463/076a20463372449297bc507140358528.gif"></div><br>  <b>Figure 1.</b> The simplest INS for memorizing sentences <p></p><br><p>  Implemented network in MATLAB, training is the method of back propagation of an error.  The training sample contains 32 sentences ... </p><br><p>  More and did not take ... </p><br><p>  The INS could not remember more than 15 sentences, as the following graph shows (Figure 2).  The error is calculated as the modulus of the difference between the current output of the NA and the required one. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/3c1/1b5/b57/3c11b5b5751745fca4c35b67a6959bbc.gif"></div><br>  <b>Figure 2.</b> NA error when learning 32 examples <br><p></p><br><p>  Dialogue example (all questions from the training set): <br></p><table><tbody><tr><td><ul><li>  Hello </li><li>  hello you </li></ul><br><ul><li>  how old are you </li><li>  four months </li></ul><br><ul><li>  you're good </li><li>  I do not know what to say </li></ul><br></td><td><ul><li>  you are a good robot </li><li>  *I do not know what to say* </li></ul><br><ul><li>  friendship is good </li><li>  OK </li></ul><br><ul><li>  until </li><li>  see you </li></ul><br></td></tr></tbody></table><p></p><br><p>  As a result of testing for various numbers of examples, it became clear that even the training sample of the INS remembers with great difficulty (as can be seen from Figure 2).  Even for a million epochs, the error could not be reduced to the required value. </p><br><br><h3>  Architecture 2. A two-layer neural network of direct distribution </h3><br><p>  The next way to encode words for ANN is one-hot encoding <sup>[4]</sup> .  Its essence is as follows: let the dictionary have <math> </math> $ inline $ D $ inline $   alphabetically arranged words.  Each word of such a dictionary is encoded by a length vector. <math> </math> $ inline $ D $ inline $   containing a unit in place corresponding to the number of the word in the dictionary and zeros in other places. </p><br><p>  For experiments, a dictionary was created from <math> </math> $ inline $ D = 468 $ inline $   words and a training set of 95 sentences.  Six words were delivered to the input of the National Assembly and the answer was also considered from six words. </p><br><p>  The number of neurons in the hidden layer was determined by the dependence of the number of connections on the number of examples that the network can learn without errors. </p><br><table><tbody><tr><td><ul><li>  Hello to you </li><li>  is hi good to be human? </li></ul><br><ul><li>  man to be good </li><li>  not </li></ul><br><ul><li>  what do you like </li><li>  I like to study </li></ul><br></td><td><ul><li>  what do you like </li><li>  isn't it good? </li></ul><br><ul><li>  how are you </li><li>  *I do not know what to say* </li></ul><br><ul><li>  good to be a robot </li><li>  but me? </li></ul><br></td></tr></tbody></table><br><p>  The results show that now the system can memorize more words.  Almost a victory ... but another problem arises - the definition of synonyms and similar words <sup>[4]</sup> . </p><br><h3>  Architecture 3. Two-layer direct distribution neural network <br>  with one hidden layer and word2vec encoding </h3><br><p>  To solve the problem of the similarity of words and synonyms, I decided to try word2vec <sup>[4]</sup> , which allows you to encode words as necessary. </p><br><p>  For experiments on the network used a dictionary of word2vec length vectors <math> </math> $ inline $ D = 50 $ inline $   trained at the neural network training base. </p><br><p>  Six words (the vector of length 300) are given to the input of the neural network and it is proposed to receive an answer, also consisting of six words.  In the case of reverse coding, the sentence vector is divided into six word vectors, for each of which in the dictionary the maximum possible match is found for the cosine of the angle between the vectors <math> </math> $ inline $ A $ inline $   and <math> </math> $ inline $ b $ inline $   : <br></p><p><math> </math> $$ display $$ cos [A, B] = \ frac {\ sum_ {d = 1} ^ {D} {(A_d B_d)}} {\ sqrt {\ sum_ {d = 1} ^ {D} {A_d ^ 2}} \ sqrt {\ sum_ {d = 1} ^ {D} {B_d ^ 2}}} $$ display $$ </p><br><p>  But even with this implementation, word2vec does not make the necessary connections between words from the point of view of the Russian language.  To create a dictionary in which exactly the synonyms will be located as close as possible, a training building with grouped synonyms was formed, as far as possible combined in meaning with each other: </p><br><table><tbody><tr><td>  TO ME MY ME MY MY <br>  YOU YOU YOU YOUR YOUR YOUR YOUR YOU <br>  NO ON ON WITH S AND YES TO ABOUT WHAT ALSO EVEN ONLY THIS <br>  WHO IS WHAT <br>  BORN BORN <br>  ROBOT ROBOT ROBOT ROBOT <br></td></tr></tbody></table><br><p>  As a result of such a presentation, there is no need to memorize many synonyms that can be given the same answer (like ‚Äúhello‚Äù, ‚Äúhello‚Äù, ‚Äúgreet‚Äù).  For example, only ‚Äúhello - hello‚Äù participated in the training sample, the remaining answers were received due to the large cosine proximity of ‚Äúhello‚Äù, ‚Äúhello‚Äù and ‚Äúgreet‚Äù. </p><br><table><tbody><tr><td><ul><li>  Hello </li><li>  HELLO </li></ul><br><ul><li>  Hello </li><li>  HELLO </li></ul><br><ul><li>  welcome </li><li>  HELLO ROBOTS </li></ul><br></td><td><ul><li>  you can speak </li><li>  I CAN SELECT ANSWERED </li></ul><br><ul><li>  How do you study </li><li>  I am talking and remembering my words </li></ul><br><ul><li>  you're good </li><li>  THANK YOU TO TRY TO BE BETTER </li></ul><br></td></tr></tbody></table><br><p>  However, at the same time, because of the close proximity of the synonyms in the answer (conversation = conversation = talked = ..., I = me = mine = me = ...), they are often confused when they slightly reformulate the question ("How do you study?" Instead of "How do you learn from a person? ‚Äù). <br><br></p><h3>  Misadventure </h3><br><p>  As you can see, when I tried to use the INS to communicate with a person, I had ‚Äútwo blondes‚Äù: one cannot remember more than 15 sentences, and the second knows a lot, but does not understand anything. </p><br><p>  Judging by the descriptions both on Habrahabr and on other sites, not everyone faces such a problem.  Therefore, the question arises: where is the dog buried?  What approach should be used to get an INS that can memorize and understand at least 100 - 200 phrases? </p><br><p>  Who faced similar questions, I ask for your advice and suggestions. </p><br><h3>  Bibliography </h3><br><ol><li>  <a href="http://alexsosn.github.io/ml/2015/11/16/LSTM.html">How to understand LSTM networks</a> </li><li>  <a href="https://habrahabr.ru/company/meanotek/blog/256987/">Development: Chatbot on neural networks</a> </li><li>  <a href="https://habrahabr.ru/company/meanotek/blog/271053/">Development: Google TensorFlow machine learning library - first impressions and comparison with its own implementation</a> </li><li>  <a href="https://habrahabr.ru/company/meanotek/blog/256593/">Development: Classification of proposals using neural networks without pre-processing</a> </li><li>  <a href="https://habrahabr.ru/company/meanotek/blog/280268/">Development: Russian Neural Network Chatbot</a> </li></ol><p></p><p></p><p></p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/321996/">https://habr.com/ru/post/321996/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../321976/index.html">How we moved from widgets and ‚Äúbricks‚Äù to an intuitive layout with the ability to embed html, css and javascript</a></li>
<li><a href="../321978/index.html">With the help of LAMP, I created a SaaS service bringing $ 3,700 a month. My history</a></li>
<li><a href="../321982/index.html">Its WEB application, with MVC and registry</a></li>
<li><a href="../321986/index.html">How was the frame rendered in the 1998 Thief game</a></li>
<li><a href="../321992/index.html">DDoS attacks: attack and defense</a></li>
<li><a href="../321998/index.html">Using Tarantool in a .NET project on Windows</a></li>
<li><a href="../322000/index.html">Snake and coconut</a></li>
<li><a href="../322008/index.html">Create a development application using Android Studio</a></li>
<li><a href="../322010/index.html">RDPPatcher sells access to your computer at a low price.</a></li>
<li><a href="../322012/index.html">Public data management: preparation and delivery</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>