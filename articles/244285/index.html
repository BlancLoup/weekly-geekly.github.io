<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to find out more about your users? Application of Data Mining in Mail.Ru Rating</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Any Internet project can be done better. Implement new features, add servers, remake an interface, or release a new version of the API. Your users wil...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to find out more about your users? Application of Data Mining in Mail.Ru Rating</h1><div class="post__text post__text-html js-mediator-article"> <a href="http://habrahabr.ru/company/mailru/blog/244285/"><img src="https://habrastorage.org/files/2cb/fb8/980/2cbfb8980b1d45a197c220b06eb16efd.png"></a> <br><br>  Any Internet project can be done better.  Implement new features, add servers, remake an interface, or release a new version of the API.  Your users will love this.  Or not?  And in general, what kind of people?  Young or aged?  Secured or rather the opposite?  From Moscow?  Peter?  San Francisco, California?  And why, in the end, those hundred warm blankets that you bought back in May, are gathering dust in the warehouse, and t-shirts with octocotes disperse like hot cakes?  Get answers help project <a href="http://top.mail.ru/">Rating Mail.Ru.</a>  This article is about how we use data mining to answer the most difficult questions. <br><a name="habracut"></a><br>  Of course, it is impossible to tell about everything that we are doing in one post, so I will illustrate our approach with an example of solving a specific task - determining the category of user income.  Suppose you run a blog about cars and show in it advertisements of expensive car dealerships.  Having learned that your blog is read mostly by not very well-to-do users, you will reconsider your strategy and will show advertisements of spare parts.  For Oka. <br><br>  Where does the rating get its data?  If you need to know the gender of a user, Mail.Ru Mail registration data is used (users who cheat during registration are quite few) and profiles from social networks.  Need to know the city - IP to help.  Determining whether a given user has a high income or not is much more difficult. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      To begin, we will need a training set.  To get such a sample is quite realistic - you can use the results of opinion polls, panel research or data from friendly projects.  The training sample for the task of determining the category of income is compiled on the basis of data on the salary wishes of users of the HeadHunter service.  We know a number of characteristics of users from the sample, including gender, age, city of residence, income category, area of ‚Äã‚Äãemployment.  And the Rating ID Mail.Ru.  The latter is especially important because we will model the behavior of users from the sample using the Rating data.  Given this, we can formulate a problem that we will solve. <br><br>  <b>Objective:</b> using a training sample containing about a million users with a known category of individual income, develop a model that allows, according to the Rating data, to predict which of the five categories the new untagged user belongs to: 1 - low income, 2 - income below average, 3 - average income , 4 - income above average, 5 - high income. <br><br><h1>  Input data - log of page visits </h1><br>  In order to get statistics on the site, you need to add a Rating counter code to the pages.  Data on each visit is collected by the counter and sent to the server, where it is saved as a log.  Each visit (synonym: hit) in the log corresponds to one line.  For example, in the picture below, the first line corresponds to a visit to hi-tech.mail.ru by user A21CE around noon on April 9, and the second line corresponds to a visit to horo.mail.ru. <br><br><img src="https://habrastorage.org/files/0ee/b35/c56/0eeb35c56da04ebd97e07029b8b6314d.png"><br><br>  It turns out that having enough of such data on the pages visited by the user, one can quite successfully predict the category of income of this user.  To illustrate the idea, the picture shows the radars of the distribution of users of several sites by income category. <br><br> <a href="http://http:"><img src="https://habrastorage.org/files/063/bc0/491/063bc0491af64bb9a17f2ad259766822.png"></a> <br>  <sup>(clickable)</sup> <br><br>  The prior distribution is indicated by the blue area - for example, the share of users with income below the average is 0.39, and the share of users with high income is 0.08.  On the left graph it is clear that this ratio is different for visitors to hi-tech.mail.ru: almost 12% of users of this site are rich, while the proportion of poor is less than in the general population.  For the rabota.mail.ru website, the situation is reversed: among people who are looking for work, the proportion of the poor is greater than ‚Äúusually‚Äù.  For many other sites, the situation is similar - for example, wealthy users go to airline sites more, and the users are usually looking for poorer loans.  Therefore, knowing what sites a user visits allows you to make reasonable assumptions about the category of income.  This can be helped by other user information available in the logs, for example, gender (the average graph is higher for men) or the browser used (the right graph is Safari lovers suddenly richer than people who prefer IE). <br><br>  Summing up the subtotal.  Here is a plan of action: we analyze the sites visited by the user, and other available information, and make a reasonable assumption about the category of income.  It sounds easy.  But there is a problem: manual analysis is impossible, because there are many users.  Tens of millions.  And the size of the compressed log reaches 500MB per minute.  Without data mining is not enough. <br><br><h1>  Convert log of visits to feature vectors </h1><br>  According to the KDD process (and any of its analogues: <a href="http://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining%3Foldformat%3Dtrue">one</a> , <a href="http://en.wikipedia.org/wiki/SEMMA%3Foldformat%3Dtrue">two</a> ), the solution of the data mining problem begins with feature engineering.  This stage is the most important and at the same time the most laborious.  We select features from a semi-structured log of page visits.  The figure shows day logs for three real users.  Minutes of the day (0-1439) are plotted on the horizontal axis, seconds - in the corresponding minute (0-59) - on the vertical axis.  Each point - visiting the site.  For clarity, sites are shown in different colors.  Users from the top graphics are mainly interested in dating (red hits correspond to the popular dating service) and computer hardware (blue and green hits).  And the main activity falls on the night (someone recognized himself?).  The second user is also not opposed to meet (green hits), but during working hours.  The third spends on this non-working time - evening and presumably lunch break. <br><br> <a href="http://http:"><img src="https://habrastorage.org/files/801/798/2c6/8017982c6e844cf88da65363236a4b51.png"></a> <br>  <sup>(clickable)</sup> <br><br>  Although the users represented have a similar interest in dating, the pattern of behavior is different: the periods of activity, the number and frequency of hits differ.  In order to smooth this difference, in web mining it is customary to break the flow of user hits into sessions - intervals of continuous activity on the Internet.  There are many approaches to the allocation of sessions, we stopped at the following simple rules: <br><br>  <i>(1) two hits belong to the same session, if the interval between them does not exceed 20 minutes</i> <i><br></i>  <i>(2) session lasts 20 minutes after the last hit</i> <br><br>  Sessions of our users on the graphs are highlighted in gray.  Based on the received sessions, we will convert the history of user visits into a matrix that can be input to machine learning algorithms.  Each row in this matrix corresponds to a user, each column corresponds to a feature.  Habr is not rubber, so here I will focus on the simplest of the options we use: each sign is a word extracted from the URL of the visited page.  For example, a piece of log already familiar to us: <br><br><img src="https://habrastorage.org/files/ba1/c7e/a5c/ba1c7ea5cb3e4cde80fc86a79f73204c.png"><br><br>  converted to the following matrix: <br><table><tbody><tr><td></td><td>  <b>hi-tech</b> </td><td>  <b>mail</b> </td><td>  <b>horo</b> </td><td>  <b>example</b> </td></tr><tr><td>  <i>A21CE</i> </td><td>  one </td><td>  2 </td><td>  2 </td><td>  0 </td></tr><tr><td>  <i>B0BB1</i> </td><td>  0 </td><td>  one </td><td>  0 </td><td>  one </td></tr></tbody></table><br>  The value of the attribute for a specific user in this case is the number of user sessions in which this characteristic occurred.  In principle, a sign value can be binarized using a threshold, or one of many weighing options can be applied, for example, by <a href="http://en.wikipedia.org/wiki/Tf%25E2%2580%2593idf">idf</a> .  Experiments show that any of these normalizations leads to an improvement in the quality of the model.  We stopped on binarization. <br><br><h1>  Feature selection </h1><br>  Not all signs are equally helpful.  Let's say that the presence of a non-zero value of the page attribute does not say anything about the user's income, because page is just a technical word for page numbering.  In contrast, a user with a non-zero value of the jewelery tag may be looking for jewelry, and this will help to classify it into the ‚Äúrich‚Äù category.  When analyzing the signs, we use the following considerations: <br><br>  <i>(1) the trait should give statistically reliable estimates.</i>  <i>For example, <a href="http://en.wikipedia.org/wiki/Multinomial_test">multinomial tests</a> are used for verification.</i> <i><br></i>  <i>(2) the sign should be informative.</i>  <i>Information criteria such as <a href="http://en.wikipedia.org/wiki/Mutual_information%3Foldformat%3Dtrue">redundancy</a> are used for verification.</i> <br><br>  The figure shows the signs that best reflect each of the categories.  For example, a large proportion of rich users have a non-zero weight in the matrix column corresponding to the IT-news feature.  Apparently, being an IT professional is quite profitable.  The categories of the poor are more suitable for signs related to low-skilled work (‚Äúseller-consultant‚Äù, ‚Äúwithout experience‚Äù). <br><br> <a href="http://http:"><img src="https://habrastorage.org/files/d52/b08/c57/d52b08c571444d9ba3226fe224637815.png"></a> <br>  <sup>(clickable)</sup> <br><br>  Selecting only informative and statistically reliable signs, we obtain a smaller matrix, which will become input data for machine learning algorithms.  The element of this matrix <i>x [i, j]</i> is the weight of the <i>j</i> -th attribute of the <i>i</i> -th user.  For the sake of fairness, I note that in addition to the training sample, which reflects the pages visited by the user, we are building a few more - for example, on demographic data and on the content of the pages.  For each training sample, a separate model is built, after which all models are combined. <br><br><h1>  Simulation and results </h1><br>  The specificity of our classification problem is that the data matrices have a large dimension (millions of rows and tens of thousands of columns) and at the same time are very sparse.  Therefore, implementations of learning algorithms capable of working with sparse matrices are used.  Here we do not invent bicycles - we use linear model ensembles with cross-entropy loss functions: logistic regression and maxent.  Such models are good for a number of reasons.  First, cross-entropy naturally arises under rather general assumptions, in particular, when the distribution <i>p (x | class)</i> belongs to an exponential family of distributions (details can be found in Pattern Recognition and Machine Learning // C. Bishop and Machine Learning: A Probabilistic Perspective // ‚Äã‚ÄãK. Murphy).  Secondly, the SGD algorithm allows you to effectively train such a model, including on sparse data.  Thirdly, the model serialization is reduced to a simple storage of the weights vector, therefore, the saved model occupies the entire O (number of attributes) memory.  Fourth, the application of the model requires only a quick calculation of the decision function.  For our teaching matrix, we build an ensemble of linear models using bagging.  The final prediction is made on the basis of ensemble voting. <br><br>  To assess the quality of classification in our problems, the main metric is Affinity (the more commonly used name is Lift).  This metric shows how our model is better than random.  For example, consider the category of rich users.  The share of such users in the total population is 7%.  If our model with acceptable coverage (Recall) gives an accuracy (Precision) of 21%, then Affinity is counted as Affinity = 0.21 / 0.07 = 3.0.  In this case, the recall is configured so that the output distribution of users by categories approximately coincides with the input.  The table shows the cross-validation metrics of the constructed model.  It is seen that the model gives a very good result. <br><table><tbody><tr><td>  <b>Class</b> </td><td>  <b>Baseline distribution</b> </td><td>  <b>The resulting distribution</b> </td><td>  <b>Precision</b> </td><td>  <b>Recall</b> </td><td>  <b>Affinity</b> </td></tr><tr><td>  one </td><td>  69720 (0.125) </td><td>  18609 (0.033) </td><td>  0.281 </td><td>  0.075 </td><td>  2.245 </td></tr><tr><td>  2 </td><td>  223358 (0.400) </td><td>  274519 (0.492) </td><td>  0.492 </td><td>  0.605 </td><td>  1.229 </td></tr><tr><td>  3 </td><td>  134024 (0.240) </td><td>  93075 (0.167) </td><td>  0.316 </td><td>  0.220 </td><td>  1.317 </td></tr><tr><td>  four </td><td>  91448 (0.164) </td><td>  118506 (0.212) </td><td>  0.246 </td><td>  0.319 </td><td>  1.502 </td></tr><tr><td>  five </td><td>  39353 (0.071) </td><td>  53194 (0.095) </td><td>  0.233 </td><td>  0.315 </td><td>  3.301 </td></tr></tbody></table><br>  Let's take a closer look at the classification results.  It is clear that in our case, due to the orderliness of the categories, not all errors are equally serious.  It‚Äôs not scary to assign a ‚Äúabove average‚Äù category to a ‚Äúrich‚Äù user, it‚Äôs quite another thing to call him ‚Äúpoor‚Äù.  The figure shows an Affinity map for comparing the predicted and real categories of users.  The value of each cell is calculated as: <br><div style="text-align:center;"><img src="https://habrastorage.org/files/606/9ac/c42/6069acc42b12464d9bf53a0b4d79de11.png"></div><br>  In other words, the proportion of users of category <i>i</i> among those whom we predicted is <i>j</i> divided by the proportion of users of category <i>i</i> in the general population.  On the diagonal so we have Precision / TrueFraction = Affinity.  The ideal classifier gives nonzero values ‚Äã‚Äãof the elements of such a matrix only on the diagonal.  It can be seen from the figure that in our case the neighboring categories can be mixed up (3 "squares": rich - above average, medium - above average, poor - below average), but the proportion of gross errors is rather small.  Given that the division into categories of income is rather conditional (try to honestly define your own), the blurring between neighboring categories is quite understandable.  We also note that the extremes are best defined - the rich and the poor, and worst of all - the category of medium.  This is successful because it is the extremes that usually interest a business (who is my economy or premium audience?). <br><br><img src="https://habrastorage.org/files/375/6e3/80a/3756e380a2e74c6498aa17dc94820513.png"><br><br>  The final step in the process is to apply the model on unallocated users.  We have tens of millions of such users, for each of them a vector of selected features is calculated and a trained model is applied.  The final quality check of the model is carried out on the basis of data on users participating in the panel study.  The predicted income category is recorded in the Rating cookie.  Counters read these cookies and counts the statistics for your service.  It is these statistics that you see in the interface and can be used to make business decisions.  The task is closed. <br><br><h1>  Final Considerations </h1><br>  In the article, using the example of the task of determining the category of income, I described the approach used to solve the problems of classifying users of the project. Rating Mail.Ru.  The overview format of the article did not allow me to delve into the technical details, for example, in the subtleties of the implementation of certain stages of the process (mainly on Hadoop).  Also for brevity, we had to omit several important subtasks - filtering bots and one-day users, constructing semantic features, composing ensembles from models built for different training samples, an approach to validating the constructed models, and much more.  These are already slightly different stories, we will leave them for future publications. </div><p>Source: <a href="https://habr.com/ru/post/244285/">https://habr.com/ru/post/244285/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../244273/index.html">Development system combining ExtJS and Node.JS</a></li>
<li><a href="../244275/index.html">Document with information about mobile publishers and samizdat opportunities</a></li>
<li><a href="../244277/index.html">Everything you need to develop "dynamic thinking"</a></li>
<li><a href="../244279/index.html">Automatically test Android applications with love</a></li>
<li><a href="../244283/index.html">Video from Vitaly Friedman's report ‚ÄúResponsive Web Design: Tricks and Tricks‚Äù</a></li>
<li><a href="../244287/index.html">3 in 1: Hour Code, Computer Science Day and Volunteer Day</a></li>
<li><a href="../244289/index.html">Freelance in Cyprus</a></li>
<li><a href="../244291/index.html">How to build a studio for shooting video with minimal cost and for three days</a></li>
<li><a href="../244293/index.html">Study in Mathematica: Benedict Cumberbatch successfully parodies other actors, but can he fool the computer?</a></li>
<li><a href="../244295/index.html">Prospects for data centers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>