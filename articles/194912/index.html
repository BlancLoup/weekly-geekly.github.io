<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Intel¬Æ Parallel Studio XE 2013 Service Pack 1 - what's new?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The Intel Parallel Studio XE package has long been known to developers, including by publishing on the Intel Habr√© blog. Recently there was an update ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Intel¬Æ Parallel Studio XE 2013 Service Pack 1 - what's new?</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/storage3/f7b/cc2/53f/f7bcc253fd1cce09eee03542b38a48c9.png"><br><br>  The <i>Intel Parallel Studio XE</i> package has long been known to developers, including by publishing on the Intel Habr√© blog.  Recently there was an update - <i>Intel Parallel Studio XE 2013 Service Pack 1 (SP1)</i> , which has a number of interesting innovations.  It becomes easier to program for co-processors and integrated graphics, largely due to the support of the OpenMP 4.0 standard (partial).  The search for errors has become more flexible, memory leaks are now detected before the process is completed, i.e.  You can search for them in long-playing services and "falling" applications.  Finding bottlenecks in performance will be easier thanks to a new view of the call tree, an assessment of overhead costs and detailed information on parallel structures. <br><a name="habracut"></a><br>  <b>Intel Composer XE</b> combines C / C ++ and Fortran compilers, multithreaded, mathematical and other libraries.  In Intel Composer XE 2013 SP1, many improvements have appeared, here we will dwell on the possibilities of vectorization and the use of co-processors and accelerators, which appeared in the OpenMP * 4.0 standard. <br><br><h5>  OpenMP * SIMD Design </h5><br>  Using SIMD (Single Instruction Multiple Data) instructions, or vectorization, allows you to implement data parallelism.  This is one of the most effective ways to optimize software performance.  The Intel compiler is doing everything possible to vectorize the code automatically, but this is not always possible if, for example, it suspects the possibility of dependencies.  The OpenMP 4.0 standard allows you to explicitly tell the compiler that there are no dependencies and you can vectorize using the ‚Äú#pragma omp simd‚Äù construct (there is an analogue for Fortran).  If the cycle is preceded by ‚Äúomp simd‚Äù, the compiler must generate vector instructions that can process several iterations simultaneously.  Additional constructions can be used, for example, for ‚Äúreduction‚Äù: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre><code class="hljs lua">double <span class="hljs-built_in"><span class="hljs-built_in">pi</span></span>() { double <span class="hljs-built_in"><span class="hljs-built_in">pi</span></span> = <span class="hljs-number"><span class="hljs-number">0.0</span></span>; double t; #pragma omp simd private(t) reduction(+:<span class="hljs-built_in"><span class="hljs-built_in">pi</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (i=<span class="hljs-number"><span class="hljs-number">0</span></span>; i&lt;count; i++) { t = (double)((i+<span class="hljs-number"><span class="hljs-number">0.5</span></span>)/count); <span class="hljs-built_in"><span class="hljs-built_in">pi</span></span> += <span class="hljs-number"><span class="hljs-number">4.0</span></span>/(<span class="hljs-number"><span class="hljs-number">1.0</span></span>+t*t); } <span class="hljs-built_in"><span class="hljs-built_in">pi</span></span> /= count <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-built_in"><span class="hljs-built_in">pi</span></span>; }</code> </pre> <br>  You can define "element-by-element" SIMD functions, in which the developer describes the operations performed on individual data elements.  The compiler knows that the function can be safely used in the SIMD cycle.  To define the SIMD function, use the ‚Äúomp declare‚Äù construction <br><br><pre> <code class="hljs mel">#pragma omp declare simd notinbranch <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> <span class="hljs-keyword"><span class="hljs-keyword">min</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">float</span></span> a, <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> b) { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> a &lt; b ? a : b; } #pragma omp declare simd notinbrach <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> distsq(<span class="hljs-keyword"><span class="hljs-keyword">float</span></span> x, <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> y) { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (x - y) * (x - y); }</code> </pre><br>  Such things allow you to combine data parallelism (SIMD) within a single core, and parallelism across threads or tasks running on multiple cores.  The following loop is first vectorized, then the remaining number of iterations can be performed by different threads: <br><br><pre> <code class="hljs swift">#pragma omp parallel <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> simd <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (i=<span class="hljs-number"><span class="hljs-number">0</span></span>; i&lt;<span class="hljs-type"><span class="hljs-type">N</span></span>; i++) d[i] = <span class="hljs-built_in"><span class="hljs-built_in">min</span></span>(distsq(a[i], b[i]), <span class="hljs-built_in"><span class="hljs-built_in">c</span></span>[i]);</code> </pre><br><h5>  Using the co-processor </h5><br>  Accelerators and co-processors, such as Intel Xeon Phi, are gaining popularity.  In OpenMP 4.0, you can send calculations (which are well parallelized) to a co-processor using omp target.  This construction starts the code block on the accelerator following it.  If it does not exist or is not supported, the code will run on the CPU in the normal mode.  The ‚Äúmap‚Äù construction allows you to organize data sent to the co-processor: <br><br><pre> <code class="hljs swift">#pragma omp target <span class="hljs-built_in"><span class="hljs-built_in">map</span></span>(to(b:<span class="hljs-built_in"><span class="hljs-built_in">count</span></span>)) <span class="hljs-built_in"><span class="hljs-built_in">map</span></span>(to(<span class="hljs-built_in"><span class="hljs-built_in">c</span></span>,d)) <span class="hljs-built_in"><span class="hljs-built_in">map</span></span>(from(a:<span class="hljs-built_in"><span class="hljs-built_in">count</span></span>)) { #pragma omp parallel <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (i=<span class="hljs-number"><span class="hljs-number">0</span></span>; i&lt;<span class="hljs-built_in"><span class="hljs-built_in">count</span></span>; i++) a[i] = b[i] * <span class="hljs-built_in"><span class="hljs-built_in">c</span></span> + d; }</code> </pre><br>  <b>Intel Advisor XE</b> models parallel execution of sequential code.  It is used for prototyping parallel algorithms, giving architects the opportunity to quickly experiment with different design options, before significant resources are spent on implementation.  The result is a prediction of program execution acceleration and scalability, as well as an indication of the race data that may appear in the parallel version. <br><br><h5>  Advanced acceleration and scalability analysis </h5><br>  Earlier, scalability was estimated from 2 to 32 cores.  This was usually enough for the CPU, but not for the Intel Xeon Phi co-processor, which has more than 240 hardware threads.  The new version of Advisor XE evaluates the scalability of the algorithm on an unlimited number of cores.  The picture below shows an example with 512 cores.  Now you can evaluate how ready your algorithm is for porting to Intel Xeon Phi in terms of the number of threads (you also need to evaluate the ‚Äúvectorization‚Äù of the code, memory limitations, etc., which the Advisor XE does not yet do). <br><br><img src="http://habrastorage.org/storage3/27d/3a2/d71/27d3a2d71d7f4ac934033bb1a5e856e9.png"><br><br><h5>  Experiment snapshot </h5><br>  Inspector XE and VTune Amplifier XE users have the ability to do many tests and store all the results, tracking changes in performance and problem status.  Until recently, Advisor XE was deprived of such an opportunity - the result of only the last experiment was kept, due to the complexity of the profile structure and its close connection with the current version of the code. <br><br>  Intel Advisor XE 2013 update 4, supplied with Parallel Studio XE 2013 SP1, has the ability to make a copy of the experiment (snapshot).  This copy is read-only, but it allows you to see what the performance evaluations were in earlier versions of the code, and compare them with the current one: <br><br><img src="http://habrastorage.org/storage3/df6/5fd/bc6/df65fdbc6e19acc060665e408f7d2457.png"><br><img src="http://habrastorage.org/storage3/689/8be/f23/6898bef23cd4073cf2350d30fd81d68c.png"><br><br><h5>  Analysis of individual sections of the code </h5><br>  Advisor XE analysis can add significant overhead, because  complex simulation of parallel execution ‚Äúin real time‚Äù is being carried out.  To save time, you can now mark the individual sections of the code that need to be analyzed.  In addition, such a "narrowing" can improve the accuracy of the analysis by eliminating the influence of other parts of the program.  All this is implemented by new types of ‚Äúannotations‚Äù: ANNOTATE_DISABLE_COLLECTION_PUSH and ANNOTATE_DISABLE_COLLECTION_POP (there are also buttons for manual control): <br><br><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">int</span></span> main(<span class="hljs-type"><span class="hljs-type">int</span></span> argc, <span class="hljs-type"><span class="hljs-type">char</span></span>* argv[]) { ANNOTATE_DISABLE_COLLECTION_PUSH // <span class="hljs-keyword"><span class="hljs-keyword">Do</span></span> initialization <span class="hljs-keyword"><span class="hljs-keyword">work</span></span> here ANNOTATE_DISABLE_COLLECTION_POP // <span class="hljs-keyword"><span class="hljs-keyword">Do</span></span> interesting <span class="hljs-keyword"><span class="hljs-keyword">work</span></span> here ANNOTATE_DISABLE_COLLECTION_PUSH // <span class="hljs-keyword"><span class="hljs-keyword">Do</span></span> finalization <span class="hljs-keyword"><span class="hljs-keyword">work</span></span> here ANNOTATE_DISABLE_COLLECTION_POP <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span>; }</code> </pre><br>  <b>Intel Inspector XE</b> - debugger of memory and threads, performs dynamic analysis during the execution of the application and can be integrated with standard debuggers.  It can find errors in working with memory, such as leaks, incorrect memory access, etc., and errors in working with streams - interlocks, data races, and others.  Such problems can be missed by normal functional testing or static code analysis. <br><br><h5>  Importing Valgrind * and Rational Purify * Suppression Rules </h5><br>  Inspector XE has the ability to suppress individual problems or groups of problems that are not of interest to the developer.  For example, problems in foreign modules or false positives.  Similar functionality is in other tools.  The rules of suppression in large projects are stored in separate files that define the place in the code, the module, the type of problem and other information. <br><br>  The new version of Inspector XE - update 7 has the ability to import files with the rules of suppression, generated by other tools - Valgrind and Rational Purify.  These files are converted to Inspector XE format.  This simplifies the transition to the Inspector XE from other tools, saving time and past investments in the formation of the base rules of suppression. <br><br>  In addition, the Inspector XE suppression rules are now stored as text and can be edited manually. <br><br><h5>  Search for memory leaks before application termination </h5><br>  Memory leak detection is one of the most popular ways to analyze memory problems in the Inspector XE.  In previous versions, leak detection was necessary for the program to run from start to finish, so that the Inspector XE could track all allocations and free up memory.  This is not always convenient if, for example, an application runs for a long time, or even forever, like a demon or a service.  In addition, if the program terminates incorrectly, ‚Äúcrashes,‚Äù it will also not be possible to track leaks in this way. <br><br>  In the new version of Inspector XE, a programmer can mark a region of code in which to look for memory leaks.  Restrict such a region can be a special API in the source code.  You can call a report on the leaks detected to date by the buttons in the graphical interface and commands in the command line. <br><br>  Those.  You can search for leaks in a particular section of the code, and learn about them without waiting for the process to complete. <br><br>  <b>Intel VTune Amplifier XE</b> is a performance profiler.  It shows the most expensive in terms of CPU resources areas of your application with different granularities - by threads, modules, functions, instructions, etc.  The tool provides information about the load balance between threads, latency and synchronization objects that cause these waits.  VTune Amplifier XE allows you to detect micro-architectural performance problems, such as cache misses, "false sharing" and many others. <br><br><h5>  Detailed Overhead Report </h5><br>  In a multi-threaded program, a part of the CPU time is inevitably spent on synchronizing threads, distributing work between them, and so on.  This time spent not on basic calculations is called overhead or ‚Äúoverhead‚Äù.  These overheads need to be minimized to improve performance.  VTune Amplifier XE can now give detailed information about overhead and spin waiting.  You can estimate the amount of CPU time spent on overhead in a particular function, module or instruction.  The tool can show "overhead" coming from OpenMP, Intel Threading Building Blocks or Intel Cilk Plus.  You can use both the time values ‚Äã‚Äãin the table and the graphical representation on the timeline: <br><br><img src="http://habrastorage.org/storage3/531/2f1/21b/5312f121b921e5ab5eeb65c4fa0152a7.png"><br><br><h5>  Improved analysis of OpenMP * applications </h5><br>  The new VTune Amplifier XE has received advanced analysis of OpenMP regions.  Grouping by ‚ÄúFrame Domain‚Äù in the Bottom-up panel shows the OpenMP regions as frames - see the picture below.  Using filters, you can narrow the performance profile to a separate parallel region, estimate how much time was spent there, what was the load balance, etc.  The string ‚Äú[No frame domain - Outside any frame]‚Äù represents the sequential part of your program, so that you can evaluate the degree of ‚Äúparallelism‚Äù of the code (recall <a href="http://ru.wikipedia.org/wiki/%25D0%2597%25D0%25B0%25D0%25BA%25D0%25BE%25D0%25BD_%25D0%2590%25D0%25BC%25D0%25B4%25D0%25B0%25D0%25BB%25D0%25B0">Amdahl‚Äôs law</a> ). <br><br><img src="http://habrastorage.org/storage3/5a6/d94/429/5a6d944291cf05ad5e884a58060b6479.png"><br><br>  Overhead costs and active wait times related to OpenMP are defined not only for Intel OpenMP, but also for GCC * and Microsoft OpenMP *.  In the picture above (Advanced Hotspots analysis), ‚Äú[OpenMP worker]‚Äù and ‚Äú[OpenMP for]‚Äù refer to the Microsoft OpenMP * library ‚Äî the vcomp100.dll module. <br><br><h5>  View source code and assembler from the command line </h5><br>  In some cases, the command line is more convenient to use than the graphical interface.  For example, when you are working on a remote Linux server via SSH.  Now you can view the sources in the VTune Amplifier XE profile directly from the command line.  So you may not need to copy the profiling results from a remote machine or configure VNC - you can quickly look at the profile from the same command interpreter from which you started the data collection: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># amplxe-cl -report hotspots -source-object function=grid_intersect -r r000hs/ Source Line Source CPU Time:Self ----------- ------------------------------------------------------------ ------------- 460 return 1; 461 } 462 463 464 /* the real thing */ 465 static void grid_intersect(grid * g, ray * ry) 0.036 466 { 467 468 469 flt tnear, tfar, offset; 470 vector curpos, tmax, tdelta, pdeltaX, pdeltaY, pdeltaZ, nXp, nYp, nZp; 471 gridindex curvox, step, out; 472 int voxindex; 473 objectlist * cur; 474 475 if (ry-&gt;flags &amp; RT_RAY_FINISHED) 476 return; 477 478 if (!grid_bounds_intersect(g, ry, &amp;tnear, &amp;tfar)) 479 return; 480 481 if (ry-&gt;maxdist &lt; tnear) 0.020</span></span></code> </pre><br><h5>  Caller / Callee Call Tree Analysis </h5><br>  New tab Caller / Callee combines the best of Bottom-up and Top-down.  It shows the own execution time of each function and the total time, taking into account the called functions.  For the selected function, its parents (Callers) are shown in the upper right window, and the called functions (Callees) are shown in the lower right window.  In the Caller / Callee window, you can explore the call sequences, and the contribution of each level to CPU consumption.  You can filter on the full time of any function and get all the trees that have this function on any level.  So you can find the most critical branch of calls. <br><br><img src="http://habrastorage.org/storage3/9f0/82f/74d/9f082f74d08573de3660aee53fdd0717.png"><br><br><h5>  GPU profiling </h5><br>  VTune Amplifier XE can now profile code running on Intel Processor Graphics.  You can track the overall activity of the GPU: is it used for video decoding?  What CPU threads run GPU computing?  Are all GPU resources used?  This feature is especially interesting for computational tasks running on a GPU through OpenCL *.  VTune Amplifier XE recognizes OpenCL kernels (or computing task), you can see the size of the work and microarchitectural problems, such as L3 cache misses.  On the timeline, OpenCL kernels are marked on the CPU threads that started them.  You can observe the state of the GPU (GPU Execution Units) computational cores over time: active, idle, or stalled.  Having data on the loading of the GPU, you can assess the extent to which the program's performance is limited: CPU or GPU, which OpenCL kernels spend more resources, where there are opportunities for more loading of the GPU and, accordingly, CPU load. <br><br><img src="http://habrastorage.org/storage3/e6f/688/01d/e6f68801dfafacfd2fbf77fc97b60700.png"><br><br><h5>  Top-down performance analysis on fourth-generation Intel Core processors </h5><br>  The study of micro-architectural problems is not a trivial task.  An understanding of the CPU micro-architecture and knowledge of the hardware events that register what is happening in the processor is required.  To give more structure and friendliness to such an analysis, the data from the hardware counters in the General Exploration analysis were reduced to more comprehensible metrics, and organized from top to bottom, or from general to specific.  The tool itself counts the metrics for the platform on which the data was collected, and highlights potential problems that limit performance.  The hierarchical presentation of data makes it possible to control the level of detail, navigation becomes more convenient. <br><br><img src="http://habrastorage.org/storage3/cc8/78d/744/cc878d744138a92b1ea34a928fb68f0f.png"><br><br>  <b>Summary</b> <br>  Intel Parallel Studio XE 2013 SP1 provides new opportunities for efficiently programming for co-processors, getting more from parallel models (such as OpenMP), easier to find and fix complex performance problems on new microarchitectures, creating excellent software products.  <a href="http://software.intel.com/en-us/intel-parallel-studio-xe">Download the new version</a> , check how new features can improve your current project. </div><p>Source: <a href="https://habr.com/ru/post/194912/">https://habr.com/ru/post/194912/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../194896/index.html">Google Constitute - Comparison of 160 World Constitutions</a></li>
<li><a href="../194898/index.html">Brute force Facebook username</a></li>
<li><a href="../194904/index.html">Realization of the ability to download directories by site users</a></li>
<li><a href="../194906/index.html">Migrating a running Ubuntu system to encrypted drives</a></li>
<li><a href="../194908/index.html">EPC Gen2 RFID Systems</a></li>
<li><a href="../194914/index.html">Intellectual Internet Spider Architecture</a></li>
<li><a href="../194916/index.html">Information security in Australia, and why pentest there is no longer a cake</a></li>
<li><a href="../194918/index.html">Mysql Delayed Replication or Salespeople with Mysql-slave</a></li>
<li><a href="../194920/index.html">Introducing Rollercoaster.io</a></li>
<li><a href="../194922/index.html">Deep Impact lost due to software failure</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>