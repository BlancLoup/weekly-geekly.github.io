<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Distributed learning neural networks with MXNet. Part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Today we will answer the simple question: "How does distributed learning work (in the context of MXNet )?" 


 All code samples tested on MXNet v0.10....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Distributed learning neural networks with MXNet. Part 1</h1><div class="post__text post__text-html js-mediator-article"><p>  Today we will answer the simple question: "How does distributed learning work (in the context of <a href="http://mxnet.io/">MXNet</a> )?" </p><br><p>  All code samples tested on MXNet v0.10.0 and may not work (or work differently) in other versions, but I believe that the general concepts will be unchanged for a long time. </p><br><p>  Well, the last thing before we get to the main part, I want to express my gratitude for the help in writing the article to my colleagues, without which this article would not have been possible: </p><br><ul><li>  Madan Jampani; </li><li>  Suneel Marthi; </li></ul><br><p>  I would also like to recommend raising the typewriter with <a href="https://habrahabr.ru/post/333380/">DLAMI</a> and doing all the examples from the article myself, especially since they are quite simple.  To execute the code it is quite suitable for a free machine on AWS. </p><br><p>  With the preamble over, we climb under the cat ... </p><a name="habracut"></a><br><h2 id="raspredelennoe-obuchenie-kakim-mxnet-ego-vidit">  Distributed learning how MXNet sees it </h2><br><p>  In MXNet, all participants in the learning process are divided into 3 logical groups: </p><br><ul><li>  scheduler; </li><li>  server; </li><li>  worker (worker); </li></ul><br><p>  This is a purely logical distribution, so that all participants can work on the same machine. </p><br><p>  To begin with, let's look at a superficial explanation of what each of the participants represents: </p><br><h3 id="planirovschik">  Scheduler </h3><br><p>  The scheduler is the central node of the cluster, it is responsible for the initial configuration of the cluster, providing the necessary information for each participant in the learning process and ... nothing more.  We will see how he falls into hibernation as soon as the cluster is ready to start learning.  And even when the cluster finishes its training, its task will only be to turn itself off. </p><br><p>  I think everyone already guessed that there can be only one scheduler in a cluster. </p><br><h3 id="server">  Server </h3><br><p>  The server acts as a repository of learning model parameters.  That is, if a model is taught in the style: Y = AX + B, the server stores the A and B vectors. He is also responsible for correct updating them.  Servers can be more than one, and accordingly there is a rule according to which the model is distributed to several servers.  But this is a topic for a separate article. </p><br><h3 id="rabochiy">  Working </h3><br><p>  These are actually those cluster participants who directly perform the model training.  Each worker receives his part of the data, which needs to be trained, considers the gradient step and sends it to the servers for updating the model. </p><br><h3 id="primer-klastera">  Cluster example </h3><br><p>  Let's take the sham cluster example with: </p><br><ul><li>  scheduler on a separate machine; </li><li>  two servers; </li><li>  three cars with workers; </li></ul><br><p>  The cluster itself will look like this: </p><br><p><img src="https://habrastorage.org/web/499/29b/654/49929b654444493fa993ea3fbb4f4543.png"></p><br><p>  This picture, like the described configuration, will be used only to visualize the data flow. </p><br><h2 id="inicializaciya-klastera">  Cluster initialization </h2><br><p>  In practice, we will not create such a large cluster as described above, but we will manage with a much smaller cluster with 3 nodes on one physical machine.  There are several reasons for this: </p><br><ul><li>  easier and cheaper; </li><li>  all logs will be in one place, which will simplify the explanation; </li></ul><br><p>  Before continuing, you need to clarify one detail.  For MXNet, distributed learning essentially means using KVStore.  The name is an acronym for "Key Value Storage".  And in essence, it is a distributed storage that runs on servers and has some additional functionality (for example, it knows exactly how to update the model, having received a gradient step from the worker). </p><br><blockquote>  Also, KVStore support is available only in one of two options: <br><ul><li>  MXNet was compiled manually, with the USE_DIST_KVSTORE = 1 flag turned on or </li><li>  <a href="https://habrahabr.ru/post/333380/">DLAMI</a> was used (since the framework was built manually with the USE_DIST_KVSTORE flag = 1 enabled) </li></ul><br><br>  In this article, I assume that MXNet will be used from the Jun / Jul DLAMI release (MXNet 0.10.0). <br>  There is also a non-zero probability that at the time of reading, the official MXNet pip package will have support for KVStore. </blockquote><p>  It is time to start creating logical cluster members.  To create a member, you just need to create some environment variables and then import the mxnet module. </p><br><h3 id="planirovschik-1">  Scheduler </h3><br><p>  First of all, let's run the scheduler: </p><br><pre><code class="python hljs">ubuntu:~$ python &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> subprocess &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os &gt;&gt;&gt; scheduler_env = os.environ.copy() &gt;&gt;&gt; scheduler_env.update({ ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_ROLE"</span></span>: <span class="hljs-string"><span class="hljs-string">"scheduler"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_PS_ROOT_PORT"</span></span>: <span class="hljs-string"><span class="hljs-string">"9000"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_PS_ROOT_URI"</span></span>: <span class="hljs-string"><span class="hljs-string">"127.0.0.1"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_NUM_SERVER"</span></span>: <span class="hljs-string"><span class="hljs-string">"1"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_NUM_WORKER"</span></span>: <span class="hljs-string"><span class="hljs-string">"1"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"PS_VERBOSE"</span></span>: <span class="hljs-string"><span class="hljs-string">"2"</span></span> ‚Ä¶ }) &gt;&gt;&gt; subprocess.Popen(<span class="hljs-string"><span class="hljs-string">"python -c 'import mxnet'"</span></span>, shell=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, env=scheduler_env) &lt;subprocess.Popen object at <span class="hljs-number"><span class="hljs-number">0x7facb0622850</span></span>&gt;</code> </pre> <br><p>  Let's stop here for a second to get an idea of ‚Äã‚Äãwhat's going on.  The first 4 lines of code should not cause many questions for Python programmers: just import dependencies and create an OS environment.  What is interesting here is exactly what updates to environmental variables will be made: </p><br><p>  We start by looking at DMLC_ROLE.  Let's see exactly where it is used, namely in the <a href="https://github.com/dmlc/ps-lite">ps-lite</a> package.  In accordance with the official <a href="">README</a> (loosely translated): </p><br><blockquote>  Easy and efficient server implementation for storing parameters. <br>  Well, the exact place where the environment variable is read <a href="">here</a> (by the way, all references to specific commits). </blockquote><br><pre> <code class="cpp hljs">val = CHECK_NOTNULL(Environment::Get()-&gt;find(<span class="hljs-string"><span class="hljs-string">"DMLC_ROLE"</span></span>)); <span class="hljs-comment"><span class="hljs-comment">// here std::string role(val); is_worker_ = role == "worker"; is_server_ = role == "server"; is_scheduler_ = role == "scheduler"; // and later here verbose_ = GetEnv("PS_VERBOSE", 0);</span></span></code> </pre> <br><p>  I think it is not necessary to be a C ++ guru to understand what is happening here.  The logical role of the node is determined by the string in this variable "DMLC_ROLE".  Funny, but it seems there is no check that this variable contains one of the allowed values.  This, potentially, can lead to interesting problems. </p><br><p>  The second thing that interests us is not only where the variable is read, but also where it is used.  To tell about this, you need to refer to the file van.cc, which we will meet more than once, here is a specific line where the variable is <a href="">used</a> and the variable "is_scheduler" is created: </p><br><pre> <code class="cpp hljs">scheduler_.hostname = <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">string</span></span>(CHECK_NOTNULL(Environment::Get()-&gt;find(<span class="hljs-string"><span class="hljs-string">"DMLC_PS_ROOT_URI"</span></span>))); scheduler_.port = atoi(CHECK_NOTNULL(Environment::Get()-&gt;find(<span class="hljs-string"><span class="hljs-string">"DMLC_PS_ROOT_PORT"</span></span>))); scheduler_.role = Node::SCHEDULER; scheduler_.id = kScheduler; is_scheduler_ = Postoffice::Get()-&gt;is_scheduler(); <span class="hljs-comment"><span class="hljs-comment">// here</span></span></code> </pre> <br><p>  If you quickly run further through the code to see what is happening there, you can see the following interesting place: </p><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">// get my node info if (is_scheduler_) { my_node_ = scheduler_; } else { auto role = is_scheduler_ ? Node::SCHEDULER : (Postoffice::Get()-&gt;is_worker() ? Node::WORKER : Node::SERVER);</span></span></code> </pre> <br><p>  In this particular example, the role variable will never be equal to Node :: SCHEDULER.  So you have a chance to create a pull-request to fix it (if nobody has done it yet). </p><br><p>  Just looking at this place you understand that there is not so much work for a planner.  This is because, unlike the worker and the server, the scheduler uses the IP address and port that were transferred to it, rather than looking for a free port in the system. </p><br><p>  Go further parameter: DMLC_PS_ROOT_PORT.  With this we quickly understand with regard to the already existing knowledge.  Here is the code that you have already seen: </p><br><pre> <code class="cpp hljs">scheduler_.hostname = <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">string</span></span>(CHECK_NOTNULL(Environment::Get()-&gt;find(<span class="hljs-string"><span class="hljs-string">"DMLC_PS_ROOT_URI"</span></span>))); scheduler_.port = atoi(CHECK_NOTNULL(Environment::Get()-&gt;find(<span class="hljs-string"><span class="hljs-string">"DMLC_PS_ROOT_PORT"</span></span>))); <span class="hljs-comment"><span class="hljs-comment">// here scheduler_.role = Node::SCHEDULER; scheduler_.id = kScheduler; is_scheduler_ = Postoffice::Get()-&gt;is_scheduler();</span></span></code> </pre> <br><p>  Again, this is from <a href="">van.cc.</a>  As it is not difficult to guess, this is the port on which the scheduler should listen to messages. </p><br><p>  I hope at this stage it is clear that DMLC_PS_ROOT_URI is just the ip address of the scheduler.  So let's jump right away to discussing DMLC_NUM_SERVER and DMLC_NUM_WORKER. </p><br><p>  It so happened that every MXNet logical node in a cluster should know about all the other nodes.  So, for each node, before it starts, the environment variables write how many workers and servers are in the cluster (the number of schedulers is unnecessary, because it is always 1).  By the way, this information is stored in the <a href="">Postoffice</a> class (along with other information about the cluster). </p><br><p>  Well, the last parameter, but perhaps one of the most important archives is PS_VERBOSE.  This will force our newly created process to display debug information, which is vital for us now. </p><br><p>  From the point of view of our sham chart, our cluster now looks something like this: </p><br><p><img src="https://habrastorage.org/web/a23/e3a/5bf/a23e3a5bf57b41ee92da701e13603f56.png"></p><br><h3 id="zapuskaem-server">  We start the server </h3><br><p>  Now that we have a scheduler, let's raise the server.  Since we are raising all the logical nodes on the same machine, we will have to create a copy of the environment parameters and re-make the necessary changes there in order to start the server: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>server_env = os.environ.copy() &gt;&gt;&gt; server_env.update({ ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_ROLE"</span></span>: <span class="hljs-string"><span class="hljs-string">"server"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_PS_ROOT_URI"</span></span>: <span class="hljs-string"><span class="hljs-string">"127.0.0.1"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_PS_ROOT_PORT"</span></span>: <span class="hljs-string"><span class="hljs-string">"9000"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_NUM_SERVER"</span></span>: <span class="hljs-string"><span class="hljs-string">"1"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_NUM_WORKER"</span></span>: <span class="hljs-string"><span class="hljs-string">"1"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"PS_VERBOSE"</span></span>: <span class="hljs-string"><span class="hljs-string">"2"</span></span> ‚Ä¶ }) &gt;&gt;&gt; subprocess.Popen(‚Äúpython -c <span class="hljs-string"><span class="hljs-string">'import mxnet'</span></span>‚Äù, shell=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, env=server_env) &lt;subprocess.Popen object at <span class="hljs-number"><span class="hljs-number">0x7facb06228d0</span></span>&gt;</code> </pre> <br><p>  I hope now what is happening in the code raises no questions, but just in case: </p><br><ul><li>  we say that the new process is the server (DMLC_ROLE); </li><li>  we say what IP is in the scheduler (DMLC_PS_ROOT_URI); </li><li>  we speak on which port the scheduler listens for incoming connections (DMLC_PS_ROOT_PORT); </li><li>  we tell the server how many workers are in the cluster (DMLC_NUM_WORKER) </li><li>  we tell the server how many servers are in a cluster (DMLC_NUM_SERVER) </li><li>  well, and set the output to debug mode (2) </li></ul><br><p>  Then someone might ask: hey, I thought that DMLC_PS_ROOT_PORT and DMLC_PS_ROOT_URI to specify the IP and port of the logical node that we run?  The answer will be - no, this is the address and port of the scheduler, but everyone else must figure out what their address is and find an available port in the system.  They need information about the scheduler to knock on him and ask him to add them to the cluster. </p><br><p>  After starting the servers, our chart looks like this: </p><br><p><img src="https://habrastorage.org/web/a3c/b17/4c0/a3cb174c098b4ba4b1d3786c912ea49e.png"></p><br><h3 id="zapuskaem-rabochego">  We start the worker </h3><br><p>  It is time to launch the actual worker itself and create the KVStore: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>os.environ.update({ ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_ROLE"</span></span>: <span class="hljs-string"><span class="hljs-string">"worker"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_PS_ROOT_URI"</span></span>: <span class="hljs-string"><span class="hljs-string">"127.0.0.1"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_PS_ROOT_PORT"</span></span>: <span class="hljs-string"><span class="hljs-string">"9000"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_NUM_SERVER"</span></span>: <span class="hljs-string"><span class="hljs-string">"1"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"DMLC_NUM_WORKER"</span></span>: <span class="hljs-string"><span class="hljs-string">"1"</span></span>, ‚Ä¶ <span class="hljs-string"><span class="hljs-string">"PS_VERBOSE"</span></span>: <span class="hljs-string"><span class="hljs-string">"2"</span></span> ‚Ä¶ }) &gt;&gt;&gt; worker_env = os.environ.copy() &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mxnet &gt;&gt;&gt; kv_store = mxnet.kv.create(<span class="hljs-string"><span class="hljs-string">'dist_async'</span></span>)</code> </pre> <br><p>  By the way, KVStore can work in two modes: </p><br><ul><li>  dist_sync </li><li>  dist_async </li></ul><br><p>  I will leave to the inquisitive reader the question of how these modes differ, you can read <a href="http://mxnet.io/api/python/kvstore.html">about it here</a> . </p><br><p>  After running the workers, our chart will look like this: </p><br><p><img src="https://habrastorage.org/web/499/29b/654/49929b654444493fa993ea3fbb4f4543.png"></p><br><h2 id="zhiznennyy-cikl-nody-van">  The life cycle of the node (Van) </h2><br><p>  Before you rush to discuss what is happening at the moment of KVStore creation, you need to tell that each node has a life cycle, which has the following events: </p><br><ul><li>  <a href="">Start</a> - start </li><li>  <a href="">Stop</a> - stop </li><li>  <a href="">Receiving</a> - receiving a message </li></ul><br><p>  Also, the very same class ( <a href="">Van</a> ) that is responsible for handling these events has several other equally important methods.  We will talk about some of them in detail later in other articles, and now we will simply list: </p><br><ul><li>  <a href="">Send</a> - sends the message </li><li>  <a href="">PackMeta</a> - convert model to proto message </li><li>  <a href="">UnpackMeta</a> - unpacks the proto message and creates a model </li><li>  <a href="">HeartBeat</a> - sends a message that he is still alive </li></ul><br><p>  Here is what each node performs at the moment when the Start signal arrives: </p><br><ul><li>  <a href="">loads into memory all the data about the scheduler</a> </li><li>  <a href="">loads into memory information about what role is assigned to the node (worker, scheduler, server)</a> </li><li>  for non-schedulers - <a href="">find a free port and collect all the data about yourself</a> that will need to be sent to the scheduler (if necessary, the port can be set via the environment variable) </li><li>  <a href="">tie yourself to the found port</a> </li><li>  <a href="">start a stream that listens to incoming messages</a> </li><li>  <a href="">for non-schedulers - send a message to the scheduler with a request to add yourself to the cluster (discussed later)</a> </li><li>  <a href="">Run the stream that is responsible for sending the signal that the node is alive</a> </li></ul><br><h2 id="inicializaciya-klastera-1">  Cluster initialization </h2><br><p>  As soon as all the commands above are executed, a lot of debugging information should appear on the screen, which comes from three previously running processes at the same time.  Now we will go through each line to discuss in detail what is happening and how our diagram will look like at each stage. </p><br><blockquote>  [00:33:12] src / van.cc: 75: Bind to role = worker, ip = 1.1.1.1, port = 37350, is_recovery = 0 </blockquote><p>  This starts the worker process.  In this case, it is the <a href="">Start</a> method, which tells us that its address is 1.1.1.1, the role of "worker" and the port that it found 37350. Now it will instantly try to notify the scheduler that it is ready to be added to the cluster, specifying its address and port : </p><br><blockquote>  [00:33:12] src / van.cc: 136 :?  =&gt; 1. Meta: request = 0, timestamp = 3, control = {cmd = ADD_NODE, node = {role = worker, ip = 1.1.1.1, port = 37350, is_recovery = 0}} </blockquote><p>  This particular message is generated in the <a href="">Send</a> method, right here.  It is necessary to pay attention to several things: </p><br><ul><li>  is_recovery = 0 - reports that it is not in recovery mode, this part is beyond the scope of this article </li><li>  cmd = ADD_NODE - the team scheduler to add a worker to the cluster </li><li>  ?  =&gt; 1 - each node has its own rank.  Rank assigned by the planner.  The scheduler itself has rank 1. In our case, a node without a rank sends a message to a node with rank 1 (scheduler). </li></ul><br><p>  In our diagram, this messaging looks like this: </p><br><p><img src="https://habrastorage.org/web/fa3/7e7/2db/fa37e72dbb2340f3bae71941930b66ce.png"></p><br><p>  Go ahead </p><br><blockquote>  [00:33:13] src / van.cc: 75: Bind to role = server, ip = 2.2.2.2, port = 54160, is_recovery = 0 </blockquote><p>  This is our server awake.  I found a port (54160) and immediately the scheduler tries to notify about it: </p><br><blockquote>  [00:33:13] src / van.cc: 136 :?  =&gt; 1. Meta: request = 0, timestamp = 0, control = {cmd = ADD_NODE, node = {role = server, ip = 2.2.2.2, port = 54160, is_recovery = 0}} </blockquote><p>  On the diagram, it looks like this: </p><br><p><img src="https://habrastorage.org/web/3c6/68f/de9/3c668fde93e64469bd616c0e7d4bdec0.png"></p><br><p>  Just as in the case of a worker, our server sends the command "ADD_NODE" to be registered in the cluster.  So, as the server is not yet registered in the cluster and does not have a rank, then we see: "? =&gt; 1". </p><br><blockquote>  [00:33:13] src / van.cc: 75: Bind to role = scheduler, id = 1, ip = 127.0.0.1, port = 9000, is_recovery = 0 </blockquote><p>  Finally, the scheduler is running.  It uses the local IP and port 9000 (all nodes in the cluster should already know about its address and port).  Since the scheduler is up, it is logical to expect that at this moment he will receive all incoming messages that were sent to him and ... voila: </p><br><blockquote>  [00:33:13] src / van.cc: 161 :?  =&gt; 1. Meta: request = 0, timestamp = 0, control = {cmd = ADD_NODE, node = {role = server, ip = 2.2.2.2, port = 54160, is_recovery = 0}} </blockquote><p>  Message from the server.  This part of the logs generated by the method of the <a href="">Receive</a> , to be even more accurate <a href="">here</a> .  The scheduler immediately receives a second message, this time from the worker: </p><br><blockquote>  [00:33:13] src / van.cc: 161 :?  =&gt; 1. Meta: request = 0, timestamp = 3, control = {cmd = ADD_NODE, node = {role = worker, ip = 1.1.1.1, port = 37350, is_recovery = 0}} </blockquote><p>  First of all, the scheduler is taken to assign ranks, first working (9): </p><br><blockquote>  [00:33:13] src / van.cc: 235: assign rank = 9 to node role = worker, ip = 1.1.1.1, port = 37350, is_recovery = 0 </blockquote><p>  Now to server (8): </p><br><blockquote>  [00:33:13] src / van.cc: 235: assign rank = 8 to node role = server, ip = 2.2.2.2, port = 54160, is_recovery = 0 </blockquote><p>  After comes the rather important part: </p><br><blockquote>  [00:33:13] src / van.cc: 136 :?  =&gt; 9. Meta: request = 0, timestamp = 0, control = {cmd = ADD_NODE, node = {role = worker, id = 9, ip = 1.1.1.1, port = 37350, is_recovery = 0 role = server, id = 8, ip = 2.2.2.2, port = 54160, is_recovery = 0 role = scheduler, id = 1, ip = 127.0.0.1, port = 9000, is_recovery = 0}} </blockquote><p>  Messages like these indicate that the scheduler received the "ADD_NODE" commands from all the nodes of the cluster (in our case from the 1st worker and the 1st server) and now began to notify all the nodes back about their ranks and information about all the other nodes in the cluster.  That is, the scheduler sends ALL information about EACH cluster node to EACH cluster node. </p><br><p>  In this particular message, we see all the data about the cluster and this message is sent to the node with a rank of 9 (this is an employee).  Cluster information is vital because it is needed by the worker, for example, to understand which server to send the model update to. </p><br><p>  On the diagram, this process looks like this: </p><br><p><img src="https://habrastorage.org/web/aca/b9c/414/acab9c414e934aa5b33e01ee9057fdec.png"></p><br><p>  The following output: </p><br><blockquote>  [00:33:13] src / van.cc: 136 :?  =&gt; 8. Meta: request = 0, timestamp = 1, control = {cmd = ADD_NODE, node = {role = worker, id = 9, ip = 1.1.1.1, port = 37350, is_recovery = 0 role = server, id = 8, ip = 2.2.2.2, port = 54160, is_recovery = 0 role = scheduler, id = 1, ip = 127.0.0.1, port = 9000, is_recovery = 0}} </blockquote><p>  The scheduler sends the same confirmation to the node with a rank of 8 (server).  The diagram looks like this: </p><br><p><img src="https://habrastorage.org/web/35b/26f/105/35b26f1050ae4edbb85e862248c1b0ce.png"></p><br><blockquote>  [00:33:13] src / van.cc: 251: the scheduler is connected to 1 workers and 1 servers </blockquote><p>  Scheduler happily reported that he is connected to one worker and one server (to all the nodes of the cluster). </p><br><p>  Reminder - when running on a real cluster, all these logs are on different machines, so now it may seem that there is more information than necessary. </p><br><blockquote>  [00:33:13] src / van.cc: 161: 1 =&gt; 2147483647. Meta: request = 0, timestamp = 0, control = {cmd = ADD_NODE, node = {role = worker, id = 9, ip = 1.1.1.1, port = 37350, is_recovery = 0 role = server, id = 8, ip = 2.2.2.2, port = 54160, is_recovery = 0 role = scheduler, id = 1, ip = 127.0.0.1, port = 9000, is_recovery = 0}} <br>  [00:33:13] src / van.cc: 281: W [9] is connected to others </blockquote><p>  This worker received messages from the scheduler and reports that he is connected to the cluster.  You can ask what is "2147483647".  Answer - I have no idea =) most likely a bug, I would expect to see: "1 =&gt; 9".  So, as a worker correctly sees his rank: "W [9]", the bug is most likely somewhere in the logging process, so you can fix it and become a project contributor. </p><br><blockquote>  [00:33:13] src / van.cc: 161: 1 =&gt; 2147483647. Meta: request = 0, timestamp = 1, control = {cmd = ADD_NODE, node = {role = worker, id = 9, ip = 1.1.1.1, port = 37350, is_recovery = 0 role = server, id = 8, ip = 2.2.2.2, port = 54160, is_recovery = 0 role = scheduler, id = 1, ip = 127.0.0.1, port = 9000, is_recovery = 0}} <br>  [00:33:13] src / van.cc: 281: S [8] is connected to others </blockquote><p>  Same for the server: he received a message and was pleased to tell about this world. </p><br><blockquote>  [00:33:13] src / van.cc: 136 :?  =&gt; 1. Meta: request = 1, timestamp = 4, control = {cmd = BARRIER, barrier_group = 7} <br>  [00:33:13] src / van.cc: 136 :?  =&gt; 1. Meta: request = 1, timestamp = 2, control = {cmd = BARRIER, barrier_group = 7} <br>  [00:33:13] src / van.cc: 136 :?  =&gt; 1. Meta: request = 1, timestamp = 1, control = {cmd = BARRIER, barrier_group = 7} </blockquote><p>  Another important part.  So far, we have only seen one "ADD_NODE" command.  Here we are seeing a new one: "BARRIER".  In short, this concept of barriers, which I hope the reader is familiar with in multithreaded programming, means: "stop until everyone reaches this barrier."  The scheduler is responsible for letting you know exactly when everyone has reached the barrier and can continue execution.  The first barrier is located immediately after the cluster has started, but before the start of training.  All three nodes (including the scheduler itself) sent messages, which essentially means: "I reached the barrier, let me know when you can move on." </p><br><p>  Also, as can be seen from the message, there is a concept of a barrier group (barrier_group).  Barrier group is a group of nodes that participate in this or that barrier.  These groups are: </p><br><p>  1 - scheduler <br>  2 - servers <br>  4 - workers </p><br><p>  As it is not difficult to guess, this is a power of two, so our group of 7 is: 4 + 2 + 1. In essence, this barrier applies to everyone. </p><br><p>  Well, by itself, since in our logs, we saw three sending messages, it is logical to expect three lines about the receipt of these messages by the scheduler: </p><br><blockquote>  [00:33:13] src / van.cc: 161: 1 =&gt; 1. Meta: request = 1, timestamp = 2, control = {cmd = BARRIER, barrier_group = 7} <br>  [00:33:13] src / van.cc: 291: Barrier count for 7: 1 <br>  [00:33:13] src / van.cc: 161: 9 =&gt; 1. Meta: request = 1, timestamp = 4, control = {cmd = BARRIER, barrier_group = 7} <br>  [00:33:13] src / van.cc: 291: Barrier count for 7: 2 <br>  [00:33:13] src / van.cc: 161: 8 =&gt; 1. Meta: request = 1, timestamp = 1, control = {cmd = BARRIER, barrier_group = 7} <br>  [00:33:13] src / van.cc: 291: Barrier count for 7: 3 </blockquote><p>  What happens in our diagram looks like this: </p><br><p><img src="https://habrastorage.org/web/227/8b3/697/2278b3697f50447ca7e52e5216256467.png"></p><br><p>  Now it's time to discuss what the planner is doing when it receives a new message that the node has reached a barrier in a particular group: </p><br><ul><li>  it increases the count of the number of nodes that sent the BARRIER command in a particular group ( <a href="">here</a> ) </li><li>  when the counter is equal to the number of nodes in the group, it sends confirmation to all that it is possible to continue normal work </li></ul><br><p>  In the logs above you can see how the counter increased as each new message was received.  Well, at the moment when it reached the expected size (3), the scheduler began to send confirmations: </p><br><blockquote>  [00:33:13] src / van.cc: 136 :?  =&gt; 9. Meta: request = 0, timestamp = 3, control = {cmd = BARRIER, barrier_group = 0} <br>  [00:33:13] src / van.cc: 136 :?  =&gt; 8. Meta: request = 0, timestamp = 4, control = {cmd = BARRIER, barrier_group = 0} <br>  [00:33:13] src / van.cc: 136 :?  =&gt; 1. Meta: request = 0, timestamp = 5, control = {cmd = BARRIER, barrier_group = 0} </blockquote><p>  In our diagram, it looks like this: </p><br><p><img src="https://habrastorage.org/web/9fb/453/64d/9fb45364d3ce4b61813a262ef2c77c3e.png"></p><br><p>  As you can see, the scheduler even sends confirmation to itself.  Well, of course, since the message was sent from the scheduler (as many as 3), then we should see <a href="">logs that these messages were received</a> : </p><br><blockquote>  [00:33:13] src / van.cc: 161: 1 =&gt; 9. Meta: request = 0, timestamp = 3, control = {cmd = BARRIER, barrier_group = 0} <br>  [00:33:13] src / van.cc: 161: 1 =&gt; 8. Meta: request = 0, timestamp = 4, control = {cmd = BARRIER, barrier_group = 0} <br>  [00:33:13] src / van.cc: 161: 1 =&gt; 1. Meta: request = 0, timestamp = 5, control = {cmd = BARRIER, barrier_group = 0} </blockquote><p>  Well, the last touch.  At the moment, the planner has reached the second barrier, which will be reached by all the nodes after graduation, however, since the planner does not take part in the training, he has already reached this barrier.  So he sends the barrier_group = 7 to the group that he has reached the barrier, with instant acknowledgment of receipt of the message and the installation of the 7-by-1 barrier group counter. </p><br><blockquote>  [00:33:13] src / van.cc: 136 :?  =&gt; 1. Meta: request = 1, timestamp = 6, control = {cmd = BARRIER, barrier_group = 7} <br>  [00:33:13] src / van.cc: 161: 1 =&gt; 1. Meta: request = 1, timestamp = 6, control = {cmd = BARRIER, barrier_group = 7} <br>  [00:33:13] src / van.cc: 291: Barrier count for 7: 1 </blockquote><p>  At this stage, the cluster is initialized, you can start learning ... </p><br><h2 id="obuchenie">  Training </h2><br><p>  Having executed all the code, we have an initialized KVstore.  What now?  Let's use it for direct learning.  I will use a very simple example of a linear regressor taken <a href="http://mxnet.io/tutorials/python/linear-regression.html">from here</a> .  Just ask, before continuing, follow the example to understand what is happening.  In order to do the training in the described example distributed, you need to change only 1 line in the code.  Instead: </p><br><pre> <code class="python hljs">model.fit(train_iter, eval_iter, optimizer_params={ <span class="hljs-string"><span class="hljs-string">'learning_rate'</span></span>:<span class="hljs-number"><span class="hljs-number">0.005</span></span>, <span class="hljs-string"><span class="hljs-string">'momentum'</span></span>: <span class="hljs-number"><span class="hljs-number">0.9</span></span>}, num_epoch=<span class="hljs-number"><span class="hljs-number">50</span></span>, eval_metric=<span class="hljs-string"><span class="hljs-string">'mse'</span></span>, batch_end_callback = mx.callback.Speedometer(batch_size, <span class="hljs-number"><span class="hljs-number">2</span></span>))</code> </pre> <br><p>  You need to write: </p><br><pre> <code class="python hljs">model.fit(train_iter, eval_iter, optimizer_params={ <span class="hljs-string"><span class="hljs-string">'learning_rate'</span></span>:<span class="hljs-number"><span class="hljs-number">0.005</span></span>, <span class="hljs-string"><span class="hljs-string">'momentum'</span></span>: <span class="hljs-number"><span class="hljs-number">0.9</span></span>}, num_epoch=<span class="hljs-number"><span class="hljs-number">50</span></span>, eval_metric=<span class="hljs-string"><span class="hljs-string">'mse'</span></span>, batch_end_callback = mx.callback.Speedometer(batch_size, <span class="hljs-number"><span class="hljs-number">2</span></span>), kvstore=kv_store) <span class="hljs-comment"><span class="hljs-comment"># updated line</span></span></code> </pre> <br><p>  So simple?  in short, yes. </p><br><h2 id="nebolshoe-zaklyuchenie">  Small conclusion </h2><br><p>  I hope the reader now has a more detailed understanding of what is happening with the MXNet cluster at the time of its launch.  Also, I hope this article will help with debugging the cluster in case of any problems.  Well, plus, with this knowledge, you can draw some conclusions about the characteristics of the network for the cluster, namely: </p><br><ul><li>  it‚Äôs not critical for the scheduler to quickly connect to the rest </li><li>  servers are not critical to have a fast connection between them </li><li>  every worker should have a fast connection with every server </li><li>  workers are not critical to have a quick connection with each other </li></ul><br><p>  <a href="https://blog.kovalevskyi.com/mxnet-distributed-training-explained-in-depth-part-1-b90c84bda725">I would be very grateful for the recommendations of the original article on Medium</a> .  Also, if all of a sudden you are building AWS-based distributed machine learning systems using MXNet and you have any questions, I‚Äôm glad to help and answer (viacheslav@kovalevskyi.com). </p><br><h2 id="ssylki">  References: </h2><br><ul><li>  <a href="https://github.com/blog-kovalevskyi-com/mxnet_distributed_lr_model_training">full version of the code from the article</a> </li><li>  <a href="http://draw.io/">Draw.IO</a> , a drawing service for <a href="http://draw.io/">diagrams</a> </li><li>  <a href="https://github.com/dmlc/ps-lite">MXNet ps-lite repository</a> </li></ul></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/334968/">https://habr.com/ru/post/334968/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../334956/index.html">How to start young mobile game developers from Russia in the current realities</a></li>
<li><a href="../334958/index.html">The digest of interesting materials for the mobile developer # 215 (July 31 - August 5)</a></li>
<li><a href="../334960/index.html">OpenDataScience and Mail.Ru Group will conduct an open machine learning course</a></li>
<li><a href="../334964/index.html">Javascript as thoughtvirus</a></li>
<li><a href="../334966/index.html">Hacking Wi-Fi</a></li>
<li><a href="../334970/index.html">Create a mass of asynchronous requests using Grequests</a></li>
<li><a href="../334972/index.html">Where is the logic? Learn to think systemically. Part 3</a></li>
<li><a href="../334974/index.html">ICO: legalization of funds received. Part I</a></li>
<li><a href="../334976/index.html">Rotate 180. From CRM systems to game dev # 1</a></li>
<li><a href="../334978/index.html">Shops / developers / buyers / publishers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>