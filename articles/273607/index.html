<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Data Driven Realtime Rule Engine in Wargaming: Data Collection</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The field of activity of our company extends far beyond the game development. In parallel with it, we have dozens of internal projects, and the Data D...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Data Driven Realtime Rule Engine in Wargaming: Data Collection</h1><div class="post__text post__text-html js-mediator-article">  The field of activity of our company extends far beyond the game development.  In parallel with it, we have dozens of internal projects, and the Data Driven Realtime Rule Engine (DDRRE) is one of the most ambitious ones. <br><br>  Data Driven Realtime Rule Engine is a special system that, by analyzing large amounts of data in real time, allows you to personalize the interaction with the player through the recommendations received by the user based on the context of his latest gaming experience. <br><br>  DDRRE allows our players to get more pleasure from the game, improves their user experience, and also eliminates the viewing of unnecessary promotional and promotional messages. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>DDRRE architecture</b> <br><img src="https://habrastorage.org/files/5ac/6dc/987/5ac6dc9870d04351b395b2c5449b4126.jpg"><br><a name="habracut"></a><br>  Data Driven Realtime Rule Engine can be divided into several components: RAW Data Collection, WG HUB and Business Rule Engine.  Their architecture can be seen in the diagram. <br>  In this article we will talk about adapters for data collection and analysis, and in the following publications we will look at other components of the system in detail. <br><hr><br>  Data collection is carried out using a common bus, which is used as Kafka.  All game subsystems in real time write logs of the set format to the bus.  For subsystems that, due to technical limitations, cannot do this, we wrote adapters that collect and redirect logs to Kafka.  In particular, our stack contains adapters for MySQL, PSQL, RabbitMQ, as well as an adapter for loading archived data from DWH, via the Hive JDBC interface.  Each of them exports metrics about processing speed and lagging behind the source to JMX, where Grafana is used for data visualization, and Zabbix is ‚Äã‚Äãused to report problems.  All adapters are designed as standalone Java applications for Java 8 and Scala. <br><br>  <b>Adapter for MySQL, psql</b> <br>  It is based on the Tungsten replicator, to which the producer is written in Kafka.  We use replication as it is a reliable way to get data without additional load on the database server of the data source. <br><br>  The current pipeline in Tungsten is as follows: <br><br><blockquote>  replicator.pipelines = slave <br>  replicator.pipeline.slave = d-binlog-to-q, q-to-kafka <br>  replicator.pipeline.slave.stores = parallel-queue <br>  replicator.pipeline.slave.services = datasource <br>  replicator.pipeline.slave.syncTHLWithExtractor = false <br><br>  replicator.stage.d-binlog-to-q = com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask <br>  replicator.stage.d-binlog-to-q.extractor = dbms <br>  replicator.stage.d-binlog-to-q.applier = parallel-q-applier <br>  replicator.stage.d-binlog-to-q.filters = replicate, colnames, schemachange <br>  replicator.stage.d-binlog-to-q.blockCommitRowCount = $ {replicator.global.buffer.size} <br><br>  replicator.stage.q-to-kafka = com.continuent.tungsten.replicator.pipeline.SingleThreadStageTask <br>  replicator.stage.q-to-kafka.extractor = parallel-q-extractor <br>  replicator.stage.q-to-kafka.applier = asynckafka <br>  replicator.stage.q-to-kafka.taskCount = $ {replicator.global.apply.channels} <br>  replicator.stage.q-to-kafka.blockCommitRowCount = $ {replicator.global.buffer.size} </blockquote><br><br>  where the asynckafka module is written by us. <br><br>  Asynckafka receives data from the previous stage and writes it to Kafka.  The last recorded offset is stored in the zookeeper, because it is always there with Kafka.  As an option, tungsten can save data to a file or MySQL, but this is not very reliable in case of loss of the host with the adapter.  In our case, with the crash, the module reads the offset, and binlog processing continues from the last value saved in Kafka. <br><br>  <i>Record in Kafka</i> <br><br><pre><code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">commit</span></span></span></span>(): <span class="hljs-type"><span class="hljs-type">Unit</span></span> = { <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scala.collection.<span class="hljs-type"><span class="hljs-type">JavaConversions</span></span>._ <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> msgs : java.util.concurrent.<span class="hljs-type"><span class="hljs-type">ConcurrentLinkedQueue</span></span>[(<span class="hljs-type"><span class="hljs-type">String</span></span>,<span class="hljs-type"><span class="hljs-type">String</span></span>,<span class="hljs-type"><span class="hljs-type">String</span></span>,<span class="hljs-type"><span class="hljs-type">Option</span></span>[<span class="hljs-type"><span class="hljs-type">Callback</span></span>])] = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> java.util.concurrent.<span class="hljs-type"><span class="hljs-type">ConcurrentLinkedQueue</span></span>[(<span class="hljs-type"><span class="hljs-type">String</span></span>,<span class="hljs-type"><span class="hljs-type">String</span></span>,<span class="hljs-type"><span class="hljs-type">String</span></span>,<span class="hljs-type"><span class="hljs-type">Option</span></span>[<span class="hljs-type"><span class="hljs-type">Callback</span></span>])]() data.foreach(e =&gt; { msgs.addAll(ruleProcessor.get.processToMsg(e._1, e._2).map(e =&gt; (e._1, e._2, e._3, <span class="hljs-type"><span class="hljs-type">None</span></span>))) }) kafkaSender.get.send(msgs.toSeq:_*) } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> kpe: <span class="hljs-type"><span class="hljs-type">KafkaProducerException</span></span> =&gt; { logger.error(kpe.getMessage, kpe) <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">ReplicatorException</span></span>(kpe); } } lastHeader.map(saveLastHeader(_)) resetEventsToSend() }</code> </pre> <br>  <i>Saving offset</i> <br><br><pre> <code class="scala hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">saveLastHeader</span></span></span></span>(header: <span class="hljs-type"><span class="hljs-type">ReplDBMSHeader</span></span>): <span class="hljs-type"><span class="hljs-type">Unit</span></span> = { zkCurator.map { zk =&gt; <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> dhd = <span class="hljs-type"><span class="hljs-type">DbmsHeaderData</span></span>( header.getSeqno, header.getFragno, header.getLastFrag, header.getSourceId, header.getEpochNumber, header.getEventId, header.getShardId, header.getExtractedTstamp.getTime, header.getAppliedLatency, <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (<span class="hljs-literal"><span class="hljs-literal">null</span></span> == header.getUpdateTstamp) { <span class="hljs-number"><span class="hljs-number">0</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { header.getUpdateTstamp.getTime }, <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (<span class="hljs-literal"><span class="hljs-literal">null</span></span> == header.getTaskId) { <span class="hljs-number"><span class="hljs-number">0</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { header.getTaskId }) logger.info(<span class="hljs-string"><span class="hljs-string">"{}"</span></span>, writePretty(dhd)) zk.setData().forPath(getZkDirectoryPath(context), writePretty(dhd).getBytes(<span class="hljs-string"><span class="hljs-string">"utf8"</span></span>)) } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> t: <span class="hljs-type"><span class="hljs-type">Throwable</span></span> =&gt; logger.error(<span class="hljs-string"><span class="hljs-string">"error while safe last header to zk"</span></span>, t) } } }</code> </pre><br><br>  <i>Recovery offset</i> <br><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getLastEvent</span></span></span></span>: <span class="hljs-type"><span class="hljs-type">ReplDBMSHeader</span></span> = { lastHeader.getOrElse { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> result = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">ReplDBMSHeaderData</span></span>(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-literal"><span class="hljs-literal">false</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Timestamp</span></span>(<span class="hljs-type"><span class="hljs-type">System</span></span>.currentTimeMillis()), <span class="hljs-number"><span class="hljs-number">0</span></span>) zkCurator.map { zk =&gt; <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> json = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">String</span></span>(zk.getData().forPath(getZkDirectoryPath(context)), <span class="hljs-string"><span class="hljs-string">"utf8"</span></span>) logger.info(<span class="hljs-string"><span class="hljs-string">"found previous header {}"</span></span>, json) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> headerDto = read[<span class="hljs-type"><span class="hljs-type">DbmsHeaderData</span></span>](json) result = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">ReplDBMSHeaderData</span></span>(headerDto.seqno, headerDto.fragno, headerDto.lastFrag, headerDto.sourceId, headerDto.epochNumber, headerDto.eventId, headerDto.shardId, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Timestamp</span></span>(headerDto.extractedTstamp), headerDto.appliedLatency, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Timestamp</span></span>(headerDto.updateTstamp), headerDto.taskId) } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> t: <span class="hljs-type"><span class="hljs-type">Throwable</span></span> =&gt; logger.error(<span class="hljs-string"><span class="hljs-string">"error while safe last header to zk"</span></span>, t) } } result } }</code> , headerDto.lastFrag, headerDto.sourceId, headerDto.epochNumber, headerDto.eventId, headerDto.shardId, new Timestamp (headerDto.extractedTstamp), headerDto.appliedLatency, new Timestamp (headerDto. <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getLastEvent</span></span></span></span>: <span class="hljs-type"><span class="hljs-type">ReplDBMSHeader</span></span> = { lastHeader.getOrElse { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> result = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">ReplDBMSHeaderData</span></span>(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-literal"><span class="hljs-literal">false</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-string"><span class="hljs-string">""</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Timestamp</span></span>(<span class="hljs-type"><span class="hljs-type">System</span></span>.currentTimeMillis()), <span class="hljs-number"><span class="hljs-number">0</span></span>) zkCurator.map { zk =&gt; <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> json = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">String</span></span>(zk.getData().forPath(getZkDirectoryPath(context)), <span class="hljs-string"><span class="hljs-string">"utf8"</span></span>) logger.info(<span class="hljs-string"><span class="hljs-string">"found previous header {}"</span></span>, json) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> headerDto = read[<span class="hljs-type"><span class="hljs-type">DbmsHeaderData</span></span>](json) result = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">ReplDBMSHeaderData</span></span>(headerDto.seqno, headerDto.fragno, headerDto.lastFrag, headerDto.sourceId, headerDto.epochNumber, headerDto.eventId, headerDto.shardId, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Timestamp</span></span>(headerDto.extractedTstamp), headerDto.appliedLatency, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Timestamp</span></span>(headerDto.updateTstamp), headerDto.taskId) } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> t: <span class="hljs-type"><span class="hljs-type">Throwable</span></span> =&gt; logger.error(<span class="hljs-string"><span class="hljs-string">"error while safe last header to zk"</span></span>, t) } } result } }</code> </pre><br><br>  <b>Adapter for RabbitMQ</b> <br>  A fairly simple adapter that shifts data from one queue to another.  Records are transferred one by one to Kafka, after which they are acknowledged in RabbitMQ.  The service delivers the message guaranteed at least once, deduplication occurs on the data side. <br><pre> <code class="java hljs"> RabbitMQConsumerCallback callback = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> RabbitMQConsumerCallback() { <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">apply</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">byte</span></span></span></span><span class="hljs-function"><span class="hljs-params">[] body)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// callback-     RabbitMQ String routingKey = envelope.getRoutingKey(); Tuple3&lt;String, String, String&gt; routingExpr = routingExprMap.get(routingKey); //  topic    Kafka       routingKey if (routingExpr == null) throw new RuntimeException("No mapping for routing key " + routingKey); String expr = routingExpr._1(), topic = Objects.firstNonNull(routingExpr._2(), kafkaProducerMainTopic), sourceDoc = routingExpr._3(); Object data = rabbitMQConsumerSerializer.deserialize(body); //   ,     RabbitMQMessageEnvelope msgEnvelope = new RabbitMQMessageEnvelope(envelope, properties, data, sourceDoc); //        byte[] key = getValueByExpression(data, expr).getBytes(); byte[] msg = kafkaProducerSerializer.serialize(msgEnvelope); kafkaProducer.addMsg(topic, key, msg, envelope.getDeliveryTag()); //    Kafka try { checkForSendBatch(); } catch (IOException e) { this.errBack(e); } } @Override public void errBack(Exception e) { logger.error("{}", e.fillInStackTrace()); close(); }</span></span></code> </pre><br><br>  <b>Adapter for DWH</b> <br>  When it is necessary to process historical data, we turn to DWH.  The storage is based on Hadoop technology, so we use Hive or Impala for data acquisition.  To make the loading interface more universal, we implemented it via JDBC.  The main problem of working with DWH is that the data in it is normalized, and to collect the entire document, it is necessary to combine several tables. <br><br>  What we have at the entrance: <br>  ‚Ä¢ data of the required tables is partitioned by date <br>  ‚Ä¢ the period for which we want to load data is known <br>  ‚Ä¢ the document grouping key is known for each table. <br><br>  To group tables: <br>  ‚Ä¢ use Spark SQL Data Frame <br>  ‚Ä¢ we integrate by a cycle on dates from the set range <br>  ‚Ä¢ we merge several DataFrame by grouping key into one document and write to Kafka using Spark. <br><br>  An example of setting a datasource using the property file. <br><pre> <code class="scala hljs">hdfs_kafka.dataframe.df1.uri=<span class="hljs-string"><span class="hljs-string">"jdbc:hive2://[HiveUri]:10000/test;user=hdfs"</span></span> <span class="hljs-comment"><span class="hljs-comment">// jbdc uri hdfs_kafka.dataframe.df1.sql=select * from test.log_arenas_p1_v1 where dt='%s' hdfs_kafka.dataframe.df1.keyField=arena_id // SQL-  '%s'  hdfs_kafka.dataframe.df1.outKeyField=arena_id // ,       . hdfs_kafka.dataframe.df1.tableName=test.log_arenas_p1_v hdfs_kafka.dataframe.df2.uri="jdbc:hive2://[HiveUri]:10000/test;user=hdfs" hdfs_kafka.dataframe.df2.sql=select * from test.log_arenas_members where dt='%s' hdfs_kafka.dataframe.df2.keyField=arena_id hdfs_kafka.dataframe.df2.outKeyField=arena_id // ,       Kafka hdfs_kafka.dataframe.df2.tableName=test.log_arenas_members_p1_v //  ,    </span></span></code> </pre><br><br>  In this example, we are building two DataFrame. <br><br>  The application counts the number of days between the specified dates and performs a cycle from the configuration file: <br>  hdfs_kafka.from = 2015-06-25 <br>  hdfs_kafka.to = 2015-06-26 <br><br><pre> <code class="java hljs">val dates = Utils.getRange(configuration.dateFormat, configuration.from, configuration.to) <span class="hljs-comment"><span class="hljs-comment">//   ,    sql     dates.map( date =&gt; { //    val dataFrames = configuration.dataframes.map( dfconf =&gt; { val df = executeJdbc(sqlContext, Utils.makeQuery(dfconf.sql, date), dfconf.uri) (dfconf, df) }) val keysExtracted = dataFrames.map( e =&gt; { //   DataFrame dataFrameProcessor.extractKey(e._2.rdd, e._1.keyField, e._1.tableName) }) //   RDD[Key, Row]  keyBy   keyField   val grouped = keysExtracted.reduce(_.union(_)).map( e =&gt; (e._1, Seq(e._2))) //   dataFrame   grouped.reduceByKey(_ ++ _) //  Row   dataFrameProcessor.applySeq(grouped) }) //    </span></span></code> </pre><br><hr><br>  We will describe how the collected information is processed, as well as other components of DDRRE, in the next post.  If you have any questions about the described technologies - be sure to ask them in the comments. </div><p>Source: <a href="https://habr.com/ru/post/273607/">https://habr.com/ru/post/273607/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../273597/index.html">"Countdown by simple means - or the rule" 3-2-1 "for disk storages"</a></li>
<li><a href="../273599/index.html">Capital Projects: from idea to specifications and drawings</a></li>
<li><a href="../273601/index.html">Stephen Wolfram‚Äôs Book An Elementary Introduction to the Wolfram Language</a></li>
<li><a href="../273603/index.html">Results of 2015: mobile solutions market</a></li>
<li><a href="../273605/index.html">Recipes from CHEF: automated deployment of business application environments using HPE OneView</a></li>
<li><a href="../273611/index.html">Scalding: a reason to switch from Java to Scala</a></li>
<li><a href="../273613/index.html">Price of using frameworks</a></li>
<li><a href="../273615/index.html">Developing applications on Yii2 without experience - a direct path to AD</a></li>
<li><a href="../273617/index.html">Remote registration of SPD (FOP) in Ukraine - 2.0</a></li>
<li><a href="../273619/index.html">Powershell String Optimization</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>