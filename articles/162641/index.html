<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Multithreading and Task Analysis in Intel¬Æ VTune ‚Ñ¢ Amplifier XE 2013</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="One of the methods for improving the efficiency of parallelization of algorithms of a certain class is the pipelining of execution phases, both sequen...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Multithreading and Task Analysis in Intel¬Æ VTune ‚Ñ¢ Amplifier XE 2013</h1><div class="post__text post__text-html js-mediator-article"><img align="left" src="https://habrastorage.org/storage2/e48/f61/ce7/e48f61ce799f4de09b837f1f4a456215.jpg" alt="image">  One of the methods for improving the efficiency of parallelization of algorithms of a certain class is the pipelining of execution phases, both sequential and parallel.  The Intel TBB Library can help reduce the effort and time required to implement pipelined algorithms, taking care of task management and load distribution between threads in the system.  However, the formulation and formation of tasks that make up the phases of an algorithm can be a non-trivial problem, depending on the complexity of the algorithm, which often happens in real-world applications.  It may be even more difficult to control the execution of tasks if the algorithm itself does not contain means for control.  The toolkit for analyzing computational tasks in Intel VTune Amplifier helps developers present the structure of execution in a multi-threaded environment in a convenient graphical form, increasing the efficiency of analysis and significantly reducing the time to develop applications.  In this article, we will look at a simple example of a pipelined task, and step by step we will parallelize it using the TBB pipeline class, analyze it using the VTune Amplifier, and improve the performance of the implementation based on the analysis results. <br><a name="habracut"></a><br><h2>  Introduction </h2><br>  As is known, a computational task in general can be divided into subtasks (decomposition), which are performed in parallel by different computing devices.  Depending on the type of task, or input data, decomposition can be applied to the data, or to the task itself.  Simply put, data decomposition can be considered as separating the input dataset for processing by the same algorithm, but in different calculators.  Decomposition into tasks is, on the contrary, several different algorithms executed by different devices, either on the same data or on different parts of them.  The main purpose of decomposition for computing devices is to ensure that all available devices are as busy as possible all the time while the task is being calculated.  This achieves maximum performance for the existing set of devices when performing this task, which means reducing the time to complete it.  In this regard, the ability to analyze the implementation of both each part of the task and the task as a whole in the computing system is of great importance for the optimization of the algorithm.  In this article, we consider the analysis of computational tasks parallelized using the Intel TBB library using the new version of the VTune Amplifier XE 2013 profiler. <br><br>  Immediately I will ask to excuse me for the abundance of English terms without translation.  This was done intentionally, because using the tools you still have to use them, and attempts to translate them into Russian will only confuse the reader.  (UPD: I was told that the untranslated text in the pictures hurts my eyes - I work on the translation and will update the pictures soon) <br><br>  The introduction does not accidentally contain an abundance of generalizing expressions, such as a computer system, a data set, etc., because we start with a generalized example that has little to do with real-world tasks, and then gradually move on to a living example, which we analyze .  Let's consider a spherical algorithm that runs in a vacuum, but it strongly resembles the real task of obtaining data from any source, performing some sorting procedures on this data, and writing the processed data to the receiver (Fig. 1).  This structure is quite typical for many applications, among which, codecs, filters, or communication protocols most quickly come to mind. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/storage2/24b/17b/ede/24b17bedebd8f99fad695bd06b46dbfd.jpg"><br>  Pic1 <br><br>  To maintain the depth of the vacuum and simplify the example, we will not take into account the possible relationship between the parts of the data coming from the source.  Although this is not correct, as, for example, when processing video frames in a codec, the dependence of neighboring pixels is taken into account by the algorithm.  However, in the general case, one can always isolate a sufficiently large data array, which can be considered independent of other arrays.  At the same time, it is quite safe to apply decomposition to execute a computational algorithm in parallel over different parts of the data, thereby distorting the sphericity of the algorithm and making something similar to the scheme shown in Fig. 2: the data are sequentially read from the source, processed in parallel blocks (P) , and then again sequentially recorded (W) at the receiver. <br><br><img src="https://habrastorage.org/storage2/bf5/e24/631/bf5e24631a09791b83869cb38652b3a3.jpg"><br>  Pic2 <br><br>  If the data source is a serial device, such as a disk, or a network interface, then the process of reading and writing will also be sequential.  And even if you have free computing threads in the system, the only stage of execution that can be parallelized is the execution phase of the algorithm.  According to <a href="http://ru.wikipedia.org/wiki/%25D0%2597%25D0%25B0%25D0%25BA%25D0%25BE%25D0%25BD_%25D0%2590%25D0%25BC%25D0%25B4%25D0%25B0%25D0%25BB%25D0%25B0">Amdal's law</a> , the increase in the performance of the entire task due to parallelization will be limited to these successive phases. <br><br>  Trying to overcome this limitation, we can redistribute parts of the task in such a way as to ensure that all executive threads are constantly busy with its execution.  This is possible if parts of the data are independent and their processing order is not important, or can be restored at the output.  To do this, we build a simple pipeline that consists of the same Read / Process / Write stages performed by each thread (Figure 3).  The Read and Write stages are still consistent, but they are distributed between the threads.  It should be borne in mind that no thread can begin the stage of reading or writing, while any other thread performs the same stage.  However, two different threads can quite simultaneously perform both reading and writing different data. <br><br><img src="https://habrastorage.org/storage2/551/efc/8ae/551efc8ae41d12d27a765c7fe1992450.jpg"><br>  Pic.3 <br><br>  Having restructured the algorithm in such a way, formally we used decomposition by tasks, that is, different phases of the task were performed by different threads simultaneously.  At the same time, dividing data into parts, and performing the same operations on them in different streams, we use data decomposition.  Such a combined approach allows us to ensure that the threads are constantly busy working, thereby maximizing the efficiency of the execution of the entire task.  However, the sphericity of the algorithm still remains the same, since we do not take into account possible delays in the read / write devices, and also assume that there is a lot of work for computational flows to smooth out irregularities in the data input, and computing devices are always available for execution algorithm. <br><br>  In real life, things can be more complicated.  For example, no one guarantees a stable value of the time spent by input / output devices for data transfer, or that the processors are free at the moment, or that the data is homogeneous and require the same time to be processed by the same algorithm.  Most often, the input data may arrive with a delay, due to the peculiarities of the source.  The processor may be busy with other tasks with a higher priority.  Some data may require more processing time, for example, frames with fine details are processed much longer by a compression algorithm than frames with a uniform background.  Recording operations are usually not a problem, since in most systems they are buffered and physically recording data to the receiver can be delayed until later.  Although this only works if the output is not used again at the input.  In general, moving from a spherical-vacuum model to real implementations of the problem in the existing system, all conceived efficiency of the conveyor may come to naught, and our complex scheme will remain as ineffective as was the original and simple (Figure 4). <br><br><img src="https://habrastorage.org/storage2/3e7/1ca/432/3e71ca432444d19f77517fdc1bccac64.jpg"><br>  Pic.4 <br><br>  To eliminate the inefficiencies that have arisen due to load imbalances, we can try to dynamically distribute the work performed by each thread.  This will require monitoring the employment status of the streams, managing the size of the data processed by the algorithm, and distributing the subtasks among the free streams.  The implementation of such an infrastructure is possible with small efforts, but only for simple cases.  In general, such an infrastructure can be quite complex to develop from scratch.  And then it's time to advertise the capabilities of the Intel TBB library, which already contains a mechanism for dynamic load balancing between threads.  And for our example, TBB contains a built-in pipeline algorithm called the <i>pipeline class</i> , suitable for use in our task. <br><br><h2>  Example </h2><br>  We will not go into much detail about the implementation of the pipeline algorithm in TBB, since all the information, including open source, can be found at <a href="http://threadingbuildingblocks.org/">threadingbuildingblocks.org</a> .  Below, we will simply take an example of our task, how to create a pipeline using TBB.  A sample of the code is in the library directory or on the website. <br><br>  In a nutshell, a pipeline is the application of a sequence of execution units to a stream of objects.  Executive units can implement stages of the task that perform certain actions on the data stream.  In the TBB library, these stages can be defined as instances of the <i>filter</i> class.  Thus, the pipeline is built as a sequence of filters.  Some stages (such as Processing) can work in parallel in several streams with different data, so they must be defined as <i>parallel filter</i> Class.  The remaining stages, such as Read and Write, must be performed sequentially and in a specific order (if the order change is not provided), and we define them as the <i>serial_in_order</i> filter Class.  The library contains abstract classes for these types, and we need to inherit our classes from them.  Below is an example that, with certain simplifications, shows how to do this. <br><table><tbody><tr><td><pre><code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">class</span></span> MyReadFilter: <span class="hljs-built_in"><span class="hljs-built_in">public</span></span> tbb::<span class="hljs-keyword"><span class="hljs-keyword">filter</span></span> { FILE* input_file; DataItem* next_item; <span class="hljs-comment"><span class="hljs-comment">/*override*/</span></span> <span class="hljs-type"><span class="hljs-type">void</span></span>* <span class="hljs-keyword"><span class="hljs-keyword">operator</span></span>()(<span class="hljs-type"><span class="hljs-type">void</span></span>*); <span class="hljs-built_in"><span class="hljs-built_in">public</span></span>: MyReadFilter( FILE* <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ); };</code> </pre> </td><td><pre> <code class="hljs swift"><span class="hljs-type"><span class="hljs-type">MyReadFilter</span></span>:: <span class="hljs-type"><span class="hljs-type">MyReadFilter</span></span>( <span class="hljs-type"><span class="hljs-type">FILE</span></span>* <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ) : <span class="hljs-built_in"><span class="hljs-built_in">filter</span></span>(serial_in_order), input_file(<span class="hljs-keyword"><span class="hljs-keyword">in</span></span>), next_item(<span class="hljs-type"><span class="hljs-type">DataItem</span></span>*) { }</code> </pre></td></tr><tr><td><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">class</span></span> MyWriteFilter: <span class="hljs-built_in"><span class="hljs-built_in">public</span></span> tbb::<span class="hljs-keyword"><span class="hljs-keyword">filter</span></span> { FILE* output_file; <span class="hljs-comment"><span class="hljs-comment">/*override*/</span></span> <span class="hljs-type"><span class="hljs-type">void</span></span>* <span class="hljs-keyword"><span class="hljs-keyword">operator</span></span>()(<span class="hljs-type"><span class="hljs-type">void</span></span>*); <span class="hljs-built_in"><span class="hljs-built_in">public</span></span>: MyWriteFilter( FILE* <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> ); };</code> </pre></td><td><pre> <code class="hljs pgsql">MyWriteFilter:: MyWriteFilter( FILE* <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> ) : <span class="hljs-keyword"><span class="hljs-keyword">filter</span></span>(serial_in_order), output_file(<span class="hljs-keyword"><span class="hljs-keyword">out</span></span>), { }</code> </pre></td></tr></tbody></table><br>  In our example, the data is contained in a file, so you need to define a private member of the class for the file pointer.  Similarly, we define the MyWriteFilter class for the write stage, and store the pointer to the output file.  These classes will be responsible for allocating memory for the data and passing objects along the pipeline.  The main work is performed in the <i>operator ()</i> method defined in the base class.  We only need to override these methods by implementing accordingly reading data from the input file to the container and writing data from the container to the output file. <br><table><tbody><tr><td><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">void</span></span>* MyReadFilter::<span class="hljs-keyword"><span class="hljs-keyword">operator</span></span>()(<span class="hljs-type"><span class="hljs-type">void</span></span>*) { // ALLOCATEMEMORY // <span class="hljs-keyword"><span class="hljs-keyword">READ</span></span> A DATA ITEM <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> FILE // PUT DATA ITEM <span class="hljs-keyword"><span class="hljs-keyword">TO</span></span> CONTAINER }</code> </pre></td><td><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">void</span></span>* MyWriteFilter::<span class="hljs-keyword"><span class="hljs-keyword">operator</span></span>()(<span class="hljs-type"><span class="hljs-type">void</span></span>*) { // <span class="hljs-keyword"><span class="hljs-keyword">GET</span></span> DATA ITEM <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> CONTAINER // <span class="hljs-keyword"><span class="hljs-keyword">WRITE</span></span> THE DATA ITEM <span class="hljs-keyword"><span class="hljs-keyword">TO</span></span> FILE // <span class="hljs-keyword"><span class="hljs-keyword">DEALLOCATE</span></span> MEMORY }</code> </pre></td></tr></tbody></table><br>  Our Process stage can run in parallel in streams, therefore we define it as a <i>parallel filter</i> class, and the <i>operator ()</i> method should contain the ‚Äúmeat‚Äù of the data stream processing algorithm. <br><table><tbody><tr><td><pre> <code class="hljs cpp"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MyProcessFilter</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> tbb::filter { <span class="hljs-keyword"><span class="hljs-keyword">public</span></span>: MyProcessFilter(); <span class="hljs-comment"><span class="hljs-comment">/*override*/</span></span><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function">* </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">operator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">void</span></span></span></span><span class="hljs-function"><span class="hljs-params">* item )</span></span></span></span>; }; MyProcessFilter:: MyProcessFilter() : tbb::filter(parallel) {}</code> </pre></td><td><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">void</span></span>* MyProcessFilter::<span class="hljs-keyword"><span class="hljs-keyword">operator</span></span>()( <span class="hljs-type"><span class="hljs-type">void</span></span>* item ) { // FIND A <span class="hljs-keyword"><span class="hljs-keyword">CURRENT</span></span> DATA ITEM <span class="hljs-keyword"><span class="hljs-keyword">IN</span></span> CONTAINER // PROCESS THE ITEM }</code> </pre></td></tr></tbody></table><br>  It remains for us only to finally assemble the pipeline: create objects of the class <i>filter</i> , an object of the class <i>pipeline</i> , and connect all the stages.  After building the pipeline, you must call the <i>run ()</i> method of the class class <i>pipeline</i> , specifying the number of tokens (sorry for such a transfer).  In this case, the number of tokens means the number of data objects that can be executed in the pipeline simultaneously.  Choosing the right amount can be a separate topic for discussion, we simply follow the recommendations in the documents for TBB and choose a number equal to twice the number of streams available in the system.  This will give us the opportunity to have enough data objects to keep the pipeline busy and, at the same time, ensures that the queue of data objects does not grow if the first stage processes them much faster than subsequent ones. <br><br><pre> <code class="hljs css"> <span class="hljs-selector-tag"><span class="hljs-selector-tag">tbb</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">::pipeline</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">pipeline</span></span>; <span class="hljs-selector-tag"><span class="hljs-selector-tag">MyReadFilter</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">r_filter</span></span>( <span class="hljs-selector-tag"><span class="hljs-selector-tag">input_file</span></span> ); <span class="hljs-selector-tag"><span class="hljs-selector-tag">pipeline</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.add_filter</span></span>( <span class="hljs-selector-tag"><span class="hljs-selector-tag">r_filter</span></span> ); <span class="hljs-selector-tag"><span class="hljs-selector-tag">MyProcessFilter</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">p_filter</span></span>; <span class="hljs-selector-tag"><span class="hljs-selector-tag">pipeline</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.add_filter</span></span>(<span class="hljs-selector-tag"><span class="hljs-selector-tag">p_filter</span></span> ); <span class="hljs-selector-tag"><span class="hljs-selector-tag">MyWriteFilter</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">w_filter</span></span>( <span class="hljs-selector-tag"><span class="hljs-selector-tag">output_file</span></span> ); <span class="hljs-selector-tag"><span class="hljs-selector-tag">pipeline</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.add_filter</span></span>( <span class="hljs-selector-tag"><span class="hljs-selector-tag">w_filter</span></span> ); <span class="hljs-selector-tag"><span class="hljs-selector-tag">pipeline</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.run</span></span>(2*<span class="hljs-selector-tag"><span class="hljs-selector-tag">nthreads</span></span>)</code> </pre><br><br>  Thus, we created three subtasks in the pipeline that will be controlled by the TBB library to maximize the use of processors available in the system - the Process subtask will be parallelized, and the remaining subtasks will be dynamically distributed between threads depending on their availability.  We only needed to correctly connect these stages.  As you can see, in the example there is no flow control code ‚Äî all of this is hidden in the library's task scheduler. <br><br>  And as is often the case, somewhere in the next-to-last series (or, vice versa, from the first), a hand rises, and a very meticulous listener asks a very good question: How can we be sure that the constructed conveyor is efficient?  And if you dig deeper, how do you know how exactly the subtasks are performed in the pipeline, and what are the overhead costs of managing subtasks, and are there any gaps in their execution? <br><br>  It is meaningless to suggest comparing its implementation with the original, inefficient implementation of the algorithm.  First, so we will not understand anything, except that it will work faster.  And secondly, it is possible that it will work faster and will not. <br><br>  That is, there are no simple answers to the questions posed.  Need to measure and analyze.  Profiling with VTune Amplifier XE will make this process easier, more visual, and most importantly, fast.  VTune Amplifier will help us determine the efficiency of loading processors in the system, visually show the TBB task management scheme, and detect possible errors that we made in the process of constructing the pipeline that affect the performance of the algorithm implementation. <br><br>  Let's start with Hotspot analysis for educational purposes.  We will analyze an example of code that reads integer values ‚Äã‚Äãfrom a file in text format [Read stage], squares them in [Process] and writes numbers in text format to the [Write] output file.  We will assume that the reader is familiar with the basics of working with the VTune profiler, and will not go into details, which button to press to start the profiling. <br><br><img src="https://habrastorage.org/storage2/52a/c80/a99/52ac80a9943a0ce2021fc062f7ff8ba9.jpg"><br>  Fig.5. <br><br>  The results of the Hotspot analysis (Fig. 5) are quite expected - the test program was executed in 4 threads on a 4-core processor;  The <i>MyProcessClass :: operator ()</i> method, called from the TBB pipeline, is the hottest function, since it converts from a text to an integer value, calculates the square of the value, and performs the inverse conversion to text.  Interestingly, some function <i>[TBB Dispatch Loop]</i> (not really a function, but a conventional name that hides the ‚Äúinsides‚Äù of the TBB scheduler) is also on the hot list, which can tell us about the possible presence of planning overheads.  Let's continue with the Concurrency analysis so far, and determine the efficiency of the algorithm parallelization. <br><br><img src="https://habrastorage.org/storage2/dd0/4c4/56a/dd04c456a66474ea444820202cef0689.jpg"><br>  Fig.6. <br><br>  From the resulting histogram of parallelism of the program, we see that we are far from maximum efficiency: in general, the application was executed in less than 4 streams.  (The blue bars in the histogram show on the Y axis how long the application worked in X threads. Ideally, we would like to see a single bar with a parallelism level of 4 [concurrency level4], which would mean that our application was working in 4 threads all the time) . <br>  Even a quick glance at the results in the Bottom-Up View is enough to see the redundant synchronization that occurred between the threads (Fig. 7).  You can hover the mouse on the yellow lines of the transition control (transitions) and see in the pop-up window a link to the source of this synchronization. <br><br><img src="https://habrastorage.org/storage2/451/e4c/354/451e4c35451552f2bd4acb356ab08961.jpg"><br>  Fig.7. <br><br>  At this point, even the most patient user would have thrown further research, incidentally accusing the developers of TBB in an inefficient implementation of the library.  I would have done the same thing if I didn‚Äôt have a tool at hand to analyze the work of the scheduler.  Theoretically, one can imagine the independent tracing of tasks using source code instrumentation, and parsing the trace ‚Äúmanually‚Äù or using some script to identify the time relationships of the tasks.  However, this path does not look attractive because of the laboriousness of processing the collected trasses.  Therefore, I advise you to use a toolkit that already exists in VTune Amplifier and which can be used to quickly restore the structure of user tasks in a convenient graphical form. <br><br>  To enable the task analyzer, we need to instrument the source code using special functions from the Task API, which is available in VTune Amplifier.  Listed below are the steps you need to do to do this: <br><br><ol><li>  Include the header file from the User API in your source <br><pre> <code class="hljs cpp"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"ittnotify.h"</span></span></span></span></code> </pre> </li><li>  Determine the domain of the task.  It is often very convenient to separate tasks performed in different domains, for example, threads in different parallel infrastructures (TBB, OpenMP, etc.) <br><pre> <code class="hljs pgsql">__itt_domain* <span class="hljs-keyword"><span class="hljs-keyword">domain</span></span> = __itt_domain_create("PipelineTaskDomain");</code> </pre> </li><li>  Identify the task descriptor that describes the stage. <br><pre> <code class="hljs objectivec">__itt_string_handle* hFileReadSubtask = __itt_string_handle_create(<span class="hljs-string"><span class="hljs-string">"Read"</span></span>); __itt_string_handle* hFileWriteSubtask = __itt_string_handle_create(<span class="hljs-string"><span class="hljs-string">"Write"</span></span>); __itt_string_handle* hDoSquareSubtask = __itt_string_handle_create(<span class="hljs-string"><span class="hljs-string">"Do Square"</span></span>);</code> </pre></li><li>  Now wrap the source code, executed in the task stages, with calls to the instrumental API functions: <i>__itt_task_begin</i> and <i>__itt_task_end</i> .  For example, the read and write stages can be interpreted as follows: <br><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">void</span></span>* MyReadFilter::<span class="hljs-keyword"><span class="hljs-keyword">operator</span></span>()(<span class="hljs-type"><span class="hljs-type">void</span></span>*) { __itt_task_begin(<span class="hljs-keyword"><span class="hljs-keyword">domain</span></span>, __itt_null, __itt_null, hFileReadSubtask); // ALLOCATE MEMORY // <span class="hljs-keyword"><span class="hljs-keyword">READ</span></span> A DATA ITEM <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> FILE // PUT DATA ITEM <span class="hljs-keyword"><span class="hljs-keyword">TO</span></span> CONTAINER __itt_task_end(<span class="hljs-keyword"><span class="hljs-keyword">domain</span></span>); }</code> </pre><br><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">void</span></span>* MyWriteFilter::<span class="hljs-keyword"><span class="hljs-keyword">operator</span></span>()(<span class="hljs-type"><span class="hljs-type">void</span></span>*) { __itt_task_begin(<span class="hljs-keyword"><span class="hljs-keyword">domain</span></span>, __itt_null, __itt_null, hFileWriteSubtask); // <span class="hljs-keyword"><span class="hljs-keyword">GET</span></span> DATA ITEM <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> CONTAINER // <span class="hljs-keyword"><span class="hljs-keyword">WRITE</span></span> THE DATA ITEM <span class="hljs-keyword"><span class="hljs-keyword">TO</span></span> FILE // <span class="hljs-keyword"><span class="hljs-keyword">DEALLOCATE</span></span> MEMORY __itt_task_end(<span class="hljs-keyword"><span class="hljs-keyword">domain</span></span>); }</code> </pre><br>  Likewise, you can wrap and the stage of processing numbers (Process stage).  (Further information on User API functions can be found in the product documentation). <br><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">void</span></span>* MyProcessFilter::<span class="hljs-keyword"><span class="hljs-keyword">operator</span></span>()( <span class="hljs-type"><span class="hljs-type">void</span></span>* item ) { __itt_task_begin(<span class="hljs-keyword"><span class="hljs-keyword">domain</span></span>, __itt_null, __itt_null, hDoSquareSubtask); // FIND A <span class="hljs-keyword"><span class="hljs-keyword">CURRENT</span></span> DATA ITEM <span class="hljs-keyword"><span class="hljs-keyword">IN</span></span> CONTAINER // PROCESS THE ITEM __itt_task_end(<span class="hljs-keyword"><span class="hljs-keyword">domain</span></span>); }</code> </pre><br></li><li>  Do not forget to add the path to the VTune Amplifier header files to your project: <br>  <i>$ (VTUNE_AMPLIFIER_XE_2013_DIR) include</i> </li><li>  Statically link your executable with the library <i>libittnotify.lib</i> , the path to which is located <br>  <i>$ (VTUNE_AMPLIFIER_XE_2013_DIR) lib [32 | 64]</i> depending on the ‚Äúbitness‚Äù of your system. </li></ol><br><br>  Finally, it remains for us to enable the analysis of user tasks in the analysis configuration window (Fig. 8) and run any type of profiling that we need. <br><br><img src="https://habrastorage.org/storage2/8f1/468/868/8f1468868392ce1aab90c18734ba65b9.jpg"><br>  Fig.8. <br><br>  After the Concurrency profiling is finished, we switch to the Tasks View tab (Fig.9).  Here, we are somewhat hampered by the yellow lines of flow control transitions - they can be turned off in the panel to the right, as are the CPU Time graphics, which may close the necessary information.  However, even with this, tasks are poorly distinguishable on a timeline;  color bars representing a graphic representation of tasks are too thin to distinguish them.  In this case, select the smaller time interval and zoom in (Zoom-In) from the context menu by right-clicking the mouse, or using the toolbar. <br><br><img src="https://habrastorage.org/storage2/b19/6af/a1b/b196afa1b26d21638e4b73bc1b4a1fd9.jpg"><br>  Fig.9. <br><br>  In the enlarged image of the tasks we can notice a couple of problems (Fig. 10).  The first is the huge (compared to the duration of the tasks themselves) intervals between tasks, during which the threads are in an inactive state, that is, the processors are idle waiting for synchronization events.  The second problem is the duration of the task phases.  For example, the ‚ÄúDo Squire‚Äù subtask is only about 0.1 milliseconds.  This is a critically small execution interval, given that the tasks are managed by the TBB scheduler, and it takes some time to plan and assign them to workflows (TBB Worker Thread).  That is, it turns out that the overhead of managing subtasks is commensurate with the execution time of the task itself.  This should not be allowed, so it is necessary to increase the time spent on work in the subtask, that is, in other words, give it more work. <br><br><img src="https://habrastorage.org/storage2/db4/d4f/6a0/db4d4f6a0d7d2d5fbde1fb3a7b542dd4.jpg"><br>  Fig.  ten. <br><br>  In our example, it is already clear what functions in the code perform what tasks, but this was done intentionally to simplify the presentation in the article.  In a real-world application in the execution of tasks dozens of functions can be involved.  To find all the functions that influenced the execution of tasks, and increase their workload, we can switch to the Bottom-Up view results for Concurrency, or the Hotspot profiling.  Now we simply change the grouping of functions by task type (TaskType / Function) and get in the table a list of subtasks that we created by instrumenting the application.  Opening the task, clicking on the ‚Äú+‚Äù icon, we get a list and a tree of function calls that participated in the execution of this subtask, occupying a statistically significant processor time (Fig. 11). <br><br><img src="https://habrastorage.org/storage2/265/6b1/81c/2656b181c56cc9f652963aa29b9fe67b.jpg"><br>  Fig.11. <br><br>  Next, by double-clicking, we go to the source code of the hot function <i>MyProcessFilter :: operator ()</i> and find that it works with a text segment transmitted to it (text slice) (Fig. 12).  Inside this function, it iterates over characters in the text, converts to an integer type, multiplies the value by itself, and converts it back to a text character.  The most obvious way to increase the load for this subtask would be to increase the size of a piece of text ‚Äî this will linearly increase the number of operations performed in this subtask.  We simply choose the new size of the maximum number of characters in the MAX_CHAR_PER_INPUT_SLICE segment, which will be, say, 100 times greater than the original (based on the time indicators we received during the profiling).  We also assume that the efficiency of read and write operations will also increase with increasing data size for a single object. <br><br><img src="https://habrastorage.org/storage2/6ad/943/5f9/6ad9435f95e4ddfa786c992730c39a6b.jpg"><br>  Fig.12. <br><br>  We recompile the application and profile it again using task analysis (Figure 13).  The Do Square subtask is now executed in about 10 milliseconds (hover the mouse over the task image to get numerical characteristics).  There are also practically no ‚Äúgaps‚Äù between the subtasks, which makes the threads occupied more time.  You can also notice how the TBB scheduler builds up the same, but shorter subtasks, such as the Write phase, connecting them into a longer queue, and assigning them to execute in one thread, thereby reducing the cost of additional synchronization. <br><br><img src="https://habrastorage.org/storage2/d22/5cc/dae/d225ccdae38a6f160822574fb1b4347f.jpg"><br>  Fig.13. <br><br>  It will also be interesting for us to check the total parallelism throughout the program.  As can be seen in Figure 14, the application was performed mainly by 3 or 4 threads, which is a significant achievement compared to the initial results.  You can also compare the average value of the Concurrency Level, but you need to remember that it affects the sequential part of the application, including the one that we did not parallelize. <br><br>  Yes, the serial part of the program remains, but it has decreased by a third.  It can also be seen in the results in TimeLine View.  If we select this sequential part and filter the rest of the time, then we can see that it corresponds to the initialization phase of the entire application in the main thread.  If we want to determine exactly what part corresponded to the work of the pipeline and does not relate to initialization, then we can use the User API functions that control the profiling ‚Äî starting the collection immediately before creating the conveyor, and ending the collection after doing the work. <br><br><img src="https://habrastorage.org/storage2/e87/f8a/87a/e87f8a87ac36dd029788c875dd1adcc6.jpg"><br>  Fig.14. <br><br>  In addition to graphs that give an estimate of the improvement in the efficiency of implementation, we can see and compare the numerical indicators of application performance.  All results in VTune Amplifier can be compared using the corresponding comparison functionality.  I will simply bring two pictures side by side, for a better perception of the compliance of the results of the initial Hotspots analysis, and obtained after changing the code (Figure 15).  What is interesting, although the program execution time (Elapsed Time) has decreased, the time spent in the <i>MyProcessFilter :: operator ()</i> function <i>has</i> not changed, since we have not changed the total amount of work, but it has simply been redistributed.  At the same time, the cost of scheduling TBB tasks was significantly reduced.  In addition, the total time of reading and writing data also decreased, obviously, due to more efficient work with large segments of text. <br><br><img src="https://habrastorage.org/storage2/d0e/005/127/d0e005127433ec6513f1731b872c3006.jpg"><br>  Fig.15. <br><br><h2>  Conclusion </h2><br>  Data and task decomposition is effectively used for parallelization of algorithms.  Some classes of algorithms can be improved by using the method of pipelining sequential and parallel phases of execution.  The Intel TBB library can help significantly reduce the time required to implement pipelining algorithms, while taking on the task of scheduling tasks and balancing their execution in the streams available in the system.  Formation of tasks based on the phases of the algorithm can be a non-trivial task, depending on the complexity of the implemented algorithm, and managing the execution of tasks can be even more difficult.  VTune Amplifier‚Äôs Task Analysis toolkit helps developers present the structure of task execution in a multi-threaded environment in a convenient graphical form, increasing analysis efficiency and significantly reducing application development time. </div><p>Source: <a href="https://habr.com/ru/post/162641/">https://habr.com/ru/post/162641/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../162629/index.html">Reading books in English and learning the language is easier with WordMemo</a></li>
<li><a href="../162631/index.html">What do businesses actually sell to customers?</a></li>
<li><a href="../162633/index.html">Dell goes out of the smartphone business, stops working with Android</a></li>
<li><a href="../162635/index.html">Add a download icon to large CSS images.</a></li>
<li><a href="../162637/index.html">Cappa - an interesting project of personal hydroelectric power station</a></li>
<li><a href="../162647/index.html">New telecommunications agreement signed in Dubai led to a split in ITU</a></li>
<li><a href="../162649/index.html">From December 1, 2013 it will be possible to keep your number when changing the cellular operator</a></li>
<li><a href="../162651/index.html">Raspberry Pi GPIO Web Control</a></li>
<li><a href="../162653/index.html">Head of Intel may be Nvidia CEO Jen-Sen Juan</a></li>
<li><a href="../162655/index.html">Do you play computer games?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>