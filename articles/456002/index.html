<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Dedicated hearth memory and OOM Killer intervention</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello again! The translation of the next article was prepared specifically for students of the course ‚ÄúInfrastructure platform based on Kubernetes‚Äù , ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Dedicated hearth memory and OOM Killer intervention</h1><div class="post__text post__text-html js-mediator-article">  Hello again!  The translation of the next article was prepared specifically for students of the course <a href="https://otus.pw/oBRu/">‚ÄúInfrastructure platform based on Kubernetes‚Äù</a> , which is being launched this month. Let's start. <br><br><img src="https://habrastorage.org/webt/uv/io/fj/uviofjoh-y_arqntutialytzgcs.png"><br><br>  In recent days, some of my pods have constantly crashed, leaving an entry in the OS system log stating that the OOM Killer destroyed the container process.  I decided to figure out why this is happening. <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Cgroup memory limit and memory parameters </h2><br>  Let's do a test on the K3s distribution.  We create 123MB (123 Mi) with a characteristic memory limit. <br><br><pre><code class="go hljs">kubectl run --restart=Never --rm -it --image=ubuntu --limits=<span class="hljs-string"><span class="hljs-string">'memory=123Mi'</span></span> -- sh If you don<span class="hljs-string"><span class="hljs-string">'t see a command prompt, try pressing enter. root@sh:/#</span></span></code> </pre> <br>  In another console, find out the <code>uid</code> hearth. <br><br><pre> <code class="go hljs">kubectl get pods sh -o yaml | grep uid uid: bc001ffa<span class="hljs-number"><span class="hljs-number">-68f</span></span>c<span class="hljs-number"><span class="hljs-number">-11e9</span></span><span class="hljs-number"><span class="hljs-number">-92d</span></span>7<span class="hljs-number"><span class="hljs-number">-5</span></span>ef9efd9374c</code> </pre> <br>  On the server where it runs under, we find out the parameters of the <code>cgroup</code> , specifying the <code>uid</code> desired sub. <br><br><pre> <code class="go hljs">cd /sys/fs/cgroup/memory/kubepods/burstable/podbc001ffa<span class="hljs-number"><span class="hljs-number">-68f</span></span>c<span class="hljs-number"><span class="hljs-number">-11e9</span></span><span class="hljs-number"><span class="hljs-number">-92d</span></span>7<span class="hljs-number"><span class="hljs-number">-5</span></span>ef9efd9374c cat memory.limit_in_bytes <span class="hljs-number"><span class="hljs-number">128974848</span></span></code> </pre> <br>  128974848 is exactly 123 MiB (123 * 1024 * 1024).  The situation clears up.  It turns out that in Kubernetes the memory limit is specified via cgroup.  As soon as the memory limit grows larger, the cgroup initiates the destruction of the container process. <br><br><h2>  Stress test </h2><br>  Let's install the stress testing tools through an open command console session. <br><br><pre> <code class="go hljs">root@sh:/# apt update; apt install -y stress</code> </pre> <br>  At the same time, we will keep track of the system log entries with the <code>dmesg -Tw</code> . <br><br>  First, run the stress test utility, allocating 100 MB to it in memory.  The process started successfully. <br><br><pre> <code class="go hljs">root@sh:/# stress --vm <span class="hljs-number"><span class="hljs-number">1</span></span> --vm-bytes <span class="hljs-number"><span class="hljs-number">100</span></span>M &amp; [<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-number"><span class="hljs-number">271</span></span> root@sh:/# stress: info: [<span class="hljs-number"><span class="hljs-number">271</span></span>] dispatching hogs: <span class="hljs-number"><span class="hljs-number">0</span></span> cpu, <span class="hljs-number"><span class="hljs-number">0</span></span> io, <span class="hljs-number"><span class="hljs-number">1</span></span> vm, <span class="hljs-number"><span class="hljs-number">0</span></span> hdd</code> </pre> <br>  Now we will carry out the second stress test. <br><br><pre> <code class="go hljs">root@sh:/# stress --vm <span class="hljs-number"><span class="hljs-number">1</span></span> --vm-bytes <span class="hljs-number"><span class="hljs-number">50</span></span>M stress: info: [<span class="hljs-number"><span class="hljs-number">273</span></span>] dispatching hogs: <span class="hljs-number"><span class="hljs-number">0</span></span> cpu, <span class="hljs-number"><span class="hljs-number">0</span></span> io, <span class="hljs-number"><span class="hljs-number">1</span></span> vm, <span class="hljs-number"><span class="hljs-number">0</span></span> hdd stress: FAIL: [<span class="hljs-number"><span class="hljs-number">271</span></span>] (<span class="hljs-number"><span class="hljs-number">415</span></span>) &lt;-- worker <span class="hljs-number"><span class="hljs-number">272</span></span> got signal <span class="hljs-number"><span class="hljs-number">9</span></span> stress: WARN: [<span class="hljs-number"><span class="hljs-number">271</span></span>] (<span class="hljs-number"><span class="hljs-number">417</span></span>) now reaping child worker processes stress: FAIL: [<span class="hljs-number"><span class="hljs-number">271</span></span>] (<span class="hljs-number"><span class="hljs-number">451</span></span>) failed run completed in <span class="hljs-number"><span class="hljs-number">7s</span></span></code> </pre> <br>  The launch led to the instantaneous destruction of the first stress test process (PID 271) at signal 9. <br><br>  Meanwhile, the following entries appeared in the system log: <br><br> <code>[Sat Apr 27 22:56:09 2019] stress invoked oom-killer: gfp_mask=0x14000c0(GFP_KERNEL), nodemask=(null), order=0, oom_score_adj=939 <br> [Sat Apr 27 22:56:09 2019] stress cpuset=a2ed67c63e828da3849bf9f506ae2b36b4dac5b402a57f2981c9bdc07b23e672 mems_allowed=0 <br> [Sat Apr 27 22:56:09 2019] CPU: 0 PID: 32332 Comm: stress Not tainted 4.15.0-46-generic #49-Ubuntu <br> [Sat Apr 27 22:56:09 2019] Hardware name: BHYVE, BIOS 1.00 03/14/2014 <br> [Sat Apr 27 22:56:09 2019] Call Trace: <br> [Sat Apr 27 22:56:09 2019] dump_stack+0x63/0x8b <br> [Sat Apr 27 22:56:09 2019] dump_header+0x71/0x285 <br> [Sat Apr 27 22:56:09 2019] oom_kill_process+0x220/0x440 <br> [Sat Apr 27 22:56:09 2019] out_of_memory+0x2d1/0x4f0 <br> [Sat Apr 27 22:56:09 2019] mem_cgroup_out_of_memory+0x4b/0x80 <br> [Sat Apr 27 22:56:09 2019] mem_cgroup_oom_synchronize+0x2e8/0x320 <br> [Sat Apr 27 22:56:09 2019] ? mem_cgroup_css_online+0x40/0x40 <br> [Sat Apr 27 22:56:09 2019] pagefault_out_of_memory+0x36/0x7b <br> [Sat Apr 27 22:56:09 2019] mm_fault_error+0x90/0x180 <br> [Sat Apr 27 22:56:09 2019] __do_page_fault+0x4a5/0x4d0 <br> [Sat Apr 27 22:56:09 2019] do_page_fault+0x2e/0xe0 <br> [Sat Apr 27 22:56:09 2019] ? page_fault+0x2f/0x50 <br> [Sat Apr 27 22:56:09 2019] page_fault+0x45/0x50 <br> [Sat Apr 27 22:56:09 2019] RIP: 0033:0x558182259cf0 <br> [Sat Apr 27 22:56:09 2019] RSP: 002b:00007fff01a47940 EFLAGS: 00010206 <br> [Sat Apr 27 22:56:09 2019] RAX: 00007fdc18cdf010 RBX: 00007fdc1763a010 RCX: 00007fdc1763a010 <br> [Sat Apr 27 22:56:09 2019] RDX: 00000000016a5000 RSI: 0000000003201000 RDI: 0000000000000000 <br> [Sat Apr 27 22:56:09 2019] RBP: 0000000003200000 R08: 00000000ffffffff R09: 0000000000000000 <br> [Sat Apr 27 22:56:09 2019] R10: 0000000000000022 R11: 0000000000000246 R12: ffffffffffffffff <br> [Sat Apr 27 22:56:09 2019] R13: 0000000000000002 R14: fffffffffffff000 R15: 0000000000001000 <br> [Sat Apr 27 22:56:09 2019] Task in /kubepods/burstable/podbc001ffa-68fc-11e9-92d7-5ef9efd9374c/a2ed67c63e828da3849bf9f506ae2b36b4dac5b402a57f2981c9bdc07b23e672 killed as a result of limit of /kubepods/burstable/podbc001ffa-68fc-11e9-92d7-5ef9efd9374c <br> [Sat Apr 27 22:56:09 2019] memory: usage 125952kB, limit 125952kB, failcnt 3632 <br> [Sat Apr 27 22:56:09 2019] memory+swap: usage 0kB, limit 9007199254740988kB, failcnt 0 <br> [Sat Apr 27 22:56:09 2019] kmem: usage 2352kB, limit 9007199254740988kB, failcnt 0 <br> [Sat Apr 27 22:56:09 2019] Memory cgroup stats for /kubepods/burstable/podbc001ffa-68fc-11e9-92d7-5ef9efd9374c: cache:0KB rss:0KB rss_huge:0KB shmem:0KB mapped_file:0KB dirty:0KB writeback:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB <br> [Sat Apr 27 22:56:09 2019] Memory cgroup stats for /kubepods/burstable/podbc001ffa-68fc-11e9-92d7-5ef9efd9374c/79fae7c2724ea1b19caa343fed8da3ea84bbe5eb370e5af8a6a94a090d9e4ac2: cache:0KB rss:48KB rss_huge:0KB shmem:0KB mapped_file:0KB dirty:0KB writeback:0KB inactive_anon:0KB active_anon:48KB inactive_file:0KB active_file:0KB unevictable:0KB <br> [Sat Apr 27 22:56:09 2019] Memory cgroup stats for /kubepods/burstable/podbc001ffa-68fc-11e9-92d7-5ef9efd9374c/a2ed67c63e828da3849bf9f506ae2b36b4dac5b402a57f2981c9bdc07b23e672: cache:0KB rss:123552KB rss_huge:0KB shmem:0KB mapped_file:0KB dirty:0KB writeback:0KB inactive_anon:0KB active_anon:123548KB inactive_file:0KB active_file:0KB unevictable:0KB <br> [Sat Apr 27 22:56:09 2019] [ pid ] uid tgid total_vm rss pgtables_bytes swapents oom_score_adj name <br> [Sat Apr 27 22:56:09 2019] [25160] 0 25160 256 1 28672 0 -998 pause <br> [Sat Apr 27 22:56:09 2019] [25218] 0 25218 4627 872 77824 0 939 bash <br> [Sat Apr 27 22:56:09 2019] [32307] 0 32307 2060 275 57344 0 939 stress <br> [Sat Apr 27 22:56:09 2019] [32308] 0 32308 27661 24953 253952 0 939 stress <br> [Sat Apr 27 22:56:09 2019] [32331] 0 32331 2060 304 53248 0 939 stress <br> [Sat Apr 27 22:56:09 2019] [32332] 0 32332 14861 5829 102400 0 939 stress <br> [Sat Apr 27 22:56:09 2019] Memory cgroup out of memory: Kill process 32308 (stress) score 1718 or sacrifice child <br> [Sat Apr 27 22:56:09 2019] Killed process 32308 (stress) total-vm:110644kB, anon-rss:99620kB, file-rss:192kB, shmem-rss:0kB <br> [Sat Apr 27 22:56:09 2019] oom_reaper: reaped process 32308 (stress), now anon-rss:0kB, file-rss:0kB, shmem-rss:0kB</code> <br> <br>  The process with PID 32308 on the host was destroyed due to low memory (OOM).  But the most interesting thing is hidden at the end of the journal entries: <br><br><img src="https://habrastorage.org/webt/fz/dn/pb/fzdnpbniemrgqc8qvmkxvttose0.png"><br><br>  Here are the processes of this pod, which are marked as candidates for destruction by the OOM Killer component.  The basic <code>pause</code> process, which stores the network namespaces, received an <code>oom_score_adj</code> rating of <code>-998</code> , meaning the process is guaranteed not to be destroyed.  The remaining processes in the container received an <code>oom_score_adj</code> rating of <code>939</code> .  You can check this value using the formula from the Kubernetes documentation below: <br><br><pre> <code class="go hljs">min(max(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1000</span></span> - (<span class="hljs-number"><span class="hljs-number">1000</span></span> * memoryRequestBytes) / machineMemoryCapacityBytes), <span class="hljs-number"><span class="hljs-number">999</span></span>)</code> </pre> <br>  Find out the amount of memory available to the node: <br><br><pre> <code class="go hljs">kubectl describe nodes k3s | grep Allocatable -A <span class="hljs-number"><span class="hljs-number">5</span></span> Allocatable: cpu: <span class="hljs-number"><span class="hljs-number">1</span></span> ephemeral-storage: <span class="hljs-number"><span class="hljs-number">49255941901</span></span> hugepages<span class="hljs-number"><span class="hljs-number">-1</span></span>Gi: <span class="hljs-number"><span class="hljs-number">0</span></span> hugepages<span class="hljs-number"><span class="hljs-number">-2</span></span>Mi: <span class="hljs-number"><span class="hljs-number">0</span></span> memory: <span class="hljs-number"><span class="hljs-number">2041888</span></span>Ki</code> </pre> <br>  If the amount of requested memory is not specified, by default it will be equal to the limit.  Substituting the values, we get the following <code>oom_score_adj</code> value: <code>1000‚Äì123*1024/2041888=938.32</code> , which is very close to the value <code>939</code> specified in the system log.  (I don‚Äôt know how the OOM Killer gets the exact value of 939.) <br><br>  So, all processes in the container have the same oom_score_adj value.  The OOM Killer component calculates the OOM value based on memory usage and adjusts the result based on the oom_score_adj estimate.  And, ultimately, it destroys the process of the first stress test, which has eaten out most of the memory, 100 MB, which corresponds to an estimate of oom_score = 1718. <br><br><h2>  Conclusion </h2><br>  Kubernetes controls the memory limit of the hearth through the parameters cgroup and the OOM Killer component.  It is necessary to carefully coordinate the conditions of the OOM operating system and the OOM flows. <br><br>  How do you like the material?  Anyone who wants to learn more about the course is invited to the June 17 <a href="https://otus.pw/1UR7/">free webinar</a> , where we will explore the possibilities of Kubernetes for organizing the practice of continuous delivery (CI / CD) and approaches for a small team with several applications, and for a large organization with a large number of systems. </div><p>Source: <a href="https://habr.com/ru/post/456002/">https://habr.com/ru/post/456002/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455996/index.html">Digital transformation of advertising in online retail. In the footsteps of "Video Analytics in Retail"</a></li>
<li><a href="../456/index.html">The second round - myself</a></li>
<li><a href="../4560/index.html">The secret service of the Ministry of Internal Affairs - on cybercrime and fight against them</a></li>
<li><a href="../45600/index.html">Google Maps Street View looks new</a></li>
<li><a href="../456000/index.html">Creating a Tic-Tac-Toe game using TypeScript, React and Mocha</a></li>
<li><a href="../456004/index.html">We invite you to mitap on frontend-development in high-load services</a></li>
<li><a href="../45601/index.html">Platform 2009 is waiting for habra people</a></li>
<li><a href="../456010/index.html">How java 10 changes the way anonymous inner classes are used</a></li>
<li><a href="../456014/index.html">On the localization of products. Part 2: How is the price formed?</a></li>
<li><a href="../456018/index.html">How to implement warmth from GitHub to production server using Webhook</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>