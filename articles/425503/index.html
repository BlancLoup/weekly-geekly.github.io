<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Cassandra Sink for Spark Structured Streaming</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A couple of months ago, I started exploring Spark, and at some point I was faced with the problem of saving Structured Streaming calculations in the C...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Cassandra Sink for Spark Structured Streaming</h1><div class="post__text post__text-html js-mediator-article">  A couple of months ago, I started exploring Spark, and at some point I was faced with the problem of saving Structured Streaming calculations in the Cassandra database. <br><br>  In this post, I give a simple example of creating and using Cassandra Sink for Spark Structured Streaming.  I hope that the post will be useful to those who have recently started working with Spark Structured Streaming and are wondering how to upload the results of calculations to the database. <br><br>  The idea of ‚Äã‚Äãthe application is very simple - to receive and parse messages from the Kafka, perform simple transformations in the Spark and save the results in Cassandra. <br><a name="habracut"></a><br><h3>  Pros Structured Streaming </h3><br>  About Structured Streaming can be read in detail in the <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">documentation</a> .  In short, Structured Streaming is a highly scalable stream processing engine that is based on the Spark SQL engine.  It allows you to use Dataset / DataFrame to aggregate data, calculate window functions, connections, etc. That is, Structured Streaming allows you to use good old SQL to work with data streams. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  What is the problem? </h3><br>  Stable release Spark Structured Streaming was released in 2017.  That is, this is a fairly new API in which the basic functionality is implemented, but some things will have to be done by ourselves.  For example, in Structured Streaming, there are standard functions for recording output data to a file, tab, console, or memory, but in order to save data to the database, you will have to use the <i>foreach</i> receiver in Structured Streaming and implement the <i>ForeachWriter</i> interface.  <b>Starting with Spark 2.3.1, this functionality can only be implemented on Scala and Java</b> . <br><br>  I assume that the reader already knows how Structured Streaming works in general terms, knows how to implement the necessary transformations and is now ready to upload the results to the base.  If some of the steps above are unclear, official documentation can be a good starting point for learning Structured Streaming.  In this article, I would like to focus on the last step when you need to save the results in a database. <br><br>  Below, I will describe an example of implementing Cassandra sink for Structured Streaming and explain how to run it in a cluster.  The full code is available <a href="https://github.com/epishova/Structured-Streaming-Cassandra-Sink">here</a> . <br><br>  When I first encountered the above problem, <a href="https://github.com/polomarcus/Spark-Structured-Streaming-Examples">this project</a> turned out to be very useful.  However, it may seem a bit tricky if the reader has just started working with Structured Streaming and is looking for a simple example of how to upload data to a cassandra.  In addition, the project is written to work in local mode and requires some changes to run in a cluster. <br><br>  I also want to give examples of how to save data in <a href="https://jira.mongodb.org/browse/SPARK-134">MongoDB</a> and any other database that uses <a href="https://docs.databricks.com/_static/notebooks/structured-streaming-etl-kafka.html">JDBC</a> . <br><br><h3>  A simple solution </h3><br>  To upload data to an external system, you must use a <i>foreach</i> receiver.  Read more about it <a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html">here</a> .  In short, you need to implement the <i>ForeachWriter</i> interface.  That is, it is necessary to determine how to open a connection, how to process each piece of data, and how to close the connection at the end of processing.  The source code is as follows: <br><br><pre><code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraSinkForeach</span></span></span><span class="hljs-class">(</span><span class="hljs-params"></span><span class="hljs-class"><span class="hljs-params"></span>) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ForeachWriter</span></span></span><span class="hljs-class">[org.apache.spark.sql.</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">Row</span></span></span><span class="hljs-class">] </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This class implements the interface ForeachWriter, which has methods that get called // whenever there is a sequence of rows generated as output val cassandraDriver = new CassandraDriver(); def open(partitionId: Long, version: Long): Boolean = { // open connection println(s"Open connection") true } def process(record: org.apache.spark.sql.Row) = { println(s"Process new $record") cassandraDriver.connector.withSessionDo(session =&gt; session.execute(s""" insert into ${cassandraDriver.namespace}.${cassandraDriver.foreachTableSink} (fx_marker, timestamp_ms, timestamp_dt) values('${record(0)}', '${record(1)}', '${record(2)}')""") ) } def close(errorOrNull: Throwable): Unit = { // close the connection println(s"Close connection") } }</span></span></code> </pre> <br>  I will describe the definition of <i>CassandraDriver</i> and the structure of the output tables later, but for now let's take a closer look at how the above code works.  To connect to the casandra from Spark, I create a <i>CassandraDriver</i> object that provides access to the <i>CassandraConnector</i> , a connector designed by <a href="https://github.com/datastax/spark-cassandra-connector">DataStax</a> .  CassandraConnector is responsible for opening and closing the connection to the database, so I just output debug messages in the <i>open</i> and <i>close</i> methods of the <i>CassandraSinkForeach</i> class. <br><br>  The above code is called from the main application as follows: <br><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> sink = parsed .writeStream .queryName(<span class="hljs-string"><span class="hljs-string">"KafkaToCassandraForeach"</span></span>) .outputMode(<span class="hljs-string"><span class="hljs-string">"update"</span></span>) .foreach(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">CassandraSinkForeach</span></span>()) .start()</code> </pre><br>  <i>CassandraSinkForeach</i> is created for each row of data, so each working node inserts its own part of the rows into the database.  That is, each worker node executes <i>val cassandraDriver = new CassandraDriver ();</i>  This is what CassandraDriver looks like: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">CassandraDriver</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// This object will be used in CassandraSinkForeach to connect to Cassandra DB from an executor. // It extends SparkSessionBuilder so to use the same SparkSession on each node. val spark = buildSparkSession import spark.implicits._ val connector = CassandraConnector(spark.sparkContext.getConf) // Define Cassandra's table which will be used as a sink /* For this app I used the following table: CREATE TABLE fx.spark_struct_stream_sink ( fx_marker text, timestamp_ms timestamp, timestamp_dt date, primary key (fx_marker)); */ val namespace = "fx" val foreachTableSink = "spark_struct_stream_sink" }</span></span></code> </pre><br>  Let's take a closer look at the <i>spark</i> object.  The code for <i>SparkSessionBuilder is</i> as follows: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Serializable</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Build a spark session. Class is made serializable so to get access to SparkSession in a driver and executors. // Note here the usage of @transient lazy val def buildSparkSession: SparkSession = { @transient lazy val conf: SparkConf = new SparkConf() .setAppName("Structured Streaming from Kafka to Cassandra") .set("spark.cassandra.connection.host", "ec2-52-23-103-178.compute-1.amazonaws.com") .set("spark.sql.streaming.checkpointLocation", "checkpoint") @transient lazy val spark = SparkSession .builder() .config(conf) .getOrCreate() spark } }</span></span></code> </pre><br>  On each working node, <i>SparkSessionBuilder</i> provides access to the <i>SparkSession</i> that was created on the driver.  To make such access possible, it is necessary to serialize <i>SparkSessionBuilder</i> and use a <i><a href="https://habr.com/users/transient/" class="user_link">transient</a> lazy val</i> , which allows the serialization system to ignore <i>conf</i> and <i>spark</i> objects during initialization of the program until the moment when the objects are accessed.  Thus, when launching the program, <i>buildSparkSession is</i> serialized and sent to each working node, but <i>conf</i> and <i>spark</i> objects are resolved only at the moment when the working node accesses them. <br><br>  Now let's look at the main application code: <br><br><pre> <code class="scala hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">object</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">KafkaToCassandra</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SparkSessionBuilder</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">// Main body of the app. It also extends SparkSessionBuilder. def main(args: Array[String]) { val spark = buildSparkSession import spark.implicits._ // Define location of Kafka brokers: val broker = "ec2-18-209-75-68.compute-1.amazonaws.com:9092,ec2-18-205-142-57.compute-1.amazonaws.com:9092,ec2-50-17-32-144.compute-1.amazonaws.com:9092" /*Here is an example massage which I get from a Kafka stream. It contains multiple jsons separated by \n {"timestamp_ms": "1530305100936", "fx_marker": "EUR/GBP"} {"timestamp_ms": "1530305100815", "fx_marker": "USD/CHF"} {"timestamp_ms": "1530305100969", "fx_marker": "EUR/CHF"} {"timestamp_ms": "1530305100011", "fx_marker": "USD/CAD"} */ // Read incoming stream val dfraw = spark .readStream .format("kafka") .option("kafka.bootstrap.servers", broker) .option("subscribe", "currency_exchange") .load() val schema = StructType( Seq( StructField("fx_marker", StringType, false), StructField("timestamp_ms", StringType, false) ) ) val df = dfraw .selectExpr("CAST(value AS STRING)").as[String] .flatMap(_.split("\n")) val jsons = df.select(from_json($"value", schema) as "data").select("data.*") // Process data. Create a new date column val parsed = jsons .withColumn("timestamp_dt", to_date(from_unixtime($"timestamp_ms"/1000.0, "yyyy-MM-dd HH:mm:ss.SSS"))) .filter("fx_marker != ''") // Output results into a database val sink = parsed .writeStream .queryName("KafkaToCassandraForeach") .outputMode("update") .foreach(new CassandraSinkForeach()) .start() sink.awaitTermination() } }</span></span></code> </pre><br>  When an application is sent for execution, <i>buildSparkSession is</i> serialized and sent to the working nodes, however <i>conf</i> and <i>spark</i> objects remain unresolved.  The driver then creates a spark object inside the <i>KafkaToCassandra</i> and distributes the work between the working nodes.  Each working node reads data from the kafka, does simple transformations on the received portion of records, and when the working node is ready to write the results to the database, it allows <i>conf</i> and <i>spark</i> objects, thereby gaining access to the <i>SparkSession</i> created on the driver. <br><br><h3>  How to build and run the application? </h3><br>  When I switched from PySpark to Scala, it took me some time to figure out how to build the application.  Therefore, I included Maven <i>pom.xml</i> in my project.  A reader can build an application using Maven by running the <i>mvn package</i> command.  After the application can be sent for execution using <br><br><pre> <code class="bash hljs">./bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.1,datastax:spark-cassandra-connector:2.3.0-s_2.11 --class com.insight.app.CassandraSink.KafkaToCassandra --master spark://ec2-18-232-26-53.compute-1.amazonaws.com:7077 target/cassandra-sink-0.0.1-SNAPSHOT.jar</code> </pre><br>  In order to build and run the application, it is necessary to replace the names of my AWS machines with their own (i.e., replace everything that looks like ec2-xx-xxx-xx-xx.compute-1.amazonaws.com). <br><br>  Spark and Structured Streaming in particular is a new topic for me, so I will be very grateful to readers for comments, discussion and corrections. </div><p>Source: <a href="https://habr.com/ru/post/425503/">https://habr.com/ru/post/425503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../425489/index.html">Automation: exaggerated threat of robots</a></li>
<li><a href="../425493/index.html">Configuring MikroTik hAP mini for IPTV Beeline</a></li>
<li><a href="../425497/index.html">Tutu PHP Meetup # 2: Live Event Broadcast</a></li>
<li><a href="../425499/index.html">HyperX Impact DDR4 - SO-DIMM, which could! Or why in a laptop 64 GB of memory with a frequency of 3200 MHz?</a></li>
<li><a href="../425501/index.html">A / V tests on Android from A to Z</a></li>
<li><a href="../425505/index.html">Analysis of the Linux kernel boot process</a></li>
<li><a href="../425507/index.html">Parsim Wikipedia for NLP tasks in 4 teams</a></li>
<li><a href="../425511/index.html">Unobvious features of Rotativa for generating PDF in an ASP.NET MVC application</a></li>
<li><a href="../425513/index.html">Deputies seriously undertook the taxation of miners</a></li>
<li><a href="../425515/index.html">Apple blocks the possibility of independent repair of new MacBook models</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>