<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Smart IDReader SDK - add recognition to Android apps</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! In one of our past articles, the issue of embedding the Smart IDReader recognition core in an iOS application was studied. It's time to disc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Smart IDReader SDK - add recognition to Android apps</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/web/21d/486/78c/21d48678cb424ff9aaa09e54dd5b6031.png" align="right">  Hi, Habr!  In one of <a href="https://habrahabr.ru/company/smartengines/blog/329574">our</a> past <a href="https://habrahabr.ru/company/smartengines/blog/329574">articles, the</a> issue of embedding the Smart IDReader recognition core in an iOS application was studied.  It's time to discuss the same problem, but for the Android OS.  Due to the large number of versions of the system and a wide fleet of devices, it will be more complicated than for iOS, but still quite a solvable task.  Disclaimer - the information below is not true in the last resort, if you know how to simplify the process of embedding / working with the camera or do it differently - welcome to the comments! </p><a name="habracut"></a><br><p>  Suppose we want to add document recognition functionality to our application, and for this we have the Smart IDReader SDK, which consists of the following parts: </p><br><ul><li> <code>bin</code> - build the <code>libjniSmartIdEngine.so</code> kernel <code>libjniSmartIdEngine.so</code> for 32-bit ARMv7 architecture </li><li>  <code>bin-64</code> - build <code>libjniSmartIdEngine.so</code> kernel <code>libjniSmartIdEngine.so</code> for 64-bit ARMv8 architecture </li><li>  <code>bin-x86</code> - build the <code>libjniSmartIdEngine.so</code> kernel <code>libjniSmartIdEngine.so</code> for 32-bit x86 architecture </li><li>  <code>bindings</code> - JNI <code>jniSmartIdEngineJar.jar</code> wrapper over <code>libjniSmartIdEngine.so</code> library </li><li>  <code>data</code> - kernel configuration files </li><li>  <code>doc</code> - SDK documentation </li></ul><br><p>  Some comments on the content of the SDK. </p><br><p>  The presence of three library assemblies for different platforms is a fee for a wide variety of devices on the Android OS (we do not build for MIPS because there are no devices of this architecture).  The assemblies for ARMv7 and ARMv8 are basic, the x86 version is usually used by our clients for specific devices based on mobile Intel processors. </p><br><p>  JNI (Java Native Interface) <code>jniSmartIdEngineJar.jar</code> is required to call C ++ code from Java applications.  The build of the wrapper is automated using the <a href="https://en.wikipedia.org/wiki/SWIG">SWIG</a> toolkit <a href="https://en.wikipedia.org/wiki/SWIG">(simplified wrapper and interface generator)</a> . </p><br><p>  So, as the French say, revenons √† nos moutons!  We have an SDK and we need to build it into the project with minimal effort and start using it.  This will require the following steps: </p><br><ol><li>  Adding the necessary files to the project </li><li>  Data preparation and engine initialization </li><li>  Connecting the camera to the application </li><li>  Transferring data and getting results </li></ol><br><p>  In order for everyone to play around with the library, we prepared and posted the source code of Smart IDReader Demo for Android on <a href="https://github.com/SmartEngines/SmartIDReader-Android-SDK">Github</a> .  The project is made for Android Studio and shows an example of working with the camera and the core based on a simple application. </p><br><h3 id="dobavlenie-neobhodimyh-faylov-k-proektu">  Adding the necessary files to the project </h3><br><p>  Consider this process on the example of the application project under Android Studio, for users of other IDE process is not particularly different.  By default, in every project, Android Studio creates a <code>libs</code> folder, from which the Gradle collector picks up and adds JAR files to the project.  This is where we copy the JNI <code>jniSmartIdEngineJar.jar</code> wrapper.  There are several ways to add kernel libraries, the easiest way to do this is using a JAR archive.  Create an archive in the <code>libs</code> folder with the name <code>native-libs.jar</code> (this is important!) And inside the archive subfolders <code>lib/armeabi-v7a</code> and <code>lib/arm64-v8a</code> and place the corresponding versions of libraries there (for x86 libraries, the subfolder is <code>lib/x86</code> ). </p><br><p>  In this case, the Android OS after installing the application will automatically deploy the required version of the library for this device.  The accompanying engine configuration files are added to the project‚Äôs assets folder; if this folder is missing, you can create it manually or by using the <code>File|New|Folder|Assets Folder</code> command.  As you can see, adding files to the project is very simple and takes very little time. </p><br><h3 id="podgotovka-dannyh-i-inicializaciya-dvizhka">  Data preparation and engine initialization </h3><br><p>  So, we added the necessary files to the application and even successfully collected it.  Hands and stretch to try a new functionality in, but for this you need a little more work :-) Namely, do the following: </p><br><ul><li>  Deploy kernel configuration files from assets </li><li>  Download the library and initialize the engine </li></ul><br><p>  In order for the library to access the configuration files, it is necessary to transfer them from assets to the working folder of the application.  It is enough to do this once at startup and then update only when a new version is released.  The easiest way to do such a check, based on the version of the application code, and if it has changed then update the files. </p><br><pre> <code class="objectivec hljs"><span class="hljs-comment"><span class="hljs-comment">//     PackageInfo packageInfo = getPackageManager().getPackageInfo(getPackageName(), 0); int version_code = packageInfo.versionCode; SharedPreferences sPref = PreferenceManager.getDefaultSharedPreferences(this); //     int version_current = sPref.getInt("version_code", -1); //       need_copy_assets = version_code != version_current; //      SharedPreferences.Editor ed = sPref.edit(); ed.putInt("version_code", version_code); ed.commit(); ‚Ä¶ if (need_copy_assets == true) copyAssets();</span></span></code> </pre> <br><p>  The copying procedure itself is not complicated and consists in retrieving data from files located in the assets of the application and writing this data to the files in the working directory.  An example of the code of the function that performs this copying can be seen in the example on <a href="https://github.com/SmartEngines/SmartIDReader-Android-SDK">Github</a> . </p><br><p>  It remains only to load the library and initialize the kernel.  The whole procedure takes a certain time, so it is reasonable to perform it in a separate thread, so as not to slow down the main GUI thread.  AsyncTask based initialization example </p><br><pre> <code class="objectivec hljs">private <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> RecognitionEngine engine; private <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> SessionSettings sessionSettings; private <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> RecognitionSession session; ... lass InitCore extends AsyncTask&lt;Void, Void, Void&gt; { @Override protected Void doInBackground(Void... unused) { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (need_copy_assets) copyAssets(); <span class="hljs-comment"><span class="hljs-comment">//   configureEngine(); return null; } @Override protected void onPostExecute(Void aVoid) { super.onPostExecute(aVoid); if(is_configured) { //      (, rus.passport.*     ) sessionSettings.AddEnabledDocumentTypes(document_mask); //      StringVector document_types = sessionSettings.GetEnabledDocumentTypes(); ... } } } ‚Ä¶ private void configureEngine() { try { //    System.loadLibrary("jniSmartIdEngine"); //      String bundle_path = getFilesDir().getAbsolutePath() + File.separator + bundle_name; //   engine = new RecognitionEngine(bundle_path); //    sessionSettings = engine.CreateSessionSettings(); is_configured = true; } catch (RuntimeException e) { ... } catch(UnsatisfiedLinkError e) { ... } }</span></span></code> </pre> <br><h3 id="podklyuchenie-kamery-k-prilozheniyu">  Connecting the camera to the application </h3><br><p>  If your application already uses the camera, you can safely skip this section and go to the next one.  For the rest, let's consider the issue of using the camera for working with video stream for document recognition by means of Smart IDReader.  Immediately make a reservation that we use the class Camera, and not Camera2, although it is declared as deprecated since API version 21 (Android 5.0).  This is done deliberately for the following reasons: </p><br><ul><li>  The Camera class is much easier to use and contains the necessary functionality. </li><li>  Support for older Android 2.3.x and 4.xx devices is still relevant. </li><li>  The Camera class is still well supported, whereas at the beginning of the launch of Android 5.0, many manufacturers had problems with implementing Camera2 </li></ul><br><p>  To add camera support to the application, you need to register the following lines in the manifest: </p><br><pre> <code class="objectivec hljs">&lt;uses-permission android:name=<span class="hljs-string"><span class="hljs-string">"android.permission.CAMERA"</span></span> /&gt; &lt;uses-feature android:name=<span class="hljs-string"><span class="hljs-string">"android.hardware.camera"</span></span> /&gt;</code> </pre> <br><p>  A good tone is to request permission to use the camera, implemented in Android 6.x and higher.  In addition, users of these systems can always take away the permissions from the application in the settings, so you still need to check. </p><br><pre> <code class="objectivec hljs"><span class="hljs-comment"><span class="hljs-comment">//   -   if( needPermission(Manifest.permission.CAMERA) == true ) requestPermission(Manifest.permission.CAMERA, REQUEST_CAMERA); ‚Ä¶ public boolean needPermission(String permission) { //   int result = ContextCompat.checkSelfPermission(this, permission); return result != PackageManager.PERMISSION_GRANTED; } public void requestPermission(String permission, int request_code) { //       ActivityCompat.requestPermissions(this, new String[]{permission}, request_code); } @Override public void onRequestPermissionsResult(int requestCode, String permissions[], int[] grantResults) { switch (requestCode) { case REQUEST_CAMERA: { //       boolean is_granted = false; for(int grantResult : grantResults) { if(grantResult == PackageManager.PERMISSION_GRANTED) //   is_granted = true; } if (is_granted == true) { camera = Camera.open(); //   .... } else toast("Enable CAMERA permission in Settings"); } default: super.onRequestPermissionsResult(requestCode, permissions, grantResults); } }</span></span></code> </pre> <br><p>  An important part of working with the camera is setting its parameters, namely the focus mode and the resolution of the preview.  Due to the wide variety of devices and the characteristics of their cameras, this issue should be given special attention.  If the camera does not support focusing, then you have to work with a fixed focus or directed to infinity.  In this case, especially nothing can be done, we receive images from the camera as is.  And if we are lucky and the focus is available, then we check whether the <code>FOCUS_MODE_CONTINUOUS_PICTURE</code> or <code>FOCUS_MODE_CONTINUOUS_VIDEO</code> modes <code>FOCUS_MODE_CONTINUOUS_PICTURE</code> <code>FOCUS_MODE_CONTINUOUS_VIDEO</code> , which means the constant process of focusing on the <code>FOCUS_MODE_CONTINUOUS_VIDEO</code> in the process.  If these modes are supported, then we set them in the parameters.  If not, then you can make the next trick - start the timer and call the focus function on the camera with a specified frequency. </p><br><pre> <code class="objectivec hljs">Camera.Parameters params = camera.getParameters(); <span class="hljs-comment"><span class="hljs-comment">//     List&lt;String&gt; focus_modes = params.getSupportedFocusModes(); String focus_mode = Camera.Parameters.FOCUS_MODE_AUTO; boolean isAutoFocus = focus_modes.contains(focus_mode); if (isAutoFocus) { if (focus_modes.contains(Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE)) focus_mode = Camera.Parameters.FOCUS_MODE_CONTINUOUS_PICTURE; else if (focus_modes.contains(Camera.Parameters.FOCUS_MODE_CONTINUOUS_VIDEO)) focus_mode = Camera.Parameters.FOCUS_MODE_CONTINUOUS_VIDEO; } else { //          focus_mode = focus_modes.get(0); } //    params.setFocusMode(focus_mode); //          if (focus_mode == Camera.Parameters.FOCUS_MODE_AUTO) { timer = new Timer(); timer.schedule(new Focus(), timer_delay, timer_period); } ‚Ä¶ //    private class Focus extends TimerTask { public void run() { focusing(); } } public void focusing() { try{ Camera.Parameters cparams = camera.getParameters(); //         if( cparams.getMaxNumFocusAreas() &gt; 0) { camera.cancelAutoFocus(); cparams.setFocusMode(Camera.Parameters.FOCUS_MODE_AUTO); camera.setParameters(cparams); } }catch(RuntimeException e) { ... } }</span></span></code> </pre> <br><p>  Setting the preview resolution is quite simple, the basic requirements are for the camera's aspect ratio to match the sides of the display area for no distortion when viewing, and it is desirable that the resolution be as high as possible, since the quality of document recognition depends on it.  In our example, the application displays the preview on full screen, so we choose the maximum resolution corresponding to the aspect ratio of the screen. </p><br><pre> <code class="objectivec hljs">DisplayMetrics metrics = new DisplayMetrics(); getWindowManager().getDefaultDisplay().getMetrics(metrics); <span class="hljs-comment"><span class="hljs-comment">//    float best_ratio = (float)metrics.heightPixels / (float)metrics.widthPixels; List&lt;Camera.Size&gt; sizes = params.getSupportedPreviewSizes(); Camera.Size preview_size = sizes.get(0); //        final float tolerance = 0.1f; float preview_ratio_diff = Math.abs( (float) preview_size.width / (float) preview_size.height - best_ratio); //    preview      for (int i = 1; i &lt; sizes.size() ; i++) { Camera.Size tmp_size = sizes.get(i); float tmp_ratio_diff = Math.abs( (float) tmp_size.width / (float) tmp_size.height - best_ratio); if( Math.abs(tmp_ratio_diff - preview_ratio_diff) &lt; tolerance &amp;&amp; tmp_size.width &gt; preview_size.width || tmp_ratio_diff &lt; preview_ratio_diff) { preview_size = tmp_size; preview_ratio_diff = tmp_ratio_diff; } } //   preview    params.setPreviewSize(preview_size.width, preview_size.height);</span></span></code> </pre> <br><p>  It remains quite a bit - to set the camera orientation and display preview on the surface of the Activity.  By default, the angle of 0 degrees corresponds to the landscape orientation of the device; when the screen rotates, it must be changed accordingly.  Here you can still remember the good word Nexus 5X from Google, the matrix of which is installed in the device upside down and for which a separate orientation test is needed. </p><br><pre> <code class="objectivec hljs">private boolean is_nexus_5x = Build.MODEL.contains(<span class="hljs-string"><span class="hljs-string">"Nexus 5X"</span></span>); SurfaceView surface = (SurfaceView) findViewById(R.id.preview); ... <span class="hljs-comment"><span class="hljs-comment">//   camera.setDisplayOrientation(!is_nexus_5x ? 90: 270); //  preview    camera.setPreviewDisplay(surface.getHolder()); //   preview camera.startPreview();</span></span></code> </pre> <br><h3 id="peredacha-dannyh-i-poluchenie-rezultata">  Transferring data and getting results </h3><br><p>  So, the camera is connected and working, the only thing left is to use the core and get the result.  We start the recognition process by starting a new session and setting a callback to receive frames from the camera in the preview mode. </p><br><pre> <code class="objectivec hljs"><span class="hljs-keyword"><span class="hljs-keyword">void</span></span> start_session() { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (is_configured == <span class="hljs-literal"><span class="hljs-literal">true</span></span> &amp;&amp; camera_ready == <span class="hljs-literal"><span class="hljs-literal">true</span></span>) { <span class="hljs-comment"><span class="hljs-comment">//   ,  - sessionSettings.SetOption("common.sessionTimeout", "5.0"); //    session = engine.SpawnSession(sessionSettings); try { session_working = true; //         frame_waiting = new Semaphore(1, true); frame_ready = new Semaphore(0, true); //      AsyncTask new EngineTask().execute(); } catch (RuntimeException e) { ... } //  callback      camera.setPreviewCallback(this); } }</span></span></code> </pre> <br><p>  The <code>onPreviewFrame()</code> function receives the current image from the camera as an array of YUV NV21 format bytes.  Since it can only be called in the main thread, in order not to slow down the kernel calls for image processing, they are placed in a separate thread using AsyncTask, the process is synchronized using semaphores.  After receiving the image from the camera, we give a signal to the workflow to start its processing, and when it is finished - a signal to receive a new image. </p><br><pre> <code class="objectivec hljs"><span class="hljs-comment"><span class="hljs-comment">//   private static volatile byte[] data; ... @Override public void onPreviewFrame(byte[] data_, Camera camera) { if(frame_waiting.tryAcquire() &amp;&amp; session_working) { data = data_; //      frame_ready.release(); } } ‚Ä¶ class EngineTask extends AsyncTask&lt;Void, RecognitionResult, Void&gt; { @Override protected Void doInBackground(Void... unused) { while (true) { try { frame_ready.acquire(); //    if(session_working == false) //     break; Camera.Size size = camera.getParameters().getPreviewSize(); //        RecognitionResult result = session.ProcessYUVSnapshot(data, size.width, size.height, !is_nexus_5x ? ImageOrientation.Portrait : ImageOrientation.InvertedPortrait); ... //     frame_waiting.release(); }catch(RuntimeException e) { ... } catch(InterruptedException e) { ... } } return null; }</span></span></code> </pre> <br><p>  After processing each image, the kernel returns the current recognition result.  It includes the document areas found, text fields with values ‚Äã‚Äãand confidence flags, as well as graphic fields, such as photographs or captions.  If the data is recognized correctly or a timeout has occurred, the IsTerminal flag is set, signaling the completion of the process.  For intermediate results, you can draw the found zones and fields, show the current progress in recognition quality and much more, it all depends on your imagination. </p><br><pre> <code class="objectivec hljs"><span class="hljs-keyword"><span class="hljs-keyword">void</span></span> show_result(RecognitionResult result) { <span class="hljs-comment"><span class="hljs-comment">//      StringVector texts = result.GetStringFieldNames(); //    ,   ,     StringVector images = result.GetImageFieldNames(); for (int i = 0; i &lt; texts.size(); i++) //    { StringField field = result.GetStringField(texts.get(i)); String value = field.GetUtf8Value(); //   boolean is_accepted = field.IsAccepted(); ..   ... } for (int i = 0; i &lt; images.size(); i++) //    { ImageField field = result.GetImageField(images.get(i)); Bitmap image = getBitmap(field.GetValue()); //  Bitmap ... } ... }</span></span></code> </pre> <br><p>  After that, we can only stop the process of obtaining images from the camera and stop the process of recognition. </p><br><pre> <code class="objectivec hljs"><span class="hljs-keyword"><span class="hljs-keyword">void</span></span> stop_session() { session_working = <span class="hljs-literal"><span class="hljs-literal">false</span></span>; data = null; frame_waiting.release(); frame_ready.release(); camera.setPreviewCallback(null); <span class="hljs-comment"><span class="hljs-comment">//       ... }</span></span></code> </pre> <br><h3 id="zaklyuchenie">  Conclusion </h3><br><p>  As you can see in our example, the process of connecting the Smart IDReader SDK to Android applications and working with the camera are not difficult, just follow some rules.  A number of our customers successfully apply our technologies in their mobile applications, and the process of adding new functionality takes very little time.  We hope with the help of this article and you could be convinced of it! </p><br><p>  PS To see how Smart IDReader looks like in our performance after embedding, you can download free full versions of applications from the <a href="https://itunes.apple.com/ru/app/smart-idreader/id1157877082">App Store</a> and <a href="https://play.google.com/store/apps/details%3Fid%3Dbiz.smartengines.smartid">Google Play</a> . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/332670/">https://habr.com/ru/post/332670/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../332658/index.html">Personal experience: as an IT specialist, move to work in the USA, relying only on himself</a></li>
<li><a href="../332660/index.html">Script for express recovery Excel files after damage</a></li>
<li><a href="../332662/index.html">Interview in SD podCast with Pavel Odintsov, author of FastNetMon, a tool for detecting and repelling DDoS attacks</a></li>
<li><a href="../332664/index.html">Automata programming. Part 3. State and transition diagram. Continuation</a></li>
<li><a href="../332668/index.html">How to confuse analytics - 4. Probability and accuracy</a></li>
<li><a href="../332672/index.html">Google will soon cease to trust all certificates WoSign and StartCom</a></li>
<li><a href="../332674/index.html">Why I switched from React to Cycle.js</a></li>
<li><a href="../332676/index.html">3D mouse integration in Renga</a></li>
<li><a href="../332678/index.html">Understanding oracles in the blockchain</a></li>
<li><a href="../332682/index.html">Oracle Data Integrator. SubstitutionAPI: The order of the substitutions. Part 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>