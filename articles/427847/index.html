<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Curiosity and procrastination in machine learning</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Reinforcement training (RL) is one of the most promising machine learning techniques that is currently being actively developed. Here the AI ‚Äã‚Äãagent r...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Curiosity and procrastination in machine learning</h1><div class="post__text post__text-html js-mediator-article">  <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement training</a> (RL) is one of the most promising machine learning techniques that is currently being actively developed.  Here the AI ‚Äã‚Äãagent receives a positive reward for the correct actions, and a negative reward for the wrong ones.  This method of <a href="https://en.wikipedia.org/wiki/Carrot_and_stick">carrot and stick</a> is simple and versatile.  With it, DeepMind taught the <a href="https://deepmind.com/research/dqn/">DQN</a> algorithm to play the old Atari video games, and <a href="https://deepmind.com/blog/alphago-zero-learning-scratch/">AlphaGoZero the</a> ancient Go game.  So OpenAI taught the <a href="https://blog.openai.com/openai-five/">OpenAI-Five</a> algorithm to play the modern Dota video game, and Google taught robotic hands to <a href="https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html">grab new objects</a> .  Despite the success of RL, there are still many problems that reduce the effectiveness of this technique. <br><br>  RL algorithms are <a href="https://pathak22.github.io/noreward-rl/">hard to work</a> in an environment where the agent rarely receives feedback.  But this is typical of the real world.  As an example, imagine searching for your favorite cheese in a large maze like a supermarket.  You are looking for and looking for a department with cheeses, but you can‚Äôt find it.  If at each step you receive neither a ‚Äústick‚Äù nor a ‚Äúcarrot‚Äù, then it is impossible to say whether you are moving in the right direction.  In the absence of a reward, what keeps you from wandering around forever?  Nothing, except perhaps your curiosity.  It motivates to go to the grocery department, which looks unfamiliar. <br><a name="habracut"></a><br>  The scientific work <a href="https://arxiv.org/abs/1810.02274">"Episodic curiosity through attainability"</a> is the result of collaboration between <a href="https://ai.google/research/teams/brain">the Google Brain team</a> , <a href="https://deepmind.com/">DeepMind</a> and the <a href="https://www.ethz.ch/en.html">Swiss Technical School of Zurich</a> .  We offer a new episodic memory-based RL reward model.  She looks like a curiosity that allows you to explore the environment.  Since the agent must not only study the environment, but also solve the original problem, our model adds a bonus to the initially sparse reward.  The combined reward is no longer sparse, which allows standard RL algorithms to learn from it.  Thus, our method of curiosity extends the set of problems solved with the help of RL. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4e9/462/46c/4e946246c97aafab60f3dbe954ca6ed2.png"><br>  <i><font color="gray">Episodic curiosity through attainability: observational data is added to memory, the reward is calculated based on how far the current observation is from similar observations in memory.</font></i>  <i><font color="gray">The agent receives a greater reward for observations that are not yet represented in the memory.</font></i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The main idea of ‚Äã‚Äãthe method is to store the observations of the environmental agent in episodic memory, as well as to reward the agent for viewing observations that are not yet represented in memory.  ‚ÄúLack of memory‚Äù is the definition of novelty in our method.  The search for such observations means the search for a stranger.  Such a desire to search for a stranger will lead the AI ‚Äã‚Äãagent to new locations, thereby preventing him from wandering around in a circle, and ultimately helps him stumble upon a target.  As we will discuss later, our formulation can keep an agent from undesirable behavior, which some other formulations are subject to.  Much to our surprise, this behavior has some similarities with what a non-expert would call ‚Äúprocrastination.‚Äù <br><br><h4>  Previous curiosity wording </h4><br>  Although in the past there have been many attempts to formulate curiosity <sup>[1] [2] [3] [4]</sup> , in this article we will focus on one natural and very popular approach: curiosity through surprise based on prediction.  This technique is described in a recent article, <a href="https://pathak22.github.io/noreward-rl/">‚ÄúSurveying the environment with the help of curiosity through prediction under independent control‚Äù</a> (usually referred to as ICM).  To illustrate the connection of surprise with curiosity, we again use the analogy of cheese search in the supermarket. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b93/003/a3c/b93003a3c6790968f3922777926a494a.jpg"><br>  <i><font color="gray">Illustration of <a href="https://www.behance.net/gallery/71741137/Illustration-for-an-article-in-aigoogleblogcom">Indira Pasko</a> , under license <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.en_US">CC BY-NC-ND 4.0</a></font></i> <br><br>  Wandering around the store, you are trying to predict the future ( <i>"Now I am in the meat department, so I think that the department around the corner is the fish department, they are usually near this supermarket chain"</i> ).  If the forecast is incorrect, you are surprised ( <i>‚ÄúActually, the vegetables department is here. I didn‚Äôt expect it!‚Äù</i> ) - and thus you get a reward.  This increases the motivation in the future to look around the corner again, exploring new places just to check that your expectations correspond to reality (and, perhaps, stumble upon cheese). <br><br>  Similarly, the ICM method builds a predictive model of the dynamics of the world and gives the agent a reward if the model fails to make good predictions - a marker of surprise or novelty.  Please note that exploring new places is not directly articulated in ICM curiosity.  For ICM, visiting them is only a way to get more ‚Äúsurprises‚Äù and thus maximize the overall reward.  As it turns out, in some environments there may be other ways to cause surprise, which leads to unexpected results. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a56/253/b56/a56253b56c9a991ce20b2c744f42d5a9.gif"></div><br>  <i><font color="gray">An agent with a curiosity system based on surprise hangs when meeting a TV.</font></i>  <i><font color="gray">Animation from <a href="https://youtu.be/C3yKgCzvE_E">Deepak Patac</a> 's video, used under license <a href="https://creativecommons.org/licenses/by/2.0/">CC BY 2.0</a></font></i> <br><br><h4>  The danger of "procrastination" </h4><br>  In the article <a href="https://pathak22.github.io/large-scale-curiosity/resources/largeScaleCuriosity2018.pdf">‚ÄúLarge-scale study of curiosity-based learning,‚Äù the</a> authors of the ICM method along with researchers from OpenAI show the hidden danger of maximizing surprise: agents can learn to indulge in procrastination instead of doing something useful for the task.  To understand why this is so, consider a thought experiment, which the authors call the "problem of television noise."  Here the agent is placed in a labyrinth with the task of finding a very useful item (like ‚Äúcheese‚Äù in our example).  The environment has a TV, and the agent has a remote control.  There is a limited number of channels (on each individual program), and each press on the remote switches the TV to a random channel.  How will the agent act in such an environment? <br><br>  If curiosity is formed on the basis of surprise, then changing channels will give more reward, since every change is unpredictable and unexpected.  It is important to note that even after a cyclic review of all available channels, random selection of the channel ensures that each new change will still be unexpected - the agent makes a prediction that he will show TV after switching the channel, and most likely the prediction will be wrong, which will cause surprise.  It is important to note that even if the agent has already seen every program on every channel, the change is still unpredictable.  Because of this, the agent, instead of looking for a very useful item, will eventually remain in front of the TV - it looks like a procrastination.  How to change the wording of curiosity to prevent such behavior? <br><br><h4>  Episodic curiosity </h4><br>  In the article <a href="https://arxiv.org/abs/1810.02274">‚ÄúEpisodic curiosity through attainability,‚Äù</a> we explore the episodic curiosity model based on memory, which is less prone to instant pleasures.  Why is that?  If we take the example above, then after a while the channel is switched all the transmissions will eventually end up in memory.  Thus, the TV will lose its attractiveness: even if the order of the programs on the screen is random and unpredictable, they are all in memory!  This is the main difference from the method based on surprise: our method is not even trying to predict the future, it is difficult (or even impossible) to predict it.  Instead, the agent examines the past and checks whether there are observations in the memory <i>like the</i> current one.  Thus, our agent is not inclined to instant pleasures, which are given by ‚Äútelevision noise‚Äù.  The agent will have to go and explore the world outside of the TV in order to get more rewards. <br><br>  But how do we decide whether the agent sees the same thing that is stored in memory?  Checking the exact match is meaningless: in a real environment, an agent rarely sees the same thing twice.  For example, even if the agent returns to the same room, he will still see this room from a different angle. <br><br>  Instead of checking the exact match, we use a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural network</a> that is trained to measure how similar the two experiences are.  To train this network, we must guess how closely the observations occurred.  Proximity is a good indicator of whether two observations should be considered part of the same thing.  Such training leads to a general concept of novelty through attainability, which is illustrated below. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/52c/f43/629/52cf436298b7bbf44550ba9770e84f1b.png"><br>  <i><font color="gray">Reach graph determines novelty.</font></i>  <i><font color="gray">In practice, this graph is not available - so we train the neural network approximator to estimate the number of steps between observations.</font></i> <br><br><h4>  Experimental Results </h4><br>  To compare the performance of different approaches to describing curiosity, we tested them in two visually rich 3D environments: <a href="https://arxiv.org/abs/1605.02097">ViZDoom</a> and <a href="https://arxiv.org/abs/1612.03801">DMLab</a> .  Under these conditions, the agent was assigned various tasks, such as finding a target in a maze, collecting good objects and evading bad ones.  In the DMLab environment, the agent is equipped by default with a fantastic gadget like a laser, but if the gadget is not needed for a specific task, the agent can not use it freely.  Interestingly, an ICM agent based on surprise actually used a laser very often, even if it was useless for the task!  As in the case of the TV, instead of searching for a valuable item in the maze, he chose to spend time shooting at the walls, because it gave a lot of reward in the form of surprise.  Theoretically, the result of firing on the walls should be predictable, but in practice it is too difficult to predict.  This probably requires a deeper knowledge of physics than is available to the standard AI agent. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/497/4e6/be3/4974e6be39718b8ab6dbd9cdfcdab273.gif"></div><br>  <i><font color="gray">An ICM agent based on surprise constantly firing into the wall instead of exploring the maze</font></i> <br><br>  In contrast, our agent has mastered sensible environmental behavior.  This happened because he does not try to predict the result of his actions, but rather looks for observations that are ‚Äúfarther‚Äù from those that are in episodic memory.  In other words, the agent implicitly pursues goals that require more effort than a simple shot at the wall. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b5a/002/381/b5a00238194970be7a6d6cf8969ac7f7.gif"></div><br>  <i><font color="gray">Our method demonstrates reasonable environmental behavior.</font></i> <br><br>  It is interesting to observe how our approach to remuneration punishes an agent running in a circle, because after completing the first circle, the agent does not encounter new observations and, thus, does not receive any reward: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/489/ef2/a8b/489ef2a8b67c8c087cff940bdfeeee9f.gif"></div><br>  <i><font color="gray">The visualization of the award: red corresponds to negative reward, Green to positive.</font></i>  <i><font color="gray">From left to right: an award map, a map with locations in memory, first-person view</font></i> <br><br>  At the same time, our method promotes good environmental research: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/075/ae3/952/075ae39528e77477099fad13610d2e4a.gif"></div><br>  <i><font color="gray">The visualization of the award: red corresponds to negative reward, Green to positive.</font></i>  <i><font color="gray">From left to right: an award map, a map with locations in memory, first-person view</font></i> <br><br>  We hope that our work contributes to a new wave of research that will go beyond the scope of the technique of surprise, in order to train agents to more intelligent behavior.  For an in-depth analysis of our method, please take a look at the <a href="https://arxiv.org/abs/1810.02274">preprint of scientific work</a> . <br><br><h4>  Thanks: </h4><br>  This project is the result of a collaboration between the Google Brain team, DeepMind and the Swiss High School of Zurich.  The main research team: Nikolay Savinov, Anton Raichuk, Raphael Marinier, Damien Vincent, Mark Pollefeys, Timothy Lillicrap and Sylvain Zheli.  We wanted to thank Olivier Pietkina, Carlos Riquelme, Charles Blundell and Sergei Levine for discussing this document.  We are grateful to Indira Pasco for helping with the illustrations. <br><br><h4>  References to the literature: </h4><br>  [1] <a href="https://arxiv.org/abs/1703.01310">‚ÄúStudy of the environment on the basis of counting with neuron density models‚Äù</a> , Georg Ostrovsky, Mark G. Bellemar, Aaron Van den Oord, Remi Munoz <br>  [2] <a href="https://arxiv.org/abs/1611.04717">‚ÄúLearning the environment on the basis of counting for in-depth training with reinforcements‚Äù</a> , Khaoran Tan, Rein Huthuft, Davis Foote, Adam Knock, Xi Chen, Yan Duan, John Schulman, Philip de Tours, Peter Abbel <br>  [3] <a href="https://arxiv.org/abs/1803.00781">‚ÄúTeaching without a teacher the location of goals for internally motivated research‚Äù</a> , Alexander Pere, Sebastien Forestier, Olivier Sego, Pierre-Yves Udeye <br>  [4] <a href="https://arxiv.org/abs/1605.09674">‚ÄúVIME: intelligence with the maximization of changing information‚Äù</a> , Rein Huthuft, Xi Chen, Yan Duan, John Schulman, Philippe de Turk, Peter Abbel </div><p>Source: <a href="https://habr.com/ru/post/427847/">https://habr.com/ru/post/427847/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../427837/index.html">The first days in the development team - as it happens with us</a></li>
<li><a href="../427839/index.html">Authorization of users in Django through GSSAPI and delegation of user rights to the server</a></li>
<li><a href="../427841/index.html">Scam company Magic Leap</a></li>
<li><a href="../427843/index.html">How to sleep right and wrong</a></li>
<li><a href="../427845/index.html">How to fit in the iPhone million stars</a></li>
<li><a href="../427849/index.html">Straight line with TM. v3.0</a></li>
<li><a href="../427851/index.html">Mortal Kombat source leak</a></li>
<li><a href="../427853/index.html">Reflections on TDD. Why this methodology is not widely accepted</a></li>
<li><a href="../427855/index.html">MOSDROID mitap in FunCorp</a></li>
<li><a href="../427857/index.html">Tax and legal issues for beginner freelancers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>