<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Fast TCP Sockets on Erlang</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Processing TCP connections can easily be a bottleneck when speed approaches 10,000 requests per second: effective reading and writing becomes a separa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Fast TCP Sockets on Erlang</h1><div class="post__text post__text-html js-mediator-article">  Processing TCP connections can easily be a bottleneck when speed approaches 10,000 requests per second: effective reading and writing becomes a separate problem, and most of the cores are idle. <br><br>  In this article, I propose optimizations that improve the three components of working with TCP: receiving connections, receiving messages and responding to them. <br><br>  The article is addressed to both Erlang programmers and everyone who is simply interested in Erlang.  Profound knowledge of the language is not required. <br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I divide ‚ÄúWork with TCP‚Äù into three parts: <br><ol><li>  Receiving connections </li><li>  Receive messages </li><li>  Reply to messages </li></ol><br>  Depending on the task, any of these parts may be the bottleneck. <br><br>  I will consider two approaches to writing TCP services - directly through <a href="http://erlang.org/doc/man/gen_tcp.html">gen_tcp</a> and with the help of <a href="https://github.com/ninenines/ranch">ranch</a> , the most popular library for connection pools on Erlang.  Some of the proposed optimizations will be applicable only in one of the cases. <br><br>  In order to evaluate the performance change, I use <a href="https://github.com/machinezone/mzbench">MZBench</a> with tcp_worker, which implements the connect and request functions plus the synchronization functions.  Two scripts will be used: fast_connect and fast_receive.  The first one opens connections with a linearly increasing speed, and the second one tries to send as many packets as possible to the already open connections.  Each of the scripts was run on c4.2xlarge Amazon node.  Erlang version - 18. <br><br>  Scripts and feature codes for MZBench <a href="https://github.com/parsifal-47/server-examples">are available on GitHub</a> . <br><br><h2>  Receiving connections </h2><br>  Fast acceptance of connections is important if you have many clients that are constantly reconnected, for example, if client processes are very limited in time or do not support persistent connections. <br><br><h3>  Optimize ranch </h3><br>  TCP services using <a href="https://github.com/ninenines/ranch">ranch</a> are pretty simple.  I will change the code for the example <a href="https://github.com/ninenines/ranch/tree/master/examples/tcp_echo">echo service that comes with the ranch</a> to answer ‚Äúok‚Äù to any incoming packet, below the difference: <br><br><pre><code class="erlang hljs">--- a/examples/tcp_echo/src/echo_protocol.erl +++ b/examples/tcp_echo/src/echo_protocol.erl @@ -<span class="hljs-number"><span class="hljs-number">16</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span> +<span class="hljs-number"><span class="hljs-number">16</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span> @@ init(Ref, Socket, Transport, _Opts = []) -&gt; loop(Socket, Transport) -&gt; <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> Transport:recv(Socket, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">5000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> - {ok, Data} -&gt; - Transport:send(Socket, Data), + {ok, _Data} -&gt; + Transport:send(Socket, &lt;&lt;<span class="hljs-string"><span class="hljs-string">"ok"</span></span>&gt;&gt;), loop(Socket, Transport); _ -&gt; ok = Transport:close(Socket) --- a/examples/tcp_echo/src/tcp_echo_app.erl +++ b/examples/tcp_echo/src/tcp_echo_app.erl @@ -<span class="hljs-number"><span class="hljs-number">11</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span> +<span class="hljs-number"><span class="hljs-number">11</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span> @@ <span class="hljs-comment"><span class="hljs-comment">%% API. start(_Type, _Args) -&gt; - {ok, _} = ranch:start_listener(tcp_echo, 1, - ranch_tcp, [{port, 5555}], echo_protocol, []), + {ok, _} = ranch:start_listener(tcp_echo, 100, + ranch_tcp, [{port, 5555}, {max_connections, infinity}], echo_protocol, []), tcp_echo_sup:start_link().</span></span></code> </pre> <br><br>  I'll start by running the ‚Äúfast_connect‚Äù script (at an increasing rate of opening connections): <br><img src="https://habrastorage.org/files/ec5/58c/52f/ec558c52f8d94558bb2d09c31c0bb6e3.png"><br><br>  The graph on the left shows a burst of 214ms in size, the other lines correspond to percentile time delays divided into five-second intervals.  The graph on the right is the speed at which connections are open, for example, in the release area, it was about 3.5 thousand connections per second.  In this scenario, one message is sent each time, so the number of messages corresponds to the number of open connections. <br><br>  A further increase in speed gives the following results: <br><br><img src="https://habrastorage.org/files/89c/5cc/ea0/89c5ccea098a4f0d92ac3d9bba1a4e49.png"><br><br>  Emissions of 1000 msec correspond to timeout.  If you continue to increase the opening rate of the compounds, emissions will become more frequent.  The first outliers appear at a speed of 5k rps and are constantly present at a speed of 11k rps. <br><br><h4>  Replacing timeout when receiving a packet with timer: sleep () </h4><br>  I found that the simple exception of the timeout parameter when receiving a message greatly increases the speed of establishing connections.  In order not to poll the socket with the maximum speed, I added a timer: sleep (20): <br><br><pre> <code class="erlang hljs">--- a/examples/tcp_echo/src/echo_protocol.erl +++ b/examples/tcp_echo/src/echo_protocol.erl @@ -<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span> +<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">11</span></span> @@ init(Ref, Socket, Transport, _Opts = []) -&gt; loop(Socket, Transport). loop(Socket, Transport) -&gt; - <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> Transport:recv(Socket, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">5000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> - {ok, Data} -&gt; - Transport:send(Socket, Data), + <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> Transport:recv(Socket, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> + {ok, _Data} -&gt; + Transport:send(Socket, &lt;&lt;<span class="hljs-string"><span class="hljs-string">"ok"</span></span>&gt;&gt;), loop(Socket, Transport); + {error, timeout} -&gt; timer:sleep(<span class="hljs-number"><span class="hljs-number">20</span></span>), loop(Socket, Transport); _ -&gt; ok = Transport:close(Socket) <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>.</code> </pre><br><br>  With this optimization, the ranch application can take more soda, the first burst appears only at 11k rps: <br><br><img src="https://habrastorage.org/files/4f5/59c/2ca/4f559c2ca8694cc6a4b03ec32434984c.png"><br><br>  Emissions become even greater if you try to increase the speed further.  Thus, the maximum number is 24k rps. <br><br>  <b>Conclusion</b> <br>  With the proposed optimization, I got about double the gain in receiving connections, from 11k to 24k rps. <br><br><h3>  Gen_tcp optimization </h3><br>  Below is a clean implementation using gen_tcp, similar to what I did with ranch (the text is available as <a href="">simple.erl</a> in the repository with examples): <br><br><pre> <code class="erlang hljs"><span class="hljs-keyword"><span class="hljs-keyword">-export</span></span><span class="hljs-params"><span class="hljs-params">([service/</span><span class="hljs-number"><span class="hljs-params"><span class="hljs-number">1</span></span></span><span class="hljs-params">])</span></span>. -define(Options, [ binary, {backlog, <span class="hljs-number"><span class="hljs-number">128</span></span>}, {active, false}, {buffer, <span class="hljs-number"><span class="hljs-number">65536</span></span>}, {keepalive, true}, {reuseaddr, true} ]). -define(Timeout, <span class="hljs-number"><span class="hljs-number">5000</span></span>). main([Port]) -&gt; {ok, ListenSocket} = gen_tcp:listen(list_to_integer(Port), ?Options), accept(ListenSocket). accept(ListenSocket) -&gt; <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> gen_tcp:accept(ListenSocket) <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> {ok, Socket} -&gt; erlang:spawn(?MODULE, service, [Socket]), accept(ListenSocket); {error, closed} -&gt; ok <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>. service(Socket) -&gt; <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> gen_tcp:recv(Socket, <span class="hljs-number"><span class="hljs-number">0</span></span>, ?Timeout) <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> {ok, _Binary} -&gt; gen_tcp:send(Socket, &lt;&lt;<span class="hljs-string"><span class="hljs-string">"ok"</span></span>&gt;&gt;), service(Socket); _ -&gt; gen_tcp:close(Socket) <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>.</code> </pre><br><br>  Running the same script, I got the results: <br><br><img src="https://habrastorage.org/files/338/7fd/e6b/3387fde6bff94c05850de745baaf7012.png"><br><br>  As you can see, approximately around 18k rps, the reception of connections becomes unreliable.  We assume that it turns out to take 18k. <br><br><h4>  Replacing timeout when receiving a packet with timer: sleep () </h4><br>  I will simply apply the same optimization as for the ranch: <br><br><pre> <code class="erlang hljs"><span class="hljs-function"><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">service</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Socket)</span></span></span><span class="hljs-function"> -&gt;</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> gen_tcp:recv(Socket, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> {ok, _Binary} -&gt; gen_tcp:send(Socket, &lt;&lt;<span class="hljs-string"><span class="hljs-string">"ok"</span></span>&gt;&gt;), service(Socket); {error, timeout} -&gt; timer:sleep(<span class="hljs-number"><span class="hljs-number">20</span></span>), service(Socket); _ -&gt; gen_tcp:close(Socket) <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>.</code> </pre><br><br>  In this case, it turns out to handle 23k rps: <br><br><img src="https://habrastorage.org/files/23d/ef7/84d/23def784d1ad4a27b121aeb7220c4acf.png"><br><br><h4>  Add host processes </h4><br>  The second idea is to increase the number of processes accepting a connection.  This can be achieved by calling gen_tcp: accept from several processes: <br><br><pre> <code class="erlang hljs"><span class="hljs-function"><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">([Port])</span></span></span><span class="hljs-function"> -&gt;</span></span> {ok, ListenSocket} = gen_tcp:listen(list_to_integer(Port), ?Options), erlang:spawn(?MODULE, accept, [ListenSocket]), erlang:spawn(?MODULE, accept, [ListenSocket]), accept(ListenSocket).</code> </pre><br><br>  Testing under load gives 32k rps: <br><br><img src="https://habrastorage.org/files/ad5/4fe/b71/ad54feb71e02469881525813323a94ba.png"><br><br>  With further increase in load, delays grow. <br><br>  <b>Conclusion</b> <br>  Optimizing the timeout for gen_tcp increases the reception speed by 5k rps, from 18k to 23k. <br>  With multiple host processes, gen_tcp handles 32k rps, which is 1.8 times more than without optimizations. <br><br><h3>  Results </h3><br><ul><li>  It is better not to use the timeout parameter in the call function, timer: sleep is better.  This applies to both the ranch and the pure gen_tcp.  For ranch, this doubles the rate at which connections are received. </li><li>  From several processes, connections are accepted faster.  This is applicable only for pure gen_tcp.  In my case, this gave a 40% improvement in the speed of receiving connections in combination with replacing timeout with timer: sleep (). </li></ul><br><br><h2>  Receive messages </h2><br>  This is part of how to receive a large number of short messages on already established connections.  New connections rarely open; you need to read and respond to messages as quickly as possible.  This script is implemented in loaded applications with web sockets. <br><br>  I open 25k connections from several nodes and gradually increase the speed of sending messages. <br><br><h3>  Ranch optimization </h3><br>  Below are the results for a non-optimized code using a ranch (on the left are time delays, on the right are the message processing speed): <br><img src="https://habrastorage.org/files/e70/328/f74/e70328f747e644f38d415b915d4fb468.png"><br><br>  Without optimizations, the ranch processes 70k rps with a maximum time delay of 800ms. <br><br><h4>  Increase linux buffers </h4><br>  A fairly popular optimization is <a href="http://www.linux-admins.net/2010/09/linux-tcp-tuning.html">increasing linux buffers for sockets</a> .  Let's see how this optimization will affect the results: <br><br><img src="https://habrastorage.org/files/63d/7fb/486/63d7fb486bc04fde99d6014ebf562916.png"><br><br>  <b>Conclusion</b> <br>  In this case, increasing buffers does not give a big win. <br><br><h3>  Get_tcp optimization </h3><br>  Below, I checked the packet processing speed with the gen_tcp solution from the previous part of the article: <br><br><img src="https://habrastorage.org/files/de8/28c/116/de828c116f934d15b4bf79f798b58adc.png"><br>  70k rps, as well as ranch. <br><br><h4>  Reducing the number of reading processes </h4><br>  In the previous case, I have 25k processes read from sockets - one process per connection.  Now I will try to reduce this amount and check the results. <br><br>  I will create 100 processes and will allocate new sockets between them: <br><br><pre> <code class="erlang hljs"><span class="hljs-function"><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">([Port])</span></span></span><span class="hljs-function"> -&gt;</span></span> {ok, ListenSocket} = gen_tcp:listen(list_to_integer(Port), ?Options), Readers = [erlang:spawn(?MODULE, reader, []) || _X &lt;- lists:seq(<span class="hljs-number"><span class="hljs-number">1</span></span>, ?Readers)], accept(ListenSocket, Readers, []). accept(ListenSocket, [], Reversed) -&gt; accept(ListenSocket, lists:reverse(Reversed), []); accept(ListenSocket, [Reader | Rest], Reversed) -&gt; <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> gen_tcp:accept(ListenSocket) <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> {ok, Socket} -&gt; Reader ! Socket, accept(ListenSocket, Rest, [Reader | Reversed]); {error, closed} -&gt; ok <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>. reader() -&gt; reader([]). read_socket(S) -&gt; <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> gen_tcp:recv(S, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> {ok, _Binary} -&gt; gen_tcp:send(S, &lt;&lt;<span class="hljs-string"><span class="hljs-string">"ok"</span></span>&gt;&gt;), <span class="hljs-literal"><span class="hljs-literal">true</span></span>; {error, timeout} -&gt; <span class="hljs-literal"><span class="hljs-literal">true</span></span>; _ -&gt; gen_tcp:close(S), <span class="hljs-literal"><span class="hljs-literal">false</span></span> <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>. reader(Sockets) -&gt; Sockets2 = lists:filter(<span class="hljs-keyword"><span class="hljs-keyword">fun</span></span> read_socket/<span class="hljs-number"><span class="hljs-number">1</span></span>, Sockets), <span class="hljs-keyword"><span class="hljs-keyword">receive</span></span> S -&gt; reader([S | Sockets2]) <span class="hljs-keyword"><span class="hljs-keyword">after</span></span> ?SmallTimeout -&gt; reader(Sockets) <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>.</code> </pre><br><br>  This optimization provides significant performance gains: <br><br><img src="https://habrastorage.org/files/ebc/321/f76/ebc321f76c554bed8d524fe543d9decd.png"><br><br>  In addition to increasing the speed, the time delays look much better, and the number of packets processed is about 100k, in addition, even 120k messages can be processed, but with large time delays.  While without optimization this was not possible. <br><br>  <b>Conclusion</b> <br>  Processing multiple connections from a single process gives at least a 50% increase in performance for a pure gen_tcp server. <br><br><h4>  Increase Linux Buffers </h4><br>  I will apply the same optimization to the system with the vanilla gen_tcp script: <br><img src="https://habrastorage.org/files/5f0/b29/459/5f0b294595274f35bbb15e683bdec410.png"><br><br>  As in the case of the ranch, no significant results are visible, only additional emissions appeared in the form of large time delays. <br><br>  By applying optimization to an already optimized gen_tcp, I get a lot of time-lapse emissions: <br><br><img src="https://habrastorage.org/files/cb7/314/577/cb7314577ae049c484bcd1a016a78265.png"><br><br>  <b>Conclusion</b> <br>  Pure gen_tcp solutions also do not benefit from increased Linux buffers.  Reducing the number of processes reading from sockets gives a 50% gain in processing speed. <br><br><h3>  Results </h3><br><ul><li>  Initially, both solutions allow processing about the same number of messages, about 70k rps. </li><li>  An increase in buffers does not significantly increase the processing speed and in the case of pure gen_tcp adds choices in the form of large time delays. </li><li>  Gen_tcp solution with several sockets per process runs at least 1.5 times faster than non-optimized and has much better time delays.  Unfortunately, this optimization is not applicable to ranch without changing its architecture. </li></ul><br><br><h2>  Reply to messages </h2><br>  Formally, in previous chapters, the message processing cycle suggested an answer to it, but I did not do something to optimize this part.  I will try to apply the same ideas to the posting functions.  Here I use the script from the previous chapter, in which the packets go over already established connections. <br><br><h3>  Timeout and process optimization </h3><br>  The same ideas that I used in previous chapters can be applied to the send function: remove the timeout and respond to fewer processes.  There is no such parameter as timeout in the send function, you need to set the {send_timeout, 0} option when opening the connection. <br><br>  Unfortunately, this optimization practically does not change anything, and changing the code boils down to simply adding an option, for this reason I decided not to bother the reader with the diff and schedule. <br><br>  To check how the number of processes affects, I used the following script: <br><br><pre> <code class="erlang hljs"><span class="hljs-keyword"><span class="hljs-keyword">-export</span></span><span class="hljs-params"><span class="hljs-params">([responder/</span><span class="hljs-number"><span class="hljs-params"><span class="hljs-number">0</span></span></span><span class="hljs-params">, service/</span><span class="hljs-number"><span class="hljs-params"><span class="hljs-number">2</span></span></span><span class="hljs-params">])</span></span>. -define(Options, [ binary, {backlog, <span class="hljs-number"><span class="hljs-number">128</span></span>}, {active, false}, {buffer, <span class="hljs-number"><span class="hljs-number">65536</span></span>}, {keepalive, true}, {send_timeout, <span class="hljs-number"><span class="hljs-number">0</span></span>}, {reuseaddr, true} ]). -define(SmallTimeout, <span class="hljs-number"><span class="hljs-number">50</span></span>). -define(Timeout, <span class="hljs-number"><span class="hljs-number">5000</span></span>). -define(Responders, <span class="hljs-number"><span class="hljs-number">200</span></span>). main([Port]) -&gt; {ok, ListenSocket} = gen_tcp:listen(list_to_integer(Port), ?Options), Responders = [erlang:spawn(?MODULE, responder, []) || _X &lt;- lists:seq(<span class="hljs-number"><span class="hljs-number">1</span></span>, ?Responders)], accept(ListenSocket, Responders, []). accept(ListenSocket, [], Reversed) -&gt; accept(ListenSocket, lists:reverse(Reversed), []); accept(ListenSocket, [Responder | Rest], Reversed) -&gt; <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> gen_tcp:accept(ListenSocket) <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> {ok, Socket} -&gt; erlang:spawn(?MODULE, service, [Socket, Responder]), accept(ListenSocket, Rest, [Responder | Reversed]); {error, closed} -&gt; ok <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>. responder() -&gt; <span class="hljs-keyword"><span class="hljs-keyword">receive</span></span> S -&gt; gen_tcp:send(S, &lt;&lt;<span class="hljs-string"><span class="hljs-string">"ok"</span></span>&gt;&gt;), responder() <span class="hljs-keyword"><span class="hljs-keyword">after</span></span> ?SmallTimeout -&gt; responder() <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>. service(Socket, Responder) -&gt; <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> gen_tcp:recv(Socket, <span class="hljs-number"><span class="hljs-number">0</span></span>, ?Timeout) <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> {ok, _Binary} -&gt; Responder ! Socket, service(Socket, Responder); _ -&gt; gen_tcp:close(Socket) <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>.</code> </pre><br><br>  Here, the responding processes are shared with the readers;  I have 25,000 readers and 200 respondents. <br><br>  But again, this optimization does not show a significant performance increase compared to the gen_tcp solution from the previous section: <br><img src="https://habrastorage.org/files/8b5/bd9/f25/8b5bd9f25d654e8da1677fbc24ac7da3.png"><br><br><h3>  Tuning Erlang </h3><br>  If one process is used to work with several sockets, one slow client can slow down all the others.  In order to avoid such a situation, you can set {send_timeout, 0} when opening a socket and, in case of failure, repeat sending in the next cycle. <br><br>  Unfortunately, the send function does not return the number of bytes sent.  Only a POSIX error is returned, or an ‚Äúok‚Äù atom.  This makes it impossible to send from the last successfully sent byte.  In addition, knowing this amount you can use the network more efficiently, which becomes especially important if customers have bad channels. <br><br>  Next, I give an example of how this can be fixed: <br><br><ol><li>  Download the Erlang source from the official site: <br><pre> <code class="bash hljs">$ wget http://erlang.org/download/otp_src_18.2.1.tar.gz $ tar -xf otp_src_18.2.1.tar.gz $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> otp_src_18.2.1</code> </pre><br></li><li>  Update the inet erts / emulator / drivers / common / inet_drv.c driver function: <br><ol><li>  Add the ability to respond with a number: <br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">inet_reply_ok_int</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(inet_descriptor* desc, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> Val)</span></span></span><span class="hljs-function"> </span></span>{ ErlDrvTermData spec[<span class="hljs-number"><span class="hljs-number">2</span></span>*LOAD_ATOM_CNT + <span class="hljs-number"><span class="hljs-number">2</span></span>*LOAD_PORT_CNT + <span class="hljs-number"><span class="hljs-number">2</span></span>*LOAD_TUPLE_CNT]; ErlDrvTermData caller = desc-&gt;caller; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i = LOAD_ATOM(spec, i, am_inet_reply); i = LOAD_PORT(spec, i, desc-&gt;dport); i = LOAD_ATOM(spec, i, am_ok); i = LOAD_INT(spec, i, Val); i = LOAD_TUPLE(spec, i, <span class="hljs-number"><span class="hljs-number">2</span></span>); i = LOAD_TUPLE(spec, i, <span class="hljs-number"><span class="hljs-number">3</span></span>); ASSERT(i == <span class="hljs-keyword"><span class="hljs-keyword">sizeof</span></span>(spec)/<span class="hljs-keyword"><span class="hljs-keyword">sizeof</span></span>(*spec)); desc-&gt;caller = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> erl_drv_send_term(desc-&gt;dport, caller, spec, i); }</code> </pre><br></li><li>  Remove the dispatch of the atom ‚Äúok‚Äù from the function tcp_inet_commandv: <br><br><pre> <code class="cpp hljs"> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> inet_reply_error(INETP(desc), ENOTCONN); } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (desc-&gt;tcp_add_flags &amp; TCP_ADDF_PENDING_SHUTDOWN) tcp_shutdown_error(desc, EPIPE); &gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> tcp_sendv(desc, ev); DEBUGF((<span class="hljs-string"><span class="hljs-string">"tcp_inet_commandv(%ld) }\r\n"</span></span>, (<span class="hljs-keyword"><span class="hljs-keyword">long</span></span>)desc-&gt;inet.port)); }</code> </pre><br></li><li>  Let's add sending int instead of returning 0 in in the tcp_sendv function: <br><pre> <code class="cpp hljs"> <span class="hljs-keyword"><span class="hljs-keyword">default</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (len == <span class="hljs-number"><span class="hljs-number">0</span></span>) &gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> inet_reply_ok_int(desc, <span class="hljs-number"><span class="hljs-number">0</span></span>); h_len = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">break</span></span>; } ----------------------------------- <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (n == ev-&gt;size) { ASSERT(NO_SUBSCRIBERS(&amp;INETP(desc)-&gt;empty_out_q_subs)); &gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> inet_reply_ok_int(desc, n); } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { DEBUGF((<span class="hljs-string"><span class="hljs-string">"tcp_sendv(%ld): s=%d, only sent "</span></span> LL<span class="hljs-string"><span class="hljs-string">U"/%d of "</span></span>LL<span class="hljs-string"><span class="hljs-string">U"/%d bytes/items\r\n"</span></span>, (<span class="hljs-keyword"><span class="hljs-keyword">long</span></span>)desc-&gt;inet.port, desc-&gt;inet.s, (<span class="hljs-keyword"><span class="hljs-keyword">llu_t</span></span>)n, vsize, (<span class="hljs-keyword"><span class="hljs-keyword">llu_t</span></span>)ev-&gt;size, ev-&gt;vsize)); } DEBUGF((<span class="hljs-string"><span class="hljs-string">"tcp_sendv(%ld): s=%d, Send failed, queuing\r\n"</span></span>, (<span class="hljs-keyword"><span class="hljs-keyword">long</span></span>)desc-&gt;inet.port, desc-&gt;inet.s)); driver_enqv(ix, ev, n); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (!INETP(desc)-&gt;is_ignored) sock_select(INETP(desc),(FD_WRITE|FD_CLOSE), <span class="hljs-number"><span class="hljs-number">1</span></span>); } &gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> inet_reply_ok_int(desc, n);</code> </pre><br></li></ol><br></li><li>  Run / configure &amp;&amp; make &amp;&amp; make install. </li></ol><br><br>  And that's it, now the function gen_tcp: send will return {ok, Number} if successful.  The above code snippet will print ‚Äú9‚Äù: <br><br><pre> <code class="erlang hljs"> {ok, Sock} = gen_tcp:connect(SomeHostInNet, <span class="hljs-number"><span class="hljs-number">5555</span></span>, [binary, {packet, <span class="hljs-number"><span class="hljs-number">0</span></span>}]), {ok, N} = gen_tcp:send(Sock, <span class="hljs-string"><span class="hljs-string">"Some Data"</span></span>), io:format(<span class="hljs-string"><span class="hljs-string">"~p"</span></span>, [N])</code> </pre><br><br>  <b>Conclusion</b> <br>  If you handle multiple connections from one process, you must use the {send_timeout, 0} option when creating a socket, otherwise one slow client can slow down sending to all others. <br><br>  If your protocol can process partial messages, it is better to patch the OTP and take into account the number of bytes sent. <br><br><h2>  Briefly </h2><br><ul><li>  If you need to quickly accept connections, you need to accept them from several processes. </li><li>  If you need to quickly read from sockets, you need to handle several sockets from one process and not use ranch. </li><li>  An increase in linux buffers leads to a decrease in system stability and does not give a significant performance gain. </li><li>  When using multiple sockets from the same process, it is necessary to remove the timeout for sending. </li><li>  If you need to know the exact number of bytes sent, you can patch the OTP. </li></ul><br><br><h2>  Links </h2><br><ul><li>  A library that allows you to handle sockets according to the script described above: <a href="https://github.com/parsifal-47/socketpool">https://github.com/parsifal-47/socketpool</a> </li><li>  Examples from the article: <a href="https://github.com/parsifal-47/server-examples">https://github.com/parsifal-47/server-examples</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/306590/">https://habr.com/ru/post/306590/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../306580/index.html">Docker Volume plugin for Nutanix on AOS 4.7</a></li>
<li><a href="../306582/index.html">Rust: & and ref in patterns</a></li>
<li><a href="../306584/index.html">Node.js and JavaScript instead of the old web</a></li>
<li><a href="../306586/index.html">Rating of countries in which it is more favorable to deploy a server farm</a></li>
<li><a href="../306588/index.html">Our development centers around the country with "teleports" to any city</a></li>
<li><a href="../306592/index.html">How to sell: how the seller should work</a></li>
<li><a href="../306594/index.html">In search of mutual understanding: ‚Äúbad advice‚Äù for customers</a></li>
<li><a href="../306596/index.html">Gitlab-ci</a></li>
<li><a href="../306598/index.html">ITU new generation Cisco Firepower 4100 series close-up</a></li>
<li><a href="../306600/index.html">In Q2 2016, encryptors top the list of cyber attacks</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>