<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Using DPDK to provide high performance application solutions (part 0)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kernel is the root of all evil ‚äô.‚òâ 
 Now it‚Äôs hardly possible to surprise anyone with the use of epoll () / kqueue () in the event pollers. To solve t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Using DPDK to provide high performance application solutions (part 0)</h1><div class="post__text post__text-html js-mediator-article"><h1>  Kernel is the root of all evil ‚äô.‚òâ </h1><br>  Now it‚Äôs hardly possible to surprise anyone with the use of <a href="https://ru.wikipedia.org/wiki/Epoll">epoll ()</a> / <a href="http://www.opennet.ru/base/dev/kqueue_overview.txt.html">kqueue ()</a> in the event pollers.  To solve the <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25BE%25D0%25B1%25D0%25BB%25D0%25B5%25D0%25BC%25D0%25B0_10000_%25D1%2581%25D0%25BE%25D0%25B5%25D0%25B4%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B9">C10K</a> problem, there are quite a lot of various solutions ( <a href="http://libevent.org/">libevent</a> / <a href="http://software.schmorp.de/pkg/libev.html">libev</a> / <a href="https://github.com/libuv/libuv">libuv</a> ), with different performance and rather high overhead costs.  The article discusses the use of <a href="http://dpdk.org/">DPDK</a> for solving the problem of processing 10 million connections (C10M), and achieving maximum performance gains when processing network requests in common application solutions.  The main feature of this task is the delegation of responsibility for processing traffic from the OS kernel to the user space (userspace), precise control of interrupt handling and <a href="http://habrahabr.ru/post/37455/">DMA</a> channels, the use of <a href="https://www.kernel.org/doc/Documentation/vfio.txt">VFIO</a> , and many other not very clear words.  Java <a href="http://netty.io/">Netty</a> was selected as the target application environment using the <a href="http://habrahabr.ru/post/130113/">Disruptor</a> pattern and <a href="http://habrahabr.ru/company/odnoklassniki/blog/148139/">offheap caching</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/cb3/524/32f/cb352432f1b94505b8357ba2088ff893.png"></div><br><br>  In short, this is a very efficient way to handle traffic, in terms of performance close to existing hardware solutions.  The overhead of using funds provided by the OS kernel itself is too high, and for such tasks it is the source of most problems.  The difficulty lies in the support of the drivers of the target network interfaces, and the architectural features of the applications as a whole. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The article discusses in great detail the issues of installation, configuration, use, debugging, profiling and deployment of <a href="http://dpdk.org/">DPDK</a> for building high-performance solutions. <br><br><a name="habracut"></a><br><h2>  Why <a href="http://dpdk.org/">DPDK</a> ? </h2><br>  There are also <a href="http://info.iet.unipi.it/~luigi/netmap/">Netmap</a> , <a href="http://www.openonload.org/">OpenOnload</a> and <a href="http://www.ntop.org/products/packet-capture/pf_ring/">pf_ring</a> . <br><br><h4>  netmap </h4><br>  The main task in the development of <i>netmap</i> was the development of an easy-to-use solution, so the most common synchronous interface <a href="https://www.opennet.ru/cgi-bin/opennet/man.cgi%3Ftopic%3Dselect%26category%3D2">select () is</a> provided for it, which significantly simplifies porting existing solutions.  From the point of view of flexibility and abstraction of iron, <i>netmap</i> 's obviously lacks functionality.  Nevertheless, this is the most accessible and widespread solution (even under <s>godless</s> Windows).  Now netmap comes directly <a href="https://www.freebsd.org/cgi/man.cgi%3Fquery%3Dnetmap%26sektion%3D4">as part of freebsd</a> and there is pretty good support for <a href="https://github.com/luigirizzo/netmap-libpcap">libpcap</a> .  Supported by the forces of Luigi Rizzo and Alessio Faina, it is a project of the University of Pisa.  Naturally there is no talk of any commercial support, although it is done in such a way that there is nothing to fall off. <br><br><h4>  pf_ring </h4><br>  <i>pf_ring</i> appeared as a means of overclocking <a href="https://ru.wikipedia.org/wiki/Pcap">pcap</a> 'a, and historically it was that at the time of development there were no ready-to-use, stable solutions.  There are not many obvious advantages over the same netmap, but there is support for <i>IOMMU</i> in the proprietary <a href="http://www.ntop.org/products/packet-capture/pf_ring/pf_ring-zc-zero-copy/">ZC</a> version.  By itself, the product has long been not distinguished by high performance or quality, is nothing more than a means of collecting and analyzing <i>pcap</i> dumps and was not intended to handle traffic in user applications.  The main feature of <i>pf_ring</i> 'a <i>ZC</i> is complete independence from existing network interface drivers. <br><br><h4>  OpenOnload </h4><br>  <i>OpenOnload</i> highly specialized, high-performance, <s>ancient</s> network stack from <a href="http://www.solarflare.com/">SolarFlare</a> .  They are engaged in the release of branded 10 / 40GbE adapters for <i>HP</i> , <i>IBM</i> , <i>Lenovo</i> , <i>Stratus</i> .  Unfortunately, <i>OpenOnload</i> itself <i>does</i> not support all existing <i>SolarFlare</i> adapters.  The main feature of <i>OpenOnload</i> is the complete replacement of the <i>BSD</i> sockets API, including the <i>epoll ()</i> mechanism.  Yes, now your <i>nginx</i> can <a href="http://www.solarflare.com/Content/UserFiles/Documents/Solarflare_OpenOnload_TechBrief.pdf">overcome the 38Gbit bar</a> without any third-party modifications.  <i>SolarFlare</i> provides commercial support and has a lot of respectable customers.  I do not know how things are going with virtualization in <i>OpenOnload</i> , but if you are sitting on containers behind a <i>nginx</i> balancer, this is the simplest and most affordable solution, without unnecessary problems.  Buy, use, pray that it would not fall off, and you can no longer read. <br><br><h4>  Other </h4><br>  There are also <a href="http://www.napatech.com/products">Napatech</a> solutions, but, as <a href="http://www.napatech.com/products">far</a> as I know, they have just a library with their API there, without a <i>hardware program</i> like <i>SolarFlare</i> , so their solutions are less common. <br><br>  Naturally, I didn‚Äôt consider all existing solutions - I just couldn‚Äôt face everything, but I don‚Äôt think that they can be very different from what was described above. <br><br><h4>  DPDK </h4><br>  Historically, the most common adapters for working with 10 / 40GbE are <i>Intel</i> adapters serviced by <i>e1000</i> <i>igb</i> <i>ixgbe</i> <i>i40e</i> drivers.  Therefore, they are frequent target adapters for high-performance traffic processing tools.  So it was with <i>Netmap</i> and <i>pf_ring</i> , the developers of which are <s>perhaps good</s> <a href="http://www.ntop.org/pf_ring/dna-vs-netmap/">acquaintances</a> .  It would be strange if <i>Intel</i> did not start developing its own traffic processing tool - it is <i>DPDK</i> . <br><br>  <i>DPDK</i> is <i>Intel</i> 's OpenSource project, on the basis of which entire offices were built ( <a href="http://www.6wind.com/products/">6WIND</a> ) and for which manufacturers rarely provide drivers, for example, <a href="http://www.mellanox.com/page/products_dyn%3Fproduct_family%3D209%26mtag%3Dpmd_for_dpdk">Mellanox</a> .  Naturally, commercial support for solutions based on it is simply wonderful, it provides a fairly large number of vendors (6WIND, Aricent, ALTEN Calsoft Labs, Advantech, Brocade, Radisys, Tieto, Wind River, Lanner, Mobica) <br><br>  <i>DPDK</i> has the broadest functionality and best abstracts existing iron. <br>  It is not created conveniently - it is created flexible enough to achieve high, possibly maximum, performance. <br><br><h3>  List of supported drivers and cards </h3><br><ul><li>  <i>Chelsio</i> cxgbe ( <a href="http://www.chelsio.com/nic/terminator-5-asic/">Terminator 5</a> ) </li><li>  <i>Cisco</i> enic (all <a href="http://www.cisco.com/c/en/us/products/interfaces-modules/ucs-virtual-interface-card-1385/index.html">Virtual Interface Card</a> series) </li><li>  <i>Emulex</i> oce ( <a href="http://www.emulex.com/products/ethernet-networking-storage-connectivity/ethernet-networking-adapters/emulex-branded/selection-guide/">OneConnect OCe14000</a> family) </li><li>  <i>Mellanox</i> mlx4 ( <a href="http://www.mellanox.com/page/products_dyn%3Fproduct_family%3D162%26mtag%3Dconnectx_3_pro_en_card">ConnectX-3</a> , <a href="http://www.mellanox.com/page/products_dyn%3Fproduct_family%3D127%26mtag%3Dconnectx_3_en">ConnectX-3 Pro</a> ) </li><li>  <i>QLogic / Broadcom</i> bnx2x ( <a href="http://www.cisco.com/c/dam/en/us/products/collateral/servers-unified-computing/ucs-b-series-blade-servers/1CS57711-TB100-D5.pdf">NetXtreme II</a> ) </li></ul><br>  <i>Intel</i> all existing drivers in the linux kernel <br><ul><li>  e1000 (82540, 82545, 82546) </li><li>  e1000e (82571..82574, 82583, ICH8..ICH10, PCH..PCH2) </li><li>  igb (82575..82576, 82580, I210, I211, I350, I354, DH89xx) </li><li>  ixgbe (82598..82599, X540, X550) </li><li>  i40e (X710, XL710) </li><li>  fm10k </li></ul><br>  All of them are ported as <i>Poll Mode</i> drivers for execution in user space ( <i>usermode</i> ). <br><br><h3>  Something else ? </h3><br>  Actually, yes, there is still support <br><ul><li>  virtualization based on <i>QEMU</i> , <i>Xen</i> , <i>VMware ESXi</i> </li><li>  paravirtualized network interfaces based on copying buffers, <s>even though it is evil</s> </li><li>  <i>AF_PACKET</i> sockets and <i>PCAP</i> dumps for testing </li><li>  network adapters with ring buffers </li></ul><br><br><h1>  DPDK Architecture </h1><br><div style="text-align:center;"><img src="https://habrastorage.org/files/c30/012/aa7/c30012aa7b4249d79050429f87ae8f73.png"></div><br>  * this is it in my head so funtsiruet, the reality may be slightly different <br><br>  DPDK itself consists of a set of libraries (the contents of the <i>lib folder</i> ): <br><ul><li>  librte_acl - <s>CEP</s> access control lists for <a href="http://xgu.ru/wiki/VLAN">VLANs</a> </li><li>  librte_compat - compatibility of exported binary interfaces (ABI) </li><li>  librte_ <b>ether</b> - control of ethernet adapter, work with ethernet frames </li><li>  librte_ <b>ivshmem</b> - sharing buffers with ivshmem </li><li>  librte_ <b>kvargs</b> - parsing key-value arguments </li><li>  librte_ <b>mbuf</b> - <i>message buffer</i> management ( <i>message buffer - mbuf</i> ) </li><li>  librte_ <b>net</b> - a piece of BSD's IP stack with ARP / IPv4 / IPv6 / TCP / UDP / SCTP </li><li>  librte_ <b>power</b> - power and frequency management ( <i>cpufreq</i> ) </li><li>  librte_ <b>sched</b> - QOS hierarchical scheduler </li><li>  librte_ <b>vhost</b> - virtual network adapters </li><li>  librte_ <b>cfgfile</b> - parsing configuration files </li><li>  librte_ <b>distributor</b> - a means of <b>distributing</b> packages between existing tasks </li><li>  librte_ <b>hash</b> - hash functions </li><li>  librte_ <b>jobstats</b> - measuring task execution time </li><li>  librte_lpm - Longest Prefix Match functions, used to look for forwarding tables </li><li>  librte_mempool - memory object pool manager </li><li>  librte_ <b>pipeline</b> - package framework <b>pipeline</b> </li><li>  librte_ <b>reorder</b> - sorting packets in a message buffer </li><li>  librte_ <b>table</b> - lookup table implementation (lookup table) </li><li>  librte_ <b>cmdline</b> - parsing command line arguments </li><li>  librte_ <b>eal</b> - platform dependent environment </li><li>  librte_ip_frag - IP packet fragmentation </li><li>  librte_ <b>kni</b> - API for interacting with KNI </li><li> librte_ <b>malloc</b> - easy to guess </li><li>  librte_ <b>meter</b> - QOS metric </li><li>  librte_ <b>port</b> - implementing ports for network packets </li><li>  librte_ <b>ring</b> - ring lock-free FIFO queues </li><li>  librte_ <b>timer</b> - timers and counters </li></ul><br>  UIO drivers ( <i>lib / librte_eal / linuxapp</i> ) network interfaces under linux: <br><ul><li>  uio_igb - ethernet network adapter </li><li>  xen_dom0 - clear from the title </li></ul><br>  and BSD <br><ul><li>  nic_uio </li></ul><br>  And the aforementioned <i>Poll Mode</i> drivers ( <i>PMD</i> ) that run in the user space ( <i>userspace</i> ): e1000, e1000e, igb, ixgbe, i40e, fm10k and others. <br><br>  <i>Kernel Network Interface</i> (KNI) is a specialized driver that allows you to interact with the kernel network API, perform <i>ioctl</i> calls to ports of interfaces that work with <i>DPDK</i> , use common utilities ( <i>ethtool</i> , <i>ifconfig</i> , <i>tcpdump</i> ) to manage them. <br><br>  As you can see, <i>DPDK</i> , in comparison with other <s>netmap</s> solutions, has a <s>hell of a</s> lot of buns for the implementation of <s>SDNs</s> , which attract the dark side of hardware art. <br><br><h1>  Requirements and fine tuning of the target system </h1><br>  The main recommendations of the official documentation have been translated and supplemented. <br>  The issue of setting up <i>XEN</i> and <i>VMware</i> hypervisor for working with <i>DPDK is</i> not affected. <br><br><h2>  Are common </h2><br>  If you put your <i>DPDK</i> under the <i>Intel</i> Communications Chipset 89xx, then <a href="https://01.org/sites/default/files/page/330750-004_qat_gsg.pdf">you are here</a> . <br>  To build you need <i>coreutils</i> , <i>gcc</i> , kernel headers, <i>glibc</i> headers. <br>  It seems to support <i>clang</i> , and there is support for <i>Intel</i> 's <i>icc</i> . <br>  To run auxiliary scripts - <i>Python</i> 2.6 / 2.7 <br><br>  The Linux kernel must be compiled with UIO support and monitoring of process address spaces, these are the kernel parameters: <br>  <a href="http://kernel.xc.net/html/linux-3.2.28/x86/CONFIG_UIO">CONFIG_UIO</a> <br>  <a href="http://kernel.xc.net/html/linux-3.2.28/x86/CONFIG_UIO_PDRV">CONFIG_UIO_PDRV</a> <br>  <a href="http://kernel.xc.net/html/linux-3.2.28/x86/CONFIG_UIO_PDRV_GENIRQ">CONFIG_UIO_PDRV_GENIRQ</a> <br>  <a href="http://kernel.xc.net/html/linux-3.2.28/x86/CONFIG_UIO_PCI_GENERIC">CONFIG_UIO_PCI_GENERIC</a> <br>  and <br>  <a href="http://kernel.xc.net/html/linux-3.2.28/x86/PROC_PAGE_MONITOR">CONFIG_PROC_PAGE_MONITOR</a> <br><br>  I want to draw attention to the fact that in <i>grsecurity the</i> parameter PROC_PAGE_MONITOR is considered too informative - it helps in exploiting kernel vulnerabilities and bypassing the <i>ASLR</i> . <br><br><h2>  <a href="https://ru.wikipedia.org/wiki/HPET">HPET</a> </h2><br>  For organizing periodic interruptions of high accuracy, an <a href="https://ru.wikipedia.org/wiki/HPET">HPET</a> timer is needed. <br><br>  You can look availability <pre><code class="bash hljs">grep hpet /proc/timer_list</code> </pre>  Go to enable BIOS <blockquote>  Advanced -&gt; PCH-IO Configuration -&gt; High Precision Timer </blockquote>  And build a kernel with <a href="http://kernel.xc.net/html/linux-2.6.18/i386/HPET">CONFIG_HPET</a> and <a href="http://kernel.xc.net/html/linux-2.6.18/i386/HPET_MMAP">CONFIG_HPET_MMAP enabled</a> . <br><br>  By default, <i>HPET</i> support is disabled in the <i>DPDK</i> itself, so you need to enable it by setting the CONFIG_RTE_LIBEAL_USE_HPET flag manually in the <i>config / common_linuxapp file</i> . <br><br>  In some cases it is advisable to use <i>HPET</i> , in others - <a href="https://en.wikipedia.org/wiki/Time_Stamp_Counter">TSC</a> . <br>  To implement a high-performance solution, you need to use both, since they have a different purpose and they compensate for each other‚Äôs shortcomings.  Usually, the default is <i>TSC</i> .  Initialization and availability check of the <i>HPET</i> timer is performed by calling <b>rte_eal_hpet_init</b> (int <b>make_default</b> ) &lt; <a href="http://dpdk.org/doc/api/rte__cycles_8h_source.html">rte_cycles.h</a> &gt;.  It's strange that the API documentation misses it. <br><br><h2>  Core insulation </h2><br>  For unloading the system scheduler, a fairly common practice is to isolate the logical cores of the processor for the needs of high-performance applications.  This is especially true for dual-processor systems. <br>  If your application runs on even-numbered kernels 2, 4, 6, 8, 10, you can add a kernel parameter to your favorite bootloader <br><blockquote>  isolcpus = 2,4,6,8,10 </blockquote>  For the widespread <a href="https://ru.wikipedia.org/wiki/GNU_GRUB">grub</a> 'a, this is the GRUB_CMDLINE_LINUX_DEFAULT parameter in the <i>/ etc / default / grub</i> config. <br><br><h2>  Hugepages </h2><br>  <a href="https://wiki.debian.org/Hugepages">Large pages are</a> needed to allocate memory for network buffers.  Allocating large pages has a positive effect on performance since fewer calls are needed to translate virtual memory addresses into <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D1%2583%25D1%2584%25D0%25B5%25D1%2580_%25D0%25B0%25D1%2581%25D1%2581%25D0%25BE%25D1%2586%25D0%25B8%25D0%25B0%25D1%2582%25D0%25B8%25D0%25B2%25D0%25BD%25D0%25BE%25D0%25B9_%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D1%2581%25D0%25BB%25D1%258F%25D1%2586%25D0%25B8%25D0%25B8">TLBs</a> .  True, they should stand out in the process of loading the kernel to avoid fragmentation. <br>  To do this, add a kernel parameter: <br><blockquote>  hugepages = 1024 </blockquote>  This will allocate 1024 pages of 2 MB each. <br><br>  To highlight four pages per gigabyte: <br><blockquote>  default_hugepagesz = 1G hugepagesz = 1G hugepages = 4 </blockquote>  But we need the appropriate support - the <b>pdpe1gb</b> processor <b>flag</b> in <i>/ proc / cpuinfo</i> . <br><pre> <code class="bash hljs">grep pdpe1gb /proc/cpuinfo | uniq</code> </pre><br>  For 64-bit applications, using 1GB pages is preferred. <br><br>  To obtain information about the distribution of pages between the cores in the <i>NUMA</i> system, you can use the following command <br><pre> <code class="bash hljs">cat /sys/devices/system/node/node*/meminfo | fgrep Huge</code> </pre><br>  You can read more about managing the policy of allocating and freeing large pages in <a href="https://ru.wikipedia.org/wiki/Non-Uniform_Memory_Access">NUMA</a> systems in the <a href="https://www.kernel.org/doc/Documentation/vm/hugetlbpage.txt">official documentation</a> . <br><br>  To support large pages, you need to build a kernel with the parameter <a href="http://kernel.xc.net/html/linux-3.3.1/x86/HUGETLBFS">CONFIG_HUGETLBFS</a> <br><br>  Management of allocated memory areas for large pages is carried out by the <a href="https://www.kernel.org/doc/Documentation/vm/transhuge.txt">Transparent Hugepage</a> mechanism, which performs defragmentation in a separate <b>khugepaged</b> kernel <b>stream</b> .  To support it, you need to collect with the <a href="http://kernel.xc.net/html/linux-3.3.1/x86/TRANSPARENT_HUGEPAGE">CONFIG_TRANSPARENT_HUGEPAGE</a> parameter and the <a href="http://kernel.xc.net/html/linux-3.3.1/x86/TRANSPARENT_HUGEPAGE_ALWAYS">CONFIG_TRANSPARENT_HUGEPAGE_ALWAYS policies</a> or <a href="http://kernel.xc.net/html/linux-3.3.1/x86/TRANSPARENT_HUGEPAGE_ALWAYS">CONFIG_TRANSPARENT_HUGEPAGE_MADVISE policies</a> . <br><br>  This mechanism remains relevant even in the case of allocation of large pages during the OS boot, since, nevertheless, there remains the probability of not being able to allocate continuous memory areas for 2 MB pages, for various reasons. <br><br>  There is a <i>NUMA</i> <a href="http://habrahabr.ru/company/intel/blog/171079/">blockbuster</a> and memory from <i>Intel</i> 's adepts. <br>  There is a small <a href="https://access.redhat.com/solutions/46111">article</a> about using large pages from Rad Hat. <br><br>  After configuring and selecting pages, you need to mount them; to do this, add the corresponding mount point to <i>/ etc / fstab</i> <br><pre> <code class="hljs lua">nodev /mnt/<span class="hljs-built_in"><span class="hljs-built_in">huge</span></span> hugetlbfs defaults <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre><br>  For 1GB pages, the page size must be specified as an additional parameter. <br><pre> <code class="hljs lua">nodev /mnt/<span class="hljs-built_in"><span class="hljs-built_in">huge</span></span> hugetlbfs pagesize=<span class="hljs-number"><span class="hljs-number">1</span></span>GB <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre><br>  According to my personal observations, the most problems with setting up and operating <i>DPDK</i> arise with large pages.  It is worth paying special attention to the administration of large pages. <br><br>  By the way, in <i>Power8, the</i> size of large pages is 16 MBytes and 16 GB, which, as for me, is a little overkill. <br><br><h2>  Energy Management </h2><br>  The DPDK already has the means to control the frequencies of the processor, so that the standard policies "do not stick in the wheels." <br>  To use them you need to enable <a href="https://ru.wikipedia.org/wiki/SpeedStep">SpeedStep</a> and <a href="https://software.intel.com/ru-ru/articles/power-management-states-p-states-c-states-and-package-c-states">C3 C6</a> . <br><br>  In <i>BIOS, the</i> path to the settings might look like this <br><blockquote>  Advanced-&gt; Processor Configuration-&gt; Enhanced Intel SpeedStep Tech <br>  Advanced-&gt; Processor Configuration-&gt; Processor C3 Advanced-&gt; Processor Configuration-&gt; Processor C6 <br></blockquote>  The <a href="http://www.dpdk.org/browse/dpdk/plain/examples/l3fwd-power/">l3fwd-power</a> application provides an example of an L3 switch using power management features. <br><br><h2>  Access rights </h2><br>  It is clear that it is very insecure to execute an application with root access rights. <br>  It is advisable to use ACLs to <a href="https://www.opennet.ru/docs/RUS/posixacl/posixacls5.html">create permissions</a> for a particular user group. <br><pre> <code class="bash hljs">setfacl -su::rwx,g::rwx,o:---,g:dpdk:rw- /dev/hpet setfacl -su::rwx,g::rwx,o:---,g:dpdk:rwx /mnt/huge setfacl -su::rwx,g::rwx,o:---,g:dpdk:rw- /dev/uio0 setfacl -su::rwx,g::rwx,o:---,g:dpdk:rw- /sys/class/uio/uio0/device/config setfacl -su::rwx,g::rwx,o:---,g:dpdk:rwx /sys/class/uio/uio0/device/resource*</code> </pre><br>  That will add full access for the dpdk user group for the resources used and the uio0 device. <br><br><h2>  Firmware </h2><br>  For 40GbE network adapters, processing small packets is quite a challenge, and from firmware to firmware, <i>Intel</i> introduces additional optimizations.  Support for <b>FLV3E</b> series <b>firmware is</b> implemented in DPDK 2.2-rc2, but for now the most optimal version is <b>4.2.6</b> .  You can either contact vendor support or directly contact <i>Intel</i> for an update, or upgrade it yourself. <br><br><h2>  Extended labels, size of request and read handles in PCIe devices </h2><br>  The PCIe bus parameters <b>extended_tag</b> and <b>max_read_request_size</b> significantly affect the processing speed of small packets - on the order of 100 byte 40GbE adapters.  In some versions of the BIOS, you can install them manually - 125 Bytes and "1", respectively, for 100 byte packets. <br><br>  Values ‚Äã‚Äãcan be set in the config <i>/ common_linuxapp config</i> when building DPDK using the following parameters: <br><blockquote>  CONFIG_RTE_PCI_CONFIG <br>  CONFIG_RTE_PCI_EXTENDED_TAG <br>  CONFIG_RTE_PCI_MAX_READ_REQUEST_SIZE </blockquote>  Or using the <b>setpci</b> <b>lspci</b> commands. <br><br>  <a href="http://www.xilinx.com/support/answers/36596.html">This is the</a> difference between the MAX_REQUEST and MAX_PAYLOAD parameters for PCIe devices, but there is only MAX_REQUEST in the configs. <br><br>  For the <i>i40e</i> driver, it makes sense to reduce the size of the read handles to 16 bytes, you can do this by setting the following parameter: CONFIG_RTE_LIBRTE_I40E_16BYTE_RX_DESC in <i>config / common_linuxapp</i> or in <i>config / common_bsdapp,</i> respectively. <br>  You can also specify the minimum interval between the processing of the write interrupts CONFIG_RTE_LIBRTE_I40E_ITR_INTERVAL depending on the existing priorities: maximum throughput or per packet latency. <br><br>  Also, there are similar parameters for the driver <i>Mellanox</i> mlx4. <br><blockquote>  CONFIG_RTE_LIBRTE_MLX4_SGE_WR_N <br>  CONFIG_RTE_LIBRTE_MLX4_MAX_INLINE <br>  CONFIG_RTE_LIBRTE_MLX4_TX_MP_CACHE <br>  CONFIG_RTE_LIBRTE_MLX4_SOFT_COUNTERS </blockquote>  Which for certain somehow influence productivity. <br><br>  All other parameters of network adapters are associated with debugging modes that allow very finely profile and debug the target application, but more on that later. <br><br><h2>  IOMMU for working with <i>Intel</i> VT-d </h2><br>  You need to build a kernel with parameters <br>  <a href="http://kernel.xc.net/html/linux-3.2.28/x86/IOMMU_SUPPORT">CONFIG_IOMMU_SUPPORT</a> <br>  <a href="http://kernel.xc.net/html/linux-3.2.28/x86/CONFIG_IOMMU_API">CONFIG_IOMMU_API</a> <br>  <a href="http://kernel.xc.net/html/linux-3.2.28/x86/CONFIG_INTEL_IOMMU">CONFIG_INTEL_IOMMU</a> <br>  For <b>igb_uio</b> driver, the boot parameter must be set. <br><blockquote>  iommu = pt </blockquote>  Which leads to the correct translation of <i>DMA</i> addresses ( <i>DMA remapping</i> ).  <i>IOMMU</i> support for the target network adapter in the hypervisor is turned <b>off</b> .  By itself, <i>IOMMU is</i> quite wasteful for high-performance network interfaces.  DPDK implements one-to-one mapping, so full <i>IOMMU</i> support is not required, even though this is another security breach. <br><br>  If the <a href="http://kernel.xc.net/html/linux-3.2.28/x86/INTEL_IOMMU_DEFAULT_ON">INTEL_IOMMU_DEFAULT_ON</a> flag is set when building the kernel, then the boot parameter should be used <br><blockquote>  intel_iommu = on </blockquote>  That guarantees the correct initialization of Intel <i>IOMMU</i> . <br><br>  I want to note that the use of <i>UIO</i> ( <i>uio_pci_generic</i> , <i>igb_uio</i> ) is optional for kernels supporting <i>VFIO</i> (vfio-pci), which are used to interact with the target network interfaces. <br><br>  <i>igb_uio is</i> needed if there are no support for some interrupts and / or virtual functions by the target network adapters, otherwise you can safely use <i>uio_pci_generic</i> . <br><br>  Despite the fact that the iommu = pt parameter is mandatory for the igb_uio driver, the vfio-pci driver functions correctly both with the iommu = pt parameter and with iommu = on. <br><br>  By itself, the <i>VFIO</i> functions quite <s>stubbornly</s> strangely, due to the peculiarities of the <i>IOMMU</i> groups: some devices require that all their ports are binded under the <i>VFIO</i> , others need only some, the third doesn't need to bind anything at all. <br><br>  If your device is located behind a <i>PCI-to-PCI</i> bridge, then the bridge driver will be included in the same <i>IOMMU</i> group as the target adapter, so the bridge driver must be unloaded ‚Äî so that the <i>VFIO</i> can pick up the devices behind the bridge. <br><br>  You can check the location of existing devices and the drivers used by the script <br><pre> <code class="bash hljs">./tools/dpdk_nic_bind.py --status</code> </pre><br>  You can also explicitly bind drivers to specific network devices. <br><pre> <code class="bash hljs">./tools/dpdk_nic_bind.py --<span class="hljs-built_in"><span class="hljs-built_in">bind</span></span>=uio_pci_generic 04:00.1 ./tools/dpdk_nic_bind.py --<span class="hljs-built_in"><span class="hljs-built_in">bind</span></span>=uio_pci_generic eth1</code> </pre><br>  It is convenient however. <br><br><h1>  Installation </h1><br>  <a href="http://dpdk.org/download">We take the source</a> and collect as described below. <br>  <i>DPDK</i> itself comes with a set of example applications, where you can test the correctness of the system setup. <br><br>  Configuring the DPDK, as mentioned above, is done by setting the parameters in the <i>config / common_linuxapp</i> and <i>config / common_bsdapp files</i> .  Standard values ‚Äã‚Äãfor platform-specific parameters are stored in <i>config / defconfig_ *</i> files. <br><br>  First, the configuration template is applied, the <i>build</i> folder is created with all the living creatures and targets: <br><pre> <code class="bash hljs">make config T=x86_64-native-linuxapp-gcc</code> </pre><br>  The following target environments are available in <i>DPDK</i> 2.2 (mine) <br><pre> <code class="bash hljs"> arm-armv7a-linuxapp-gcc arm64-armv8a-linuxapp-gcc arm64-thunderx-linuxapp-gcc arm64-xgene1-linuxapp-gcc i686-native-linuxapp-gcc i686-native-linuxapp-icc ppc_64-power8-linuxapp-gcc tile-tilegx-linuxapp-gcc x86_64-ivshmem-linuxapp-gcc x86_64-ivshmem-linuxapp-icc x86_64-native-bsdapp-clang x86_64-native-bsdapp-gcc x86_64-native-linuxapp-clang x86_64-native-linuxapp-gcc x86_64-native-linuxapp-icc x86_x32-native-linuxapp-gcc</code> </pre><br>  <i>ivshmem</i> is a <a href="https://ru.wikipedia.org/wiki/QEMU">QEMU</a> mechanism that seems to allow sharing a memory area between several guest virtual machines without copying, by means of a common specialized device.  Although copying to <i>shared</i> memory is necessary in the case of communication between guest OSs, this is not the case with <i>DPDK</i> .  By itself, <i>ivshmem is</i> implemented quite <a href="">simply</a> . <br><br>  The purpose of the rest of the configuration templates should be obvious, otherwise why are you reading this at all? <br><br>  In addition to the configuration template, there are other optional parameters. <br><pre> <code class="bash hljs"> EXTRA_CPPFLAGS -   EXTRA_CFLAGS -   EXTRA_LDFLAGS -   EXTRA_LDLIBS -   RTE_KERNELDIR -      CROSS -   V=1 -      D=1 -    O -   `build` DESTDIR -    `/usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>`</code> </pre><br>  Further, just good old <br><pre> <code class="bash hljs">make</code> </pre><br>  The target list for <i>make is</i> pretty trite <br><pre> <code class="bash hljs"> all build clean install uninstall examples examples_clean</code> </pre><br>  To work, you need to load <i>UIO</i> modules <br><pre> <code class="bash hljs">sudo modprobe uio_pci_generic</code> </pre>  or <br><pre> <code class="bash hljs">sudo modprobe uio sudo insmod kmod/igb_uio.ko</code> </pre><br>  If <i>VFIO is</i> used <br><pre> <code class="bash hljs">sudo modprobe vfio-pci</code> </pre><br>  If <i>KNI is</i> used <br><pre> <code class="bash hljs">insmod kmod/rte_kni.ko</code> </pre><br><br><h1>  Build and run examples </h1><br>  DPDK uses 2 environment variables to build examples: <br><ul><li>  RTE_SDK - path to the folder where <i>DPDK is</i> installed </li><li>  RTE_TARGET - the name of the configuration template used for the assembly </li></ul><br>  They are used in the respective <i>Makefile</i> 'ah. <br><br>  EAL already provides some command line parameters to configure the application: <br><ul><li>  -c &lt;mask&gt; - a hexadecimal mask of logical cores on which the application will be executed </li><li>  -n &lt;number&gt; of memory channels per processor </li><li>  -b &lt;domain: bus: identifier.function&gt;, ... - black list of <i>PCI</i> devices </li><li>  --use-device &lt;domain: bus: identifier.function&gt;, ... - white list of <i>PCI</i> devices, cannot be used simultaneously with black </li><li>  --socket-mem MB - amount of memory allocated for large pages per processor socket </li><li>  -m MB - the amount of memory allocated for large pages, ignoring the physical location of the processor </li><li>  -r &lt;number&gt; of memory slots </li><li>  -v - version </li><li>  - huge-dir - folder to which large pages are mounted </li><li>  --file-prefix - the prefix of files that are stored in the file system of large pages </li><li>  --proc-type is a process instance, used together with --file-prefix to run an application in several processes </li><li>  --xen-dom0 - execution in <i>Xen domain0</i> without support for large pages </li><li>  --vmware-tsc-map - use <i>TSC</i> counter provided by <i>VMWare</i> , instead of <i>RDTSC</i> </li><li>  --base-virtaddr - base virtual address </li><li>  --vfio-intr - interrupt type used by <i>VFIO</i> </li></ul><br><br>  To check the numbering of cores in the system, you can use the <i>lstopo</i> command from the <a href="http://www.open-mpi.org/projects/hwloc/">hwloc</a> package. <br><br>  It is recommended to use all the memory allocated in the form of large pages, this is the default behavior if the -m and --socket-mem parameters are not used.  Allocating contiguous areas of memory less than what is available in large pages can lead to <i>EAL</i> initialization errors, and sometimes to undefined behavior. <br><br>  To allocate 1GB of memory <br><ul><li>  on zero socket () you need to specify - socket-mem = 1024 </li><li>  on the first - socket-mem = 0.1024 </li><li>  on zero and second - socket-mem = 1024,0,1024 </li></ul><br><br>  To build and run Hello World <br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">export</span></span> RTE_SDK=~/src/dpdk <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> <span class="hljs-variable"><span class="hljs-variable">${RTE_SDK}</span></span>/examples/helloworld make ./build/helloworld -cf -n 2</code> </pre><br>  Thus, the application will run on four cores, taking into account that 2 memory bars have been installed. <br>  And we get 5 hello worlds from different cores. <br><br><h3>  Chicken, egg and pterodactyl problem </h3><br>  I chose Java as the target platform because of the relatively high performance of the virtual machine and the possibility of introducing additional memory management mechanisms.  The question of how to allocate responsibility: where to allocate memory, where to manage flows, how to perform task scheduling, and what is special about <i>DPDK</i> mechanisms is quite complex and two-digit.  I had to uncommonly pokolovatsya in the source <i>DPDK</i> , <i>Netty,</i> and most <i>OpenJDK</i> .  As a result, specialized versions of <i>netty</i> components with very deep <i>DPDK</i> integration were <i>developed</i> . <br><br>  To be continued. </div><p>Source: <a href="https://habr.com/ru/post/267591/">https://habr.com/ru/post/267591/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../267577/index.html">Trojan Android app bypasses Google Bouncer checks</a></li>
<li><a href="../267579/index.html">Tehnokniga, part 3: literature on the design of highly loaded systems, on the security of web applications, on ensuring the quality of development and on mobile development</a></li>
<li><a href="../267583/index.html">Symantec voluntarily issued a certificate for google.com and www.google.com</a></li>
<li><a href="../267587/index.html">Deep Linking for Mobile Applications</a></li>
<li><a href="../267589/index.html">Notifications are available to everyone. Expansion of functions of integration with RSS + Push from Vkontakte groups</a></li>
<li><a href="../267593/index.html">Porting C ++ applications to the nanoCAD platform, using project property pages</a></li>
<li><a href="../267595/index.html">19 daily tips for working with Git</a></li>
<li><a href="../267597/index.html">Purposeful and conscious de-automation of business</a></li>
<li><a href="../267599/index.html">DIY MAB Library for Microsoft NPS</a></li>
<li><a href="../267601/index.html">The difficulties and joys of developing the first game on Android</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>