<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Patience and labor all text will extract</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="During the training session (May-June and December-January), users ask us to check for the presence of borrowing up to 500 documents every minute. Doc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Patience and labor all text will extract</h1><div class="post__text post__text-html js-mediator-article"><p>  During the training session (May-June and December-January), users ask us to check for the presence of borrowing up to 500 documents every minute.  Documents come in files of various formats, the complexity of working with each of them is different.  To check the document for borrowing, we first need to extract its text from the file, and at the same time to deal with formatting.  The task is to implement high-quality extraction of half a thousand texts with formatting per minute, while falling infrequently (and it is better not to fall at all), consume few resources and do not pay for the development and operation of the final creation half of the galactic budget. </p><br><p>  Yes, yes, we, of course, know that from three things - quickly, cheaply and efficiently - you need to choose any two.  But the worst thing is that in our case we can not cross out.  The question is how well we did it ... </p><br><p><img src="https://habrastorage.org/webt/us/nu/t4/usnut4dlt0wwlqzbkwhnmlud1uo.png"></p><br><p>  <sub><em>Image source: <a href="">Wikipedia</a></em></sub> <br></p><p><a name="habracut"></a></p><br><p>  We are often told that the fate of people depends on the quality of our work.  Therefore it is necessary to cultivate perfectionists in oneself.  Of course, we are constantly improving the quality of the system (in all aspects), as unscrupulous authors come up with new ways to get around.  And I hope that the day is close when the complexity of deception, on the one hand, and a feeling of satisfaction from a quality job, on the other, will induce an absolute majority of students to give up their favorite desire to cheat.  At the same time, we understand that the price of error may be the possible suffering of innocent people, if we suddenly cheat. </p><br><p>  Why am I doing this?  If we were perfectionists, we would thoughtfully approach the writing of a series of articles on the work <a href="https://www.antiplagiat.ru/">of the Antiplagiat system</a> .  We would painstakingly formulate a publication plan in order to present everything in the most logical and expected way for the reader: </p><br><ul><li>  First, we would talk about how our system works (the <a href="https://habr.com/ru/company/antiplagiat/blog/429634/">fifth publication</a> on Habr√©), and describe the three main stages of processing a document when it is checked for borrowing: <br><ol><li>  Extract the text of the document (you are here!); </li><li>  Search for borrowing (pieces already exist <a href="https://habr.com/ru/company/antiplagiat/">in several of our articles</a> ); </li><li>  Building a report on the document (in the plans). </li></ol></li><li>  Further, we would begin to initiate the reader into the device of interesting auxiliary mechanisms, such as the search for transferable borrowings ( <a href="https://habr.com/ru/company/antiplagiat/blog/354142/">first article</a> ), the definition of paraphrase ( <a href="https://habr.com/ru/company/antiplagiat/blog/422941/">fourth</a> ) and thematic classification ( <a href="https://habr.com/ru/company/antiplagiat/blog/413361/">second</a> ). </li><li>  And finally, we got to the search engine - the index of shingles ( <a href="https://habr.com/ru/company/antiplagiat/blog/445952/">seventh article</a> ). </li></ul><br><p>  The attentive reader probably noticed that we still do not suffer from excessive perfectionism, so the time has come to consider the first stage - extracting text and formatting documents.  We will deal with this today, thinking about the frailty of being and the light at the end of the tunnel, about the non-existence of anything ideal and about striving for perfection, about having a plan and following it and about compromises that we always incline towards life. </p><br><h1>  In the beginning was the word </h1><br><p>  First, we extracted from the documents only the most necessary things to check for borrowing - the text of the documents themselves.  Main formats were supported - docx, doc, txt, pdf, rtf, html.  Then less common ppt, pptx, odt, epub, fb2, djvu were added, however, it was <abbr title="Since the very low demand did not justify the efforts expended on supporting these formats">necessary to refuse to</abbr> work with most of them <abbr title="Since the very low demand did not justify the efforts expended on supporting these formats">in the future</abbr> .  Each of them was processed in its own way - somewhere by a separate library, somewhere by its parser.  On average, text extraction took about hundreds of milliseconds.  It would seem that the main and almost the only difficulty in extracting text is the ‚Äúparsing‚Äù of the format itself, which is especially important for binary pdf and doc formats (and the <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25BE%25D0%25BF%25D1%2580%25D0%25B8%25D0%25B5%25D1%2582%25D0%25B0%25D1%2580%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BE%25D0%25B1%25D0%25B5%25D1%2581%25D0%25BF%25D0%25B5%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">proprietary nature of the</a> latter makes working with it even more problematic).  However, already at this stage, when our desires were limited only to extracting the text, it became clear that any way of reading the formats we need brings with it a number of unpleasant features.  The most significant of them are: </p><br><ul><li>  There are exceptions even when processing some valid documents, not to mention processing incorrectly formed ‚Äúbroken‚Äù documents.  Even more problems are caused by the fact that the <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B0%25D1%2588%25D0%25B8%25D0%25BD%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BA%25D0%25BE%25D0%25B4">native code</a> can fall, and the handling of such situations in the .net code is difficult; </li><li>  Inadequately high memory consumption, which can hurt both neighboring processes and the current processing "problem" document (out of memory in a managed or unmanaged code); </li><li>  Document processing is too long, which is aggravated by the lack of cancellation mechanisms for most libraries, and sometimes by the complexity (read: almost impossible) of canceling the unmanaged code from the managed one; </li><li>  "Text extraction from documents."  The formation of the text of the pdf-document (and this format is the key for us), the parsing of which has already been done, contrary to expectations, is not a trivial task.  The fact is that <a href="https://ru.wikipedia.org/wiki/Portable_Document_Format">the pdf format</a> was originally developed primarily for the electronic presentation of printing materials.  The text in pdfs is a set of text blocks located on the pages of the document.  Moreover, a block can be a paragraph of text, or a single character.  The task of restoring text in its original form from a given set of blocks falls on the library (code / program) that reads the document.  Yes, the format, starting with a certain version of it, provides the ability to set the order of the blocks, but, unfortunately, documents with a marked order of text blocks are quite rare.  Therefore, libraries of reading text pdf'ok contain a number of heuristics (well, it‚Äôs standard here: machine learning, <s>bigdata, blockchain</s> , ...), allowing to restore the text in one form or another with the exact accuracy, and, as expected, the result is different from library to library . </li></ul><br><p><img src="https://habrastorage.org/webt/nr/hu/jb/nrhujbad4ji9xkwm5qzajzmvq24.png"></p><br><p>  <sub><em>Source of the bottom image: <a href="http://old2.adygnet.ru/sites/default/files/%25203%2520-%25204-5_2013.pdf">Article</a></em></sub> </p><br><p>  <sub><em>Upper Image Source: <a href="https://imgur.com/gallery/yopBh">Hmm ...</a></em></sub> </p><br><h1>  Need more data! </h1><br><p>  If for analyzing a document for borrowing, we had enough textual background of the document, then the implementation of a number of new features is impossible or very difficult without extracting additional data from the document.  Today, in addition to the text substrate, we also extract the formatting of the document and <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25BD%25D0%25B4%25D0%25B5%25D1%2580%25D0%25B8%25D0%25BD%25D0%25B3">render the</a> images of the pages.  We use the latter for optical text recognition ( <a href="http://www.unkniga.ru/innovation/tehnology/8017-raspoznavanie-izobrazheniy-na-sluzhbe-u-antiplagiata.html">OCR</a> ), as well as for defining some varieties of detours. </p><br><p>  Document formatting includes the geometric arrangement of all words and characters on the pages, as well as the font size of all characters.  This information allows us to: </p><br><ul><li>  Beautifully display the report on the verification of the document, drawing the borrowings found directly on the original document; <br><img src="https://habrastorage.org/webt/hk/uo/am/hkuoamt7hjscszy6lcgoc4vjeoy.png"><br></li><li>  Determine document blocks (title page, <a href="https://habr.com/ru/company/antiplagiat/blog/449124/">bibliography</a> ) with greater accuracy and extract its metadata (authors, title of work, year and place of work, etc.); </li><li>  <a href="http://www.unkniga.ru/innovation/tehnology/9514-kto-ne-spryatalsya-ya-ne-vinovat.html">Detect attempts to bypass the system.</a> </li></ul><br><p>  To unify the processing of documents and a set of extracted data, we convert documents of all formats supported by us into pdf.  Thus, the procedure for extracting document data is performed in two stages: </p><br><ul><li>  Document conversion to pdf; </li><li>  Extract data from pdf. </li></ul><br><h1>  Convert to pdf.  Library selection </h1><br><p>  Since it is not so easy to take and convert a document into pdf, we decided not to reinvent the wheel and explore the ready-made solutions, choosing the most suitable for us.  It was back in 2017. </p><br><p>  Candidate selection criteria: </p><br><ul><li>  Library on .net, ideally - .net core and cross-platform <div class="spoiler">  <b class="spoiler_title">Spoiler!</b> <div class="spoiler_text">  As a result, at that time, the ideal was unreachable. </div></div></li><li>  Support for required formats - doc, docx, rtf, odf, ppt, pptx </li><li>  Stability </li><li>  Performance </li><li>  Quality technical support </li><li>  Issue price </li></ul><br><p>  We analyzed the available solutions, selecting among them the 6 most appropriate for our tasks: </p><br><div class="scrollable-table"><table><tbody><tr><th>  Library </th><th>  Surface problems </th></tr><tr><td>  MS Word.  Interop </td><td>  <b>Requires:</b> MS Word.  Call Microsoft Word via COM.  The method has many drawbacks: the need to install MS Word, instability, poor performance.  There are licensing restrictions. </td></tr><tr><td>  <a href="https://documentation.devexpress.com/OfficeFileAPI/14911/Office-File-API">DevExpress (17)</a> </td><td>  Does not support ppt, pptx </td></tr><tr><td>  <a href="https://products.groupdocs.com/conversion/net">Groupdocs</a> </td><td>  - </td></tr><tr><td>  <a href="https://www.syncfusion.com/products/file-formats/docio">Syncfusion</a> </td><td>  Does not support ppt, pptx, odt </td></tr><tr><td>  <a href="https://neevia.com/products/dcpro/">Neevia Document Converter Pro</a> </td><td>  Requires: MS Word.  Does not support odt </td></tr><tr><td>  <a href="https://www.dynamicpdf.com/">DynamicPdf</a> </td><td>  Requires: MS Word, Internet Explorer.  Does not support odt </td></tr></tbody></table></div><br><p>  MS Word Interop, Neevia Document Converter Pro and DynamicPdf require the installation of MS Office on production, which could permanently and permanently link us to Windows.  Therefore, we no longer considered these options. </p><br><p>  Thus, we have three main candidates, and only one of them fully supports all the formats we need.  Well, it's time to see what they can do. </p><br><p>  To test the libraries, we have formed a sample of 120,000 real user documents, the ratio of formats in which roughly corresponds to what we see every day in production. </p><br><p>  So, the first round.  Let's see what percentage of documents can be successfully converted into pdf by the libraries in question.  Successfully, in our case, is not throwing an exception, meeting a 3-minute timeout and returning a non-empty text. </p><br><div class="scrollable-table"><table><tbody><tr><th rowspan="2">  Converter </th><th rowspan="2">  Success (%) </th><th colspan="5">  Not successful (%) </th></tr><tr><th>  Total </th><th>  Blank text </th><th>  Exception </th><th>  Timeout (3 minutes) </th><th>  Process crash </th></tr><tr><td>  Groupdocs </td><td>  99.012 </td><td>  0.988 </td><td>  0.039 </td><td>  0.873 </td><td>  0.076 </td><td>  0 </td></tr><tr><td>  DevExpress </td><td>  99.819 </td><td>  0.181 </td><td>  0.123 </td><td>  0.019 </td><td>  0.039 </td><td>  0 </td></tr><tr><td>  Syncfusion </td><td>  98.358 </td><td>  1.632 </td><td>  0.039 </td><td>  0.848 </td><td>  0.745 </td><td>  0.01 </td></tr></tbody></table></div><br><p>  Syncfusion immediately stood out, which not only was able to successfully process the smallest number of documents, but also dumped the entire process on some documents (by generating exceptions like OutOfMemoryException or exceptions from the native code that were not caught without dancing with a tambourine). </p><br><p> GroupDocs could not process approximately <b>5.5</b> times more documents than DevExpress (everything can be seen on the table above).  This is despite the fact that GroupDocs has a license for one developer approximately 9 times more expensive than a license for one developer from DevExpress.  This is so, by the way. </p><br><p>  The second serious test is the conversion time, the same 120 thousand documents: </p><br><div class="scrollable-table"><table><tbody><tr><th>  Converter </th><th>  Mean (sec.) </th><th>  Median (sec.) </th><th>  Std (sec.) </th></tr><tr><td>  Groupdocs </td><td>  1.301966 </td><td>  0.328000 </td><td>  6.401197 </td></tr><tr><td>  DevExpress </td><td>  0.523453 </td><td>  0.252000 </td><td>  1.781898 </td></tr><tr><td>  Syncfusion </td><td>  8.922892 </td><td>  4.987000 </td><td>  12.929588 </td></tr></tbody></table></div><br><img src="https://habrastorage.org/webt/g5/2h/3k/g52h3klfh-leliffdwmrtmgapxy.png"><br><br><p>  Note that DevExpress not only processes documents on average much faster, but also shows a much more stable processing time. </p><br><p>  But the stability and processing speed mean nothing if the output is a bad pdf.  Maybe DevExpress skips half the text?  We are checking.  So, the same 120 thousand documents, this time we calculate the total amount of extracted text and the average share of vocabulary words (the more extracted words are dictionary, the less garbage / incorrectly extracted text): </p><br><div class="scrollable-table"><table><tbody><tr><th>  Converter </th><th>  Total amount of text (in characters) </th><th>  Average share of vocabulary words </th></tr><tr><td>  Groupdocs </td><td>  6 321 145 966 </td><td>  0.949172 </td></tr><tr><td>  DevExpress </td><td>  6 135 668 416 </td><td>  0.950629 </td></tr><tr><td>  Syncfusion </td><td>  5,995,008,572 </td><td>  0.938693 </td></tr></tbody></table></div><br><p>  Partly the assumption turned out to be true.  As it turned out, GroupDocs, unlike DevExpress, can work with footnotes.  DevExpress simply skips them when converting a document to pdf.  By the way, yes, the text from the received pdf'ok in all cases is retrieved by means of DevExpress. </p><br><p>  So, we have studied the speed and stability of the libraries in question, now we will carefully evaluate the quality of the conversion of pdf documents.  To do this, we will analyze not just the volume of the extracted text and the share of vocabulary words in it, but we will compare the texts extracted from the received pdf's with the texts of the pdf'ok obtained through MS Word.  We accept the result of converting a document via MS Word for the <i>reference pdf</i> .  For this test about 4500 pairs of ‚Äú <i>document, reference pdf'ka</i> ‚Äù were prepared. </p><br><div class="scrollable-table"><table><tbody><tr><th rowspan="2">  Converter </th><th rowspan="2">  <abbr title="The percentage of documents whose text the library was able to extract">Text extracted (%)</abbr> </th><th colspan="3">  Proximity to the length of the text </th><th colspan="3">  Proximity in word frequency </th></tr><tr><th>  The average </th><th>  Median </th><th>  <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2580%25D0%25B5%25D0%25B4%25D0%25BD%25D0%25B5%25D0%25BA%25D0%25B2%25D0%25B0%25D0%25B4%25D1%2580%25D0%25B0%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B5_%25D0%25BE%25D1%2582%25D0%25BA%25D0%25BB%25D0%25BE%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">SKO</a> </th><th>  The average </th><th>  Median </th><th>  <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2580%25D0%25B5%25D0%25B4%25D0%25BD%25D0%25B5%25D0%25BA%25D0%25B2%25D0%25B0%25D0%25B4%25D1%2580%25D0%25B0%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B5_%25D0%25BE%25D1%2582%25D0%25BA%25D0%25BB%25D0%25BE%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">SKO</a> </th></tr><tr><td>  Groupdocs </td><td>  99.131 </td><td>  0.985472 </td><td>  0.999756 </td><td>  0.095304 </td><td>  0.979952 </td><td>  1.000000 </td><td>  0.102316 </td></tr><tr><td>  DevExpress </td><td>  99.726 </td><td>  0.971326 </td><td>  0.996647 </td><td>  0.075951 </td><td>  0.965686 </td><td>  0.996101 </td><td>  0.082192 </td></tr><tr><td>  Syncfusion </td><td>  89.336 </td><td>  0.880229 </td><td>  0.996845 </td><td>  0.306920 </td><td>  0.815760 </td><td>  0.998206 </td><td>  0.348621 </td></tr></tbody></table></div><br><p>  For each pair of ‚Äú <i>reference pdf, conversion result</i> ‚Äù we calculated the similarity by the length of the extracted text and by the frequencies of the extracted words.  Naturally, these metrics were obtained only in cases where the conversion was successful.  Therefore, we do not consider the results of Syncfusion here.  DevExpress and GroupDocs showed similar performance.  On the DevExpress side, a significantly higher percentage of successful conversion, on the GD side, correct work with footnotes. </p><br><p>  Given the results, the choice was obvious.  We still use the solution from DevExpress and will soon plan to upgrade to its 19th version. </p><br><h1>  There is a PDF, extract text with formatting </h1><br><p>  So, we can convert documents to pdf.  Now we are faced with another task: using DevExpress to extract text, knowing all the information we need about each word.  Namely: </p><br><ul><li>  On which page the word; </li><li>  The location of the word on the page (framing rectangle); </li><li>  The font size of the word (characters of the word). </li></ul><br><p>  The image shows a breakdown of the text into pages, and also illustrates the correspondence of the words of the text of the page area. </p><br><p><img src="https://habrastorage.org/webt/sv/zy/ja/svzyjafy1nmstmlbblte4uhdpwg.png"></p><br><p>  <sub><em>Image Source: <a href="https://scholar.google.ru/scholar%3Fhl%3Dru%26as_sdt%3D0%252C5%26q%3DHeader%2BMetadata%2BExtraction%2Bfrom%2BScientific%2BDocuments%26btnG%3D">Header Metadata Extraction from Scientific Documents</a></em></sub> </p><br><p>  It would seem that everything should be simple.  We look, what API provides us DevExpress: </p><br><ul><li>  We have a method that returns the text of the entire document.  Plain <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2582%25D1%2580%25D0%25BE%25D0%25BA%25D0%25BE%25D0%25B2%25D1%258B%25D0%25B9_%25D1%2582%25D0%25B8%25D0%25BF">string</a> ; </li><li>  We have the ability to iterate through the words of the document.  For each word we can get: <br><ul><li>  Word text; </li><li>  The page on which the word is located; </li><li>  Framing a word rectangle; </li><li>  Information on individual characters of the word (the value of the symbol, the framing rectangle, the font size, ...). </li></ul><br></li></ul><br><p>  Okay, everything seems to be there.  But how to get the necessary data for each word in the text of the document, which returns DevExpress?  We don‚Äôt really want to collect the text of a document from words, because, for example, we don‚Äôt have information, where between words is just a space, and where is a line break.  We'll have to come up with heuristics based on the location of the words ... The text is here, we have already assembled it. </p><br><p><img src="https://habrastorage.org/webt/ti/d9/ru/tid9ruengnw06fe0krbvfqbpfle.jpeg"></p><br><p>  <sub><em>Image source: <a href="https://imgur.com/gallery/XfkhXrS">Eureka!</a></em></sub> </p><br><p>  The obvious solution is to match the words with the text of the document.  We look - indeed, in the text of the document the words are arranged in the same order in which the iterator returns them by the words of the document. </p><br><p>  We quickly implement a simple word matching algorithm with the text of the document, we add checks that everything is correctly matched, we start ... </p><br><p><img src="https://habrastorage.org/webt/uc/xw/ks/ucxwkst9qdybxsrwsgrxi4qncyk.png"></p><br><p>  Indeed, on the vast majority of pages, everything works correctly, but, unfortunately, not on all pages. </p><br><img src="https://habrastorage.org/webt/mm/sc/gb/mmscgb1gygbne-ail3tsccjs_ik.png"><br><p>  <sub><em>Upper Image Source: <a href="">Are you sure?</a></em></sub> </p><br><p>  On the part of the documents, we see that the words in the text are not in the order in which they go when iterating through the words of the document.  Moreover, it is clear that the opening square bracket in the text in the word list is represented as a closing bracket and is located in another ‚Äúword‚Äù.  The correct display of this text fragment can be seen by opening the document in MS Word.  What is even more interesting, if the document is not converted into pdf, and the text is directly extracted from the doc, then we get the third version of the text fragment, which does not coincide with either the correct order or the two other orders received from the library.  In this fragment, as well as in the majority of others, on which a similar problem arises, it is the case of invisible ‚ÄúRTL‚Äù symbols that change the order of the adjacent symbols / words. </p><br><p>  Here it is worth remembering that the quality of technical support was called important when choosing a library.  As practice has shown, in this aspect, the interaction with DevExpress is quite effective.  The problem with the submitted document was promptly corrected after we created the corresponding ticket.  A number of other issues related to exceptions / high memory consumption / long processing of documents were also fixed. </p><br><p>  However, while DevExpress does not provide a direct way to get the text with the right information for each word, we continue to compare the sometimes incomparable.  If we are unable to construct an exact match between the words and the text, we use a series of heuristics that allow small permutations of the words.  If nothing has helped - the document is left without formatting.  Rarely, but it happens. </p><br><p>  Until :) </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/458842/">https://habr.com/ru/post/458842/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458830/index.html">Dell Technologies Webinars: all the details about our tutorial</a></li>
<li><a href="../458834/index.html">The side of the personality of Paul Allen, about which not many people knew as I would like</a></li>
<li><a href="../458836/index.html">Index borscht. A systematic approach to assessing, comparing, determining the price / quality ratio</a></li>
<li><a href="../45884/index.html">Symfony 1.2</a></li>
<li><a href="../458840/index.html">How we pierced the Great Chinese Firewall (Part 2)</a></li>
<li><a href="../458844/index.html">Silo destruction through adaptation of the VeriSM ‚Ñ¢ approach</a></li>
<li><a href="../458846/index.html">How to develop another platformer using Unity. Another tutorial</a></li>
<li><a href="../458848/index.html">Release Rust 1.36.0: Trey Future, stabilization alloc and MaybeUninit <T></a></li>
<li><a href="../45885/index.html">when will they collide?</a></li>
<li><a href="../458850/index.html">We learn English cheaply and effectively. Part 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>