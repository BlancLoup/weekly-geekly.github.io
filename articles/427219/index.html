<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Useless deferred non-blocking messaging in MPI: light analytics and tutorial for those who are a little "in the subject"</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Most recently, I had to solve another trivial learning task from my teacher. However, solving it, I managed to pay attention to things about which I h...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Useless deferred non-blocking messaging in MPI: light analytics and tutorial for those who are a little "in the subject"</h1><div class="post__text post__text-html js-mediator-article">  Most recently, I had to solve another trivial learning task from my teacher.  However, solving it, I managed to pay attention to things about which I had not thought about at all, maybe you didn‚Äôt even think about it.  This article will most likely be useful to students and to all who begin their way into the world of parallel programming using MPI. <br><br><img src="https://habrastorage.org/webt/v0/ik/2-/v0ik2-rxhe1gexlrsumqpwhslbu.jpeg"><br><br><h2>  Our "Given:" </h2><br>  So, the essence of ours, in the essence of the computational problem, is to compare how many times a program using non-blocking delayed point-to-point transmissions is faster than one that uses blocking point-to-point transmissions.  We will carry out measurements for input arrays with dimensions of 64, 256, 1024, 4096, 8192, 16384, 65536, 262144, 1048576, 4194304, 16777216, 33554432 elements.  By default, it is proposed to solve it by four processes.  And here, actually, and that we will consider: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <a name="habracut"></a><img src="https://habrastorage.org/webt/rt/vc/vo/rtvcvob3gfonidkax7qtskxfbc0.png"><cut></cut><br><br>  At the output, we should have three vectors: Y1, Y2 and Y3, which the zero process will collect.  I will test the whole thing on my system based on <a href="https://ark.intel.com/ru/products/88967/Intel-Core-i7-6700HQ-Processor-6M-Cache-up-to-3-50-GHz-">an Intel processor</a> with 16 GB of RAM.  To develop programs, we will use the implementation of the <a href="https://www.microsoft.com/en-us/download/details.aspx%3Fid%3D56727">MPI</a> standard <a href="https://www.microsoft.com/en-us/download/details.aspx%3Fid%3D56727">from Microsoft version 9.0.1</a> (at the time of this writing, it is relevant), Visual Studio Community 2017 and not Fortran. <br><br><h2>  Materiel </h2><br>  I would not like to describe in detail how the functions of MPI work, which will be used, you can always follow it and <a href="https://docs.microsoft.com/ru-ru/">look into the documentation</a> , so I will only give a brief overview of what we will use. <br><br><h4>  Blocking exchange </h4><br>  <b>For blocking point-to-point messaging, we will use the functions:</b> <br><br>  <a href="https://docs.microsoft.com/en-us/message-passing-interface/mpi-send-function">MPI_Send</a> - performs blocking message sending, i.e.  after calling the function, the process is blocked until the data it sends will not be written from its memory into the internal buffer of the MPI environment, after the process continues to work further; <br>  <a href="https://docs.microsoft.com/en-us/message-passing-interface/mpi-recv-function">MPI_Recv</a> - performs blocking message reception, i.e.  after calling a function, the process is blocked until data from the sending process is received and until this data is completely written to the buffer of the receiving process by the MPI environment. <br><br><h4>  Deferred non-blocking exchange </h4><br>  <b>For deferred non-blocking point-to-point messaging, we will use the functions:</b> <br><br>  <a href="https://docs.microsoft.com/en-us/message-passing-interface/mpi-send-init-function">MPI_Send_init</a> - in the background prepares the medium for sending data that will occur in some future and no locks; <br>  <a href="https://docs.microsoft.com/en-us/message-passing-interface/mpi-recv-init-function">MPI_Recv_init</a> - this function works similarly to the previous one, only this time for receiving data; <br>  <a href="https://docs.microsoft.com/en-us/message-passing-interface/mpi-start-function">MPI_Start</a> - starts the process of receiving or transmitting a message, it also runs in the background ak.a.  without blocking; <br>  <a href="https://docs.microsoft.com/en-us/message-passing-interface/mpi-wait-function">MPI_Wait</a> - used to check and, if necessary, wait for the completion of the parcel or receive a message, but it just blocks the process if necessary (if the data is ‚Äúundersubtracted‚Äù or ‚Äúnot received‚Äù).  For example, a process wants to use data that has not reached it yet - not well, therefore we insert MPI_Wait before the place where it will need this data (we insert it even if there is just a risk of data corruption).  Another example, the process started the background data transfer, and after starting the data transfer, it immediately began to somehow change this data - not good, so we insert MPI_Wait before the place in the program where it starts to change this data (we also insert it here even if there is just the risk of data corruption). <br><br>  Thus, the <i>semantically</i> sequence of calls during a deferred non-blocking exchange is as follows: <br><br><ol><li>  MPI_Send_init / MPI_Recv_init - prepare the environment for reception or transmission </li><li>  MPI_Start - start the transmit / receive process </li><li>  MPI_Wait - we call if there is a risk of damage (including ‚Äúunder-sending‚Äù and ‚Äúunder-receiving‚Äù) of transmitted or received data </li></ol><br>  I also used <a href="https://docs.microsoft.com/en-us/message-passing-interface/mpi-startall-function">MPI_Startall</a> , <a href="https://docs.microsoft.com/en-us/message-passing-interface/mpi-waitall-function">MPI_Waitall</a> in my test programs, their meaning is basically the same as MPI_Start and MPI_Wait, respectively, only they operate with several packages and / or transmissions.  But this is far from the entire list of start and wait functions, there are several more functions for checking the completeness of operations. <br><br><h2>  Interprocess communication architecture </h2><br>  For clarity, we construct a graph for performing calculations using four processes.  At the same time, it is necessary to try to distribute all vector arithmetic operations evenly among the processes.  Here's what I got: <br><br><img src="https://habrastorage.org/webt/bq/_b/q4/bq_bq43yrgkgjayi8p5uy0g9ckk.png"><br><br>  See these arrays T0-T2?  These are buffers for storing intermediate results of operations.  Also on the graph in the transmission of messages from one process to another at the beginning of the arrow is the name of the array, whose data is transmitted, and at the end of the arrow - the array that receives this data. <br><br>  Well, when we finally answered the questions: <br><br><ol><li>  What kind of problem we solve? </li><li>  What tools will we use to solve it? </li><li>  How will we solve it? </li></ol><br>  It remains only to solve it ... <br><br><h2>  Our "Solution:" </h2><br>  Next, I will present the codes of the two programs that were discussed above, but first I will give a little more explanation of what and how. <br><br>  I carried out all vector arithmetic operations in separate procedures (add, sub, mul, div) in order to increase the readability of the code.  All input arrays are initialized in accordance with the formulas, which I have indicated <i>almost</i> at random.  Since the zero process assembles the results of work from all other processes, therefore, it works the longest, so it is logical to consider its work time equal to the program execution time (as we remember, we are interested in: arithmetic + messaging) in both the first and second cases.  We will measure time intervals using the <a href="https://docs.microsoft.com/en-us/message-passing-interface/mpi-wtime-function">MPI_Wtime</a> function and at the same time I decided to deduce what resolution I have there with <a href="https://docs.microsoft.com/en-us/message-passing-interface/mpi-wtick-function">MPI_Wtick</a> (somewhere in my heart I hope that they will be tied to my invariant TSC, in this case, I‚Äôm even ready to forgive them for the error associated with the call time of the function MPI_Wtime).  So, let's put together all of what I wrote above and, in accordance with the graph, we will finally develop these programs (and of course we will debug too). <br><br><hr><br>  Who is interested to see the code: <br><br><div class="spoiler">  <b class="spoiler_title">Program with blocking data transfers</b> <div class="spoiler_text"><pre><code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pch.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;fstream&gt; #include &lt;mpi.h&gt; using namespace std; void add(double *A, double *B, double *C, int n); void sub(double *A, double *B, double *C, int n); void mul(double *A, double *B, double *C, int n); void div(double *A, double *B, double *C, int n); int main(int argc, char **argv) { if (argc &lt; 2) { return 1; } int n = atoi(argv[1]); int rank; double start_time, end_time; MPI_Status status; double *A = new double[n]; double *B = new double[n]; double *C = new double[n]; double *D = new double[n]; double *E = new double[n]; double *G = new double[n]; double *T0 = new double[n]; double *T1 = new double[n]; double *T2 = new double[n]; for (int i = 0; i &lt; n; i++) { A[i] = double (2 * i + 1); B[i] = double(2 * i); C[i] = double(0.003 * (i + 1)); D[i] = A[i] * 0.001; E[i] = B[i]; G[i] = C[i]; } cout.setf(ios::fixed); cout &lt;&lt; fixed &lt;&lt; setprecision(9); MPI_Init(&amp;argc, &amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); if (rank == 0) { start_time = MPI_Wtime(); sub(A, B, T0, n); MPI_Send(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD); MPI_Send(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); div(T0, G, T1, n); MPI_Recv(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;status); add(T1, T2, T0, n); mul(T0, T1, T2, n); MPI_Recv(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD); add(T0, T2, T1, n); MPI_Recv(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;status); MPI_Recv(T2, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;status); end_time = MPI_Wtime(); cout &lt;&lt; "Clock resolution: " &lt;&lt; MPI_Wtick() &lt;&lt; " secs" &lt;&lt; endl; cout &lt;&lt; "Thread " &lt;&lt; rank &lt;&lt; " execution time: " &lt;&lt; end_time - start_time &lt;&lt; endl; } if (rank == 1) { add(C, C, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); mul(T1, G, T2, n); add(T2, C, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); sub(T1, T0, T2, n); MPI_Recv(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;status); add(T0, T2, T1, n); MPI_Send(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); } if (rank == 2) { mul(C, C, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;status); MPI_Recv(T2, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;status); MPI_Send(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD); MPI_Send(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); add(T1, T2, T0, n); mul(T0, G, T1, n); MPI_Recv(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;status); mul(T1, T2, T0, n); MPI_Recv(T1, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;status); mul(T0, T1, T2, n); MPI_Send(T2, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD); } if (rank == 3) { mul(E, D, T0, n); MPI_Send(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); sub(T0, B, T1, n); mul(T1, T1, T2, n); sub(T1, G, T0, n); mul(T0, T2, T1, n); MPI_Send(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD); } MPI_Finalize(); delete[] A; delete[] B; delete[] C; delete[] D; delete[] E; delete[] G; delete[] T0; delete[] T1; delete[] T2; return 0; } void add(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] + B[i]; } } void sub(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] - B[i]; } } void mul(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] * B[i]; } } void div(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] / B[i]; } }</span></span></span></span></code> </pre> </div></div><br><div class="spoiler">  <b class="spoiler_title">Program with deferred non-blocking data transfers</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pch.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;fstream&gt; #include &lt;mpi.h&gt; using namespace std; void add(double *A, double *B, double *C, int n); void sub(double *A, double *B, double *C, int n); void mul(double *A, double *B, double *C, int n); void div(double *A, double *B, double *C, int n); int main(int argc, char **argv) { if (argc &lt; 2) { return 1; } int n = atoi(argv[1]); int rank; double start_time, end_time; MPI_Request request[7]; MPI_Status statuses[4]; double *A = new double[n]; double *B = new double[n]; double *C = new double[n]; double *D = new double[n]; double *E = new double[n]; double *G = new double[n]; double *T0 = new double[n]; double *T1 = new double[n]; double *T2 = new double[n]; for (int i = 0; i &lt; n; i++) { A[i] = double(2 * i + 1); B[i] = double(2 * i); C[i] = double(0.003 * (i + 1)); D[i] = A[i] * 0.001; E[i] = B[i]; G[i] = C[i]; } cout.setf(ios::fixed); cout &lt;&lt; fixed &lt;&lt; setprecision(9); MPI_Init(&amp;argc, &amp;argv); MPI_Comm_rank(MPI_COMM_WORLD, &amp;rank); if (rank == 0) { start_time = MPI_Wtime(); MPI_Send_init(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[0]);// MPI_Send_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[1]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[2]);// MPI_Recv_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[3]);// MPI_Send_init(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[4]);// MPI_Recv_init(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[5]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[6]);// MPI_Start(&amp;request[2]); sub(A, B, T0, n); MPI_Startall(2, &amp;request[0]); div(T0, G, T1, n); MPI_Waitall(3, &amp;request[0], statuses); add(T1, T2, T0, n); mul(T0, T1, T2, n); MPI_Startall(2, &amp;request[3]); MPI_Wait(&amp;request[3], &amp;statuses[0]); add(T0, T2, T1, n); MPI_Startall(2, &amp;request[5]); MPI_Wait(&amp;request[4], &amp;statuses[0]); MPI_Waitall(2, &amp;request[5], statuses); end_time = MPI_Wtime(); cout &lt;&lt; "Clock resolution: " &lt;&lt; MPI_Wtick() &lt;&lt; " secs" &lt;&lt; endl; cout &lt;&lt; "Thread " &lt;&lt; rank &lt;&lt; " execution time: " &lt;&lt; end_time - start_time &lt;&lt; endl; } if (rank == 1) { MPI_Recv_init(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[0]);// MPI_Send_init(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[1]);// MPI_Recv_init(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[2]);// MPI_Send_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[3]);// MPI_Recv_init(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[4]);// MPI_Send_init(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[5]);// MPI_Start(&amp;request[0]); add(C, C, T0, n); MPI_Start(&amp;request[1]); MPI_Wait(&amp;request[0], &amp;statuses[0]); mul(T1, G, T2, n); MPI_Start(&amp;request[2]); MPI_Wait(&amp;request[1], &amp;statuses[0]); add(T2, C, T0, n); MPI_Start(&amp;request[3]); MPI_Wait(&amp;request[2], &amp;statuses[0]); sub(T1, T0, T2, n); MPI_Wait(&amp;request[3], &amp;statuses[0]); MPI_Start(&amp;request[4]); MPI_Wait(&amp;request[4], &amp;statuses[0]); add(T0, T2, T1, n); MPI_Start(&amp;request[5]); MPI_Wait(&amp;request[5], &amp;statuses[0]); } if (rank == 2) { MPI_Recv_init(T1, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[0]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;request[1]);// MPI_Send_init(T0, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[2]);// MPI_Send_init(T0, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[3]);// MPI_Recv_init(T2, n, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &amp;request[4]);// MPI_Recv_init(T1, n, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &amp;request[5]);// MPI_Send_init(T2, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &amp;request[6]);// MPI_Startall(2, &amp;request[0]); mul(C, C, T0, n); MPI_Startall(2, &amp;request[2]); MPI_Waitall(4, &amp;request[0], statuses); add(T1, T2, T0, n); MPI_Start(&amp;request[4]); mul(T0, G, T1, n); MPI_Wait(&amp;request[4], &amp;statuses[0]); mul(T1, T2, T0, n); MPI_Start(&amp;request[5]); MPI_Wait(&amp;request[5], &amp;statuses[0]); mul(T0, T1, T2, n); MPI_Start(&amp;request[6]); MPI_Wait(&amp;request[6], &amp;statuses[0]); } if (rank == 3) { MPI_Send_init(T0, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[0]); MPI_Send_init(T1, n, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &amp;request[1]); mul(E, D, T0, n); MPI_Start(&amp;request[0]); sub(T0, B, T1, n); mul(T1, T1, T2, n); MPI_Wait(&amp;request[0], &amp;statuses[0]); sub(T1, G, T0, n); mul(T0, T2, T1, n); MPI_Start(&amp;request[1]); MPI_Wait(&amp;request[1], &amp;statuses[0]); } MPI_Finalize(); delete[] A; delete[] B; delete[] C; delete[] D; delete[] E; delete[] G; delete[] T0; delete[] T1; delete[] T2; return 0; } void add(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] + B[i]; } } void sub(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] - B[i]; } } void mul(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] * B[i]; } } void div(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] / B[i]; } }</span></span></span></span></code> </pre></div></div><br><hr><br><h2>  Testing and analysis </h2><br>  Let's run our programs for arrays of different sizes and see what happens.  The test results are summarized in a table, in the last column of which we calculate and write down the acceleration factor, which we define as follows: K us = T <sub>ex.</sub>  <sub>non-block</sub>  / T <sub>block</sub> <br><br><img src="https://habrastorage.org/webt/ba/hg/zv/bahgzvsgz67vnkfsji-swsyy_cg.png"><br><br>  If you look at this table a little more attentively than usual, then you will notice that with an increase in the number of elements processed, the acceleration factor decreases somehow like this: <br><br><img src="https://habrastorage.org/webt/qq/to/hv/qqtohvv7azbc05r6x4mwtfbbxfe.png"><br><br>  Let's try to determine what is the matter?  For this, I propose to write a small test program that will measure the time of each vector arithmetic operation and carefully reduce the results into an ordinary text file. <br><br><hr><br>  Here, in fact, the program itself: <br><br><div class="spoiler">  <b class="spoiler_title">Time measurement</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pch.h"</span></span></span><span class="hljs-meta"> #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;iomanip&gt; #include &lt;Windows.h&gt; #include &lt;fstream&gt; using namespace std; void add(double *A, double *B, double *C, int n); void sub(double *A, double *B, double *C, int n); void mul(double *A, double *B, double *C, int n); void div(double *A, double *B, double *C, int n); int main() { struct res { double add; double sub; double mul; double div; }; int i, j, k, n, loop; LARGE_INTEGER start_time, end_time, freq; ofstream fout("test_measuring.txt"); int N[12] = { 64, 256, 1024, 4096, 8192, 16384, 65536, 262144, 1048576, 4194304, 16777216, 33554432 }; SetConsoleOutputCP(1251); cout &lt;&lt; "   loop: "; cin &gt;&gt; loop; fout &lt;&lt; setiosflags(ios::fixed) &lt;&lt; setiosflags(ios::right) &lt;&lt; setprecision(9); fout &lt;&lt; " : " &lt;&lt; loop &lt;&lt; endl; fout &lt;&lt; setw(10) &lt;&lt; "\n " &lt;&lt; setw(30) &lt;&lt; ".   (c)" &lt;&lt; setw(30) &lt;&lt; ".   (c)" &lt;&lt; setw(30) &lt;&lt; ".  (c)" &lt;&lt; setw(30) &lt;&lt; ".   (c)" &lt;&lt; endl; QueryPerformanceFrequency(&amp;freq); cout &lt;&lt; "\n : " &lt;&lt; freq.QuadPart &lt;&lt; " " &lt;&lt; endl; for (k = 0; k &lt; sizeof(N) / sizeof(int); k++) { res output = {}; n = N[k]; double *A = new double[n]; double *B = new double[n]; double *C = new double[n]; for (i = 0; i &lt; n; i++) { A[i] = 2.0 * i; B[i] = 2.0 * i + 1; C[i] = 0; } for (j = 0; j &lt; loop; j++) { QueryPerformanceCounter(&amp;start_time); add(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.add += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); QueryPerformanceCounter(&amp;start_time); sub(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.sub += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); QueryPerformanceCounter(&amp;start_time); mul(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.mul += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); QueryPerformanceCounter(&amp;start_time); div(A, B, C, n); QueryPerformanceCounter(&amp;end_time); output.div += double(end_time.QuadPart - start_time.QuadPart) / double(freq.QuadPart); } fout &lt;&lt; setw(10) &lt;&lt; n &lt;&lt; setw(30) &lt;&lt; output.add / loop &lt;&lt; setw(30) &lt;&lt; output.sub / loop &lt;&lt; setw(30) &lt;&lt; output.mul / loop &lt;&lt; setw(30) &lt;&lt; output.div / loop &lt;&lt; endl; delete[] A; delete[] B; delete[] C; } fout.close(); cout &lt;&lt; endl; system("pause"); return 0; } void add(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] + B[i]; } } void sub(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] - B[i]; } } void mul(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] * B[i]; } } void div(double *A, double *B, double *C, int n) { for (size_t i = 0; i &lt; n; i++) { C[i] = A[i] / B[i]; } }</span></span></span></span></code> </pre></div></div><br><hr><br>  At startup, she asks to enter the number of measurement cycles, I tested for 10,000 cycles.  At the output we get the average result for each operation <br><br><img src="https://habrastorage.org/webt/iz/0g/bs/iz0gbs8ilynlmbchxtga_0at61s.png"><br><br>  To measure time, I used a high-level <a href="https://msdn.microsoft.com/ru-ru/library/windows/desktop/ms644904(v%3Dvs.85).aspx">QueryPerformanceCounter</a> .  I strongly recommend reading <a href="https://docs.microsoft.com/ru-ru/windows/desktop/SysInfo/acquiring-high-resolution-time-stamps">this FAQ</a> so that most of the questions on the measurement of time by this function disappear by themselves.  According to my observations, she clings to TSC (but theoretically, maybe not to him), but returns, according to the certificate, the current number of ticks of the counter.  But the fact is that my meter cannot physically measure the time interval of 32 ns (see the first row of the results table).  This result is due to the fact that between two calls to the QueryPerformanceCounter, 0 ticks pass, then 1. For the first row in the table, we can only conclude that about a third of the 10,000 results equals 1 tick.  <i>So the data in this table for 64, 256 and even for 1024 elements is something quite approximate.</i>  Now, let's open any of the programs and calculate how many total operations of each type are found in it, traditionally ‚Äúspreading‚Äù everything on the next table: <br><br><img src="https://habrastorage.org/webt/et/vo/w2/etvow2fj-vxgtusc9aez__9egxq.png"><br><br>  Finally, we know the time of each vector arithmetic operation and how much it is in our program, try to find out how much time is spent on these operations in parallel programs and how much time is spent on blocking and delayed non-blocking data exchange between processes and again, for clarity, we reduce it to table: <br><br><img src="https://habrastorage.org/webt/no/k2/-0/nok2-0p1kpw8g1ahveqog4q9lzi.png"><br><br>  Based on the results of the obtained data, we plot three functions: the first describes the change in time spent on blocking transfers between processes, on the number of array elements, the second on changing the time spent on deferred non-blocking transfers between processes on the number of array elements and the third one on changing time spending on arithmetic operations, from the number of elements of the arrays: <br><br><img src="https://habrastorage.org/webt/4e/w6/5h/4ew65htbb-s3vh7anlo0txafxpg.png"><br><br>  As you have already noticed, the vertical scale of the graph is logarithmic, it is a necessary measure, since  the scatter of times is too large and on a regular chart you would not see anything at all.  Pay attention to the function of the dependence of the time spent on arithmetic on the number of elements, it safely overtakes the other two functions by about 1 million elements already.  The thing is that it grows at infinity faster than its two opponents.  Therefore, with an increase in the number of processed elements, the running time of programs is increasingly determined by arithmetic, and not by transmissions.  Suppose that you have increased the number of transfers between processes, conceptually you will see that the moment when the function of arithmetic will overtake the other two will happen later. <br><br><h2>  Results </h2><br>  Thus, continuing to increase the length of the arrays, you will come to the conclusion that a program with deferred non-blocking transfers will be only quite a bit faster than the one that uses blocking exchange.  And if you rush the length of the arrays to infinity (well, or just take too long arrays), then the time of your program will be 100% read by calculations, and the acceleration factor will safely strive for 1. </div><p>Source: <a href="https://habr.com/ru/post/427219/">https://habr.com/ru/post/427219/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../427207/index.html">Rockstar employees defended the company after criticizing 100-hour work weeks.</a></li>
<li><a href="../427209/index.html">GeoPuzzle - collect the world in pieces</a></li>
<li><a href="../427211/index.html">Electron is a flash for desktop</a></li>
<li><a href="../427215/index.html">You need to grow up to microservices, and not start with them</a></li>
<li><a href="../427217/index.html">Performance Analysis of WSGI Servers: Part Two</a></li>
<li><a href="../427221/index.html">What I realized on the way to my dream of artificial intelligence</a></li>
<li><a href="../427223/index.html">What are the responsibilities of the lead developer</a></li>
<li><a href="../427225/index.html">Oracle Database 18c XE Released</a></li>
<li><a href="../427227/index.html">How we made a board game with a remote control - Part 2</a></li>
<li><a href="../427229/index.html">4 years of the game projects management program</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>