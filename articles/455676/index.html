<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Whether the bubble of machine learning burst, or the beginning of a new dawn</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recently an article has been published that shows quite well the trend in machine learning in recent years. In short, the number of machine learning s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Whether the bubble of machine learning burst, or the beginning of a new dawn</h1><div class="post__text post__text-html js-mediator-article">  Recently <a href="https://www.getrevue.co/profile/peterzhegin/issues/ai-investment-activity-trends-of-2018-issue-8-150825%3Ffbclid%3DIwAR0tU3l4WSotv7pQpYm8PmyKgVbgUgfMLeue_IiV78lXXApH-cy9EcG2kDc">an article</a> has been published that shows quite well the trend in machine learning in recent years.  In short, the number of machine learning startups has plummeted in the past two years. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1c6/466/4fc/1c64664fcaf125f2104e67547b533e41.png" alt="image"></div><br>  Well.  Let's analyze whether the bubble has burst, how to continue to live, and we‚Äôll talk about where such a squiggle comes from. <br><a name="habracut"></a><br>  First, let's talk about what was a booster of this curve.  Where did she come from.  Probably everyone will remember the <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">victory of</a> machine learning in 2012 in the competition ImageNet.  After all, this is the first global event!  But in reality it is not.  And the growth of the curve begins a little earlier.  I would break it into a few moments. <br><br><ol><li>  2008 is the emergence of the term ‚Äúbig data‚Äù.  Real products began <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D0%25BE%25D0%25BB%25D1%258C%25D1%2588%25D0%25B8%25D0%25B5_%25D0%25B4%25D0%25B0%25D0%25BD%25D0%25BD%25D1%258B%25D0%25B5">to appear</a> from 2010.  Big data is directly related to machine learning.  Without big data, stable operation of algorithms that existed at that time is impossible.  And these are not neural networks.  Until 2012, neural networks are the lot of the marginal minority.  But then completely different algorithms began to work, which have existed for years, or even decades: <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25BE%25D0%25BF%25D0%25BE%25D1%2580%25D0%25BD%25D1%258B%25D1%2585_%25D0%25B2%25D0%25B5%25D0%25BA%25D1%2582%25D0%25BE%25D1%2580%25D0%25BE%25D0%25B2">SVM</a> (1963, 1993), <a href="https://ru.wikipedia.org/wiki/Random_forest">Random Forest</a> (1995), <a href="https://en.wikipedia.org/wiki/AdaBoos">AdaBoost</a> (2003), ... Startups of those years are primarily associated with automatic processing of structured data : box office, users, advertising, much more. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The derivative of this first wave is a set of frameworks such as XGBoost, CatBoost, LightGBM, etc. <br></li><li>  In 2011-2012, <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural networks</a> won a series of image recognition contests.  Their actual use was somewhat delayed.  I would say that massively meaningful startups and solutions began to appear in 2014.  It took two years to digest that neurons still work, make convenient frameworks that could be installed and run in a reasonable time, develop methods that would stabilize and speed up the time of convergence. <br><br>  Convolutional networks made it possible to solve problems of machine vision: the classification of images and objects in an image, the detection of objects, the recognition of objects and people, the improvement of images, etc., etc. </li><li>  2015-2017 years.  The boom of algorithms and projects linked to recurrent networks or their analogues (LSTM, GRU, TransformerNet, etc.).  Well-functioning speech-to-text algorithms, machine translation systems appeared.  Partially, they are based on convolutional networks to highlight basic features.  Partly on the fact that they learned how to collect really big and good datasets. </li></ol><br><img src="https://habrastorage.org/webt/_c/bj/f2/_cbjf2doqjypuqwfwh1d8_vx92a.png"><br><br>  ‚ÄúBubble burst?  HYIP overheated?  Did they die like a blockchain? ‚Äù <br>  Well then!  Tomorrow Siri will stop working on your phone, and the day after Tesla will not distinguish the turn from the kangaroo. <br><br>  Neural networks are already working.  They are dozens of devices.  They really allow you to make money, change the market and the world around it.  HYIP looks a little different: <br><br><img src="https://habrastorage.org/webt/zl/7m/ph/zl7mphh3m3rzprgxpbaopha3gwy.png"><br><br>  Just neural networks have ceased to be something new.  Yes, many people have high expectations.  But a large number of companies have learned how to apply neurons and make products based on them.  Neurons provide new functionality, allow to reduce workplaces, reduce the price of services: <br><br><ul><li>  Manufacturing companies integrate algorithms for the analysis of defects on the conveyor. </li><li>  Cattle farms buy systems to control cows. </li><li>  Automatic combines. </li><li>  Automated Call Centers. </li><li>  Filters in.  ( <s>well, at least something sensible!</s> ) </li></ul><br>  But the main thing, and not the most obvious: ‚ÄúNew ideas are no more, or they will not bring instant capital.‚Äù  Neural networks have solved dozens of problems.  And they will decide even more.  All the obvious ideas that were - spawned many startups.  But all that was on the surface - already collected.  Over the past two years I have not met a single new idea for the use of neural networks.  No new approach (well, ok, there are some problems with GANs). <br><br>  And each next startup is more and more difficult.  It requires no longer two guys who teach neuron on open data.  It requires programmers, a server, a team of markers, complex support, and so on. <br><br>  As a result, startups are getting smaller.  But more production.  Need to attach license plate recognition?  There are hundreds of professionals on the market with relevant experience.  You can hire and a couple of months your employee will make the system.  Or buy ready.  But to make a new startup? .. Madness! <br><br>  We need to make a visitor tracking system - why pay for a bunch of licenses, when you can make your own in 3-4 months, sharpen it for your business. <br><br>  Now neural networks go the same way that dozens of other technologies have gone. <br><br>  Remember how the concept of "site developer" has changed since 1995?  While the market is not saturated with experts.  There are very few professionals.  But I can bet that in 5-10 years there will not be much difference between a Java programmer and a developer of neural networks.  And those and those experts will be enough in the market. <br><br>  There will simply be a class of tasks for which is solved by neurons.  There was a task - you hire a specialist. <br><br>  <b>"What's next?</b>  <b>Where is the promised artificial intelligence? ‚Äù</b> <br><br>  And here there is a small but interesting neponyatchka :) <br><br>  The technology stack that exists today, apparently, will not lead us to artificial intelligence.  Ideas, their novelty - largely exhausted themselves.  Let's talk about what keeps the current level of development. <br><br><h3>  Restrictions </h3><br>  Let's start with the auto-drones.  It seems clear that it is possible to make fully autonomous cars with today's technologies.  But after how many years it will happen - it is not clear.  Tesla believes that this will happen in a couple of years - <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Ucp0TTmvqOE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  There are many other <a href="https://beth.technology/truths-autonomous-vehicles/">professionals</a> who rate this as 5-10 years. <br><br>  Most likely, in my opinion, in 15 years, the infrastructure of cities will change itself in such a way that the emergence of autonomous cars will become inevitable, become its continuation.  But this cannot be considered an intellect.  Modern Tesla is a very complex pipeline for filtering data, their search and retraining.  These are rules, rules, rules, data collection and filters above them ( <a href="http://cv-blog.ru/%3Fp%3D279">here</a> I wrote a little more about it, or look at <a href="https://www.youtube.com/watch%3Ftime_continue%3D7614%26v%3DUcp0TTmvqOE">this</a> mark). <br><br><h3>  First problem </h3><br>  And it is here that we see the <b>first fundamental problem</b> .  Big data.  This is exactly what caused the current wave of neural networks and machine learning.  Now, to do something complex and automatic you need a lot of data.  Not just a lot, but very, very much.  We need automated algorithms for their collection, markup, use.  We want to make the car see the trucks against the sun - you must first collect enough of them.  We want the car to not go crazy with the bike bolted to the trunk - more samples. <br><br>  And one example is not enough.  Hundreds?  Thousands? <br><br><img src="https://habrastorage.org/webt/hl/tm/ip/hltmipml3fq_m-md4ansomrxjmk.jpeg"><br><br><h3>  Second problem </h3><br>  <b>The second problem</b> is the visualization of what our neural network understood.  This is a very nontrivial task.  Until now, few people understand how to visualize it.  These articles are very recent, these are just a few examples, even remote ones: <br>  <a href="https://habr.com/ru/company/ods/blog/453788/">Visualization of</a> obsession with textures.  Well shows, on what the neuron is inclined to go in cycles + that it perceives as the initial information. <br><br><img src="https://habrastorage.org/webt/d7/wb/5f/d7wb5fbpcbugnrcma1qrnn9vyc0.png" alt="image"><br>  Attentive <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">visualization</a> during <a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/">translations</a> .  In reality, attenuation can often be used precisely to show what caused such a network response.  I met such things for debugging and for product solutions.  There are a lot of articles on this topic.  But the more complex the data, the harder it is to understand how to achieve sustainable visualization. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/154/93f/988/15493f988978760639233843c9c28c91.png" alt="image"><br><br>  Well, yes, the good old set of "look at the grid inside the <a href="https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030">filters</a> ."  These pictures were popular 3-4 years ago, but everyone quickly realized that the pictures were beautiful, but there was not much point in them. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f68/78c/a4a/f6878ca4a732ef4890e1ad9d8a369896.jpg" alt="image"><br><br>  I did not name dozens of other gadgets, ways, hacks, research on how to display the insides of the network.  Do these tools work?  Do they help to quickly understand what the problem is and to debug the network? .. Pull out the last percent?  Well, about the same: <br><br><img width="600" src="https://habrastorage.org/webt/eo/p0/i6/eop0i67mupthvfm86dwn139kkha.jpeg"><br><br>  You can watch any competition on Kaggle.  And a description of how people make final decisions.  We nastakali 100-500-800 models and it worked! <br><br>  I, of course, exaggerate.  But these approaches do not give quick and direct answers. <br><br>  With sufficient experience, having tried various options, you can give a verdict on why your system made such a decision.  But to correct the behavior of the system will be difficult.  To put a crutch, to move a threshold, to add dataset, to take another backend-network. <br><br><h3>  Third problem </h3><br>  <b>The third fundamental problem</b> is that grids do not teach logic, but statistics.  Statistically this <a href="https://habr.com/ru/post/417405/">face</a> : <br><br><img src="https://habrastorage.org/webt/wa/lg/6h/walg6hlyvy_cvd7i6oojypaa0lc.png" alt="image"><br><br>  Logically - not very similar.  Neural networks do not learn something difficult, if they are not forced.  They always learn the simplest signs.  Have eyes, nose, head?  So this face!  Or give an example where the eyes will not mean a face.  And again - millions of examples. <br><br><h3>  There's a lot of room at the bottom </h3><br>  I would say that these three global problems today limit the development of neural networks and machine learning.  And where these problems are not limited - is already actively used. <br><br>  <b>This is the end?</b>  <b>Neural networks got up?</b> <br><br>  Unknown.  But, of course, everyone hopes not. <br><br>  There are many approaches and directions to solving the fundamental problems that I have highlighted above.  But so far, none of these approaches has allowed for something fundamentally new to be done, to solve something that has not yet been solved.  So far, all fundamental projects are being made on the basis of stable approaches (Tesla), or remain test projects of institutions or corporations (Google Brain, OpenAI). <br><br>  Roughly speaking, the main direction is the creation of some high-level representation of input data.  In a sense, ‚Äúmemory‚Äù.  The simplest example of memory is the various ‚ÄúEmbedding‚Äù - representations of images.  Well, for example, all face recognition systems.  The network is learning to get some stable view from the face that does not depend on rotation, lighting, resolution.  In essence, the network minimizes the metric ‚Äúdifferent faces - far away‚Äù and ‚Äúsame ‚Äî close‚Äù. <br><br><img src="https://habrastorage.org/webt/8z/re/sp/8zrespvq6y2unwlyuaeovq3fj58.png"><br><br>  For such training, tens and hundreds of thousands of examples are needed.  But the result is some of the rudiments of ‚ÄúOne-shot Learning‚Äù.  Now we do not need hundreds of faces to remember a person.  Just one face, and that's it ‚Äî we'll <a href="https://github.com/davidsandberg/facenet">find out</a> ! <br>  Only here is a problem ... The grid can learn only fairly simple objects.  When trying to distinguish not individuals, but, for example, ‚Äúpeople by clothes‚Äù (the <a href="https://medium.com/%40alitech_2017/reforming-person-re-identification-with-local-convolutional-neural-networks-17148f11f17b">Re-indentification task</a> ), the quality fails by many orders of magnitude.  And the network can no longer learn enough obvious changes of perspectives. <br><br>  Yes, and learn from the millions of examples - also somehow so-so entertainment. <br><br>  There are works to significantly reduce the election.  For example, you can immediately recall one of the first works on <b>OneShot Learning</b> <a href="https://arxiv.org/pdf/1605.06065v1.pdf">from Google</a> : <br><br><img src="https://habrastorage.org/webt/dv/h5/zt/dvh5zte1ggsunwya3tm40zqwiec.png"><br><br>  There are many such works, for example, <a href="https://pdfs.semanticscholar.org/d1c4/c4c7989102e85b5248cebfcb0cb000c3b837.pdf">1</a> or <a href="https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf">2</a> or <a href="http://www.robots.ox.ac.uk/~tvg/publications/2018/0431.pdf">3</a> . <br><br>  Minus one - usually the training works quite well on some simple, ‚ÄúMNIST examples‚Äù.  And in the transition to complex tasks - you need a large base, a model of objects, or some kind of magic. <br>  In general, one-shot training is a very interesting topic.  You find a lot of ideas.  But for the most part, those two problems that I have listed (pre-training on a huge dataset / instability on complex data) - greatly impede learning. <br><br>  On the other hand, GAN - generatively competitive networks approach the topic of Embedding.  You probably read a bunch of articles on this topic on Habr√©.  ( <a href="https://habr.com/ru/company/ods/blog/340154/">1</a> , <a href="https://habr.com/ru/company/itsumma/blog/447896/">2</a> , <a href="https://habr.com/ru/company/ods/blog/322514/">3</a> ) <br>  A feature of GAN is the formation of some internal state space (essentially the same Embedding), which allows you to draw an image.  This may be a <a href="https://github.com/shaoanlu/faceswap-GAN">person</a> may be <a href="https://github.com/sergeytulyakov/mocogan">action</a> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e25/77f/f6d/e2577ff6d435046c44f861ab5d3ce2b3.jpg" alt="image"><br><br>  The GAN problem - the more complex the generated object, the more difficult it is to describe it in the ‚Äúgenerator-discriminator‚Äù logic.  As a result, of the real applications of GAN, which are widely known, only DeepFake, which, again, manipulates the ideas of individuals (for which there is a huge base). <br><br>  I have seen very few other useful uses.  Usually some whistles perdelki with drawing pictures. <br><br>  And again.  No one has an understanding of how this will allow us to move into a bright future.  Representing logic / space in a neural network is good.  But we need a huge number of examples, we do not understand how a neuron represents this in itself, we don‚Äôt understand how to make a neuron memorize some really complex representation. <br><br>  <b>Reinforcement learning</b> is a completely different approach.  Surely you remember how Google beat everyone in Go.  Recent victories in Starcraft and in Dota.  But here everything is not so rosy and promising.  The best thing about RL and its complexity is <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">this article</a> . <br><br>  To summarize what the author wrote: <br><br><ul><li>  Boxed models do not fit / work in most cases poorly. </li><li>  Practical tasks are easier to solve in other ways.  Boston Dynamics does not use RL because of its complexity / unpredictability / calculation complexity </li><li>  To make RL work, you need a complex function.  It is often difficult to create / write </li><li>  It's hard to train models.  You have to spend a lot of time to swing and remove from local optima </li><li>  As a consequence, it is difficult to repeat the model, the model‚Äôs instability with the slightest changes </li><li>  Often overfits on some left regularities, up to a random number generator </li></ul><br>  The key point is that RL is not yet in production.  Google has some experiments ( <a href="https://ai.google/research/teams/brain/robotics/">1</a> , <a href="https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html">2</a> ).  But I have not seen a single grocery system. <br><br>  <b>Memory</b> .  The disadvantage of everything described above is unstructured.  One approach to trying to tidy up all this is to give the neural network access to a separate memory.  So that she can record and re-record the results of her steps there.  Then the neural network can be determined by the current state of the memory.  This is very similar to classic processors and computers. <br><br>  The most famous and popular <a href="https://towardsdatascience.com/rps-intro-to-differentiable-neural-computers-e6640b5aa73a">article</a> is from DeepMind: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b66/f0d/3a9/b66f0d3a9da550ad89e677eb4453a6bf.png" alt="image"><br><br>  It seems that here it is, the key to understanding the intellect?  But rather not.  The system still needs a huge amount of data for training.  And it works mainly with structured tabular data.  At the same time, when Facebook <a href="https://embodiedqa.org/">solved a</a> similar problem, they took the path of ‚Äúnafig memory, just make the neuron more complicated, and there are more examples - and she will learn‚Äù. <br><br>  <b>Disentanglement</b> .  Another way to create meaningful memory is to take the same embeddings, but when learning to introduce additional criteria that would allow to highlight the ‚Äúmeanings‚Äù in them.  For example, we want to train a neural network to distinguish the behavior of a person in a store.  If we followed the standard path, we would have to make a dozen networks.  One is looking for a person, the second determines what he does, his third age, the fourth - gender.  A separate logic looks at the part of the store where he does / learns about it.  The third determines its trajectory, and so on. <br><br>  Or, if there were infinitely a lot of data, then it would be possible to train one network for all sorts of outcomes (it is obvious that such an array of data cannot be collected). <br><br>  The intelligence approach tells us - and let's teach the network so that she can distinguish between concepts.  So that she formed an embedding according to the video, where one area would define the action, one is the position on the floor in time, one is the height of a person, and another is his gender.  At the same time, when training, I would almost not like to prompt the network with such key concepts, and that she herself singled out and grouped areas.  There are quite a few such articles (some of them are <a href="https://ai.googleblog.com/2019/04/evaluating-unsupervised-learning-of.html">1</a> , <a href="http://papers.nips.cc/paper/5851-deep-convolutional-inverse-graphics-network.pdf">2</a> , <a href="https://arxiv.org/pdf/1812.02230.pdf">3</a> ) and, in general, they are quite theoretical. <br><br>  But this direction, at least theoretically, should close the problems listed at the beginning. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/314/bd2/7ab/314bd27abc46e22c8c64430c6b6b9211.gif" alt="image"><br><br>  Image decomposition according to the parameters ‚Äúcolor of walls / color of floor / shape of object / color of object / itd‚Äù <br><br><img src="https://habrastorage.org/webt/km/md/fb/kmmdfbtnliixqj3d5szfofcqe7q.jpeg"><br><br>  Decomposition of the face by the parameters ‚Äúsize, eyebrows, orientation, skin color, etc.‚Äù <br><br><h3>  Other </h3><br>  There are many other, not so much global, directions that allow you to somehow reduce the database, work with more heterogeneous data, etc. <br><br>  <b>Attention</b> .  It probably does not make sense to single it out as a separate method.  Just an approach that enhances others.  He is devoted to many articles ( <a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/">1</a> , <a href="http://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">2</a> , <a href="https://arxiv.org/abs/1706.03762">3</a> ).  The meaning of Attention is to enhance the response of the network to significant objects during training.  Often some external target designation, or a small external network. <br><br>  <b>3D simulation</b> .  If you make a good 3D engine, then they can often close 90% of the training data (I even saw an example when almost 99% of the data was closed with a good engine).  There are many ideas and hacks on how to make a network trained on a 3D engine to work on real data (Fine tuning, style transfer, etc.).  But it is often to make a good engine - several orders of magnitude more difficult than typing data.  Examples when making engines: <br>  Learning robots ( <a href="https://ai.googleblog.com/2018/06/teaching-uncalibrated-robots-to_22.html">google</a> , <a href="https://youtu.be/VZcmogKXC18">braingarden</a> ) <br>  Learning to <a href="https://neuromation.io/">recognize</a> products in the store (but in two projects that we did - we quietly did without it). <br>  Training in Tesla (again, the video that was higher). <br><br><h2>  findings </h2><br>  The whole article is in some sense conclusions.  Probably the main message I wanted to make was ‚Äúthe freebie is over, the neurons do not provide more simple solutions‚Äù.  Now it is necessary to work hard building complex decisions.  Or work hard doing complex scientific research. <br><br>  In general, the topic is debatable.  Maybe readers have more interesting examples? </div><p>Source: <a href="https://habr.com/ru/post/455676/">https://habr.com/ru/post/455676/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../45566/index.html">Social blog widget</a></li>
<li><a href="../455662/index.html">Large mechanical cam display as a decoder</a></li>
<li><a href="../455666/index.html">Building Outbound Sales in a Service IT Company</a></li>
<li><a href="../45567/index.html">New fring for iPhone / iPod touch with firmware 2.2</a></li>
<li><a href="../455670/index.html">How bones, vessels and organs are printed using 3D printers</a></li>
<li><a href="../455678/index.html">On the way of Sergei Pavlovich Korolev. Modern Russian manned project. Part 1. "Federation"</a></li>
<li><a href="../455682/index.html">How much do you spend on infrastructure? And how to save on it?</a></li>
<li><a href="../455684/index.html">Why we conducted hackathon for testers</a></li>
<li><a href="../455692/index.html">NSPA: restyling or theater begins with a hanger</a></li>
<li><a href="../455694/index.html">The architecture of billing a new generation: the transformation with the transition to Tarantool</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>