<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Learning a computer to write like Tolstoy, Volume I</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="- Eh bien, mon prince. G√™nes et Lucques ne sont plus que des apanages, des estates, de la famille Buonaparte. Je vous pr√©viens que si vous ne me dites...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Learning a computer to write like Tolstoy, Volume I</h1><div class="post__text post__text-html js-mediator-article"><blockquote>  - Eh bien, mon prince.  G√™nes et Lucques ne sont plus que des apanages, des estates, de la famille Buonaparte.  Je vous pr√©viens que si vous ne me dites pas que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, tous les atrocit√©s de cet , vous n'√™tes plus mon ami, vous n'√™tes plus my faithful slave, comme vous dites 1. Well, hello, hello.  Je vois que je vous fais peur 2, sit down and tell. </blockquote><br><h1 id="tom-pervyy">  VOLUME FIRST </h1><br><h3 id="chast-pervaya-anna-karenina">  PART ONE.  Anna Karenina </h3><br><p>  Recently, I came across this article <a href="https://habrahabr.ru/post/342738/">https://habrahabr.ru/post/342738/</a> .  And I wanted to write about word embeddings, python, gensim and word2vec.  In this part, I will try to talk about learning the basic model of w2v. </p><br><p>  So, we proceed. </p><br><ul><li>  Download anaconda.  Install. </li><li>  We also need C / C ++ tools from visual studio. </li><li>  Now install gensim.  It is for him that we need c ++. </li><li>  Install nltk. </li><li>  When installing, do not forget to download the library for Anaconda, and not for the standard interpreter.  Otherwise, it will all end in failure. </li><li>  We swing <a href="http://modernlib.ru/books/tolstoy_lev_nikolaevich/anna_karenina/">Anna Karenina</a> in TXT. </li><li> I advise you to open the file and cut out ads and headlines.  Then save in <code>utf-8</code> format. </li><li>  You can get to work. </li></ul><br><p><a name="habracut"></a>  The first step is to download data for nltk. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nltk nltk.dwonload()</code> </pre> <br><p>  In the opened window, select everything, and go to drink coffee.  It takes about half an hour. <br>  By default, there is no Russian language in the library.  But the craftsmen did everything for us.  Download <a href="https://github.com/mhq/train_punkt">https://github.com/mhq/train_punkt</a> and extract everything to a folder <br>  <code>C:\Users\&lt;username&gt;\AppData\Roaming\nltk_data\tokenizers\punkt</code> and <br>  <code>C:\Users\&lt;username&gt;\AppData\Roaming\nltk_data\tokenizers\punkt\PY3</code> . </p><br><p>  Nltk we will use to break down the text into sentences, and sentences into words.  To my surprise, it all works pretty quickly.  Well, enough of the settings, a couple already write at least a line of normal code. <br>  Create a folder where there will be scripts and data.  Create an enviroment. </p><br><pre> <code class="bash hljs">conda create -n tolstoy-like</code> </pre> <br><p>  Activate. </p><br><pre> <code class="bash hljs">activate tolstoy</code> </pre> <br><p>  There we throw the text.  <code>anna.txt</code> file <code>anna.txt</code> <br>  For owners of PyCharm, you can simply create a project and select an anaconda as an interpreter without creating an environment. </p><br><p>  Create a script <code>train-I.py</code> . </p><br><ul><li><p>  We connect dependences. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- # imports import gensim import string from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from nltk.tokenize import word_tokenize</span></span></code> </pre> <br></li><li><p>  Read the text. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># load text text = open('./anna.txt', 'r', encoding='utf-8').read()</span></span></code> </pre> <br></li><li><p>  Now it's the turn of the tokenizer of Russian sentences. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tokenize_ru</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(file_text)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># firstly let's apply nltk tokenization tokens = word_tokenize(file_text) # let's delete punctuation symbols tokens = [i for i in tokens if (i not in string.punctuation)] # deleting stop_words stop_words = stopwords.words('russian') stop_words.extend(['', '', '', '', '', '', '', '‚Äî', '‚Äì', '', '', '...']) tokens = [i for i in tokens if (i not in stop_words)] # cleaning words tokens = [i.replace("¬´", "").replace("¬ª", "") for i in tokens] return tokens</span></span></code> </pre> <br><p>  Let us dwell on this in more detail.  In the first line we break the sentence (string) into words (array of strings).  Then we delete the punctuation, which nltk, for some reason, makes as a separate word.  Now stop words.  These are words from which our model will not benefit, they will only knock it off from the main text.  These include interjections, conjunctions and some pronouns, as well as some of the favorite words, parasites.  Then remove the quotes that in this novel over the edge. </p><br></li><li><p>  Now break the text into sentences, and sentences into an array of words. </p><br><pre> <code class="python hljs">sentences = [tokenize_ru(sent) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> sent <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sent_tokenize(text, <span class="hljs-string"><span class="hljs-string">'russian'</span></span>)]</code> </pre> <br></li><li><p>  For interest, we derive the number of sentences and a couple of them. </p><br><pre> <code class="python hljs">print(len(sentences)) <span class="hljs-comment"><span class="hljs-comment"># 20024 print(sentences[200:209]) # [['', '', '', '', '', ''],...]</span></span></code> </pre> <br></li><li><p>  Now we begin to train the model.  Do not be afraid it does not take half an hour - 20024 sentences for gensim just spit it out. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># train model model = gensim.models.Word2Vec(sentences, size=150, window=5, min_count=5, workers=4)</span></span></code> </pre> <br></li><li>  Now save the model to a file. <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># save model model.save('./w2v.model') print('saved')</span></span></code> </pre> </li></ul><br><p>  Save the file.  To run, those who work in PyCharm or Spyder just press run.  Whoever writes by hand from a notebook or another editor will have to run Anaconda Promt (to do this, just enter it in the search menu), go to the directory with the script and run the command </p><br><pre> <code class="bash hljs">python train-I.py</code> </pre> <br><p>  Is done.  Now you can proudly say that you trained word2vec. </p><br><h3 id="chast-vtoraya-voyna-i-mir">  PART TWO.  War and Peace </h3><br><p>  No matter how hard we try, Anna Karenins are not enough to train the model.  Therefore, we use the second work of the author - War and Peace. </p><br><p>  You can download it <a href="http://vojnaimir.ru/download.html">from here</a> , also in TXT format.  Before use, you will have to combine two files into one.  We throw in the directory from the first chapter, call <code>war.txt</code> .  One of the charm of using gensim is that any loaded model can be updated with new data.  This is what we will do. <br>  Create a <code>train-II.py</code> script </p><br><ul><li><p>  I think that this part does not need explanations, since there is nothing new in it. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- # imports import gensim import string from nltk.tokenize import sent_tokenize from nltk.corpus import stopwords from nltk.tokenize import word_tokenize # load text text = open('./war.txt', 'r', encoding='utf-8').read() def tokenize_ru(file_text): # firstly let's apply nltk tokenization tokens = word_tokenize(file_text) # let's delete punctuation symbols tokens = [i for i in tokens if (i not in string.punctuation)] # deleting stop_words stop_words = stopwords.words('russian') stop_words.extend(['', '', '', '', '', '', '', '‚Äî', '‚Äì', '', '', '...']) tokens = [i for i in tokens if (i not in stop_words)] # cleaning words tokens = [i.replace("¬´", "").replace("¬ª", "") for i in tokens] return tokens # tokenize sentences sentences = [tokenize_ru(sent) for sent in sent_tokenize(text, 'russian')] print(len(sentences)) # 30938 print(sentences[200:209]) # [['', '', '', '', '', '', '', '', '', '', '', '', '', '', ''],...]</span></span></code> </pre> <br></li><li><p>  Then we load our model, and we feed it new data. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># train model part II model = gensim.models.Word2Vec.load('./w2v.model') model.train(sentences, total_examples=model.corpus_count, epochs=model.iter)</span></span></code> </pre> <br><p>  I‚Äôll stop here a bit.  <code>total_examples</code> sets the number of words, in our case this is the entire model dictionary ( <code>model.corpus_count</code> ), including the new ones.  A <code>`epochs</code> number of iterations.  Honestly, I myself do not know what <code>model.iter</code> means from the documentation.  Who knows, please write in the comments - correct. </p><br></li><li>  And save again. <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># save model model.save('./w2v-II.model') print('saved')</span></span></code> </pre> </li></ul><br><p>  Do not forget to run. </p><br><h3 id="epilog-a-gde-zhe-testy">  EPILOGUE.  And where are the tests? </h3><br><p>  They are not.  And until there will be.  The model is not quite perfect yet, frankly, it is terrible.  In the next article I will definitely tell you how to fix it.  But here's your last thing: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- # imports import gensim model = gensim.models.Word2Vec.load('./w2v-II.model') print(model.most_similar(positive=['', ''], negative=[''], topn=1))</span></span></code> </pre><br><p>  <strong>PS</strong> </p><br><blockquote>  Not so bad, actually.  The resulting dictionary contains about 5 thousand words with their dependencies and relationships.  In the next article I will give a more perfect model (15,000 words).  More talk about the preparation of the text.  And finally, in the third part, I will publish the final model and tell you how to write a program using neural networks to generate a text in the style of Tolstoy. </blockquote><p>  References and used literature. </p><br><ul><li>  <a href="https://habrahabr.ru/post/342738/">https://habrahabr.ru/post/342738/</a> </li><li>  <a href="https://github.com/mhq/train_punkt">https://github.com/mhq/train_punkt</a> </li><li>  <a href="https://radimrehurek.com/gensim/models/word2vec.html">https://radimrehurek.com/gensim/models/word2vec.html</a> </li><li>  <a href="https://machinelearningmastery.com/develop-word-embeddings-python-gensim/">https://machinelearningmastery.com/develop-word-embeddings-python-gensim/</a> </li><li>  <a href="https://rare-technologies.com/word2vec-tutorial/">https://rare-technologies.com/word2vec-tutorial/</a> </li></ul><br><h3 id="uspehov-vam-v-mashinnom-obuchenii">  Successes you in machine learning. </h3><br><p>  I hope you liked my article a little. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/343704/">https://habr.com/ru/post/343704/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../343692/index.html">Open the CrackMe winter contest: break-rover</a></li>
<li><a href="../343694/index.html">Arduino and segment LCD indicator</a></li>
<li><a href="../343696/index.html">Latent parasites</a></li>
<li><a href="../343700/index.html">Flying snowflakes</a></li>
<li><a href="../343702/index.html">tldr - alternative man with self-titled name</a></li>
<li><a href="../343706/index.html">I created an application that makes learning algorithms and data structures much more interesting.</a></li>
<li><a href="../343708/index.html">Code Textures</a></li>
<li><a href="../343714/index.html">Local automation of builds (Crashlytics + Slack + FastLane)</a></li>
<li><a href="../343716/index.html">Experiment to promote the game on Google Play. Part 1</a></li>
<li><a href="../343718/index.html">Sound settings in Ubuntu</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>