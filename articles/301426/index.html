<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Web scraping with Node.js</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This is the first article in the series on creating and using scripts for web-scraping using Node.js. 


1. Web scraping with Node.js 
2. Web scraping...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Web scraping with Node.js</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/files/1ad/554/426/1ad5544260064818be850112381d1e95.png" align="right">  This is the first article in the series on creating and using scripts for web-scraping using Node.js. </p><br><ol><li>  <strong>Web scraping with Node.js</strong> </li><li>  <a href="https://habrahabr.ru/post/302766/">Web scraping on Node.js and problem sites</a> </li><li>  <a href="https://habrahabr.ru/post/303726/">Node.js web scraping and bot protection</a> </li><li>  <a href="https://habrahabr.ru/post/304708/">Web scraping updated data with Node.js</a> </li></ol><br><p>  The topic of web-scraping is attracting more and more interest, at least because it is an inexhaustible source of small, but convenient and interesting orders for freelancers.  Naturally, more and more people are trying to figure out what it is.  However, it is quite difficult to understand what web-scraping is by abstract examples from the documentation for the next library.  It is much easier to understand this topic by observing the solution of the real problem step by step. </p><br><p>  Usually, the task for web-scraping looks like this: there is data available only on web pages, and you need to pull it out from there and save it in some kind of digestible format.  The final format is not important, as no one has canceled converters.  For the most part, it‚Äôs about opening a browser, clicking on the links with a mouse, and copying the necessary data from the pages.  Well, or do the same script. </p><br><p>  The purpose of this article is to show the whole process of creating and using such a script, from setting the task to obtaining the final result.  As an example, I‚Äôll consider a real task, such as those that can often be found, for example, on freelance exchanges, well, and we‚Äôll use Node.js as a tool for web-scraping. </p><a name="habracut"></a><br><h2 id="postanovka-zadachi">  Formulation of the problem </h2><br><p> <a href=""><img src="https://habrastorage.org/files/b0e/6f4/a3b/b0e6f4a3b1b24a729bbad40c93e1afc7.png" align="left"></a>  Suppose I want to get a list of all the articles and notes that I published on the website Ferra.ru.  For each publication I want to get the title, link, date and size of the text.  There is no convenient API for this site, so you have to grab the data from the pages. </p><br><p>  For many years, I did not bother to organize a separate section on the site, so that all my publications are mixed with the usual news.  The only way I know how to highlight the publications I need is filtering by author.  On the pages with a list of news, the author is not listed, so you have to check every news on the corresponding page.  I remember that I wrote only in the ‚ÄúScience and Technology‚Äù section, so you can search not for all the news, but for one section only. </p><br><p>  Here, approximately in this form, I usually receive tasks for web-scraping.  Even in such tasks there are different surprises and pitfalls, but they are not immediately visible and you have to detect and settle them right in the process.  Let's start: </p><br><h2 id="analiz-sayta">  Site analysis </h2><br><p>  We will need pages with news, links to which are collected in the Padjini list.  All the necessary pages are available without authorization.  Looking at the source page in the browser, you can make sure that all the data are contained directly in the HTML code.  Quite a simple task (actually, because I chose it).  Looks like we don‚Äôt have to mess with login, session storage, sending forms, tracking AJAX requests, parsing connected scripts and so on.  There are cases when the analysis of the target site takes much more time than designing and writing a script, but not this time.  Maybe in the following articles ... </p><br><div class="spoiler">  <b class="spoiler_title">Project preparation</b> <div class="spoiler_text"><h2 id="podgotovka-proekta">  Project preparation </h2><br><p>  I think it makes no sense to describe the creation of a project directory (and there is an empty index.js file and the simplest package.json file), the installation of Node.js and the npm package manager, and the installation and removal of modules via npm. </p><br><p>  In real life, the development of a project is accompanied by a GIT repository, but this is beyond the scope of the article, so just keep in mind that each significant code change in real life will correspond to a separate commit. </p></div></div><br><h2 id="poluchenie-stranic">  Retrieving Pages </h2><br><p> To get data from the HTML code of the page you need to get this code from the site.  This can be done using the http client from the <a href="http.html">http</a> module that is built into Node.js by default, however, to perform simple http requests it is more convenient to use different wrappers over <code>http</code> most popular of which is <a href="https://github.com/request/request">request,</a> so let's try it. </p><br><p>  The first step is to make sure that the <code>request</code> module receives from the site the same HTML code that comes to the browser.  With most sites this will be the case, but sometimes there are sites that give the browser one thing, and a script with an http client another.  Before, I first of all checked the landing pages with a GET request from <a href="https://curl.haxx.se/">curl</a> , but once I came across a site that in curl and in the <code>request</code> script produced different http-responses, so now I immediately try to run the script.  Something like this: </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> request = <span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(<span class="hljs-string"><span class="hljs-string">'request'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> URL = <span class="hljs-string"><span class="hljs-string">'http://www.ferra.ru/ru/techlife/news/'</span></span>; request(URL, <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> (</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">err, res, body</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (err) <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> err; <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(body); <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(res.statusCode); });</code> </pre> <br><p>  Run the script.  If the site is lying or with the connection problem, then an error will fall out, and if everything is good, then a long sheet of the original text of the page will fall right into the terminal window, and you can make sure that it is almost the same as in the browser.  This is good, so we don‚Äôt need to set special cookies or http headers to get the page. </p><br><p>  However, if you are not too lazy and squander text up to the Russian-language text, then you will notice that <code>request</code> incorrectly determines the encoding.  Russian news headlines, for example, look like this: </p><br><blockquote>  4.7-         iPhone 7                    <br>  PC-       DOOM                4                      </blockquote><p>  The problem with encodings is now not as common as at the dawn of the Internet, but still quite often (and on sites without an API, especially often).  The request module has an <code>encoding</code> parameter, but it only supports encodings adopted in Node.js for converting a buffer into a string.  Let me remind you, this is <code>ascii</code> , <code>utf8</code> , <code>utf16le</code> (aka <code>ucs2</code> ), <code>base64</code> , <code>binary</code> and <code>hex</code> , while we need <code>windows-1251</code> . </p><br><p>  The most common solution for this problem is to set the <code>encoding</code> to <code>null</code> in the <code>request</code> so that it places the original buffer in the <code>body</code> , and use the <a href="https://github.com/bnoordhuis/node-iconv">iconv</a> or <a href="https://github.com/ashtuchkin/iconv-lite">iconv-lite</a> module to convert it.  For example, like this: </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> request = <span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(<span class="hljs-string"><span class="hljs-string">'request'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> iconv = <span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(<span class="hljs-string"><span class="hljs-string">'iconv-lite'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> opt = { <span class="hljs-attr"><span class="hljs-attr">url</span></span>: <span class="hljs-string"><span class="hljs-string">'http://www.ferra.ru/ru/techlife/news/'</span></span>, <span class="hljs-attr"><span class="hljs-attr">encoding</span></span>: <span class="hljs-literal"><span class="hljs-literal">null</span></span> } request(opt, <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> (</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">err, res, body</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (err) <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> err; <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(iconv.decode(body, <span class="hljs-string"><span class="hljs-string">'win1251'</span></span>)); <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(res.statusCode); });</code> </pre> <br><p>  The disadvantage of this solution is that on every problem site you have to spend time trying to figure out the encoding.  If this site is not the last, then it is worthwhile to find a more automated solution.  If the coding is understood by the browser, then our script should also understand it.  The path for real geeks is to find the <code>request</code> module on GitHub and help its developers implement coding support from <code>iconv</code> .  Well, or make your own fork with blackjack and good support for encodings.  The path for experienced practitioners is to look for an alternative to the <code>request</code> module. </p><br><p>  I found the <a href="https://github.com/tomas/needle">needle</a> module in a similar situation, and was so pleased that I didn‚Äôt use the <code>request</code> anymore.  With the default settings, <code>needle</code> determines the encoding in the same way as the browser does, and automatically recodes the text of the http response.  And this is not the only thing where <code>needle</code> better than <code>request</code> . </p><br><p>  Let's try to get our problem page using <code>needle</code> : </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> needle = <span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(<span class="hljs-string"><span class="hljs-string">'needle'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> URL = <span class="hljs-string"><span class="hljs-string">'http://www.ferra.ru/ru/techlife/news/'</span></span>; needle.get(URL, <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">err, res</span></span></span><span class="hljs-function">)</span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (err) <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> err; <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(res.body); <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(res.statusCode); });</code> </pre> <br><p>  Now everything is great.  To clear your conscience, you should try the same with a <a href="http://www.ferra.ru/ru/techlife/news/2013/03/04/This-domain-is-for-sale-etc/">separate news</a> page.  There, too, everything will be fine. </p><br><h2 id="krauling">  Crawling </h2><br><p>  Now we need to get the page of each news, check the name of the author on it, and save the necessary data if it matches.  Since we do not have a ready list of links to news pages, we will get it recursively following the Padjin list.  Like search engines crawlers, only more sighting.  Thus, we need our script to take the link, send it for processing, save some useful data (if any), and put new links (for news or the next pages of the list) for the same treatment. </p><br><p>  At first it may seem that cracking is easier to carry out in several passes.  For example, first recursively collect all the pages of the Pajini list, then get all the news pages from them, and then process each news item.  This approach helps the newcomer to keep in mind the process of scraping, but in practice a single, single-level queue for requests of all types is, at a minimum, easier and faster to develop. </p><br><p>  To create such a queue, you can use the <code>queue</code> function from the famous <a href="https://github.com/caolan/async">async</a> module, but I prefer to use the <a href="https://github.com/astur/tress">tress</a> module, which is backward compatible with <code>async.queue</code> , but much smaller, since it does not contain the other functions of the <code>async</code> module.  A small module is good not because it takes up less space (this is nonsense), but because it is easier to finish it quickly if it is needed for particularly complex cracking. </p><br><p>  The <code>tress</code> works like this: </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> tress = <span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(<span class="hljs-string"><span class="hljs-string">'tress'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> needle = <span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(<span class="hljs-string"><span class="hljs-string">'needle'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> URL = <span class="hljs-string"><span class="hljs-string">'http://www.ferra.ru/ru/techlife/news/'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> results = []; <span class="hljs-comment"><span class="hljs-comment">// `tress`          var q = tress(function(url, callback){ //      url needle.get(url, function(err, res){ if (err) throw err; //      res.body //  results.push     //  q.push     callback(); // callback   }); }); //   ,      q.drain = function(){ require('fs').writeFileSync('./data.json', JSON.stringify(results, null, 4)); } //         q.push(URL);</span></span></code> </pre> <br><p>  It is worth noting that our function will each time perform an HTTP request, and while it is being executed, the script will be idle.  So the script will work for quite a long time.  To speed it up, you can pass <code>tress</code> second parameter the number of links that can be processed in parallel.  At the same time, the script will continue to work in the same process and in the same thread, while parallelism will be provided by non-blocking I / O operations in Node.js. </p><br><h2 id="parsing">  Parsing </h2><br><p>  The code that we already have can be used as the basis for scraping.  In fact, we created the simplest miniframe, which can be gradually modified every time we hit another complex website, and for simple websites (most of them), you can simply write a code snippet responsible for parsing.  The meaning of this fragment will always be the same: at the entrance, the body of the http response, and at the exit, the completion of the results array and the queue of links.  Tools for parsing the rest of the code should not affect. </p><br><p>  Parsing gurus know that the most powerful and versatile way of parsing pages is <a href="https://developer.mozilla.org/ru/docs/Web/JavaScript/Reference/Global_Objects/RegExp">regular expressions</a> .  They allow parsit pages with a very non-standard and extremely anti-semantic layout.  In general, if the data can be accurately copied from the site without knowing its language, then they can be parsed with regulars. </p><br><p>  However, most HTML pages are easily understood by DOM parsers, which are much easier and easier to read.  Regulars should only be used if DOM parsers fail.  In our case, the DOM parser is perfect.  Currently, <a href="https://github.com/cheeriojs/cheerio">cheerio</a> , the server version of the iconic jQuery, is leading the <a href="https://github.com/cheeriojs/cheerio">way</a> among DOM parsers under <a href="https://github.com/cheeriojs/cheerio">Node.js.</a> </p><br><p>  ( <em>By the way, JQuery is used on Ferra.ru. This is a fairly reliable sign that <code>cheerio</code> will cope with such a site</em> ) </p><br><p>  At first, it may seem more convenient to write a separate parser for each type of page (in our case there are two of them ‚Äî lists and news).  In fact, you can simply search the page for each type of data.  If there is no necessary data on the page, then they simply will not be found.  Sometimes you have to think about how to avoid confusion if different data looks the same on pages of different types, but I have never met a site where it would be difficult.  But I have met many sites where different data types were randomly combined on the same pages, so it‚Äôs worth getting used to writing a single parser for all pages. </p><br><p>  So, the lists of links to news are located inside the <code>div</code> element with the class <code>b_rewiev</code> .  There are other links that we do not need, but the correct links are easy to distinguish, since only such links have a parent ‚Äî the <code>p</code> element.  The link to the next page of the page is located inside the <code>span</code> element with the <code>bpr_next</code> class, and it is there alone.  There is no such item on the news pages and on the last page of the list.  It is worth considering that the links in the Padzinator are relative, so they should not be forgotten to result in absolute ones.  The name of the author is hidden in the depth of the <code>div</code> element with the class <code>b_infopost</code> .  On the pages of the list there is no such element, so if the author matches, you can stupidly collect the news data. </p><br><p>  Do not forget about broken links (spoiler: in the section that we scraped, there are one such links).  Alternatively, you can check the response code for each request, but there are sites that give the page a broken link with code <code>200</code> (even if they write ‚Äú404‚Äù on it).  Another option is to look in the code of such a page for those elements that we are going to look for by the parser.  In our case, there are no such elements on the broken link page, so the parser simply ignores such pages. </p><br><p>  Add parsing to our code using <code>cheerio</code> : </p><br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> tress = <span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(<span class="hljs-string"><span class="hljs-string">'tress'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> needle = <span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(<span class="hljs-string"><span class="hljs-string">'needle'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> cheerio = <span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(<span class="hljs-string"><span class="hljs-string">'cheerio'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> resolve = <span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(<span class="hljs-string"><span class="hljs-string">'url'</span></span>).resolve; <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> fs = <span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(<span class="hljs-string"><span class="hljs-string">'fs'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> URL = <span class="hljs-string"><span class="hljs-string">'http://www.ferra.ru/ru/techlife/news/'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> results = []; <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> q = tress(<span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">url, callback</span></span></span><span class="hljs-function">)</span></span>{ needle.get(url, <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">err, res</span></span></span><span class="hljs-function">)</span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (err) <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> err; <span class="hljs-comment"><span class="hljs-comment">//  DOM var $ = cheerio.load(res.body); //   if($('.b_infopost').contents().eq(2).text().trim().slice(0, -1) === ' '){ results.push({ title: $('h1').text(), date: $('.b_infopost&gt;.date').text(), href: url, size: $('.newsbody').text().length }); } //  $('.b_rewiev p&gt;a').each(function() { q.push($(this).attr('href')); }); // $('.bpr_next&gt;a').each(function() { //         q.push(resolve(URL, $(this).attr('href'))); }); callback(); }); }, 10); //  10   q.drain = function(){ fs.writeFileSync('./data.json', JSON.stringify(results, null, 4)); } q.push(URL);</span></span></code> </pre> <br><p>  In principle, we received a script for web-scraping, which solves our problem (for those who want the <a href="https://gist.github.com/astur/2b3258a7991d2bc83d07670f27036fb0">code on gist</a> ).  However, I would not give such a script to the customer.  Even with parallel requests, this script runs for a long time, which means that at least it needs to add an indication of the execution process.  Also now, even with a brief interruption of the connection, the script will fall without saving intermediate results, so you need to do so either that the script retains the intermediate results before the fall, or that it does not fall, but pauses.  I would also add the possibility of forcibly terminating the script, and then continue from the same place.  This is overkill, by and large, but such ‚Äúcherries on the cake‚Äù greatly strengthen the relationship with customers. </p><br><p>  However, if the customer asked to scrape the data once and simply send the file with the results, then nothing can be done.  Everything works like this (23 minutes in 10 streams, 1005 publications and one broken link found).  If you completely insolent, it would be possible not to make a recursive passage through Padzinator, but to generate links to list pages using a template during the period when I worked at Ferra.ru.  Then the script would not have worked so long.  At first, this is annoying, but the choice of such solutions is also an important part of the web scraping task. </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  In principle, knowing how to write such scrippers can take orders on freelancing exchanges and live well.  However, there are a couple of problems.  Firstly, many customers do not want final data, but a script that they can use without any problems (and they have very specific requirements).  Secondly, on the websites there are difficulties that are found only when the order has already been taken and half of the work has already been done, and you have to either lose money and reputation, or perform mental feats. </p><br><p>  In the short term, I am planning articles about more complex cases (sessions, AJAX, glitches on the site, and so on) and about bringing web scraper scripts to the presentation.  Questions and wishes are welcome. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/301426/">https://habr.com/ru/post/301426/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../301416/index.html">FlexboxLayout - part 2</a></li>
<li><a href="../301418/index.html">Creating a VoIP Provider Template in 3CX Phone System</a></li>
<li><a href="../301420/index.html">Office 365 updates</a></li>
<li><a href="../301422/index.html">IPSec VPN for OS X and iOS. Without pain</a></li>
<li><a href="../301424/index.html">SpamFireWall - prohibit access to the site for spam bots</a></li>
<li><a href="../301428/index.html">We clean the bow (but not crying): optimization techniques</a></li>
<li><a href="../301432/index.html">Warren Buffett and Dan Gilbert fight for Yahoo assets</a></li>
<li><a href="../301434/index.html">June 4, 2016 is a cloudy day 2.0. Artificial intelligence in the cloud</a></li>
<li><a href="../301436/index.html">How we parse income declarations using open data</a></li>
<li><a href="../301440/index.html">C ++ User Group, meeting in Kazan May 28, 2016</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>