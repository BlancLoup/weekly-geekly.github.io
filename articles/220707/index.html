<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Data based monitoring</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When working on Webzilla cloud services, we pay great attention to the monitoring system. We are confident that only with properly working and reliabl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Data based monitoring</h1><div class="post__text post__text-html js-mediator-article">  When working on Webzilla cloud services, we pay great attention to the monitoring system.  We are confident that only with properly working and reliable monitoring, we can provide service at the level of quality required by customers.  While working on the first of the company's cloud products, the Webzilla Instant Files cloud storage, we started building a monitoring system before we started building the product itself, we thought out monitoring for each function at the planning stage. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a6c/1ad/f2a/a6c1adf2a3b5115baad619e99acf165b.png"><br><br>  Our monitoring system has several objectives: <br><ul><li>  In the event of a failure, we should not waste time trying to determine what happened.  We must immediately and firmly know this. </li><li>  To prevent the maximum number of failures to the point where they affect customers, we need to monitor metrics and events foreshadowing problems. </li><li>  After any incident, we must have full access to all the data needed to investigate its causes, even if at the time of the removal of the cause it was not clear. </li><li>  Our support team must respond to failures promptly and correctly.  The only way to achieve this is to provide employees with a tool that does not load them with unnecessary information. </li></ul><br>  We worked on the monitoring system not less time than on the functional part of the service - and we share our accumulated experience. <br>  In general, our monitoring system consists of three main subsystems: <br><a name="habracut"></a><br><ul><li>  collection and analysis of operational data; </li><li>  collecting and analyzing logs; </li><li>  event monitoring and alerts. </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  System for collecting and analyzing operational data </h2><br>  Usually, when they say ‚Äúmonitoring,‚Äù this word means event monitoring ( <code>** PROBLEM Host Alert: server01 is DOWN **</code> ).  The cornerstone of our monitoring is continuous work with operational data.  The condition of all systems under observation is described by indicators that vary throughout the entire observation. <br><br>  In the simplest case - if the server is turned off (for example, due to a power outage) - this data has a simple form: 1 server is available less than we expected.  If the situation is more complicated: for example, for some reason, the application has gradually degraded and it has become slower to respond to requests, measurements can show, for example, that "the 95th <a href="http://ru.wikipedia.org/wiki/%25D0%259A%25D0%25B2%25D0%25B0%25D0%25BD%25D1%2582%25D0%25B8%25D0%25BB%25D1%258C">percentile of</a> response time has doubled in the last week."  At the same time, a specific point in time when the system ‚Äúbroke down‚Äù does not exist.  <a href="http://ru.wikipedia.org/wiki/%25D0%259B%25D1%258F%25D0%25B3%25D1%2583%25D1%2588%25D0%25BA%25D0%25B0_%25D0%25B2_%25D0%25BA%25D0%25B8%25D0%25BF%25D1%258F%25D1%2582%25D0%25BA%25D0%25B5">The frog</a> cannot name the exact time when boiling water formed around it. <br><br>  In other words, it is usually not difficult to reformulate usual events in a data language, but the data have their own added value.  We decided to build data-driven monitoring. <br><br>  First we collect data.  Then we make decisions. <br><br>  We chose <a href="http://ganglia.sourceforge.net/">Ganglia</a> to collect operational data.  Several of its features proved useful to us: <br><ul><li>  Out of the Box works with clusters; </li><li>  Scales naturally; </li><li>  It creates a low load on the nodes from which it collects data; </li><li>  Supports fault tolerant configuration; </li><li>  It mates well with event monitoring systems; </li><li>  The toolbar does not require modification by the file. </li></ul><br>  Ganglia consists of three component types: <br><ol><li>  Ganglia monitoring daemon (gmond) - the monitoring daemon Ganglia, running on each node.  Gmond collects data from its server and sends it to other nodes. </li><li>  Ganglia meta daemon (gmetad) is a Ganglia meta demon polling monitoring demons and collecting metrics.  The measurement results collected by the meta-daemon are recorded in a structured form and can be published to an arbitrary set of trusted hosts.  The data format is <a href="">documented</a> .  We use this data for graphs in the web interface and informing event monitoring about anomalies. </li><li>  Ganglia web interface.  In an ideal world where all processes are fully automated, it is not necessary to have a web interface to the data collection system.  We are quite actively looking at the graphics and identify patterns on their own.  You can use any web interface or write your own.  We use standard.  Each employee can create their own set of graphs for simultaneous display and thus track exactly those parameters that interest him.  It is also possible to build aggregated graphs for several hosts. </li></ol><br>  Ganglia is easy to configure so that data is available and not lost during an accident.  The system supports multicast configuration of monitoring daemons.  This means that all collected operational data is delivered to each node in the cluster and stored on it.  After that, the meta-daemon chooses which of the monitoring daemons to interrogate and polls it.  If polling fails, the next node in the cluster is polled.  The process continues until either success is achieved or the entire cluster is bypassed. <br><br>  Configuring such a system is quite simple. <br><br><h3>  Configuring Ganglia </h3><br>  Cluster membership is defined in the gmond.conf daemon configuration file <br><pre> <code class="hljs nginx"><span class="hljs-section"><span class="hljs-section">cluster</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">name</span></span> = <span class="hljs-string"><span class="hljs-string">"databases"</span></span> ... }</code> </pre><br>  All nodes of the cluster should have the following lines in the configuration of the monitoring daemon, allowing multicast data to be sent: <br><pre> <code class="hljs nginx"><span class="hljs-section"><span class="hljs-section">udp_send_channel</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">mcast_if</span></span> = eth0 mcast_join = <span class="hljs-number"><span class="hljs-number">239.2.11.71</span></span> port = <span class="hljs-number"><span class="hljs-number">8651</span></span> ttl = <span class="hljs-number"><span class="hljs-number">1</span></span> }</code> </pre><br>  and receive data from other nodes <br><pre> <code class="hljs nginx"><span class="hljs-section"><span class="hljs-section">udp_recv_channel</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">mcast_if</span></span> = eth0 mcast_join = <span class="hljs-number"><span class="hljs-number">239.2.11.71</span></span> port = <span class="hljs-number"><span class="hljs-number">8651</span></span> bind = <span class="hljs-number"><span class="hljs-number">239.2.11.71</span></span> }</code> </pre><br>  In addition, the meta-daemon must be allowed to poll: <br><pre> <code class="hljs nginx"><span class="hljs-section"><span class="hljs-section">tcp_accept_channel</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">port</span></span> = <span class="hljs-number"><span class="hljs-number">8649</span></span> }</code> </pre><br>  The meta-daemon itself through its configuration file gmetad.conf must be configured to bypass all nodes in turn: <br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">data_source</span></span> ‚Äúdatabases‚Äù node_1_ip_address node_2_ip_address ‚Ä¶ node_N_ip_address</code> </pre><br>  The configuration described above is depicted as follows: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c0c/c1c/b58/c0cc1cb58cc2a5204b5b3e86cee11be5.jpg"><br>  Ganglia monitoring demons communicate with each other.  A meta daemon polls one monitoring daemon from a cluster.  With his inaccessibility, he refers to the following - and so on.  The web frontend receives XML from the monitoring daemon and displays it in human-readable form. <br><br><h3>  Practical use </h3><br>  The described approach allows to collect operational data from all clusters.  Each cluster has its own characteristics related to the nature of the data collected.  We have many subsystems that are different from each other - but at the same time we have one method to control them all.  I will mention only two examples: <br><ol><li>  Our management systems, collection of information about resource consumption and billing are written in ruby.  For ruby, there is a <a href="http://rubygems.org/gems/gmetric">gmetric</a> gem designed by Ilya Grigorik.  With it, during the development phase, we included billing into the monitoring system in a couple of days.  Immediately after this, we found several bottlenecks and eliminated them. </li><li>  The most important software we use is <a href="http://swift.openstack.org/">Openstack Swift</a> .  Data collection from it is <a href="https://swiftstack.com/blog/2012/04/11/swift-monitoring-with-statsd/">organized using StatsD</a> .  StatsD, in turn, has a <a href="https://github.com/jbuchbinder/statsd-ganglia-backend">backend for Ganglia</a> - and with its help we send data to our collection system. </li></ol><br><br>  If you are interested to see how the Ganglia web front-end looks like, you can do this on the open <a href="http://ganglia.wikimedia.org/latest/">front end of Wikipedia</a> .  We use the same default interface.  He looks unfriendly, but not make friends with him for long.  In addition, he has a very decent <a href="http://ganglia.wikimedia.org/latest/mobile.php">mobile version</a> , which pleases the manager.  The ability to compulsively monitor the work of the system, being anywhere and at any time is priceless. <br><br><h2>  Log collection and analysis system </h2><br>  Logs are important for three reasons: <br><ol><li>  Logs are the most important element of post-mortem and incident analysis.  If something is broken, you need to examine the logs in order to understand the causes of the incident. </li><li>  Logs help identify problems.  We know for sure that this or that program behavior causes problems to our users and this behavior can be monitored using logs. </li><li>  Logs also provide some metrics.  You can consider the frequency of certain events using logs and send it to the system for collecting operational data. </li></ol><br>  For the organization of the system of collecting logs, we chose <a href="http://logstash.net/">Logstash</a> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/10d/eb5/53c/10deb553c10f2b0ab3ff0744c98830fe.png"><br>  <em>You can not go past the project with such a nice logo</em> <br><br>  We configured Logstash as an <a href="http://www.rsyslog.com/">rsyslog server</a> .  Thanks to this, Logstash can collect logs from any software that can write to syslog. <br><br>  Having a simple system for collecting logs is not enough.  We want to benefit from it.  The centralized log repository carries several advantages at once. <br><br><h3>  Indexing and searching </h3><br>  When the number of servers is measured in dozens, not to mention hundreds, the convenience of working with logs matters.  This convenience has two aspects: flexibility and ease of use. <br><br>  Logstash can be used in conjunction with software that provides these capabilities - <a href="http://www.elasticsearch.org/overview/elasticsearch/">Elasticsearch</a> and <a href="http://www.elasticsearch.org/overview/kibana/">Kibana</a> . <br><br>  Elasticsearch is a search system equipped with a REST API built on the basis of <a href="https://lucene.apache.org/">Apache Lucene</a> .  The presence of an API makes the system flexible. <br><br>  Kibana is a web front end capable of running on top of Elasticsearch.  He is responsible for ease of use.  Kibana's work can be seen on the <a href="http://demo.kibana.org/">demo page.</a> <br><br><h2>  Event Monitoring </h2><br>  When someone builds a monitoring system, it may not have a system for collecting logs or operational data (although this is wrong).  But it definitely has event monitoring - in fact, the search for ‚ÄúHabrahabru‚Äù tells us that when discussing monitoring, people mean, first of all, its event component. <br><br>  Event monitoring is important because it provides the main interfaces for the interaction of support operators with the system.  It serves as a dashboard for them and sends notifications. <br><br>  We chose <a href="http://www.shinken-monitoring.org/">Shinken</a> as an event monitoring system.  This is a Nagios-like monitoring system, but not a fork of Nagios.  Shinken was rewritten completely from scratch, while maintaining compatibility with plug-ins for Nagios. <br><br>  On the website of the system it is advertised as having the following features: <br><ul><li>  smart filtering and root problem detection leading to fewer unnecessary alerts; </li><li>  focus on ‚Äúbusiness impact‚Äù; </li><li>  scalability; </li><li>  reliability. </li></ul><br>  In ‚Äúconcentrating on the impact on business‚Äù and ‚Äúfinding root problems‚Äù we did not find much value for ourselves: in fact, the logic that Nagios provides for this is enough. <br><br>  In terms of scalability and reliability, Shinken provides interesting opportunities. <br><br><h3>  Scaling and failover </h3><br>  Shinken is a truly modular system.  One of the main innovations presented in Shinken is a clear division into various demons: <br><ul><li>  Pollers - run checks and return results; </li><li>  Reactionners - react to results by sending notifications or triggering event handlers; </li><li>  Schedulers - distribute tasks between pollers and results between reactionaries.  Schedulers have some internal intelligence that allows them, for example, if one of the checks returns an error, raise the check queues on which it depends. </li><li>  Brokers - get data from schedulers and save them to storage specified with plugins. </li><li>  Arbiter - the arbiter knows the entire system configuration and distributes the configuration parts and tasks between schedulers.  He checks the performance of other demons, and if, for example, one of the schedulers does not respond, he divides his tasks among others.  You cannot have more than one active arbiter in the same cluster. </li></ul><br><br>  Each of these demons can run completely independently of the others.  In addition, you can run as many demons as you need (except for the arbiter). <br><br>  Providing a reservation for arbitrators is fairly straightforward.  Each demon can have a spare copy.  The configuration parameter 'spare' is responsible for this.  In determining the principal arbitrator, he is set to zero, <br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">define</span></span> arbiter{ <span class="hljs-attribute"><span class="hljs-attribute">arbiter_name</span></span> arbiter-master address master_ip host_name master_hostname port <span class="hljs-number"><span class="hljs-number">7770</span></span> spare <span class="hljs-number"><span class="hljs-number">0</span></span> }</code> </pre><br>  and when configuring the spare - unit <br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">define</span></span> arbiter{ <span class="hljs-attribute"><span class="hljs-attribute">arbiter_name</span></span> arbiter-slave address spare_ip host_name slave_hostname port <span class="hljs-number"><span class="hljs-number">7770</span></span> spare <span class="hljs-number"><span class="hljs-number">1</span></span> }</code> </pre><br>  When the chief arbiter fails, the substitute takes control.  If arbitrators are configured identically (and this is the only correct way to tune them), the loss of connectivity between them does not threaten anything.  The spare will do everything the same as the main one and will not do any harm. <br><br>  A description of event monitoring would be incomplete, do not we mention the events that we are tracking.  Many of them are quite specific (and therefore of little interest to most readers), others are very general (and therefore well described in a dozen other places).  However, a couple of them are closely related to the entire system described above and therefore are of interest. <br><br><h3>  Little about checks </h3><br><h4>  Host Liveliness </h4><br>  You can come up with many definitions of a ‚Äúlive‚Äù host.  Some believe that a host responding to ICMP can be considered alive.  In reality, choosing the right definition is not a big deal.  What is important is the service running on the host, not the host itself.  The main reason why we need to know whether the host is alive or not in some formal sense is the correct definition of dependencies.  Having built the described system, we received a check for the liveliness of the host ‚Äúfor free‚Äù.  Each host must send their data to Ganglia.  As long as he really sends them away - in a sense he is "alive."  If he does not, he is not exactly fine.  We verify the availability of this data with a <a href="http://exchange.nagios.org/directory/Plugins/Network-and-Systems-Management/Others/check_ganglia/details">Ganglia plugin for Nagios</a> , slightly modified for our purposes. <br><br><h4>  Performance Check </h4><br>  Having a system for collecting operational data, it would be unwise to not use it in event monitoring.  The easiest way to do this is to compare the resulting values ‚Äã‚Äãwith some threshold values.  The Ganglia-plugin for Nagios mentioned in the previous paragraph does it out of the box. <br><br><h2>  TL; DR </h2><br>  High-level interaction monitoring component <br><br><img src="https://habrastorage.org/getpro/habr/post_images/960/ebc/098/960ebc098f89f26a782cc3d1f0024b11.jpg"><br>  It is easier to understand than to draw. <br><br>  The application cluster sends operational data to Ganglia, logs to Logstash and event checks to Shinken. <br><br>  Ganglia sends data on the ‚Äúliveliness‚Äù of the hosts and the verification of the operational data to Shinken. <br><br>  Shinken processes the output in two ways.  It sends notifications to operators and runs event handlers that fix some problems. <br><br>  Logs, operational data and information about events are sent to the ‚Äúdashboards‚Äù - Kibana, Ganglia web frontend and Shinken web, respectively, where they are watched by operators. <br><br><h2>  Why did we decide to write about it? </h2><br>  The monitoring system, which we built, did not reflect as a panacea corresponding to some ‚Äúcorrect‚Äù practices.  Nevertheless, in life she proved to be a reliable helper on whom you can rely. <br><br>  The main element of our system for the prevention and handling of emergency situations is our support team, which operates continuously.  We did not try to come up with an autonomous system capable of working without human intervention. <br><br>  The fact that she works in our environment does not mean that she will work in another without making changes to it.  Monitoring is a part of our common ecosystem, which includes both technical measures (such as the availability and proper use of the configuration management system) and business processes in the work of our support team. <br><br>  Nevertheless, we are sure that our approach is quite flexible and can be applied in one form or another in a fairly wide range of situations that are encountered in their work by teams supporting IT services. </div><p>Source: <a href="https://habr.com/ru/post/220707/">https://habr.com/ru/post/220707/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../220697/index.html">My [Credit] Cards for Android application</a></li>
<li><a href="../220699/index.html">How we increased conversion</a></li>
<li><a href="../220701/index.html">Fody and its plugins</a></li>
<li><a href="../220703/index.html">Kitten care device via the Internet</a></li>
<li><a href="../220705/index.html">December DevGamm 2013 report on COLT</a></li>
<li><a href="../220711/index.html">Intel¬Æ INDE beta Contest: The Original Android Application - Your IDF14 Pass!</a></li>
<li><a href="../220713/index.html">We build a web application in Java without JEE and Spring</a></li>
<li><a href="../220715/index.html">CSS 3 Timing Functions and what they eat</a></li>
<li><a href="../220717/index.html">Work with external device registers in C language, part 1</a></li>
<li><a href="../220723/index.html">We connect a new chip from WIZnet: W5500. IP for small things</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>