<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How Discord Scaled Elixir to 5 Million Concurrent Users</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="From the very beginning Discord actively used Elixir. Erlang virtual machine was the perfect candidate for creating a highly parallel real-time system...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How Discord Scaled Elixir to 5 Million Concurrent Users</h1><div class="post__text post__text-html js-mediator-article">  From the very beginning Discord actively used Elixir.  Erlang virtual machine was the perfect candidate for creating a highly parallel real-time system that we were going to create.  The original Discord prototype was developed by Elixir;  it is now at the core of our infrastructure.  The task and mission of Elixir is simple: access to all the power of the Erlang VM through a much more modern and user-friendly language and toolkit. <br><br>  Two years have passed.  We now have <b>five million simultaneous users</b> , and <b>millions of events</b> pass through the system <b>per second</b> .  Although we absolutely do not regret the choice of architecture, we had to do a lot of research and experimentation to achieve such a result.  Elixir is a new ecosystem, and the Erlang ecosystem lacks information about its use in production (although <a href="https://www.erlang-in-anger.com/">Erlang in Anger</a> is something).  Following the results of the whole journey, trying to adapt Elixir to work in Discord, we learned some lessons and created a number of libraries. <br><a name="habracut"></a><br><h1>  Fan Message Deployment </h1><br>  Although Discord has many functions, basically everything comes down to pub / sub.  Users connect to WebSocket and unwind a session (GenServer), which then establishes a connection with remote Erlang nodes where guild processes are running (also GenServers).  If something is published in guild (the internal naming of the ‚ÄúDiscord Server‚Äù), it is fanned out for each connected session. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9aa/d12/44f/9aad1244f8e2dc707c611cb443d551c4.gif">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      When the user goes online, he connects to the guild, and he publishes the presence status in all other connected sessions.  There is a lot of other logic, but a simplified example: <br><br><pre><code class="hljs ruby"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">handle_call</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">({</span></span><span class="hljs-symbol"><span class="hljs-function"><span class="hljs-params"><span class="hljs-symbol">:publish</span></span></span></span><span class="hljs-function"><span class="hljs-params">, message}, _from, </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">%{sessions: sessions}</span></span></span></span><span class="hljs-function"><span class="hljs-params">=state)</span></span></span></span> <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> Enum.each(sessions, &amp;send(&amp;<span class="hljs-number"><span class="hljs-number">1</span></span>.pid, message)) {<span class="hljs-symbol"><span class="hljs-symbol">:reply</span></span>, <span class="hljs-symbol"><span class="hljs-symbol">:ok</span></span>, state} <span class="hljs-keyword"><span class="hljs-keyword">end</span></span></code> </pre> <br>  This was the normal approach when we initially created Discord for groups of 25 or less users.  However, we were lucky to encounter ‚Äúgood problems‚Äù of growth when <a href="https://facebook.github.io/react/blog/2015/10/19/reactiflux-is-moving-to-discord.html">people started using Discord in large groups</a> .  As a result, we came to the conclusion that on many Discord servers like <a href="https://www.reddit.com/r/Overwatch/">/ r / Overwatch</a> there are up to 30,000 users at a time.  During peak hours, we observed that these processes could not cope with message queues.  At some point, we had to manually intervene and turn off message generation to help cope with the load.  It was necessary to deal with the problem before it becomes large-scale. <br><br>  We started with <a href="https://github.com/alco/benchfella">benchmarks</a> for the most loaded paths within the guild processes and soon identified the obvious cause of the trouble.  The exchange of messages between Erlang processes was not as effective as we thought, and the Erlang unit of work for process shading was also very expensive.  We found that the time for a single <code>send/2</code> call can vary from 30 Œºs to 70 Œºs due to the de-tuning of the Erlang call process.  This meant that at peak hours the publication of a single event from a large guild can take from 900 ms to 2.1 s!  Erlang processes are completely single-threaded, and shards are the only option for parallelization.  Such an event would require considerable forces, and we knew that there would be a better option. <br><br>  It was necessary to somehow distribute the work of sending messages.  Since the spawn processes in Erlang are cheap, our first idea was to simply spawn a new process to process each published message.  However, all publications could occur at different times, and Discord clients depend on the linearity of events.  Moreover, this solution cannot be scaled well, because the guild service has become more and more work. <br><br>  Inspired by <a href="http://www.ostinelli.net/boost-message-passing-between-erlang-nodes/">a blog post</a> about improving performance in passing messages between nodes, we created <a href="https://github.com/hammerandchisel/manifold">Manifold</a> .  Manifold distributes the work of sending messages between remote nodes with PIDs (process ID in Erlang).  This ensures that the sending processes will call <code>send/2</code> at most as many times as the remote nodes are involved.  Manifold does this by first grouping the PIDs by their remote nodes, and then sending them to the <code>Manifold.Partitioner</code> ‚Äúseparator‚Äù on each of these nodes.  Then the separator hashes the PIDs sequentially, using <code>:erlang.phash2/2</code> , groups them by the number of cores and sends them to the child workers.  In the end, workers send messages to real processes.  This ensures that the delimiter is not overloaded and continues to be linearizable, like <code>send/2</code> .  This solution was an effective replacement for <code>send/2</code> : <br><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">Manifold</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.send</span></span>(<span class="hljs-selector-attr"><span class="hljs-selector-attr">[self(), self()]</span></span>, <span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:hello)</span></span></code> </pre> <br>  A remarkable side effect of Manifold was that we managed not only to distribute the CPU load of fan messages, but also to reduce the network traffic between the nodes: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bc7/0ce/69e/bc70ce69e2bfb4c17e6c84f7114c0781.png"><br>  <font color="gray">Reducing network traffic by one Guild node</font> <br><br>  Manifold <a href="https://github.com/hammerandchisel/manifold">is on GitHub</a> , so try. <br><br><h1>  General quick access data </h1><br>  Discord is a distributed system that uses <a href="https://en.wikipedia.org/wiki/Consistent_hashing">consistent hashing</a> .  Using this method required the creation of a ring data structure that can be used to search for the nodes of a particular object.  We wanted the system to work quickly, so we chose <a href="https://github.com/chrismoos/hash-ring">Chris Musa‚Äôs</a> wonderful <a href="https://github.com/chrismoos/hash-ring">library by</a> connecting it through the Erlang C port (the process responsible for the interface with code C).  It worked fine, but as Discord was scaled, we began to notice problems during surges with reconnecting users.  The Erlang process, which is responsible for managing the ring, began to load so much work that it could not cope with requests to the ring, and the entire system could not cope with the load.  The solution at first glance looked obvious: run a variety of processes with ring data to make better use of all the cores of the machine in order to process all requests.  But this is too important a task.  Is there a better option? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/19b/4aa/58a/19b4aa58a7242486926ad186df44b8b7.gif"><br><br>  Let's look at the components. <br><br><ul><li>  The user can be in any number of guilds, but the average is 5. </li><li>  The session-responsible Erlang VM virtual machine can support up to 500,000 sessions. </li><li>  When connecting a session, she needs to find the remote node for each guild that is interesting to her. </li><li>  Communication time with another Erlang process using request / reply is about 12 ¬µs. </li></ul><br>  If the session server crashed and rebooted, it took about 30 seconds just to search in the ring.  This is not even considering desheduling by Erlang of one process involved in the work of other processes of the ring.  Can we completely eliminate these costs? <br><br>  When working with Elixir, if you need to speed up access to data, the first step is to use <a href="http://erlang.org/doc/man/ets.html">ETS</a> .  This is a quick, changeable C dictionary;  the flip side of the coin is that the data is copied there and read from there.  We couldn‚Äôt just transfer our ring to ETS because we used port C to control the ring, so we <a href="https://github.com/hammerandchisel/ex_hash_ring">rewrote the code to pure Elixir</a> .  Once this was completed, we had a process that owned the ring and continuously copied it to the ETS, so other processes could read the data directly from the ETS.  This markedly improved performance, but the ETS read operations took about 7 ¬µs and we still spent 17.5 seconds on the search operation for values ‚Äã‚Äãin the ring.  The data structure of the ring is actually quite large, and copying it to the ETS and reading from there took most of the time.  We were disappointed;  in any other programming language, you could just make the general meaning for safe reading.  There must be some way to do this on Erlang! <br><br>  After some research, we found a <a href="">mochiglobal</a> module that uses the virtual machine function: if Erlang encounters a function that constantly returns the same data, it puts this data into a read-only heap with shared access to which processes have access.  Copying is not required.  mochiglobal uses this by creating an Erlang module with one function and compiling it.  Since the data is not copied anywhere, search costs have decreased to 0.3 Œºs, which reduced the total time to 750 ms!  However, there is no complete freebie;  the creation of a module with a data structure of this size in runtime can take up to a second.  The good news is that we rarely change the ring, so we are ready to pay such a price. <br><br>  We decided to port the mochiglobal to Elixir and add some functionality to avoid atomization.  Our version is called <a href="https://github.com/hammerandchisel/fastglobal">FastGlobal</a> . <br><br><h1>  Limited concurrency </h1><br>  After solving an important problem with node search performance, we noticed that the processes responsible for processing the <code>guild_pid</code> search in the guild nodes began to reverse.  Slow searching for nodes used to protect them.  A new problem was that about 5,000,000 session processes tried to put pressure on ten of these processes (one for each guild node).  Here, speeding up the processing did not solve the problem;  The fundamental reason was that session process calls to this registry of guilds fell out in timeout and left a request in the queue to the registry.  After some time, the query was repeated, but the constantly accumulating queries went into a fatal state.  Receiving messages from other services, sessions would block these requests until they go to a timeout, which caused the message queue to inflate and, as a result, to the OOM of the entire Erlang VM, resulting in <a href="https://status.discordapp.com/incidents/dj3l6lw926kl">cascading outages</a> . <br><br>  It was necessary to make session processes smarter;  ideally, they should not even attempt to make these calls to the guild registry if an unfortunate outcome is unavoidable.  We did not want to use a <a href="https://en.wikipedia.org/wiki/Circuit_breaker_design_pattern">circuit breaker</a> to prevent a situation when a surge in timeouts leads to a temporary state, when no attempt is made at all.  We knew how to implement this in other languages, but how to do it on Elixir? <br><br>  In most other languages, we could use an atomic counter to track outgoing requests and early warning if their number is too large, effectively implementing the semaphore.  Erlang VM is built on coordination between processes, but we didn‚Äôt want to overload the process responsible for this coordination.  After some research, we stumbled upon <code>:ets.update_counter/4</code> , which performs atomic operations due to increment on the number that is in the ETS key.  Since good parallelization was needed, you could run ETS in <code>write_concurrency</code> mode, but still read the value, because <code>:ets.update_counter/4</code> returns the result.  This gave us a fundamental basis for creating the <a href="https://github.com/hammerandchisel/semaphore">Semaphore</a> library.  It is extremely easy to use, and it works very well with high bandwidth: <br><br><pre> <code class="hljs ruby">semaphore_name = <span class="hljs-symbol"><span class="hljs-symbol">:my_sempahore</span></span> semaphore_max = <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> Semaphore.call(semaphore_name, semaphore_max, fn -&gt; <span class="hljs-symbol"><span class="hljs-symbol">:ok</span></span> <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-symbol"><span class="hljs-symbol">:ok</span></span> -&gt; IO.puts <span class="hljs-string"><span class="hljs-string">"success"</span></span> {<span class="hljs-symbol"><span class="hljs-symbol">:error</span></span>, <span class="hljs-symbol"><span class="hljs-symbol">:max</span></span>} -&gt; IO.puts <span class="hljs-string"><span class="hljs-string">"too many callers"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">end</span></span></code> </pre> <br>  This library helped protect our infrastructure at Elixir.  As recently as last week, there was a situation similar to the above-mentioned cascading interruptions in service, but this time there were no interruptions.  Our presence services failed for another reason, but the session services did not even budge, and the presence services were able to recover in a few minutes after the reboot: <br><br><img src="https://habrastorage.org/web/705/30a/e3c/70530ae3c49b45cc8d71f9002f891133.png"><br>  <font color="gray">Work presence services</font> <br><br><img src="https://habrastorage.org/web/736/0d6/489/7360d6489be94df2b44b547c64a9c0cf.png"><br>  <font color="gray">The use of CPU session services for the same period</font> <br><br>  You can find our library Semaphore <a href="https://github.com/hammerandchisel/semaphore">on GitHub</a> . <br><br><h1>  Conclusion </h1><br>  Choosing and working with Erlang and Elixir has proven to be a great experience.  If we were forced to return and start anew, we would definitely choose the same path.  We hope that the story of our experience and tools will be useful to other developers of Elixir and Erlang, and we hope to continue to talk about our work, solving problems and gaining experience in the course of this work. </div><p>Source: <a href="https://habr.com/ru/post/335220/">https://habr.com/ru/post/335220/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../335210/index.html">GraphQL offline queries with Redux Offline and Apollo</a></li>
<li><a href="../335212/index.html">Algorithm for memorizing foreign words</a></li>
<li><a href="../335214/index.html">‚ÄúA train that could!‚Äù Or ‚ÄúSpecialization Machine learning and data analysis‚Äù, through the eyes of a beginner in Data Science</a></li>
<li><a href="../335216/index.html">Asymmetric cryptography and public key cryptography are not the same thing?</a></li>
<li><a href="../335218/index.html">CNCF offered free open source cloud for DevOps / microservices</a></li>
<li><a href="../335222/index.html">Microsoft Office Automation: Another Macro Virus Hole</a></li>
<li><a href="../335224/index.html">Optimization Method Trust-Region DOGLEG. Python implementation example</a></li>
<li><a href="../335226/index.html">ML Boot Camp V, solution history for 2nd place</a></li>
<li><a href="../335228/index.html">Dagaz: Steps</a></li>
<li><a href="../335230/index.html">The number game: how algorithmic trading will change the sphere of finance</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>