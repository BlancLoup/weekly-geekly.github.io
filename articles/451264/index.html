<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Practical application of ELK. Customize logstash</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 Expanding the next system, faced with the need to handle a large number of different logs. We chose ELK as a tool. This article will di...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Practical application of ELK. Customize logstash</h1><div class="post__text post__text-html js-mediator-article"><h1>  Introduction </h1><br>  Expanding the next system, faced with the need to handle a large number of different logs.  We chose ELK as a tool.  This article will discuss our experience in setting up this stack. <br><br>  We do not set goals to describe all its capabilities, but we want to concentrate on solving practical problems.  It is caused by the fact that in the presence of a sufficiently large amount of documentation and ready-made images, there are a lot of pitfalls, at least in our case they are revealed. <br><a name="habracut"></a><br>  We deployed the stack through docker-compose.  Moreover, we had a well-written docker-compose.yml, which allowed us to raise the stack with almost no problems.  And it seemed to us that the victory was already close, now we will screw up a little to fit our needs and that's it. <br><br>  Unfortunately, the attempt to tune the system to receive and process logs from our application was not crowned with success.  Therefore, we decided that it is worth examining each component separately, and then return to their connections. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      So, start with logstash. <br><br><h1>  Environment, Deployment, Logstash Launch in Container </h1><br>  For deployment, we use docker-compose, the experiments described here were conducted on MacOS and Ubuntu 18.0.4. <br><br>  The image of logstash, which was registered in our original docker-compose.yml, is docker.elastic.co/logstash/logstash:6.3.2 <br><br>  We will use it for experiments. <br><br>  To launch logstash, we wrote a separate docker-compose.yml.  Of course, it was possible to launch an image from the command line, but we did solve the specific problem, where we do everything from docker-compose. <br><br><h2>  Briefly about configuration files </h2><br>  As follows from the description, logstash can be run as for one channel, in this case, it needs to transfer the * .conf file or for several channels, in this case, it needs to transfer the file pipelines.yml, which, in turn, will refer to the files .conf for each channel. <br>  We went the second way.  It seemed to us more versatile and scalable.  Therefore, we created pipelines.yml, and made the pipelines directory into which we put the .conf files for each channel. <br><br>  Inside the container there is another configuration file - logstash.yml.  We do not touch it, use it as it is. <br><br>  So, our directory structure: <br><br><img src="https://habrastorage.org/webt/ci/zd/49/cizd49eci9alvlbi1fwk8nyyaky.png"><br><br>  For input, for the time being, we assume that this is tcp on port 5046, and for output we will use stdout. <br><br>  Here is such a simple configuration for the first run.  Because the initial task is to run. <br><br>  So, we have this docker-compose.yml <br><br><pre><code class="plaintext hljs">version: '3' networks: elk: volumes: elasticsearch: driver: local services: logstash: container_name: logstash_one_channel image: docker.elastic.co/logstash/logstash:6.3.2 networks: - elk ports: - 5046:5046 volumes: - ./config/pipelines.yml:/usr/share/logstash/config/pipelines.yml:ro - ./config/pipelines:/usr/share/logstash/config/pipelines:ro</code> </pre> <br>  What do we see here? <br><br><ol><li>  Networks and volumes were taken from the original docker-compose.yml (the one where the whole stack is started) and I think that they are not strongly affected by the overall picture. </li><li>  We create one service (services) logstash, from the image docker.elastic.co/logstash/logstash:6.3.2 and assign it the name logstash_one_channel. </li><li>  We forward port 5046 inside the container, to the same internal port. </li><li>  We map our channel configuration file ./config/pipelines.yml to the file /usr/share/logstash/config/pipelines.yml inside the container, from where it will be picked up by logstash and made it read-only, just in case. </li><li>  We display the directory ./config/pipelines, where we have the files with the channel settings in the directory / usr / share / logstash / config / pipelines and also make it read-only. </li></ol><br><img src="https://habrastorage.org/webt/5u/s3/dw/5us3dwu8forutzwmtlfnlcjt-ic.png"><br><br>  Pipelines.yml file <br><br><pre> <code class="plaintext hljs">- pipeline.id: HABR pipeline.workers: 1 pipeline.batch.size: 1 path.config: "./config/pipelines/habr_pipeline.conf"</code> </pre><br>  It describes one channel with the HABR identifier and the path to its configuration file. <br><br>  Finally, the file "./config/pipelines/habr_pipeline.conf" <br><br><pre> <code class="plaintext hljs">input { tcp { port =&gt; "5046" } } filter { mutate { add_field =&gt; [ "habra_field", "Hello Habr" ] } } output { stdout { } }</code> </pre><br>  Let's not go into its description yet, try to run: <br><br><pre> <code class="bash hljs">docker-compose up</code> </pre><br>  What do we see? <br><br>  The container has started.  We can check his work: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'13123123123123123123123213123213'</span></span> | nc localhost 5046</code> </pre><br>  And we see the answer in the container console: <br><br><img src="https://habrastorage.org/webt/uj/oy/tz/ujoytzcsc_mmiagm05savxahnzm.jpeg"><br><br>  But at the same time, we also see: <br><br>  <font color="¬´#38B9C7¬ª">logstash_one_channel |</font>  [2019-04-29T11: 28: 59,790] <font color="¬´CC0000¬ª">[ERROR] [logstash.licensechecker.licensereader] Unable to retrieve license information license server {: message =&gt; "Elasticsearch Unreachable: [http: // elasticsearch: 9200 /]</font> [Manticore :: ResolutionFailure] elasticsearch ", ... <br><br>  <font color="¬´#38B9C7¬ª">logstash_one_channel |</font>  [2019-04-29T11: 28: 59,894] [INFO] [logstash.pipeline] <font color="green">Pipeline started successfully</font> {: pipeline_id =&gt; ". Monitoring-logstash",: thread =&gt; "# &lt;Thread: 0x119abb86 run&gt;"}} <br><br>  <font color="¬´#38B9C7¬ª">logstash_one_channel |</font>  [2019-04-29T11: 28: 59,988] [INFO] [logstash.agent] Pipelines running {: count =&gt; 2,: running_pipelines =&gt; [: HABR,: ". Monitoring-logstash"],: non_running_pipelines =&gt; [ ]} <br>  <font color="¬´#38B9C7¬ª">logstash_one_channel |</font>  [2019-04-29T11: 29: 00,015] <font color="¬´CC0000¬ª">[ERROR] [logstash.inputs.metrics] X-Pack is installed on Logstash but not on Elasticsearch.</font>  <font color="¬´CC0000¬ª">Please install X-Pack on Elasticsearch to use the monitoring feature.</font>  <font color="¬´CC0000¬ª">Other features may be available.</font> <br>  <font color="¬´#38B9C7¬ª">logstash_one_channel |</font>  [2019-04-29T11: 29: 00,526] [INFO] [logstash.agent] Successfully started Logstash API endpoint {: port =&gt; 9600} <br>  <font color="¬´#38B9C7¬ª">logstash_one_channel |</font>  [2019-04-29T11: 29: 04,478] [INFO] [logstash.outputs.elasticsearch] Elasticsearch connection is working {: healthcheck_url =&gt; http: // elasticsearch: 9200 /,: path =&gt; "/"} <br>  l <font color="¬´#38B9C7¬ª">ogstash_one_channel |</font>  [2019-04-29T11: 29: 04,487] <font color="orange">[WARN] [logstash.outputs.elasticsearch] Attempted to resurrect connection.</font>  <font color="orange">{: url =&gt; " <a href="http://elasticsearch/">elasticsearch</a> : 9200 /",: error_type =&gt; LogStash :: Outputs :: ElasticSearch :: HttpClient :: Pool :: HostUnreachableError,: error =&gt; "Elasticsearch Unreachable: [http: // elasticsearch: 9200 / ] [Manticore :: ResolutionFailure] elasticsearch ¬ª}</font> <br>  <font color="¬´#38B9C7¬ª">logstash_one_channel |</font>  [2019-04-29T11: 29: 04,704] [INFO] [logstash.licensechecker.licensereader] Elasticsearch connection is working {: healthcheck_url =&gt; http: // elasticsearch: 9200 /,: path =&gt; "/"} <br>  <font color="¬´#38B9C7¬ª">logstash_one_channel |</font>  [2019-04-29T11: 29: 04,710] <font color="orange">[WARN] [logstash.licensechecker.licensereader] Attempted to resurrect connection to dead instance.</font>  <font color="orange">{: url =&gt; " <a href="http://elasticsearch/">elasticsearch</a> : 9200 /",: error_type =&gt; LogStash :: Outputs :: ElasticSearch :: HttpClient :: Pool :: HostUnreachableError,: error =&gt; "Elasticsearch Unreachable: [http: // elasticsearch: 9200 / ] [Manticore :: ResolutionFailure] elasticsearch ¬ª}</font> <br><br>  And our log crawls up all the time. <br><br>  Here I highlighted in green the message that the pipeline had started successfully, in red - the error message and in yellow - the message about the attempt to contact <a href="http://elasticsearch/">elasticsearch</a> : 9200. <br>  This is due to the fact that in the logstash.conf included in the image, it is worth checking for the availability of elasticsearch.  After all, logstash assumes that it works as part of the Elk stack, and we have separated it. <br><br>  You can work, but not convenient. <br><br>  The solution is to disable this check through the XPACK_MONITORING_ENABLED environment variable. <br><br>  Make a change to docker-compose.yml and run it again: <br><br><pre> <code class="plaintext hljs">version: '3' networks: elk: volumes: elasticsearch: driver: local services: logstash: container_name: logstash_one_channel image: docker.elastic.co/logstash/logstash:6.3.2 networks: - elk environment: XPACK_MONITORING_ENABLED: "false" ports: - 5046:5046 volumes: - ./config/pipelines.yml:/usr/share/logstash/config/pipelines.yml:ro - ./config/pipelines:/usr/share/logstash/config/pipelines:ro</code> </pre><br>  Now, everything is fine.  The container is ready for experiments. <br><br>  We can dial again in the next console: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'13123123123123123123123213123213'</span></span> | nc localhost 5046</code> </pre><br>  And see: <br><br><pre> <code class="plaintext hljs">logstash_one_channel | { logstash_one_channel | "message" =&gt; "13123123123123123123123213123213", logstash_one_channel | "@timestamp" =&gt; 2019-04-29T11:43:44.582Z, logstash_one_channel | "@version" =&gt; "1", logstash_one_channel | "habra_field" =&gt; "Hello Habr", logstash_one_channel | "host" =&gt; "gateway", logstash_one_channel | "port" =&gt; 49418 logstash_one_channel | }</code> </pre><br><h1>  Work within one channel </h1><br>  So, we started.  Now you can actually spend time configuring logstash directly.  For now, let's not touch the pipelines.yml file, see what can be obtained by working with one channel. <br><br>  I must say that the general principle of working with the channel configuration file is well described in the official manual, <a href="https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html">here</a> <br>  If you want to read in Russian, we used this <a href="https://habr.com/ru/post/165059/">article</a> (but the query syntax is old there, you need to take this into account). <br><br>  Let's go in series from the Input section.  We have already seen the work on tcp.  What else can be interesting here? <br><br><h2>  Test messages using heartbeat </h2><br>  There is such an interesting opportunity to generate automatic test messages. <br>  To do this, you need to include the heartbean plugin in the input section. <br><br><pre> <code class="plaintext hljs">input { heartbeat { message =&gt; "HeartBeat!" } }</code> </pre><br>  Turn on, start once a minute to get <br><br><pre> <code class="plaintext hljs">logstash_one_channel | { logstash_one_channel | "@timestamp" =&gt; 2019-04-29T13:52:04.567Z, logstash_one_channel | "habra_field" =&gt; "Hello Habr", logstash_one_channel | "message" =&gt; "HeartBeat!", logstash_one_channel | "@version" =&gt; "1", logstash_one_channel | "host" =&gt; "a0667e5c57ec" logstash_one_channel | }</code> </pre><br>  We want to receive more often, we need to add the interval parameter. <br>  So we will receive a message every 10 seconds. <br><br><pre> <code class="plaintext hljs">input { heartbeat { message =&gt; "HeartBeat!" interval =&gt; 10 } }</code> </pre><br><h2>  Getting data from a file </h2><br>  We also decided to watch the file mode.  If it works normally with the file, then it is possible that no agent is required, well, at least for local use. <br><br>  According to the description, the mode of operation should be similar tail -f, i.e.  reads new lines or, as an option, reads the entire file. <br><br>  So, what we want to get: <br><br><ol><li>  We want to get the lines that are added to a single log file. </li><li>  We want to receive data that is recorded in several log files, while being able to share what is received from. </li><li>  We want to check that when logstash is restarted, it will not receive this data again. </li><li>  We want to check that if logstash is disabled, and the data in the files continue to be written, then when we launch it, we will receive this data. </li></ol><br>  To conduct the experiment, add another line to docker-compose.yml, opening the directory where we put the files. <br><br><pre> <code class="plaintext hljs">version: '3' networks: elk: volumes: elasticsearch: driver: local services: logstash: container_name: logstash_one_channel image: docker.elastic.co/logstash/logstash:6.3.2 networks: - elk environment: XPACK_MONITORING_ENABLED: "false" ports: - 5046:5046 volumes: - ./config/pipelines.yml:/usr/share/logstash/config/pipelines.yml:ro - ./config/pipelines:/usr/share/logstash/config/pipelines:ro - ./logs:/usr/share/logstash/input</code> </pre><br>  And change the input section to habr_pipeline.conf <br><br><pre> <code class="plaintext hljs">input { file { path =&gt; "/usr/share/logstash/input/*.log" } }</code> </pre><br>  Starting: <br><br><pre> <code class="bash hljs">docker-compose up</code> </pre><br>  To create and record log files, we will use the command: <br><br><pre> <code class="bash hljs"> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'1'</span></span> &gt;&gt; logs/number1.log</code> </pre><br><pre> <code class="plaintext hljs">{ logstash_one_channel | "host" =&gt; "ac2d4e3ef70f", logstash_one_channel | "habra_field" =&gt; "Hello Habr", logstash_one_channel | "@timestamp" =&gt; 2019-04-29T14:28:53.876Z, logstash_one_channel | "@version" =&gt; "1", logstash_one_channel | "message" =&gt; "1", logstash_one_channel | "path" =&gt; "/usr/share/logstash/input/number1.log" logstash_one_channel | }</code> </pre><br>  Yeah, it works! <br><br>  At the same time, we see that we automatically added the path field.  So in the future, we will be able to filter records by it. <br><br>  Let's try again: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'2'</span></span> &gt;&gt; logs/number1.log</code> </pre><br><pre> <code class="plaintext hljs">{ logstash_one_channel | "host" =&gt; "ac2d4e3ef70f", logstash_one_channel | "habra_field" =&gt; "Hello Habr", logstash_one_channel | "@timestamp" =&gt; 2019-04-29T14:28:59.906Z, logstash_one_channel | "@version" =&gt; "1", logstash_one_channel | "message" =&gt; "2", logstash_one_channel | "path" =&gt; "/usr/share/logstash/input/number1.log" logstash_one_channel | }</code> </pre><br><br>  And now to another file: <br><br><pre> <code class="bash hljs"> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'1'</span></span> &gt;&gt; logs/number2.log</code> </pre><br><pre> <code class="plaintext hljs">{ logstash_one_channel | "host" =&gt; "ac2d4e3ef70f", logstash_one_channel | "habra_field" =&gt; "Hello Habr", logstash_one_channel | "@timestamp" =&gt; 2019-04-29T14:29:26.061Z, logstash_one_channel | "@version" =&gt; "1", logstash_one_channel | "message" =&gt; "1", logstash_one_channel | "path" =&gt; "/usr/share/logstash/input/number2.log" logstash_one_channel | }</code> </pre><br>  Fine!  The file picked up, path was specified correctly, everything is fine. <br><br>  Stop logstash and restart.  Let's wait.  Silence.  Those.  We do not receive these records again. <br><br>  And now the most courageous experiment. <br><br>  Put logstash and execute: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'3'</span></span> &gt;&gt; logs/number2.log <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'4'</span></span> &gt;&gt; logs/number1.log</code> </pre><br>  Run logstash again and see: <br><br><pre> <code class="plaintext hljs">logstash_one_channel | { logstash_one_channel | "host" =&gt; "ac2d4e3ef70f", logstash_one_channel | "habra_field" =&gt; "Hello Habr", logstash_one_channel | "message" =&gt; "3", logstash_one_channel | "@version" =&gt; "1", logstash_one_channel | "path" =&gt; "/usr/share/logstash/input/number2.log", logstash_one_channel | "@timestamp" =&gt; 2019-04-29T14:48:50.589Z logstash_one_channel | } logstash_one_channel | { logstash_one_channel | "host" =&gt; "ac2d4e3ef70f", logstash_one_channel | "habra_field" =&gt; "Hello Habr", logstash_one_channel | "message" =&gt; "4", logstash_one_channel | "@version" =&gt; "1", logstash_one_channel | "path" =&gt; "/usr/share/logstash/input/number1.log", logstash_one_channel | "@timestamp" =&gt; 2019-04-29T14:48:50.856Z logstash_one_channel | }</code> </pre><br>  Hooray!  Everything picked up. <br><br>  But, it is necessary to warn about the following.  If the container with logstash is removed (docker stop logstash_one_channel &amp;&amp; docker rm logstash_one_channel), then nothing will catch up.  The position of the file to which it was read was saved inside the container.  If run from scratch, it will only accept new lines. <br><br><h3>  Reading existing files </h3><br>  Suppose we run logstash for the first time, but we already have logs and we would like to process them. <br>  If we run logstash with the input section that was used above, we will not get anything.  Only new lines will be processed by logstash. <br><br>  In order to pull up lines from existing files, add an additional line to the input section: <br><br><pre> <code class="plaintext hljs">input { file { start_position =&gt; "beginning" path =&gt; "/usr/share/logstash/input/*.log" } }</code> </pre><br>  Moreover, there is a nuance, it affects only new files that logstash has not yet seen.  For the same files that were already in the logstash field of view, he already remembered their size and will now take only new entries in them. <br><br>  Let us dwell on the study of the input section.  There are still many options, but for us, for the future experiments is enough. <br><br><h2>  Routing and Data Conversion </h2><br>  Let's try to solve the following problem, let's say we have messages from one channel, some of which are informational, and some are an error message.  Different tag.  Some INFO, others ERROR. <br><br>  We need to separate them at the exit.  Those.  Informational messages are written to one channel, and error messages to another. <br><br>  For this, from the input section, go to filter and output. <br><br>  Using the filter section, we will analyze the incoming message, receiving from it hash (key-value pairs), with which you can already work, i.e.  disassemble according to the conditions.  And in the output section, select messages and send each to its own channel. <br><br><h3>  Parsing a message using grok </h3><br>  In order to parse text lines and get a set of fields from them, in the filter section there is a special plugin - grok. <br><br>  Without aiming to give here a detailed description of it (I refer to <a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html">official documentation</a> for this), I will give my simple example. <br><br>  For this, you need to decide on the format of the input lines.  I have these: <br><br>  1 INFO message1 <br>  2 ERROR message2 <br><br>  Those.  The identifier comes first, then INFO / ERROR, then a word without spaces. <br>  Not difficult, but enough to understand the principle of work. <br><br>  So, in the filter section, in the grok plugin, we need to define a pattern for parsing our strings. <br><br>  It will look like this: <br><br><pre> <code class="plaintext hljs">filter { grok { match =&gt; { "message" =&gt; ["%{INT:message_id} %{LOGLEVEL:message_type} %{WORD:message_text}"] } } }</code> </pre><br>  In essence, this is a regular expression.  Ready-made patterns are used, such as INT, LOGLEVEL, WORD.  Their description, as well as other patterns, can be found <a href="https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns">here.</a> <br><br>  Now, passing through this filter, our string will turn into hash of three fields: message_id, message_type, message_text. <br><br>  They will be displayed in the output section. <br><br><h3>  Routing messages in the output section with the if command </h3><br>  In the output section, as we remember, we were going to separate the messages into two streams.  Some - which iNFO, we will output to the console, and with errors, we will output to the file. <br><br>  How do we separate these messages?  The condition of the problem already prompts a solution - after all, we already have a selected message_type field, which can take only two INFO and ERROR values.  It is for him and make a choice with the operator if. <br><br><pre> <code class="plaintext hljs">if [message_type] == "ERROR" { #     } else { #    stdout }</code> </pre><br>  The description of work with fields and operators can be found here in this section of the <a href="https://www.elastic.co/guide/en/logstash/current/configuration.html">official manual</a> . <br><br>  Now, about the actual output itself. <br><br>  Output to the console, everything is clear here - stdout {} <br><br>  But the output to the file - we remember that we are launching all this from the container and in order for the file to which we write the result to be accessible from the outside, we need to open this directory in docker-compose.yml. <br><br>  Total: <br><br>  The output section of our file looks like this: <br><br><pre> <code class="plaintext hljs"> output { if [message_type] == "ERROR" { file { path =&gt; "/usr/share/logstash/output/test.log" codec =&gt; line { format =&gt; "custom format: %{message}"} } } else {stdout { } } }</code> </pre><br>  Add another volume to docker-compose.yml for output: <br><br><pre> <code class="plaintext hljs">version: '3' networks: elk: volumes: elasticsearch: driver: local services: logstash: container_name: logstash_one_channel image: docker.elastic.co/logstash/logstash:6.3.2 networks: - elk environment: XPACK_MONITORING_ENABLED: "false" ports: - 5046:5046 volumes: - ./config/pipelines.yml:/usr/share/logstash/config/pipelines.yml:ro - ./config/pipelines:/usr/share/logstash/config/pipelines:ro - ./logs:/usr/share/logstash/input - ./output:/usr/share/logstash/output</code> </pre><br>  We start, we try, we see division into two streams. </div><p>Source: <a href="https://habr.com/ru/post/451264/">https://habr.com/ru/post/451264/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../451254/index.html">Compilation: unboxing iron IaaS provider</a></li>
<li><a href="../451256/index.html">What is the perfect reporting system. Is it possible to understand what is happening in the company?</a></li>
<li><a href="../451258/index.html">Catch Me If You Can. Manager's letter</a></li>
<li><a href="../451260/index.html">10 thematic events at ITMO University</a></li>
<li><a href="../451262/index.html">Scientists from Stanford: a gadget placed in the ear can monitor the brain</a></li>
<li><a href="../451268/index.html">Victor Gamov about Kafka Streams IQ on jug.msk.ru</a></li>
<li><a href="../451270/index.html">B = Attention, or how to create time</a></li>
<li><a href="../451272/index.html">If already knocking on the door: how to protect information on devices</a></li>
<li><a href="../451274/index.html">Perfect Weapon, Perspective War and Human Being Reaching the Ceiling</a></li>
<li><a href="../451278/index.html">Wavelet analysis. Part 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>