<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Predict the popularity of articles on TJ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Once, on a languid evening, sitting opposite a flashing tjournal tape and sipping chamomile tea, I suddenly found myself reading an article about a So...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Predict the popularity of articles on TJ</h1><div class="post__text post__text-html js-mediator-article"><p>  Once, on a languid evening, sitting opposite a flashing tjournal tape and sipping chamomile tea, I suddenly found myself reading an article about a Soviet light bulb that had been lighting someone's entrance for 80 years.  Yes, very interesting, but I still prefer articles about <del>  policy </del>  Achievements of AI in the game of doom, the adventures of SpaceX rockets and, in the end, with the highest number of views.  And what are all the articles gaining impressive ratings?  Posts about the size of a tweet about some kind of political action or talmuda with a detailed analysis of the Russian film industry?  Well, then it's time to uncover your Jupyter notebook and display the formula for a perfect article. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/487/76d/3ee/48776d3ee7ac4672b8dfa7099e3b2b5b.jpg"></div><a name="habracut"></a><br>  So, the task is this - to predict the number of views of the article, based on its content.  To implement all this, we will use Python 3. *, a standard set from libraries for machine learning and data processing (pandas, numpy, scipy, scikit-learn), and we will write and run all this in Jupyter notebook.  Link to all the sources <a href="https://github.com/vangaa/tj_habra_article">here</a> . <p></p><br><h2 id="sbor-bazy">  Base collection </h2><br><p>  To get started is to get the data.  Since TJ doesn‚Äôt have its own API, we‚Äôll collect old good old scraping of bare html using <a href="http://docs.python-requests.org/en/master/">requests</a> and retrieving useful data from it using <a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> .  At the exit we get 39116 articles from 2014.  However, it should be noted that the article itself is written in the form of a html block with all its inherent bunch of tags, and not as simple text.  The script itself is <a href="https://github.com/vangaa/tj_habra_article/blob/master/tj_fetcher.py">here</a> . </p><br><h2 id="feature-engineering">  Feature Engineering </h2><br><p>  After an hour of the script, we have almost 39,000 articles in our hands, and it is time to pull features from them of all degrees of importance.  But first we import everything you need: </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> json <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> datetime <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> snowballstemmer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> bs4 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BeautifulSoup <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> itertools <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.sparse <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> csr_matrix, hstack <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.feature_extraction.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TfidfTransformer, CountVectorizer <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.cross_validation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.decomposition <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LatentDirichletAllocation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gensim.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Word2Vec <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> lightgbm <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> lgb <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> r2_score, mean_absolute_error <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline</code> </pre> <br><p>  We read our dataset and see its contents: </p><br><pre> <code class="python hljs">dataframe = pd.read_json(<span class="hljs-string"><span class="hljs-string">"data/tj_dataset.json"</span></span>) dataframe.head()</code> </pre> <br><p><img src="https://habrastorage.org/files/e64/9bd/7ba/e649bd7ba647450caf3319786e3cfa69.png" alt="image"></p><br><p>  Because of the available data for training, we have only the content of the post and the date of publication, then most of the engineering features will revolve around text data.  From the post itself you can get two types of features: structural and semantic (yes, the names came up with himself).  The structural ones include those that, do not believe, determine the structure of a post: the number of links to YouTube, Twitter, the number of words in the title of the post, the average number of words in text blocks, etc.  All values ‚Äã‚Äãthat could not be caught (usually due to the small amount of text data), just replace with -9999, gradient boosting will understand us. </p><br><p>  Let's start with temporary features.  And here everything is impossible - we pull out the year, month, day, hours and minutes, from engineering the features here only the day of the week: </p><br><div class="spoiler">  <b class="spoiler_title">Monthly dictionary and time calculation function</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_time</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> time = x.split()[<span class="hljs-number"><span class="hljs-number">-1</span></span>] h, m = time.split(<span class="hljs-string"><span class="hljs-string">":"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> int(h)*<span class="hljs-number"><span class="hljs-number">60</span></span> + int(m) m_keys = { <span class="hljs-string"><span class="hljs-string">''</span></span>: <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>: <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>: <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>: <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>: <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>: <span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>: <span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>: <span class="hljs-number"><span class="hljs-number">8</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>: <span class="hljs-number"><span class="hljs-number">9</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>: <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>: <span class="hljs-number"><span class="hljs-number">11</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>: <span class="hljs-number"><span class="hljs-number">12</span></span>, }</code> </pre> </div></div><br><pre> <code class="python hljs">dataframe[<span class="hljs-string"><span class="hljs-string">'month'</span></span>] = dataframe.date.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: m_keys[x.split()[<span class="hljs-number"><span class="hljs-number">1</span></span>]]) dataframe[<span class="hljs-string"><span class="hljs-string">'time'</span></span>] = dataframe.date.apply(get_time) dataframe[<span class="hljs-string"><span class="hljs-string">'day'</span></span>] = dataframe.date.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: int(x.split()[<span class="hljs-number"><span class="hljs-number">0</span></span>])) dataframe[<span class="hljs-string"><span class="hljs-string">'year'</span></span>] = dataframe.date.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: int(x.split()[<span class="hljs-number"><span class="hljs-number">2</span></span>][:<span class="hljs-number"><span class="hljs-number">-1</span></span>])) dataframe[<span class="hljs-string"><span class="hljs-string">"weekday"</span></span>] = dataframe[[<span class="hljs-string"><span class="hljs-string">"year"</span></span>, <span class="hljs-string"><span class="hljs-string">"month"</span></span>, <span class="hljs-string"><span class="hljs-string">"day"</span></span>]].apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: datetime.date(*x).weekday(), axis=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p>  And now the most creative part of the work has begun, where you can give free rein to your imagination and collect everything from your texts. </p><br><p>  First of all, you should remove bare chunks of text from the html block, tokenize them and pass through the resulting words: </p><br><pre> <code class="python hljs">stemmer = snowballstemmer.RussianStemmer() rus_chars = set([chr(i) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1072</span></span>, <span class="hljs-number"><span class="hljs-number">1104</span></span>)]) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">tokenize_sent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(sent)</span></span></span><span class="hljs-function">:</span></span> sent = sent.lower() sent = <span class="hljs-string"><span class="hljs-string">""</span></span>.join([i <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> rus_chars <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-string"><span class="hljs-string">" "</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sent]) words = stemmer.stemWords(sent.split()) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> words</code> </pre> <br><p>  Without thinking twice, we add to the list of features the number of words in the post and title, the number of html tags, the mean and root-mean-square ration of the number of words in the blocks.  Posts on tj are full of pictures, videos, references to twitter and links to other resources, which can also be used: </p><br><div class="spoiler">  <b class="spoiler_title">Functions for pulling structural features</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_p_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(article)</span></span></span><span class="hljs-function">:</span></span> html = BeautifulSoup(article, <span class="hljs-string"><span class="hljs-string">"lxml"</span></span>) data = html.findAll(<span class="hljs-string"><span class="hljs-string">"p"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [i.text <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> data] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_p_count</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(article)</span></span></span><span class="hljs-function">:</span></span> html = BeautifulSoup(article, <span class="hljs-string"><span class="hljs-string">"lxml"</span></span>) data = html.findAll(<span class="hljs-string"><span class="hljs-string">"p"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> len(data) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_p_sizes</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(article)</span></span></span><span class="hljs-function">:</span></span> html = BeautifulSoup(article, <span class="hljs-string"><span class="hljs-string">"lxml"</span></span>) data = html.findAll(<span class="hljs-string"><span class="hljs-string">"p"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> [len(preprocess_sent(i.text)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> data] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_tags_count</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(article)</span></span></span><span class="hljs-function">:</span></span> html = BeautifulSoup(article, <span class="hljs-string"><span class="hljs-string">"lxml"</span></span>) data = html.findAll() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> len(data) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">mean_p</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> len(x): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">-9999</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.mean(x) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">std_p</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> len(x): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">-9999</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.std(x)</code> </pre> </div></div><br><pre> <code class="python hljs">dataframe[<span class="hljs-string"><span class="hljs-string">'p_list'</span></span>] = dataframe.article.apply(get_p_data) dataframe[<span class="hljs-string"><span class="hljs-string">'text_chained'</span></span>] = dataframe.p_list.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: <span class="hljs-string"><span class="hljs-string">"\n"</span></span>.join(x)) dataframe[<span class="hljs-string"><span class="hljs-string">'p_list_tokenized'</span></span>] = dataframe.p_list.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: [tokenize_sent(i) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> x]) dataframe[<span class="hljs-string"><span class="hljs-string">'p_list_tokenized_joined'</span></span>] = dataframe.p_list_tokenized.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: list(itertools.chain(*x))) dataframe[<span class="hljs-string"><span class="hljs-string">'title_tokenized'</span></span>] = dataframe.title.apply(tokenize_sent) dataframe[<span class="hljs-string"><span class="hljs-string">'images_count'</span></span>] = dataframe.article.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x.count(<span class="hljs-string"><span class="hljs-string">"wrapper-image"</span></span>)) dataframe[<span class="hljs-string"><span class="hljs-string">'wide_labels_count'</span></span>] = dataframe.article.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x.count(<span class="hljs-string"><span class="hljs-string">"wrapper-wide"</span></span>)) dataframe[<span class="hljs-string"><span class="hljs-string">'link_widget_count'</span></span>] = dataframe.article.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x.count(<span class="hljs-string"><span class="hljs-string">"link-widget"</span></span>)) dataframe[<span class="hljs-string"><span class="hljs-string">'links_count'</span></span>] = dataframe.article.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x.count(<span class="hljs-string"><span class="hljs-string">"a href"</span></span>)) - \ (dataframe.images_count + dataframe.link_widget_count) dataframe[<span class="hljs-string"><span class="hljs-string">'youtube_count'</span></span>] = dataframe.article.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x.count(<span class="hljs-string"><span class="hljs-string">"wrapper-video"</span></span>)) dataframe[<span class="hljs-string"><span class="hljs-string">'tweets_count'</span></span>] = dataframe.article.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x.count(<span class="hljs-string"><span class="hljs-string">"wrapper-tweet"</span></span>)) dataframe[<span class="hljs-string"><span class="hljs-string">"tags_count"</span></span>] = dataframe.article.apply(get_tags_count) dataframe[<span class="hljs-string"><span class="hljs-string">"p_count"</span></span>] = dataframe.p_list.apply(len) dataframe[<span class="hljs-string"><span class="hljs-string">"text_sizes"</span></span>] = dataframe.p_list_tokenized.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: [len(i) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> x]) dataframe[<span class="hljs-string"><span class="hljs-string">"text_sizes_mean"</span></span>] = dataframe.text_sizes.apply(mean_p) dataframe[<span class="hljs-string"><span class="hljs-string">"text_sizes_std"</span></span>] = dataframe.text_sizes.apply(std_p) dataframe[<span class="hljs-string"><span class="hljs-string">"text_words_count"</span></span>] = dataframe.p_list_tokenized.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: len(list(itertools.chain(*x)))) dataframe[<span class="hljs-string"><span class="hljs-string">"title_words_count"</span></span>] = dataframe.title_tokenized.apply(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: len(x))</code> </pre> <br><p>  Here with semantic features the situation is more complicated.  And the first thing that comes to mind: tf-idf - a classic in the processing of text data.  But only tf-idf does not inspire confidence, so we will also include <a href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Latent Dirihlet Allocation</a> (LDA) to reduce the dimension of the post to a 10-50 element vector.  Just in case, we will try to take three LDAs of 10, 20 and 50 topics, respectively. </p><br><pre> <code class="python hljs">count_vec = CountVectorizer(tokenizer = tokenize_sent, min_df=<span class="hljs-number"><span class="hljs-number">10</span></span>, max_df=<span class="hljs-number"><span class="hljs-number">0.95</span></span>) text_count_vec = count_vec.fit_transform(dataframe.text_chained) text_tfidf_vec = TfidfTransformer().fit_transform(text_count_vec).toarray() lda_features = [] topic_counts = [<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> topics <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> topic_counts: lda = LatentDirichletAllocation(topics, n_jobs=<span class="hljs-number"><span class="hljs-number">7</span></span>, learning_method=<span class="hljs-string"><span class="hljs-string">"batch"</span></span>) feats = lda.fit_transform(text_count_vec) lda_features.append(feats) lda_features = np.concatenate(lda_features, axis=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p>  Can I add something else interesting?  Of course, <a href="https://radimrehurek.com/gensim/models/word2vec.html">word2vec</a> : </p><br><pre> <code class="python hljs">w2v = Word2Vec(dataframe.p_list_tokenized_joined, size=<span class="hljs-number"><span class="hljs-number">300</span></span>, workers=<span class="hljs-number"><span class="hljs-number">7</span></span>, min_count=<span class="hljs-number"><span class="hljs-number">5</span></span>) w2v_dict = {key:w2v.wv.syn0[val.index] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> key, val <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> w2v.wv.vocab.items()} vectors = list(w2v_dict.values()) words = list(w2v_dict.keys()) words_set = set(words)</code> </pre> <br><p>  At the exit, we have a bunch of word vectors with 300 elements each.  Now you need to somehow use them for the vectorization of the post.  To do this, you can use 2 techniques: the calculation of the average vector among all the vectors of words and more complex, using clusters. </p><br><p>  From the first, everything is clear - there is a matrix with vectors of words, just take the mean value along the first axis, the algorithm for processing empty posts is about the same as above: </p><br><div class="spoiler">  <b class="spoiler_title">The function to calculate the average vector</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_mean_vec</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(sent)</span></span></span><span class="hljs-function">:</span></span> matrix = [w2v_dict[i] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sent <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words_set] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> matrix: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.full((<span class="hljs-number"><span class="hljs-number">300</span></span>,), <span class="hljs-number"><span class="hljs-number">-9999</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.mean(matrix, axis=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> </div></div><br><p>  From the second, you need to build a certain number of clusters for all the word vectors and then assign each word in the text to belong to a particular cluster, i.e.  it will turn out something like CountVectorizer on steroids.  Just as in the case of LDA, we will try to take several KMeans with different number of clusters: </p><br><pre> <code class="python hljs">kmeans = [] clusters_counts = [<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">500</span></span>, <span class="hljs-number"><span class="hljs-number">1000</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> clusters <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> clusters_counts: kmeans.append(KMeans(clusters, precompute_distances=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">7</span></span>).fit(vectors))</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">The function to calculate the post vector for clusters</b> <div class="spoiler_text"><pre> <code class="python hljs">word_mappings = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> kmean <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> kmeans: labels = kmean.labels_ word_mappings.append(dict(zip(words, labels))) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_centroids_vec</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(sent)</span></span></span><span class="hljs-function">:</span></span> words = [i <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sent <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words_set] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> words: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.full((sum(clusters_counts),), <span class="hljs-number"><span class="hljs-number">-9999</span></span>) result_total = np.asarray([]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> cnt, mapper <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(clusters_counts, word_mappings): result = np.zeros((cnt, )) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: result[mapper[word]] += <span class="hljs-number"><span class="hljs-number">1</span></span> result_total = np.concatenate([result_total, result]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> result_total</code> </pre> </div></div><br><pre> <code class="python hljs">mean_texts = np.asarray([get_mean_vec(s) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> dataframe.p_list_tokenized_joined.tolist()]) mean_titles = np.asarray([get_mean_vec(s) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> dataframe.title_tokenized.tolist()]) clusters_texts = np.asarray([get_centroids_vec(s) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> dataframe.p_list_tokenized_joined.tolist()]) clusters_titles = np.asarray([get_centroids_vec(s) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> dataframe.title_tokenized.tolist()])</code> </pre> <br><p>  All features collected, it remains only to group them together into one large matrix: </p><br><pre> <code class="python hljs">feats = [<span class="hljs-string"><span class="hljs-string">"time"</span></span>, <span class="hljs-string"><span class="hljs-string">"day"</span></span>, <span class="hljs-string"><span class="hljs-string">"weekday"</span></span>, <span class="hljs-string"><span class="hljs-string">"month"</span></span>, <span class="hljs-string"><span class="hljs-string">"images_count"</span></span>, <span class="hljs-string"><span class="hljs-string">"wide_labels_count"</span></span>, <span class="hljs-string"><span class="hljs-string">"link_widget_count"</span></span>, <span class="hljs-string"><span class="hljs-string">"links_count"</span></span>, <span class="hljs-string"><span class="hljs-string">"youtube_count"</span></span>, <span class="hljs-string"><span class="hljs-string">"tweets_count"</span></span>, <span class="hljs-string"><span class="hljs-string">"tags_count"</span></span>, <span class="hljs-string"><span class="hljs-string">"p_count"</span></span>, <span class="hljs-string"><span class="hljs-string">"text_sizes_mean"</span></span>, <span class="hljs-string"><span class="hljs-string">"text_sizes_std"</span></span>, <span class="hljs-string"><span class="hljs-string">"text_words_count"</span></span>, <span class="hljs-string"><span class="hljs-string">"title_words_count"</span></span>] X = dataframe[feats].as_matrix() preprocessed_df = np.concatenate([X, mean_texts, mean_titles, lda_features, clusters_texts, clusters_titles], axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) full_df = hstack([csr_matrix(preprocessed_df), text_tfidf_vec]).tocsr()</code> </pre> <br><h3 id="trenirovka-modeli">  Model training </h3><br><p>  Since we have the number of views and rating here, then the regression problem directly arises.  Every month the number of subscribers to tj increases, and this means that the number of views is also increasing.  Those.  you need to somehow normalize the number of views, while minimizing the impact of emissions.  To normalize one post, you can take the mean and standard deviation of the previous 200 posts, scale the current value, while removing everything that is behind the first and 99th percentiles: </p><br><pre> <code class="python hljs">hits_y = dataframe.hits step = <span class="hljs-number"><span class="hljs-number">200</span></span> new_hits = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(step, dataframe.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]): hits = hits_y.iloc[i-step: i+<span class="hljs-number"><span class="hljs-number">1</span></span>] perc_high = np.percentile(hits, <span class="hljs-number"><span class="hljs-number">99</span></span>) perc_low = np.percentile(hits, <span class="hljs-number"><span class="hljs-number">1</span></span>) hits = hits[(hits &gt; perc_low)&amp;(hits &lt; perc_high)] mean = hits.mean() std = hits.std() val = max(min(hits_y.iloc[i], perc_high), perc_low) new_hits.append((val - mean)/std) hits_y = np.asarray(new_hits) full_df = full_df[step:]</code> </pre> <br><p>  Look at the distribution of our ratings: </p><br><pre> <code class="python hljs">plt.hist(hits_y, bins=<span class="hljs-number"><span class="hljs-number">100</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/files/530/b20/4c4/530b204c47a244aab3f7616c79ec0409.png" alt="image"></p><br><p>  It is expected bevel to the left.  This is usually treated with a logarithm: </p><br><pre> <code class="python hljs">plt.hist(np.log(hits_y - hits_y.min()+<span class="hljs-number"><span class="hljs-number">1e-8</span></span>), bins=<span class="hljs-number"><span class="hljs-number">100</span></span>)</code> </pre> <br><p><img src="https://habrastorage.org/files/aaf/0fa/cc3/aaf0facc3bfc481e823c249bcb038530.png" alt="image"></p><br><p>  Already much better. </p><br><pre> <code class="python hljs">min_val = hits_y.min() hits_log = np.log(hits_y - min_val+<span class="hljs-number"><span class="hljs-number">1e-6</span></span>)</code> </pre> <br><p>  The most difficult thing is over, it remains only to choose a model, put it on training and pray for the right choice of hyper parameters.  Usually they don‚Äôt bother with choosing a model and immediately take xgboost, which makes sense, because it has already gained its popularity on kaggle and is widely used by both novices and gurus.  But one day, accidentally stumbling upon a <a href="https://medium.com/implodinggradients/benchmarking-lightgbm-how-fast-is-lightgbm-vs-xgboost-15d224568031">comparison of benchmarks of</a> two implementations of gradient boosting, Microsoft drew attention to <a href="https://github.com/Microsoft/LightGBM">LigthGBM</a> from Microsoft with its delicious test results, which made the old man xgboost be put off as a fallback. </p><br><p>  In general, the training is the same as on xgboost - we poke back and forth max_depth and regularization to avoid overfit, we select the best number of trees under the minimum learning_rate. </p><br><p>  Dataset for LigthGBM is necessary to start with a local format: </p><br><div class="spoiler">  <b class="spoiler_title">Names of all features</b> <div class="spoiler_text"><pre> <code class="python hljs">features = (feats + list(itertools.chain(*[[<span class="hljs-string"><span class="hljs-string">f"lda_</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{lda}</span></span></span><span class="hljs-string">_</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{i}</span></span></span><span class="hljs-string">"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(lda)] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> lda <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> topic_counts])) + [<span class="hljs-string"><span class="hljs-string">f"mean_matrix_body_</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{i}</span></span></span><span class="hljs-string">"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">300</span></span>)] + [<span class="hljs-string"><span class="hljs-string">f"mean_matrix_title_</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{i}</span></span></span><span class="hljs-string">"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">300</span></span>)] + list(itertools.chain(*[[<span class="hljs-string"><span class="hljs-string">f"clusters_body_</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{i}</span></span></span><span class="hljs-string">_</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{j}</span></span></span><span class="hljs-string">"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(i)] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> clusters_counts])) + list(itertools.chain(*[[<span class="hljs-string"><span class="hljs-string">f"clusters_title_</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{i}</span></span></span><span class="hljs-string">_</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{j}</span></span></span><span class="hljs-string">"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(i)] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> clusters_counts])) + [<span class="hljs-string"><span class="hljs-string">f"tf_idf_</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">{i}</span></span></span><span class="hljs-string">"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(text_tfidf_vec.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>])])</code> </pre> </div></div><br><pre> <code class="python hljs">X_train, X_test, y_train, y_test = train_test_split(full_df, hits_log, test_size=<span class="hljs-number"><span class="hljs-number">0.2</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">753</span></span>) train_data = lgb.Dataset(X_train, label=y_train, feature_name=features) test_data = train_data.create_valid(X_test, label=y_test)</code> </pre> <br><p>  Gradient boosting parameters (obtained after long hours of training): </p><br><pre> <code class="python hljs">param = {<span class="hljs-string"><span class="hljs-string">'num_trees'</span></span>: <span class="hljs-number"><span class="hljs-number">100000</span></span>, <span class="hljs-string"><span class="hljs-string">'application'</span></span>:<span class="hljs-string"><span class="hljs-string">'regression'</span></span>, <span class="hljs-string"><span class="hljs-string">'learning_rate'</span></span>: <span class="hljs-number"><span class="hljs-number">0.01</span></span>, <span class="hljs-string"><span class="hljs-string">'num_threads'</span></span>: <span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-string"><span class="hljs-string">'max_depth'</span></span>: <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-string"><span class="hljs-string">'lambda_l2'</span></span>: <span class="hljs-number"><span class="hljs-number">1e-3</span></span>} param[<span class="hljs-string"><span class="hljs-string">'metric'</span></span>] = <span class="hljs-string"><span class="hljs-string">'mae'</span></span></code> </pre> <br><p>  Directly training itself: </p><br><pre> <code class="python hljs">bst = lgb.train(param, train_data, param[<span class="hljs-string"><span class="hljs-string">'num_trees'</span></span>], valid_sets=[test_data], early_stopping_rounds=<span class="hljs-number"><span class="hljs-number">200</span></span>)</code> </pre> <br><p>  An important feature of trees is that it is possible to appreciate the importance of the features: </p><br><pre> <code class="python hljs">importance = sorted(zip(features, bst.feature_importance()), key=<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x[<span class="hljs-number"><span class="hljs-number">1</span></span>], reverse=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> imp <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> importance[:<span class="hljs-number"><span class="hljs-number">20</span></span>]: print(<span class="hljs-string"><span class="hljs-string">"Feature '{}', importance={}"</span></span>.format(*imp))</code> </pre> <br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">Feature</span></span> <span class="hljs-string"><span class="hljs-string">'tags_count'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">546</span></span> Feature <span class="hljs-string"><span class="hljs-string">'month'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">510</span></span> Feature <span class="hljs-string"><span class="hljs-string">'weekday'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">431</span></span> Feature <span class="hljs-string"><span class="hljs-string">'images_count'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">377</span></span> Feature <span class="hljs-string"><span class="hljs-string">'lda_20_6'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">332</span></span> Feature <span class="hljs-string"><span class="hljs-string">'time'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">316</span></span> Feature <span class="hljs-string"><span class="hljs-string">'wide_labels_count'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">291</span></span> Feature <span class="hljs-string"><span class="hljs-string">'lda_50_48'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">227</span></span> Feature <span class="hljs-string"><span class="hljs-string">'text_sizes_std'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">215</span></span> Feature <span class="hljs-string"><span class="hljs-string">'mean_matrix_title_134'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">174</span></span> Feature <span class="hljs-string"><span class="hljs-string">'title_words_count'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">161</span></span> Feature <span class="hljs-string"><span class="hljs-string">'text_sizes_mean'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">160</span></span> Feature <span class="hljs-string"><span class="hljs-string">'lda_20_15'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">149</span></span> Feature <span class="hljs-string"><span class="hljs-string">'mean_matrix_body_262'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">142</span></span> Feature <span class="hljs-string"><span class="hljs-string">'mean_matrix_body_194'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">141</span></span> Feature <span class="hljs-string"><span class="hljs-string">'mean_matrix_title_115'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">138</span></span> Feature <span class="hljs-string"><span class="hljs-string">'mean_matrix_body_61'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">136</span></span> Feature <span class="hljs-string"><span class="hljs-string">'mean_matrix_title_13'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">136</span></span> Feature <span class="hljs-string"><span class="hljs-string">'tf_idf_1427'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">132</span></span> Feature <span class="hljs-string"><span class="hljs-string">'lda_10_1'</span></span>, importance=<span class="hljs-number"><span class="hljs-number">130</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Feature importance histogram</b> <div class="spoiler_text"><pre> <code class="python hljs">importance = pd.DataFrame([{<span class="hljs-string"><span class="hljs-string">"imp"</span></span>: imp, <span class="hljs-string"><span class="hljs-string">"feat"</span></span>: feat} <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> feat, imp <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> importance]) sns.barplot(x=<span class="hljs-string"><span class="hljs-string">"imp"</span></span>, y=<span class="hljs-string"><span class="hljs-string">"feat"</span></span>, data=importance.iloc[:<span class="hljs-number"><span class="hljs-number">20</span></span>])</code> </pre> <br><p><img src="https://habrastorage.org/files/c69/c4f/caa/c69c4fcaa807429cb2395858be948118.png" alt="features"></p></div></div><br><p>  And here are our results: </p><br><pre> <code class="python hljs">predictions = bst.predict(X_test) print(<span class="hljs-string"><span class="hljs-string">"r2 score = {}"</span></span>.format(r2_score(y_test, predictions))) print(<span class="hljs-string"><span class="hljs-string">"mae error = {}"</span></span>.format(mean_absolute_error(y_test, predictions)))</code> </pre> <br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">r2</span></span> score = <span class="hljs-number"><span class="hljs-number">0</span></span>.<span class="hljs-number"><span class="hljs-number">3166155559972065</span></span> mae <span class="hljs-literal"><span class="hljs-literal">error</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>.<span class="hljs-number"><span class="hljs-number">4209721870443455</span></span></code> </pre> <br><h2 id="itogi">  Results </h2><br><p>  With such parameters and features, the r2 score accuracy is 0.317, and the absolute mean error is 0.42.  Is this a good result?  Not bad, even considering that there are still ways to improve the model.  For example, try neural networks (both recurrent and convolutional, with Embedding layer and word2vec weights), add LDA to PCA, NMF and other decomposition, finally, try to distinguish other features from the category of ‚Äúratio of punctuation to number of woo line breaks. "  But further research is already leaving you. </p><br><p>  UPD. <br>  Here the user <a href="https://habrahabr.ru/users/ben_yazi/" class="user_link">ben_yazi</a> indicated in the comments a <a href="https://github.com/cmtt-ru/api">link</a> to the description of the api for TJ. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/327072/">https://habr.com/ru/post/327072/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../327062/index.html">Typography: font selection process</a></li>
<li><a href="../327064/index.html">Native American software designer tricks. Release 1</a></li>
<li><a href="../327066/index.html">Results of the second round of the Russian Code Cup 2017</a></li>
<li><a href="../327068/index.html">PHP 7 Virtual Machine</a></li>
<li><a href="../327070/index.html">‚ÄúWhy don't you just rewrite it in X?‚Äù</a></li>
<li><a href="../327074/index.html">Experience migrating applications from Unity3D to iOS sdk and SceneKit</a></li>
<li><a href="../327076/index.html">We increase the potential of the network storage abandoned by the manufacturer</a></li>
<li><a href="../327078/index.html">Investigation of the position of the eyes in more than 1,000,000 persons: the rule of the golden section or the rule of thirds?</a></li>
<li><a href="../327080/index.html">As I wrote a sentence to the standard C ++</a></li>
<li><a href="../327084/index.html">Classic and new internet marketing aids worth seeing</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>