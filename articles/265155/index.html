<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Stabilization of time-lapse video on a calculator (IPython + OpenCV)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Like many natural and seasonal lovers of astrophotography, this August I caught Perseids at night. There is a small catch, but now it‚Äôs not about him,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Stabilization of time-lapse video on a calculator (IPython + OpenCV)</h1><div class="post__text post__text-html js-mediator-article">  Like many natural and seasonal lovers of astrophotography, this August I caught <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D1%2581%25D0%25B5%25D0%25B8%25D0%25B4%25D1%258B">Perseids</a> at night.  There is a small catch, but now it‚Äôs not about him, but about the fact that a series of photos has become a side effect of such fishing, which suggested that you make a time-lapse of them.  But here's the ill luck: the installation of the camera was not as tough as we would like, and a slight shift appeared between the frames.  I tried to fix it with VirtualDub plug-in de-shaking, but the results were not pleased.  Then it was decided to make your bike: more details about the results and how they are obtained - under the cut. <br><a name="habracut"></a><br>  The traditional ‚Äúbefore‚Äù and ‚Äúafter‚Äù (a small fragment is shown here).  The picture is reduced, but even here you can see the ‚Äúcamera shake‚Äù: <br><br><img src="https://habrastorage.org/files/c76/97d/6b6/c7697d6b6e9a4ff0bab94f0955341f9a.gif"><br><br>  After treatment: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/f7a/bb7/a32/f7abb7a326ff436782f154f42dddb2cc.gif"><br><br>  What everything will be done: IPython notebook + NumPy + OpenCV. <br><br>  Necessary warning: in the post there will be neither a new word in signal processing, nor a new one about the designated language and libraries;  except that novices will find here an example of ‚Äúhow not to program‚Äù, but ‚Äúhow can you quickly come up with and test your algorithm in IPython notebook‚Äù.  Professionals, however, propose to admire the stars. <br><br><div class="spoiler">  <b class="spoiler_title">Why is a "calculator", and also about why I had to make my bike - lyrical digression</b> <div class="spoiler_text">  Having decided to refuse, whenever possible, from paid programs for which there are free analogues, I began to look for, among other things, a replacement for the matlabu.  And stopped at a bunch of IPython + SciPy [+ OpenCV].  However, I use them precisely as a large and very convenient, but calculator: for rapid prototyping of any ideas and solutions or for one-time processing, when it is easier for the computer to explain what I need from it than to look for a suitable program that may still be paid or do a little bit not what I need - this is about this case I want to tell in the post. <br></div></div><br><h5>  Preliminary data preparation </h5><br>  To improve the visibility of fixed objects, it was decided to create a special version of all the images and then to find the offset already on it.  What was done in each frame: <br><ul><li>  increased background brightness and contrast between it and dark fixed objects </li><li>  the trimming of the frame excludes trees; they are not a pattern of immobility </li><li>  file names remain unchanged - just for convenience </li></ul><br>  This is how the pre-processed frames looked like (to the right) compared to the original (left): <br><br><img src="https://habrastorage.org/files/bd3/6a8/3a8/bd36a83a8d934773b22c18b66fc970a0.png"><br><br><h5>  Reading and advanced frame processing </h5><br>  We will read the source files (which were previously prepared for estimating the displacement from them).  For simplicity, the working directory selected the same one where these frames lie, so just read their names in the sampledata array.  I will not give the details of such a serving code in a post so as not to clutter up.  They, as well as some functions written in the process of working macros, can be viewed <a href="https://habr.com/ru/post/265155/">in the original document IPython notebook</a> . <br><br>  Take a couple of frames in order to test everything on them (albeit 0 and 4).  Let us show these frames and the difference between them by simply subtracting cv2.absdiff (): <br><br><img src="https://habrastorage.org/files/28e/b58/b3a/28eb58b3a4144fb8aca6e435b5a2ca1d.png"><br><br>  The frame shift is visible by the way the edges of fixed objects appear, but the moving stars are not helpful in assessing the camera shift.  So if possible get rid of them with the operation erode.  Kernel size is chosen by practical consideration. <br><div class="spoiler">  <b class="spoiler_title">like this</b> <div class="spoiler_text"><pre><code class="python hljs">qx,qy=<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span> k=ones((qx,qy)) im1g=cv2.erode(im1g,k) im2g=cv2.erode(im2g,k) show1(im1g,<span class="hljs-string"><span class="hljs-string">u" "</span></span>)</code> </pre> <br></div></div><br><img src="https://habrastorage.org/files/4b9/285/ff4/4b9285ff47144f8ba8c471340bc411d9.png"><br><br>  It can be seen that on the frame well fixed objects, to which it will be possible to attach. <br><br><h5>  Estimation of the shift between images </h5><br>  Finding matching points on neighboring images will be done as shown in the example <a href="https://github.com/Itseez/opencv/blob/master/samples/python2/find_obj.py">find_obj.py</a> from opencv: we will find distinguishable fragments using the <a href="http://docs.opencv.org/modules/nonfree/doc/feature_detection.html%3Fhighlight%3Dsift">Scale Invariant Feature Transform (SIFT)</a> , and then we will match and filter the resulting array. <br><div class="spoiler">  <b class="spoiler_title">where does that come from</b> <div class="spoiler_text">  the `filter_matches` function is directly used from the <a href="https://github.com/Itseez/opencv/blob/master/samples/python2/find_obj.py">example</a> ,` detectandselectmatches` also borrows its functionality in many ways.  All rights to them are for the respective authors.  I will not dwell on their work in detail now, those who wish can always look at the help - everything is written there, and it‚Äôs pretty intuitive to drive <a href="https://github.com/Itseez/opencv/blob/master/samples/python2/find_obj.py">an example</a> . <br><pre> <code class="python hljs">detector = cv2.SIFT() norm = cv2.NORM_L2 matcher = cv2.BFMatcher(norm) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">filter_matches</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(kp1, kp2, matches, ratio = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.75</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> mkp1, mkp2 = [], [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> m <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> matches: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(m) == <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> m[<span class="hljs-number"><span class="hljs-number">0</span></span>].distance &lt; m[<span class="hljs-number"><span class="hljs-number">1</span></span>].distance * ratio: m = m[<span class="hljs-number"><span class="hljs-number">0</span></span>] mkp1.append( kp1[m.queryIdx] ) mkp2.append( kp2[m.trainIdx] ) p1 = np.float32([kp.pt <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> kp <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> mkp1]) p2 = np.float32([kp.pt <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> kp <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> mkp2]) kp_pairs = zip(mkp1, mkp2) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> p1, p2, kp_pairs <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">detectandselectmatches</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(fr1a,fr2a)</span></span></span><span class="hljs-function">:</span></span> kp1, desc1 = detector.detectAndCompute(fr1a, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) kp2, desc2 = detector.detectAndCompute(fr2a, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) raw_matches = matcher.knnMatch(desc1, trainDescriptors = desc2, k = <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-comment"><span class="hljs-comment">#2 p1, p2, kp_pairs = filter_matches(kp1, kp2, raw_matches) return p1, p2</span></span></code> </pre></div></div><pre> <code class="python hljs">p1, p2 = detectandselectmatches(im1g,im2g)</code> </pre><br>  The resulting arrays p1, p2 are sets of x, y coordinates of coinciding points on frames 1 and 2, respectively. <br><div class="spoiler">  <b class="spoiler_title">for example</b> <div class="spoiler_text"><pre> [665.927307129,17.939201355] [668.513000488.19.468919754]
 [744.969177246,60.6581344604] [747.49786377,61.8129844666]
 [746.388549805,77.1945953369] [749.15411377,78.5462799072]
 [892.944763184,169.295532227] [895.570373535,170.530929565]
 [906.57824707,185.634231567] [908.093933105,186.593307495]
 ...
</pre></div></div><br>  If we take the difference between them, we get an array of frame offsets according to the version of each of the points.  At this stage, you can make another filtering (sometimes some points due to an erroneous match are very strongly knocked out of the series), but if you use the median instead of the mean, then all these erroneous values ‚Äã‚Äãare simply not important.  This is clearly seen in the picture: the blue represents the offset for each of the pairs of points, the selected value is indicated by red, and for comparison green is a simple average. <br><pre> <code class="python hljs">dp=p2-p1 mx,my=np.median(dp[:,<span class="hljs-number"><span class="hljs-number">0</span></span>]),np.median(dp[:,<span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre><br><img src="https://habrastorage.org/files/575/723/3a0/5757233a0c1f4e48b73c8359d4028d7e.png"><br><br><pre>  dx = 2.68393 dy = 1.34059 </pre><br><br><h5>  Reverse shear </h5><br>  It remains to make the actual offset.  We will build the <a href="http://docs.opencv.org/modules/imgproc/doc/geometric_transformations.html%3Fhighlight%3Dwarpaffine">affine transformation</a> matrix manually, simply making the appropriate values ‚Äã‚Äãto the right place.  One could use special functions from opencv for this, but the matrix looks quite simple for rotation and displacement: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0AM%20%3D%20%5Cbegin%7Bbmatrix%7D%0A%20cos(%20%5Cphi%20)%20%26%20%20-sin%20(%5Cphi)%20%26%20%20dx%20%20%5C%5C%20%0A%20sin(%20%5Cphi%20)%20%26%20%20cos%20(%5Cphi)%20%26%20%20dy%0A%5Cend%7Bbmatrix%7D%0A" alt="M = \ begin {bmatrix} cos (\ phi) &amp; amp; -sin (\ phi) &amp; amp; dx \\ sin (\ phi) &amp; amp; cos (\ phi) &amp; amp; dy \ end {bmatrix}"></div><br>  Where <img src="http://tex.s2cms.ru/svg/%5Cphi" alt="\ phi">  - the angle of rotation, and <img src="http://tex.s2cms.ru/svg/dx" alt="dx">  and <img src="http://tex.s2cms.ru/svg/dy" alt="dy">  - corresponding offset values <br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getshiftmatrix</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( </span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(dx,dy)</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> array([[ <span class="hljs-number"><span class="hljs-number">1.</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, dx], [ <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1.</span></span>, dy]])</code> </pre><br>  and actually run the conversion <br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">shiftimg</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(im2,shift)</span></span></span><span class="hljs-function">:</span></span> tr1=getshiftmatrix(shift) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> cv2.warpAffine(im2,tr1,tuple(reversed(im2.shape[:<span class="hljs-number"><span class="hljs-number">2</span></span>]) )) im2r= shiftimg(im2,tuple(-array(shift)))</code> </pre><br>  Let's look at the result, comparing, as in the beginning, by simple subtraction: <br><br><img src="https://habrastorage.org/files/e6a/304/2a6/e6a3042a6f7a49bc87f7c053c5c1cf3d.png"><br><br>  Voila!  What was required was blackness in the place of all motionless objects.  So the frames were combined. <br><br><h5>  Processing all frames </h5><br>  Everything is working.  Can you arrange processing ... or not?  There are a few details left: <br><ul><li>  the offset was calculated on a specially processed and cropped frame, and it should be produced on the main one; </li><li>  after the offset along the edges of the frame, the black stripes will remain where the picture has been shifted from.  They can cover up from the next frame. </li></ul><br>  We take into account all this and write. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   basepath4orig        arshifts=[] im1= cv2.imread(sampledata[0]) # base frame im1g = preprocess(im1) kp1, desc1 = detector.detectAndCompute(im1g, None) imgprev=cv2.imread(basepath4orig+sampledata[0]) #base original frame for i,x in enumerate(sampledata): print x, im2g = preprocess(cv2.imread(x)) kp2, desc2 = detector.detectAndCompute(im2g, None) raw_matches = matcher.knnMatch(desc1, trainDescriptors = desc2, k = 2) #2 p1, p2, kp_pairs = filter_matches(kp1, kp2, raw_matches) dp=p2-p1 if len(dp)&lt;=0: shift=0,0 else: dx,dy=np.median(dp[:,0]),np.median(dp[:,1]) print dx,dy #process original frame imgr= shiftimg(cv2.imread(basepath4orig+x),(-dx,-dy)) if -dy&gt;0: imgr[:int(ceil(abs(dy))),:,:] = imgprev[:int(ceil(abs(dy))),:,:] if -dy&lt;0: imgr[-int(ceil(abs(dy))):,:,:] = imgprev[-int(ceil(abs(dy))):,:,:] if -dx&gt;0: imgr[:,:int(ceil(abs(dx))),:] = imgprev[:,:int(ceil(abs(dx))),:] if -dx&lt;0: imgr[:,-int(ceil(abs(dx))):,:] = imgprev[:,-int(ceil(abs(dx))):,:] imgprev=imgr cv2.imwrite('shifted_'+x+'.JPG',imgr)</span></span></code> </pre><br>  That's all - the result is obtained: fixed objects - like glued, the stars spin as they should, the clouds float about their business, and I think about what else to remove this - already specifically for the time-lapse. <br><br>  Result on youtube.com: <br><br><iframe width="420" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/j4JJIajjrAc%3Ffeature%3Doembed&amp;xid=17259,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhgrGB5DMaYEY5hir6fQlD4rXr_WUg" frameborder="0" allowfullscreen=""></iframe><br><br><div class="spoiler">  <font><b class="spoiler_title">***</b></font> <div class="spoiler_text">  <font>The most attentive ones could notice that the previous version got into youtube, without correcting the black bars along the edges, I will not replace them anymore, in the gifs in the post a normal version has already been given</font> </div></div><br><br><a name="ipnb"></a>  Link to a <a href="http://nbviewer.ipython.org/urls/dl.dropbox.com/s/y7uop7mlp2x466p/deshake_calc_2%25D1%2581_4pub-clear1.ipynb">copy of IPython notebook, in which everything was done</a> <br><br><h5>  Materials and tools used: </h5><br><ul><li>  <a href="http://opencv.org/">Opencv</a> </li><li>  <a href="http://ipython.org/notebook.html">IPython Notebook</a> </li><li>  <a href="http://www.numpy.org/">Numpy</a> </li><li>  opencv example <a href="https://github.com/Itseez/opencv/blob/master/samples/python2/find_obj.py">find_obj.py</a> </li><li>  <a href="http://docs.opencv.org/modules/nonfree/doc/feature_detection.html%3Fhighlight%3Dsift">Scale Invariant Feature Transform (SIFT)</a> </li></ul><br>  Also my thanks <ul><li>  <a href="http://locv.ru/wiki/%25D0%2593%25D0%25BB%25D0%25B0%25D0%25B2%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B8%25D1%2586%25D0%25B0">locv.ru</a> </li><li>  <a href="http://habrahabr.ru/post/121031/">post about NumPy</a> from <a href="http://habrahabr.ru/users/leomat/" class="user_link">LeoMat</a> </li><li>  <a href="http://tex.s2cms.ru/page/">Markdown &amp; Latex Editor by Roman Parpalak</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/265155/">https://habr.com/ru/post/265155/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../265139/index.html">How do we conduct iron test drives with a cast iron bridge</a></li>
<li><a href="../265141/index.html">Fall semester 2015 courses at the Computer Science Club</a></li>
<li><a href="../265143/index.html">Testing flash storage. Huawei Dorado 2100 G2</a></li>
<li><a href="../265149/index.html">Underground carders market. Translation of the book "KingPIN". Chapter 8. ‚ÄúWelcome to America‚Äù</a></li>
<li><a href="../265153/index.html">Preview of the first updates of the Microsoft Edge web platform</a></li>
<li><a href="../265157/index.html">About false posts in the VC or how many operations per second the human brain performs</a></li>
<li><a href="../265159/index.html">Our Service is both dangerous and difficult or some aspects of the survival of services in Android</a></li>
<li><a href="../265161/index.html">Another serious Android vulnerability found</a></li>
<li><a href="../265163/index.html">Google does not forgive mistakes: confrontation with Google Play</a></li>
<li><a href="../265165/index.html">Writing a webpack plugin</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>