<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>LLVM for Tensorflow, or the compiler of the era of the end of Moore's law</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The TensorFlow ecosystem contains a number of compilers and optimizers operating at different levels of software and hardware stack. For those who use...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>LLVM for Tensorflow, or the compiler of the era of the end of Moore's law</h1><div class="post__text post__text-html js-mediator-article">  The TensorFlow ecosystem contains a number of compilers and optimizers operating at different levels of software and hardware stack.  For those who use Tensorflow daily, this multilevel stack can generate difficult to understand errors, both compile times and runtimes, associated with the use of various kinds of hardware (GPU, TPU, mobile platforms, etc.) <br><br>  These components, starting with the Tensorflow graph, can be represented as such a diagram: <br><br><img src="https://habrastorage.org/webt/ly/cf/0y/lycf0y0ld6eld56mnuba9iyhjii.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i>In fact, everything is more complicated</i> <br><a name="habracut"></a><br>  In this diagram, we can see that Tensorflow graphs can be run in several different ways. <br><br><div class="spoiler">  <b class="spoiler_title">note</b> <div class="spoiler_text">  In TensorFlow 2.0, columns can be implicit, greedy execution can run operations individually, in groups, or on a full graph.  These graphs or graph fragments should be optimized and executed. <br></div></div><br>  For example: <br><br><ul><li>  We send graphs to the performer Tensorflow, which calls hand-written specialized kernels </li><li>  We convert them to <a href="https://www.tensorflow.org/xla/overview">XLA</a> HLO (XLA High-Level Optimizer representation) ‚Äîa high-level representation of the XLA optimizer, which in turn can invoke the <a href="https://llvm.org/">LLVM</a> compiler for a CPU or GPU, or continue to use XLA for <a href="https://cloud.google.com/tpu/">TPU</a> , or combine them. </li><li>  Convert them to <a href="https://developer.nvidia.com/tensorrt">TensorRT</a> , <a href="http://ngraph.nervanasys.com/index.html/">nGraph</a> , or another format for a specialized set of instructions implemented in hardware. </li><li>  Convert them to <a href="https://www.tensorflow.org/lite">TensorFlow Lite</a> format, running in TensorFlow Lite runtime, or converted to code for running on a GPU or DSP via <a href="https://developer.android.com/ndk/guides/neuralnetworks">Android Neural Networks API</a> (NNAPI) or a similar way. </li></ul><br>  There are also more sophisticated methods, including many optimization passes on each layer, such as in the Grappler framework, which optimizes operations in TensorFlow. <br><br>  Although these various implementations of compilers and intermediate representations improve performance, their diversity presents a problem for end users, such as confusing error messages when pairing these subsystems.  Also, the creators of new software and hardware stacks should adjust the optimization and conversion passes for each new case. <br><br>  And because of all this, we are pleased to announce the MLIR, Multi-Level Intermediate Representation.  It is an intermediate representation format and compilation libraries intended for use between a model representation and a low-level compiler that generates hardware-dependent code.  Introducing MLIR, we want to give way to new research in the development of optimizing compilers and implementations of compilers built on components of industrial quality. <br><br>  We expect MLIR to be of interest to many groups, including: <br><br><ul><li>  compiler researchers, as well as practitioners who want to optimize the performance and memory consumption of machine learning models; </li><li>  hardware manufacturers looking for a way to combine their hardware with Tensorflow, such as TPU, mobile neuroprocessors in smartphones, and other custom ASICs; </li><li>  people who want to give programming languages ‚Äã‚Äãthe benefits provided by optimizing compilers and hardware accelerators; </li></ul><br><h3>  What is MLIR? </h3><br>  MLIR is, in fact, a flexible infrastructure for modern optimizing compilers.  This means that it consists of a specification of an intermediate representation (IR) and a set of tools for converting this representation.  When we talk about compilers, the transition from a higher-level representation to a lower-level representation is called lowering, and we will use this term in the future. <br><br>  MLIR is built under the influence of LLVM and shamelessly borrows many good ideas from it.  It has a flexible type system, and is designed to represent, analyze and convert graphs, combining multiple levels of abstraction in one compilation level.  These abstractions include Tensorflow operations, nested regions of polyhedral cycles, LLVM instructions, and fixed-point operations and types. <br><br><h3>  MLIR dialects </h3><br>  In order to separate various software and hardware targets, MLIR has ‚Äúdialects‚Äù including: <br><br><ul><li>  TensorFlow IR, which includes everything that is possible to do in the TensorFlow columns </li><li>  XLA HLO IR, designed to get all the advantages provided by the XLA compiler, at the output of which we can get the code for TPU, and not only. </li><li>  Experimental affine dialect designed specifically for <a href="https://en.wikipedia.org/wiki/Polytope_model">polyhedral</a> representations and optimizations </li><li>  LLVM IR, 1: 1 matching LLVM's own representation, allowing MLIR to generate code for the GPU and CPU using LLVM. </li><li>  TensorFlow Lite, designed to generate code for mobile platforms </li></ul><br>  Each dialect contains a set of specific operations using invariants, such as: ‚Äúthis is a binary operator, and its input and output are of the same type‚Äù. <br><br><h3>  MLIR Extensions </h3><br>  MLIR does not have a fixed and built-in list of global intrinsic operations.  Dialects can define completely custom types, and thus MLIR can model things like the LLVM IR type system (having first class aggregates), domain language abstractions such as quantized types important for ML-optimized accelerators, and, in the future, even a Swift or Clang type system. <br><br>  If you want to attach a new low-level compiler to this system, you can create a new dialect and descend from the dialect of the TensorFlow graph to your dialect.  This simplifies the way for hardware developers and compiler developers.  You can target the dialect at different levels of the same model; high-level optimizers will be responsible for specific parts of the IR. <br><br>  For compiler researchers and framework developers, MLIR allows you to create transformations at every level, you can define your own operations and abstractions in IR, allowing you to better model your application tasks.  Thus, MLIR is more than pure compiler infrastructure, which is LLVM. <br><br>  Although MLIR works as a compiler for ML, it also allows you to use machine learning technology!  This is very important for engineers who develop digital libraries and cannot provide support for the whole variety of ML-models and hardware.  The flexibility of MLIR makes it easier to explore descent strategies for moving between levels of abstraction. <br><br><h3>  What's next </h3><br>  We have opened a <a href="https://github.com/tensorflow/mlir">GitHub repository</a> and invite everyone interested (study our tutorial!).  We will be releasing more than this toolkit - specifications for TensorFlow and TF Lite dialects in the coming months.  We can tell you more to learn more, see <a href="https://drive.google.com/file/d/1hUeAJXcAXwz82RXA5VtO5ZoH8cVQhrOK/view">Chris Luttner‚Äôs presentation</a> and our <a href="https://github.com/tensorflow/mlir">README on Github</a> . <br><br>  If you want to be aware of all things related to MLIR, join our <a href="https://groups.google.com/a/tensorflow.org/forum/">new mailing list</a> , which will soon be focused on announcements of future releases of our project.  Stay with us! </div><p>Source: <a href="https://habr.com/ru/post/457826/">https://habr.com/ru/post/457826/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../457812/index.html">‚ÄúStop! Who goes?". Video surveillance on the porch</a></li>
<li><a href="../457814/index.html">Next steps to go 2</a></li>
<li><a href="../457816/index.html">Juju - quick tour</a></li>
<li><a href="../457820/index.html">How to grow an Evangelist for your company</a></li>
<li><a href="../457824/index.html">Contagious stress: interspecies synchronization of cortisol levels on the example of dogs and their owners</a></li>
<li><a href="../45783/index.html">GECK - how to sell your MOD for Fallout 3</a></li>
<li><a href="../457836/index.html">What I learned by creating Dribbble</a></li>
<li><a href="../457838/index.html">EDR technology as an element of the SOC nuclear triad</a></li>
<li><a href="../457842/index.html">Soyuz-TM spacecraft motion control system</a></li>
<li><a href="../457844/index.html">Seven basic habits for remotely working development teams</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>