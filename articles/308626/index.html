<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The Levenberg-Marquardt algorithm for the nonlinear least squares method and its implementation in Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Finding an extremum (minimum or maximum) of an objective function is an important task in mathematics and its applications (in particular, in machine ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The Levenberg-Marquardt algorithm for the nonlinear least squares method and its implementation in Python</h1><div class="post__text post__text-html js-mediator-article"><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4ff/3da/ca4/4ff3daca463adbb35a3b725bccf05273.png"></div><br><p></p><br><p>  Finding an extremum (minimum or maximum) of an <a href="https://ru.wikipedia.org/wiki/%25D0%25A6%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25B2%25D0%25B0%25D1%258F_%25D1%2584%25D1%2583%25D0%25BD%25D0%25BA%25D1%2586%25D0%25B8%25D1%258F">objective function</a> is an important task in mathematics and its applications (in particular, in machine learning there is a problem of <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25B8%25D0%25B1%25D0%25BB%25D0%25B8%25D0%25B6%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D1%2581_%25D0%25BF%25D0%25BE%25D0%25BC%25D0%25BE%25D1%2589%25D1%258C%25D1%258E_%25D0%25BA%25D1%2580%25D0%25B8%25D0%25B2%25D1%258B%25D1%2585">curve-fitting</a> ).  Surely everyone has heard about the <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D1%2580%25D0%25B0%25D0%25B4%25D0%25B8%25D0%25B5%25D0%25BD%25D1%2582%25D0%25BD%25D1%258B%25D0%25B5_%25D0%25BC%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4%25D1%258B">method of the fastest descent</a> (MNF) and the <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%259D%25D1%258C%25D1%258E%25D1%2582%25D0%25BE%25D0%25BD%25D0%25B0">method of Newton</a> (MN).  Unfortunately, these methods have a number of significant drawbacks, in particular, the method of steepest descent can converge at the end of optimization for a very long time, and Newton's method requires the calculation of second derivatives, which requires a lot of calculations. </p><br><br><p>  To eliminate the shortcomings, as is often the case, you need to dive deeper into the subject area and add restrictions on the input data.  In particular: the MHC and MN deal with arbitrary functions.  In statistics and machine learning it is often necessary to deal with <a href="https://basegroup.ru/community/glossary/lsm">the least squares method</a> (OLS).  This method minimizes the sum of the square of errors, i.e.  the objective function is represented as </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cfrac%7B1%7D%7B2%7D%5Csum%20%5Climits_%7Bi%3D1%7D%5E%7BN%7D(y_i'-y_i)%5E2%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5Csum%20%5Climits_%7Bi%3D1%7D%5E%7BN%7Dr_i%5E2%20%5Ctag%7B1%7D%0A" alt="\ frac {1} {2} \ sum \ limits_ {i = 1} ^ {N} (y_i'-y_i) ^ 2 = \ frac {1} {2} \ sum \ limits_ {i = 1} ^ {N } r_i ^ 2 \ tag {1}"></div><p></p><br><p>  <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC_%25D0%259B%25D0%25B5%25D0%25B2%25D0%25B5%25D0%25BD%25D0%25B1%25D0%25B5%25D1%2580%25D0%25B3%25D0%25B0_%25E2%2580%2594_%25D0%259C%25D0%25B0%25D1%2580%25D0%25BA%25D0%25B2%25D0%25B0%25D1%2580%25D0%25B4%25D1%2582%25D0%25B0">The Levenberg-Marquardt algorithm</a> is a nonlinear least squares method.  The article contains: </p><br><ul><li>  algorithm explanation </li><li>  explanation of the methods: fastest descent, Nuton, Gauss-Newton </li><li>  shows implementation in Python with <a href="https://github.com/lightforever/Levenberg_Manquardt">source code on github</a> </li><li>  method comparison </li></ul><br><a name="habracut"></a><br><p>  The code uses additional libraries, such as <strong>numpy, matplotlib</strong> .  If you don‚Äôt have them, I highly recommend installing them from <a href="https://www.continuum.io/downloads">Anaconda for Python</a> </p>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>  <strong>Dependencies</strong> <br>  The Levenberg-Marquardt algorithm relies on the methods outlined in the flowchart. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0a7/a47/00b/0a7a4700be8cc96e48bc6fcc2065d380.png"></div><br><p></p><br>  Therefore, first, it is necessary to study them.  This will do <br><p>  <strong>Definitions</strong> </p><br><ul><li><img src="https://tex.s2cms.ru/svg/F%3DF(x)" alt="F = F (x)">  - our target function.  We will minimize <img src="https://tex.s2cms.ru/svg/F" alt="F">  .  In this case, <img src="https://tex.s2cms.ru/svg/F" alt="F">  is <a href="https://basegroup.ru/community/glossary/error-function">a loss function</a> </li><li><img src="https://tex.s2cms.ru/svg/%5Cnabla%20f(x)" alt="\ nabla f (x)">  - function gradient <img src="https://tex.s2cms.ru/svg/f" alt="f">  at the point <img src="https://tex.s2cms.ru/svg/x" alt="x"></li><li><img src="https://tex.s2cms.ru/svg/x%5E*" alt="x ^ *">  - <img src="https://tex.s2cms.ru/svg/x" alt="x">  in which <img src="https://tex.s2cms.ru/svg/F(x%5E*)" alt="F (x ^ *)">  is a local minimum, i.e.  if there is a punctured neighborhood <img src="https://tex.s2cms.ru/svg/%5Cdot%7BU%7D(x)" alt="\ dot {U} (x)">  such that <img src="https://tex.s2cms.ru/svg/%5Cforall%20x%20%5Cin%20%5Cdot%7BU%7D(x)%5C%20%5C%20%5C%20F(x%5E*)%5Cle%20F(x)" alt="\ forall x \ in \ dot {U} (x) \ \ \ F (x ^ *) \ le F (x)"></li><li><img src="https://tex.s2cms.ru/svg/x%5E%2B" alt="x ^ +">  - global minimum if <img src="https://tex.s2cms.ru/svg/%5Cforall%20x%20%5Cin%20R%5En%5C%20%5C%20%5C%20F(x%5E%2B)%5Cle%20F(x)" alt="\ forall x \ in R ^ n \ \ \ F (x ^ +) \ le F (x)">  i.e. <img src="https://tex.s2cms.ru/svg/F" alt="F">  It does not matter less than <img src="https://tex.s2cms.ru/svg/F(x%5E%2B)" alt="F (x ^ +)"></li><li><img src="https://tex.s2cms.ru/svg/J_f%3DJ_f(x)%20%3D%20J" alt="J_f = J_f (x) = J">  - <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B0%25D1%2582%25D1%2580%25D0%25B8%25D1%2586%25D0%25B0_%25D0%25AF%25D0%25BA%25D0%25BE%25D0%25B1%25D0%25B8">Jacobi matrix</a> for the function <img src="https://tex.s2cms.ru/svg/f%3A%20R%5En%20%5Cto%20R%5Em" alt="f: R ^ n \ to R ^ m">  at the point <img src="https://tex.s2cms.ru/svg/x" alt="x">  .  Those.  this is a table of all partial derivatives of the first order.  In fact, this is an analogue of the gradient for <img src="https://tex.s2cms.ru/svg/f%3AR%5En%20%5Cto%20R" alt="f: R ^ n \ to R">  , since in this case we are dealing with the mapping from <img src="https://tex.s2cms.ru/svg/n" alt="n">  -dimensional vector in <img src="https://tex.s2cms.ru/svg/m" alt="m">  -dimensional, so you can not just count the first derivatives of one dimension, as it happens in the gradient.  <a href="http://math.stackexchange.com/questions/1519367/difference-between-gradient-and-jacobian">Read more</a> </li><li><img src="https://tex.s2cms.ru/svg/H_f%20%3D%20H_f(x)%20%3D%20H" alt="H_f = H_f (x) = H">  - <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B5%25D1%2581%25D1%2581%25D0%25B8%25D0%25B0%25D0%25BD_%25D1%2584%25D1%2583%25D0%25BD%25D0%25BA%25D1%2586%25D0%25B8%25D0%25B8">Hesse matrix</a> (matrix of second derivatives).  Required for quadratic approximation </li></ul><br><p>  <strong>Function selection</strong> </p><br><br><p>  In mathematical optimization there are functions on which new methods are tested. <br>  One such function is the <a href="https://en.wikipedia.org/wiki/Rosenbrock_function">Rosenbrock Function</a> .  In the case of a function of two variables, it is defined as </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Af(x%2Cy)%20%3D%20(a-x)%5E2%20%2B%20b(y-x%5E2)%5E2%0A" alt="f (x, y) = (ax) ^ 2 + b (yx ^ 2) ^ 2"></div><p></p><br><p>  I accepted <img src="https://tex.s2cms.ru/svg/a%3D0.5%2C%5C%20b%3D0.5" alt="a = 0.5, \ b = 0.5">  .  Those.  function has the form: </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Af(x%2Cy)%20%3D%20%5Cfrac%7B1%7D%7B2%7D(1-x)%5E2%20%2B%20%5Cfrac%7B1%7D%7B2%7D(y-x%5E2)%5E2%0A" alt="f (x, y) = \ frac {1} {2} (1-x) ^ 2 + \ frac {1} {2} (yx ^ 2) ^ 2"></div><p></p><br><p>  We will consider the behavior of the function on the interval <img src="https://tex.s2cms.ru/svg/-2%20%5Cle%20x%2Cy%20%5Cle%202" alt="-2 \ le x, y \ le 2"></p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/4c4/d2a/d1e/4c4d2ad1e75d43548a81b634068e8e46.png"></div><br><p></p><br><p>  This function is defined non-negatively, has a minimum <img src="https://tex.s2cms.ru/svg/z%20%3D%200%24%20%D0%B2%20%D1%82%D0%BE%D1%87%D0%BA%D0%B5%20%24(x%3D1%2Cy%3D1)" alt="z = 0 $ at the point $ (x = 1, y = 1)"><br>  In code, it is easier to encapsulate all the data about the function in one class and take the class of the function that is required.  The result depends on the starting point of the optimization.  Choose it as <img src="https://tex.s2cms.ru/svg/(x%3D-2%2Cy%3D-2)" alt="(x = -2, y = -2)">  .  As can be seen from the graph, at this point, the function takes the largest value on the interval. </p><br><br><p>  <strong><em><a href="http://functions.py/">functions.py</a></em></strong> </p><br><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Rosenbrock</span></span></span><span class="hljs-class">:</span></span> initialPoint = (<span class="hljs-number"><span class="hljs-number">-2</span></span>, <span class="hljs-number"><span class="hljs-number">-2</span></span>) camera = (<span class="hljs-number"><span class="hljs-number">41</span></span>, <span class="hljs-number"><span class="hljs-number">75</span></span>) interval = [(<span class="hljs-number"><span class="hljs-number">-2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), (<span class="hljs-number"><span class="hljs-number">-2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>)] <span class="hljs-string"><span class="hljs-string">"""   """</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.5</span></span>*(<span class="hljs-number"><span class="hljs-number">1</span></span>-x[<span class="hljs-number"><span class="hljs-number">0</span></span>])**<span class="hljs-number"><span class="hljs-number">2</span></span> + <span class="hljs-number"><span class="hljs-number">0.5</span></span>*(x[<span class="hljs-number"><span class="hljs-number">1</span></span>]-x[<span class="hljs-number"><span class="hljs-number">0</span></span>]**<span class="hljs-number"><span class="hljs-number">2</span></span>)**<span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-string"><span class="hljs-string">"""    -  - r """</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">function_array</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array([<span class="hljs-number"><span class="hljs-number">1</span></span> - x[<span class="hljs-number"><span class="hljs-number">0</span></span>] , x[<span class="hljs-number"><span class="hljs-number">1</span></span>] - x[<span class="hljs-number"><span class="hljs-number">0</span></span>] ** <span class="hljs-number"><span class="hljs-number">2</span></span>]).reshape((<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)) @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">gradient</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array([-(<span class="hljs-number"><span class="hljs-number">1</span></span>-x[<span class="hljs-number"><span class="hljs-number">0</span></span>]) - (x[<span class="hljs-number"><span class="hljs-number">1</span></span>]-x[<span class="hljs-number"><span class="hljs-number">0</span></span>]**<span class="hljs-number"><span class="hljs-number">2</span></span>)*<span class="hljs-number"><span class="hljs-number">2</span></span>*x[<span class="hljs-number"><span class="hljs-number">0</span></span>], (x[<span class="hljs-number"><span class="hljs-number">1</span></span>] - x[<span class="hljs-number"><span class="hljs-number">0</span></span>]**<span class="hljs-number"><span class="hljs-number">2</span></span>)]) @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">hesse</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array(((<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">-2</span></span>*x[<span class="hljs-number"><span class="hljs-number">1</span></span>] + <span class="hljs-number"><span class="hljs-number">6</span></span>*x[<span class="hljs-number"><span class="hljs-number">0</span></span>]**<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">-2</span></span>*x[<span class="hljs-number"><span class="hljs-number">0</span></span>]), (<span class="hljs-number"><span class="hljs-number">-2</span></span> * x[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>))) @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">jacobi</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array([ [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">-2</span></span>*x[<span class="hljs-number"><span class="hljs-number">0</span></span>], <span class="hljs-number"><span class="hljs-number">1</span></span>]]) <span class="hljs-string"><span class="hljs-string">"""     : http://www.mathworks.com/help/matlab/matlab_prog/vectorization.html """</span></span> @staticmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getZMeshGrid</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X, Y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.5</span></span>*(<span class="hljs-number"><span class="hljs-number">1</span></span>-X)**<span class="hljs-number"><span class="hljs-number">2</span></span> + <span class="hljs-number"><span class="hljs-number">0.5</span></span>*(Y - X**<span class="hljs-number"><span class="hljs-number">2</span></span>)**<span class="hljs-number"><span class="hljs-number">2</span></span></code> </pre> <br><p>  <strong>The fastest descent method (Steepest Descent)</strong> </p><br><br><p>  The method itself is extremely simple.  Accept <img src="https://tex.s2cms.ru/svg/F(x)%3Df(x)" alt="F (x) = f (x)">  i.e.  objective function coincides with the given one. <br>  Need to find <img src="https://tex.s2cms.ru/svg/d_%7B%D0%BD%D1%81%7D(x)" alt="d_ {ns} (x)">  - direction of the fastest descent of the function <img src="https://tex.s2cms.ru/svg/f" alt="f">  at the point <img src="https://tex.s2cms.ru/svg/x" alt="x">  . <br><img src="https://tex.s2cms.ru/svg/f(x)" alt="f (x)">  can be linearly approximated at <img src="https://tex.s2cms.ru/svg/x" alt="x">  : </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/f(x%2Bd)%20%5Capprox%20f(x)%20%2B%20%5Cnabla%20f(x)%5ETd%2C%20%5C%20d%20%5Cin%20R%5En%20%2C%20%7C%7Cd%7C%7C%20%5Cto%200%20%5Ctag%7B2%7D" alt="f (x + d) \ approx f (x) + \ nabla f (x) ^ Td, \ d \ in R ^ n, || d || \ to 0 \ tag {2}"></div><p></p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%20%5Clim_%7Bd%20%5Cto%200%7Df(x)-f(x%2Bd)%20%3D%20-%20%5Cnabla%20f(x)%5ETd%20%20%5Cstackrel%7B(3.a)%7D%20%3D%20-%20%7C%7C%20%5Cnabla%20f(x)%5ET%7C%7C%20%5C%20%7C%7Cd%7C%7C%20cos%20%5Ctheta%20%2C%5Ctag%7B3%7D" alt="\ lim_ {d \ to 0} f (x) -f (x + d) = - \ nabla f (x) ^ Td \ stackrel {(3.a)} = - || \ nabla f (x) ^ T || \ || d || cos \ theta, \ tag {3}"></div><p></p><br><p>  Where <img src="https://tex.s2cms.ru/svg/%5Ctheta" alt="\ theta">  - angle between vector <img src="https://tex.s2cms.ru/svg/d%20%5C%20%D0%B8%20%5Cnabla%20f(x)%5ET" alt="d \ and \ nabla f (x) ^ T">  . <img src="https://tex.s2cms.ru/svg/(3.a)%20" alt="(3.a)">  follows from the <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25BA%25D0%25B0%25D0%25BB%25D1%258F%25D1%2580%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B8%25D0%25B7%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">dot product</a> </p><br><br><p>  So how do we minimize <img src="https://tex.s2cms.ru/svg/f(x)" alt="f (x)">  then the bigger the difference <img src="https://tex.s2cms.ru/svg/(3)" alt="(3)">  , all the better.  With <img src="https://tex.s2cms.ru/svg/%5Ctheta%20%3D%20%5Cpi" alt="\ theta = \ pi">  the expression will be maximally ( <img src="https://tex.s2cms.ru/svg/cos%20%5Ctheta%20%3D%20-1" alt="cos \ theta = -1">  , the vector norm is always non-negative), and <img src="https://tex.s2cms.ru/svg/%5Ctheta%20%3D%20%5Cpi" alt="\ theta = \ pi">  will only be if vectors <img src="https://tex.s2cms.ru/svg/d%20%5C%20%D0%B8%20%5C%20%20%5Cnabla%20f(x)%5ET%20%20" alt="d \ and \ \ nabla f (x) ^ T">  will be opposite, therefore </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%20d_%7B%D0%BD%D1%81%7D%20%3D%20%20-%5Cnabla%20f(x)%5ET" alt="d_ {ns} = - \ nabla f (x) ^ T"></div><p></p><br><p>  We have the right direction, but taking a step long <img src="https://tex.s2cms.ru/svg/%7C%7Cd_%7B%D0%BD%D1%81%7D%7C%7C" alt="|| d_ {ns} ||">  You can go the wrong way.  Make a step smaller: </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%20d_%7B%D0%BD%D1%81%7D%20%3D%20%20-%5Calpha%20%5Cnabla%20f(x)%5ET%2C%20%200%20%3C%20%5Calpha%20%3C%201" alt="d_ {ns} = - \ alpha \ nabla f (x) ^ T, 0 &amp; lt; \ alpha &amp; lt; one"></div><p></p><br><p>  In theory, the smaller the pitch, the better.  But then the speed of convergence will suffer.  Recommended value <img src="https://tex.s2cms.ru/svg/%5Calpha%20%3D%200.05" alt="\ alpha = 0.05"></p><br><br><p>  In code, it looks like this: first, the base optimizer class.  We transfer everything that is needed in the future (Hesse and Jacobi matrices are not needed now, but will be needed for other methods) </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Optimizer</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, function, initialPoint, gradient=None, jacobi=None, hesse=None, interval=None, epsilon=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1e-7</span></span></span></span><span class="hljs-function"><span class="hljs-params">, function_array=None, metaclass=ABCMeta)</span></span></span><span class="hljs-function">:</span></span> self.function_array = function_array self.epsilon = epsilon self.interval = interval self.function = function self.gradient = gradient self.hesse = hesse self.jacobi = jacobi self.name = self.__class__.__name__.replace(<span class="hljs-string"><span class="hljs-string">'Optimizer'</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>) self.x = initialPoint self.y = self.function(initialPoint) <span class="hljs-string"><span class="hljs-string">"      "</span></span> @abstractmethod <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">next_point</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">pass</span></span> <span class="hljs-string"><span class="hljs-string">"""     """</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">move_next</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, nextX)</span></span></span><span class="hljs-function">:</span></span> nextY = self.function(nextX) self.y = nextY self.x = nextX <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.x, self.y</code> </pre><br><br>  Optimizer code itself: <br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SteepestDescentOptimizer</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(Optimizer)</span></span></span><span class="hljs-class">:</span></span> ... <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">next_point</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> nextX = self.x - self.learningRate * self.gradient(self.x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> self.move_next(nextX)</code> </pre><br><br><p>  Optimization result: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/212/cc2/7e2/212cc27e2b054acaa57d37c30ca99c62.gif"></div><br><p></p><br><table border="1"><tbody><tr><th>  Iteration </th><th>  X </th><th>  Y </th><th>  Z </th></tr><tr><td>  25 </td><td>  0.383 </td><td>  -0.409 </td><td>  0.334 </td></tr><tr><td>  75 </td><td>  0.693 </td><td>  0.32 </td><td>  0.058 </td></tr><tr><td>  532 </td><td>  0.996 </td><td>  0.990 </td><td><img src="https://tex.s2cms.ru/svg/10%5E%7B-6%7D" alt="10 ^ {- 6}"><br></td></tr></tbody></table><br><p>  It is striking: how quickly the optimization went in 0-25 iterations, in 25-75 it was already slower, and at the end it took 457 iterations to get close to zero.  This behavior is very characteristic of the MNS: a very good convergence rate at the beginning, a bad one at the end. </p><br><br><p>  <strong>Newton's method</strong> </p><br><br><p>  <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%259D%25D1%258C%25D1%258E%25D1%2582%25D0%25BE%25D0%25BD%25D0%25B0">Newton</a> himself searches for the root of the equation, i.e.  such <img src="https://tex.s2cms.ru/svg/x" alt="x">  , what <img src="https://tex.s2cms.ru/svg/f(x)%3D0" alt="f (x) = 0">  .  This is not exactly what we need, because  the function may have an extremum not necessarily at zero. </p><br><br><p>  And then there is <a href="https://en.wikipedia.org/wiki/Newton%2527s_method_in_optimization">Newton's Method for optimization</a> .  When they talk about MN in the context of optimization, they mean it.  I myself, studying at the institute, confused these methods by foolishness and could not understand the phrase "Newton's method has a drawback - the need to count second derivatives." </p><br><br><p>  Consider for <img src="https://tex.s2cms.ru/svg/f(x)%3A%20R%20%5Cto%20R" alt="f (x): R \ to R"><br>  Accept <img src="https://tex.s2cms.ru/svg/F(x)%3Df(x)" alt="F (x) = f (x)">  i.e.  objective function coincides with the given one. </p><br><br><p>  Decompose <img src="https://tex.s2cms.ru/svg/f(x)" alt="f (x)">  in the Taylor series, but unlike the MNS, we need a quadratic approximation: </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/f(x%2Bd)%20%5Capprox%20f(x)%20%2B%20f%5E%7B'%7D(x)d%20%2B%20%5Cfrac%7B1%7D%7B2%7D%20f%5E%7B''%7D(x)d%5E2%2C%20%5C%20d%20%5Cin%20R%5En%20%2C%20%7C%7Cd%7C%7C%20%5Cto%200%20%5Ctag%7B4%7D" alt="f (x + d) \ approx f (x) + f ^ {'} (x) d + \ frac {1} {2} f ^ {' '} (x) d ^ 2, \ d \ in R ^ n, || d || \ to 0 \ tag {4}"></div><p></p><br><p>  It is easy to show that if <img src="https://tex.s2cms.ru/svg/f%7B'%7D(x)%20%5Cne%200" alt="f {'} (x) \ ne 0">  , the function cannot have an extremum in <img src="https://tex.s2cms.ru/svg/x" alt="x">  .  Point <img src="https://tex.s2cms.ru/svg/x%5E*%24%20%D0%B2%20%D0%BA%D0%BE%D1%82%D0%BE%D1%80%D0%BE%D0%B9%20%24f%7B'%7D(x)%20%3D%200" alt="x ^ * $ in which $ f {'} (x) = 0">  called <strong>stationary</strong> . </p><br><br><p>  Differentiate both parts by <img src="https://tex.s2cms.ru/svg/d" alt="d">  .  Our goal is to <img src="https://tex.s2cms.ru/svg/f(x%2Bd)%5E%7B'%7D%3D0" alt="f (x + d) ^ {'} = 0">  Therefore, we solve the equation: </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A0%20%3D%20f(x%2Bd)%5E%7B'%7D%20%3D%20f%5E%7B'%7D(x)%20%2B%20f%5E%7B''%7D(x)d%20%20%5C%5C%0Ad_%7B%D0%BD%7D%3D-%20%5Cfrac%7Bf%5E%7B'%7D(x)%7D%7Bf%5E%7B''%7D(x)%7D%0A" alt="0 = f (x + d) ^ {'} = f ^ {'} (x) + f ^ {''} (x) d \\ d_ {n} = - \ frac {f ^ {'} (x )} {f ^ {''} (x)}"></div><p></p><br><p><img src="https://tex.s2cms.ru/svg/d_%D0%BD" alt="d_n">  - This is the direction of the extremum, but it can be both a maximum and a minimum.  To find out - whether the point <img src="https://tex.s2cms.ru/svg/x%2Bd_%D0%BD" alt="x + d_n">  minimum - you need to analyze the second derivative.  If a <img src="https://tex.s2cms.ru/svg/f%5E%7B''%7D(x)%3E0%24%2C%20%D1%82%D0%BE%20%24f(x%2Bd_%D0%BD)" alt="f ^ {''} (x) &amp; gt; 0 $, then $ f (x + d_n)">  is a local minimum if <img src="https://tex.s2cms.ru/svg/f%5E%7B''%7D(x)%3C0" alt="f ^ {''} (x) &amp; lt; 0">  - maximum. </p><br><br><p>  In the multidimensional case, the first derivative is replaced by the gradient, the second - by the Hessian matrix.  It is impossible to divide matrices, instead multiply by the opposite (observing the side, since commutativity is absent): </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Af(x)%3A%20R%5En%20%5Cto%20R%20%5C%5C%0AH(x)d_%7B%D0%BD%7D%3D-%20%5Cnabla%20f(x)%5C%5C%0Ad_%7B%D0%BD%7D%3D-%20H%5E%7B-1%7D(x)%5Cnabla%20f(x)%0A" alt="f (x): R ^ n \ to R \\ H (x) d_ {n} = - \ nabla f (x) \\ d_ {n} = - H ^ {- 1} (x) \ nabla f ( x)"></div><p></p><br><p>  Similar to the one-dimensional case - you need to check whether we go right?  If the Hessian matrix is <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25BE%25D0%25BB%25D0%25BE%25D0%25B6%25D0%25B8%25D1%2582%25D0%25B5%25D0%25BB%25D1%258C%25D0%25BD%25D0%25BE_%25D0%25BE%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D1%2591%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BC%25D0%25B0%25D1%2582%25D1%2580%25D0%25B8%25D1%2586%25D0%25B0">positively defined</a> , then the direction is correct, otherwise we use the MNF. </p><br><br><p>  In the code: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">is_pos_def</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.all(np.linalg.eigvals(x) &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">NewtonOptimizer</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(Optimizer)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">next_point</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> hesse = self.hesse(self.x) <span class="hljs-comment"><span class="hljs-comment"># if Hessian matrix if positive - Ok, otherwise we are going in wrong direction, changing to gradient descent if is_pos_def(hesse): hesseInverse = np.linalg.inv(hesse) nextX = self.x - self.learningRate * np.dot(hesseInverse, self.gradient(self.x)) else: nextX = self.x - self.learningRate * self.gradient(self.x) return self.move_next(nextX)</span></span></code> </pre><br><br><p>  Result: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/b74/2a0/dc3/b742a0dc30454ed1a4a6112d16db1f59.gif"></div><br><p></p><br><table border="1"><tbody><tr><th>  Iteration </th><th>  X </th><th>  Y </th><th>  Z </th></tr><tr><td>  25 </td><td>  -1.49 </td><td>  0.63 </td><td>  4.36 </td></tr><tr><td>  75 </td><td>  0.31 </td><td>  -0.04 </td><td>  0.244 </td></tr><tr><td>  179 </td><td>  0.995 </td><td>  -0.991 </td><td><img src="https://tex.s2cms.ru/svg/10%5E%7B-6%7D" alt="10 ^ {- 6}"><br></td></tr></tbody></table><br><p>  Compare with the MHC.  There was a very strong descent to iteration 25 (almost fell from the mountain), but then the convergence slowed down a lot.  In MN, on the contrary, we first slowly descend from the mountain, but then we move faster.  The MHC took from 25 to 532 iterations to reach zero from <img src="https://tex.s2cms.ru/svg/z%3D0.334" alt="z = 0.334">  .  MN optimized <img src="https://tex.s2cms.ru/svg/4.36" alt="4.36">  for the last 154 iterations. </p><br><br><p>  This is a frequent behavior: MN has a quadratic rate of convergence, if you start from a point close to the local extremum.  The MNF works well far from the extremum. </p><br><br><p>  The MN uses the curvature information, as was seen in the figure above (smooth descent from the slide). <br>  Another example demonstrating this idea: in the figure below, the red vector is the direction of the MNF, and the green vector is the MN. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cbd/89e/9ac/cbd89e9ac806d731b042a341646bafa4.png"></div><br><p></p><br><p>  <strong>[Nonlinear vs linear] least squares method</strong> </p><br><br><p>  In MNC we have a model <img src="https://tex.s2cms.ru/svg/y%3Df(%5Cbeta_1%2C..%5Cbeta_n%3Bx)" alt="y = f (\ beta_1, .. \ beta_n; x)">  having <img src="https://tex.s2cms.ru/svg/n" alt="n">  parameters that are configured to minimize </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Cfrac%7B1%7D%7B2%7D%5Csum%20%5Climits_%7Bi%3D1%7D%5E%7BN%7D(y_i'-y_i)%5E2%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5Csum%20%5Climits_%7Bi%3D1%7D%5E%7BN%7Dr_i%5E2" alt="\ frac {1} {2} \ sum \ limits_ {i = 1} ^ {N} (y_i'-y_i) ^ 2 = \ frac {1} {2} \ sum \ limits_ {i = 1} ^ {N } r_i ^ 2"></div><p></p><br><p>  where <img src="https://tex.s2cms.ru/svg/y_i'" alt="y_i '">  - <img src="https://tex.s2cms.ru/svg/i" alt="i">  -th observation. </p><br><br><p>  In a <a href="https://en.wikipedia.org/wiki/Linear_least_squares_%2528mathematics%2529">linear MNC</a> , we have $ m $ equations, each of which we can represent as a <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25BE%25D0%25B5_%25D1%2583%25D1%2580%25D0%25B0%25D0%25B2%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">linear equation</a> </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%20x_i%5Cbeta_1%2Bx_i%5Cbeta_2%2B..x_i%5Cbeta_n%20%3D%20y_i" alt="x_i \ beta_1 + x_i \ beta_2 + .. x_i \ beta_n = y_i"></div><p></p><br><p>  For a linear MNC, the solution is unique.  There are powerful methods, such as <a href="https://en.wikipedia.org/wiki/QR_decomposition">QR decomposition</a> , <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">SVD decomposition</a> , capable of finding a solution for linear least-squares method for 1 <strong>approximate</strong> solution of the matrix equation <img src="https://tex.s2cms.ru/svg/Ax%3Db" alt="Ax = b">  . </p><br><br><p>  In <a href="https://en.wikipedia.org/wiki/Non-linear_least_squares">nonlinear OLS</a> parameter <img src="https://tex.s2cms.ru/svg/%5Cbeta_i" alt="\ beta_i">  may itself be represented by a function, for example <img src="https://tex.s2cms.ru/svg/%5Cbeta_i%5E2" alt="\ beta_i ^ 2">  .  Also, there may be a product of parameters, for example </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Cbeta_1%20%5Cbeta_2" alt="\ beta_1 \ beta_2"></div><p></p><br><p>  etc. <br>  Here we have to find a solution iteratively, and the solution depends on the choice of the starting point. </p><br><br><p>  The methods below deal with the nonlinear case.  But, first, consider the non-isolated MNC in the context of our task - minimizing the function </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Af%3AR%5E2%20%5Cto%20R%20%5C%5C%0AF(x_1%2Cx_2)%20%3D%20f(x_1%2Cx_2)%20%3D%20%5Cfrac%7B1%7D%7B2%7D(1-x_1)%5E2%20%2B%20%5Cfrac%7B1%7D%7B2%7D(x_2-x_1%5E2)%5E2%20%3D%20%20%5Cfrac%7B1%7D%7B2%7Dr_1%5E2(x_1%2Cx_2)%20%2B%20%5Cfrac%7B1%7D%7B2%7Dr_2%5E2(x_1%2Cx_2)%20%0A" alt="f: R ^ 2 \ to R \\ F (x_1, x_2) = f (x_1, x_2) = \ frac {1} {2} (1-x_1) ^ 2 + \ frac {1} {2} (x_2 -x_1 ^ 2) ^ 2 = \ frac {1} {2} r_1 ^ 2 (x_1, x_2) + \ frac {1} {2} r_2 ^ 2 (x_1, x_2)"></div><p></p><br><p>  Nothing like?  This is the form of the MNC!  We introduce a vector function <img src="https://tex.s2cms.ru/svg/r" alt="r"></p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Ar%3A%20R%5E2%20%5Cto%20R%5E2%20%5C%5C%0Ar%3D%20%5Cleft%20%5B%20%5Cbegin%7Bmatrix%7D%201-x_1%20%5C%5C%20x_2-x_1%5E2%20%5Cend%7Bmatrix%7D%20%5Cright%20%5D%0A" alt="r: R ^ 2 \ to R ^ 2 \\ r = \ left [\ begin {matrix} 1-x_1 \\ x_2-x_1 ^ 2 \ end {matrix} \ right]"></div><p></p><br><p>  and we will pick up <img src="https://tex.s2cms.ru/svg/x_1%2Cx_2" alt="x_1, x_2">  so as to solve the system of equations (at least approximately): </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cbegin%7Bcases%7D%20%0Ar_1%20%3D%201-x_1%20%3D%200%20%5C%5C%0Ar_2%20%3D%20x_2%20-%20x_1%5E2%20%3D%200%0A%5Cend%7Bcases%7D%20%5C%5C%0A" alt="\ begin {cases} r_1 = 1-x_1 = 0 \\ r_2 = x_2 - x_1 ^ 2 = 0 \ end {cases} \\"></div><p></p><br><p>  Then we need a measure - how good our approximation is.  Here she is: </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AF(x)%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5Csum_i%5Em%20r_i%5E2(x)%20%3D%20%20%5Cfrac%7B1%7D%7B2%7D%20r%5ET%20r%3D%20%20%20%5Cfrac%7B1%7D%7B2%7D%20%7C%7Cr%7C%7C%5E2%20%5Ctag%7B5%7D%0A" alt="F (x) = \ frac {1} {2} \ sum_i ^ m r_i ^ 2 (x) = \ frac {1} {2} r ^ T r = \ frac {1} {2} || r || ^ 2 \ tag {5}"></div><p></p><br><p>  I applied the inverse operation: adjusted the vector function <img src="https://tex.s2cms.ru/svg/r" alt="r">  under the target <img src="https://tex.s2cms.ru/svg/F" alt="F">  .  But it is possible and vice versa: if a vector function is given <img src="https://tex.s2cms.ru/svg/r%3A%20R%5En%20%5Cto%20R%5Em" alt="r: R ^ n \ to R ^ m">  build <img src="https://tex.s2cms.ru/svg/F(x)" alt="F (x)">  from (5).  For example: </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Ar%3D%20%5Cleft%20%5B%20%5Cbegin%7Bmatrix%7D%20x_1%5E2%20%5C%5C%20x_2%5E2%20%5Cend%7Bmatrix%7D%20%5Cright%20%5D%2C%20F(x)%20%3D%20%5Cfrac%7B1%7D%7B2%7D%20x_1%5E2%20%2B%20%20%5Cfrac%7B1%7D%7B2%7D%20x_2%5E2%0A" alt="r = \ left [\ begin {matrix} x_1 ^ 2 \\ x_2 ^ 2 \ end {matrix} \ right], F (x) = \ frac {1} {2} x_1 ^ 2 + \ frac {1} { 2} x_2 ^ 2"></div><p></p><br><p>  Finally, one very important moment.  The condition must be met <img src="https://tex.s2cms.ru/svg/m%20%5Cge%20n" alt="m \ ge n">  otherwise you cannot use the method.  In our case, the condition is satisfied </p><br><br><p>  <strong>Gauss-Newton Method</strong> </p><br><br><p>  The method is based on the same linear approximation, only now we are dealing with two functions: </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Ar(x%2Bd)%20%5Capprox%20l(d)%20%5Cequiv%20r(x)%20%2B%20J(x)d%20%5C%5C%0AF(x%2Bd)%20%20%5Capprox%20L(d)%20%5Cequiv%20%5Cfrac%7B1%7D%7B2%7Dl%5ET(d)%20l(d)%0A" alt="r (x + d) \ approx l (d) \ equiv r (x) + J (x) d \\ F (x + d) \ approx L (d) \ equiv \ frac {1} {2} l ^ T (d) l (d)"></div><p></p><br><p>  Then we do the same as in Newton's method - we solve the equation (only for <img src="https://tex.s2cms.ru/svg/L(d)" alt="L (d)">  ): </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AL%5E%7B''%7Dd_%7B%D0%B3%D0%BD%7D%20%3D%20-L%5E%7B'%7D%0A" alt="L ^ {''} d_ {n} = -L ^ {'}"></div><p></p><br><p>  It is easy to show that near <img src="https://tex.s2cms.ru/svg/%20%5Ctext(%5C%20d%20%5Cto%200)" alt="\ text (\ d \ to 0)">  : </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AL%5E%7B''%7D(d)%20%3D%20J_r%5ET%20J_r%2C%20%5C%20L%5E%7B'%7D(d)%20%3D%20J_r%5ET(d)%20r(d)%20%20%5C%5C%0A(J%5ET%20J)d_%7B%D0%B3%D0%BD%7D%20%3D%20-J%5ETr%20%5C%5C%0Ad_%7B%D0%B3%D0%BD%7D%20%3D%20-(J%5ET%20J)%5E%7B-1%7D%20J%5ETr%0A" alt="L ^ {''} (d) = J_r ^ T J_r, \ L ^ {'} (d) = J_r ^ T (d) r (d) \\ (J ^ TJ) d_ {n} = -J ^ Tr \\ d_ {n} = - (J ^ TJ) ^ {- 1} J ^ Tr"></div><p></p><br><p>  Optimizer code: </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">NewtonGaussOptimizer</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(Optimizer)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">next_point</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Solve (J_t * J)d_ng = -J*f jacobi = self.jacobi(self.x) jacobisLeft = np.dot(jacobi.T, jacobi) jacobiLeftInverse = np.linalg.inv(jacobisLeft) jjj = np.dot(jacobiLeftInverse, jacobi.T) # (J_t * J)^-1 * J_t nextX = self.x - self.learningRate * np.dot(jjj, self.function_array(self.x)).reshape((-1)) return self.move_next(nextX)</span></span></code> </pre><br><br><p>  The result exceeded my expectations.  In just 3 iterations, we came to a point <img src="https://tex.s2cms.ru/svg/(x%3D1%2C%20y%3D1)" alt="(x = 1, y = 1)">  .  To demonstrate the trajectory of the movement, I reduced the <strong><em>learningrate</em></strong> to 0.2 </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/3dd/423/6fe/3dd4236fe69e41a0b724d893b4563dbd.gif"></div><br><p></p><br><p>  <strong>Algorithm of Levenberg - Marquardt</strong> </p><br><br><p>  It is based on one of the versions of the Gauss-Newton Method (" <strong>damped version</strong> "): </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A(J%5ET%20J%20%2B%20%5Cmu%20I)d_%7B%D0%BB%D0%BC%7D%20%3D%20-J%5ETr%20%2C%20%5Cmu%20%5Cge%200%0A" alt="(J ^ T J + \ mu I) d_ {lm} = -J ^ Tr, \ mu \ ge 0"></div><p></p><br><p><img src="https://tex.s2cms.ru/svg/%5Cmu" alt="\ mu">  called <strong>regulation parameter</strong> .  Sometimes <img src="https://tex.s2cms.ru/svg/I" alt="I">  replace with <img src="https://tex.s2cms.ru/svg/diag(J%5ET%20J)" alt="diag (J ^ T J)">  to improve convergence. <br>  Diagonal elements <img src="https://tex.s2cms.ru/svg/J%5ET%20J" alt="J ^ T J">  will be positive, because  element <img src="https://tex.s2cms.ru/svg/a_%7Bii%7D" alt="a_ {ii}">  matrices <img src="https://tex.s2cms.ru/svg/J%5ET%20J" alt="J ^ T J">  is the scalar product of a row vector <img src="https://tex.s2cms.ru/svg/i" alt="i">  at <img src="https://tex.s2cms.ru/svg/J%5ET" alt="J ^ t">  on myself. </p><br><br><p>  For large <img src="https://tex.s2cms.ru/svg/%5Cmu" alt="\ mu">  the method of steepest descent is obtained, for small ones the Newton method. <br>  The algorithm itself in the optimization process selects the desired <img src="https://tex.s2cms.ru/svg/%5Cmu" alt="\ mu">  based on <strong>gain ratio</strong> , defined as: </p><br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Ag%3D%5Cfrac%7BF(x)%20-%20F(x_%7Bnew)%7D%7D%7BL(0)%20-%20L(d_%7B%D0%BB%D0%BC%7D)%7D%0A" alt="g = \ frac {F (x) - F (x_ {new)}} {L (0) - L (d_ {lm})}"></div><p></p><br><p>  If a <img src="https://tex.s2cms.ru/svg/g%3E0" alt="g &amp; gt; 0">  then <img src="https://tex.s2cms.ru/svg/L(d)" alt="L (d)">  - good approximation for <img src="https://tex.s2cms.ru/svg/F(x%2Bd)" alt="F (x + d)">  otherwise - you need to increase <img src="https://tex.s2cms.ru/svg/%5Cmu" alt="\ mu">  . <br>  Initial value <img src="https://tex.s2cms.ru/svg/%5Cmu" alt="\ mu">  set as <img src="https://tex.s2cms.ru/svg/%5Ctau%20%5Ccdot%20%20max%5C%7B%7Ba_%7Bij%7D%7D%5C%7D" alt="\ tau \ cdot max \ {{a_ {ij}} \}">  where <img src="https://tex.s2cms.ru/svg/a_%7Bij%7D" alt="a_ {ij}">  - matrix elements <img src="https://tex.s2cms.ru/svg/J%5ET%20J" alt="J ^ T J">  . <br><img src="https://tex.s2cms.ru/svg/%5Ctau" alt="\ tau">  recommended to appoint for <img src="https://tex.s2cms.ru/svg/10%5E%7B-3%7D" alt="10 ^ {- 3}">  .  The stopping criterion is to reach the global minimum, i.e. <img src="https://tex.s2cms.ru/svg/F%5E%7B'%7D(x%5E*)%3Dg(x%5E*)%3D0" alt="F ^ {'} (x ^ *) = g (x ^ *) = 0"></p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/64f/01f/396/64f01f39673249998cd194b56e395899.png"></div><br><p></p><br><p>  In optimizers, I did not implement the stopping criterion ‚Äî the user is responsible for this.  I only needed to move to the next point. </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LevenbergMarquardtOptimizer</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(Optimizer)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, function, initialPoint, gradient=None, jacobi=None, hessian=None, interval=None, function_array=None, learningRate=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> self.learningRate = learningRate functionNew = <span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: np.array([function(x)]) super().__init__(functionNew, initialPoint, gradient, jacobi, hessian, interval, function_array=function_array) self.v = <span class="hljs-number"><span class="hljs-number">2</span></span> self.alpha = <span class="hljs-number"><span class="hljs-number">1e-3</span></span> self.m = self.alpha * np.max(self.getA(jacobi(initialPoint))) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getA</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, jacobi)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.dot(jacobi.T, jacobi) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getF</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, d)</span></span></span><span class="hljs-function">:</span></span> function = self.function_array(d) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.5</span></span> * np.dot(function.T, function) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">next_point</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> self.y==<span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-comment"><span class="hljs-comment"># finished. Y can't be less than zero return self.x, self.y jacobi = self.jacobi(self.x) A = self.getA(jacobi) g = np.dot(jacobi.T, self.function_array(self.x)).reshape((-1, 1)) leftPartInverse = np.linalg.inv(A + self.m * np.eye(A.shape[0], A.shape[1])) d_lm = - np.dot(leftPartInverse, g) # moving direction x_new = self.x + self.learningRate * d_lm.reshape((-1)) # line search grain_numerator = (self.getF(self.x) - self.getF(x_new)) gain_divisor = 0.5* np.dot(d_lm.T, self.m*d_lm-g) + 1e-10 gain = grain_numerator / gain_divisor if gain &gt; 0: # it's a good function approximation. self.move_next(x_new) # ok, step acceptable self.m = self.m * max(1 / 3, 1 - (2 * gain - 1) ** 3) self.v = 2 else: self.m *= self.v self.v *= 2 return self.x, self.y</span></span></code> </pre><br><br><p>  The result was good too: </p><br><table border="1"><tbody><tr><th>  Iteration </th><th>  X </th><th>  Y </th><th>  Z </th></tr><tr><td>  0 </td><td>  -2 </td><td>  -2 </td><td>  22.5 </td></tr><tr><td>  four </td><td>  0.999 </td><td>  0.998 </td><td><img src="https://tex.s2cms.ru/svg/10%5E%7B-7%7D" alt="10 ^ {- 7}"><br></td></tr><tr><td>  eleven </td><td>  one </td><td>  one </td><td>  0 </td></tr></tbody></table><br><p>  When <strong><em>learningrate</em></strong> = 0.2: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/1ce/3e8/673/1ce3e8673e174facb8a475e6ef53b32f.gif"></div><br><p></p><br><p>  <strong>Method comparison</strong> </p><br><table border="1"><tbody><tr><th>  Method name </th><th>  Objective function </th><th>  Virtues </th><th>  disadvantages </th><th width="100">  Convergence </th></tr><tr><td>  The fastest descent method </td><td>  differentiable </td><td>  - wide range of applications <br>  -simple implementation <br><br>  - low price per iteration <br></td><td>  - the global minimum is searched worse than in other methods <br><br>  -low convergence rate near extremum <br></td><td>  local </td></tr><tr><td>  Newton's method </td><td>  twice differentiable </td><td>  -high rate of convergence near the extremum <br><br>  - uses curvature information <br></td><td>  -function must be twice differentiable <br><br>  will return an error if the Hesse matrix is ‚Äã‚Äãdegenerate (does not have an inverse) <br><br>  -There is a chance to go wrong if far away from the extremum. <br></td><td>  local </td></tr><tr><td>  Gauss-Newton Method </td><td>  nonlinear OLS </td><td>  - very high convergence rate <br><br>  -Works well with the task of <b>curve-fitting</b> <br></td><td>  - columns of matrix J should be linearly independent <br><br>  - imposes restrictions on the type of the objective function <br></td><td>  local </td></tr><tr><td>  Algorithm of Levenberg - Marquardt </td><td>  nonlinear OLS </td><td>  -labability among the considered methods <br><br>  -The greatest chance of finding a global extremum <br><br>  - very high convergence rate (adaptive) <br><br>  -Works well with the task of <b>curve-fitting</b> <br></td><td>  - columns of matrix J should be linearly independent <br><br>  - imposes restrictions on the type of the objective function <br><br>  -complicated implementation <br></td><td>  local </td></tr></tbody></table><br><p>  Despite the good results in the concrete example, the considered methods do not guarantee global convergence (to find which is an extremely difficult task).  An example of the few methods that can still achieve this is the <a href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.basinhopping.html">basin-hopping</a> algorithm. </p><br><br><p>  Combined result (specially reduced speed of the last two methods): </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/228/652/97b/22865297b57642149c7c5715287102f1.gif"></div><br><p></p><br><p>  <strong>Sources</strong> <a href="https://github.com/lightforever/Levenberg_Manquardt">can be downloaded from github</a> </p><br><br><p>  <strong>Sources</strong> </p><br><ol><li>  K. Madsen, HB Nielsen, O. Tingleff (2004): <a href="http://soe.rutgers.edu/~meer/GRAD561/ADD/nonlinadvanced.pdf">Methods for non-linear least square</a> </li><li>  Florent Brunet (2011): <a href="http://www.brnt.eu/phd/node10.html">Basics on Continuous Optimization</a> </li><li>  <a href="http://www4.ncsu.edu/~mtchu/Teaching/Lectures/MA529/chapter4.pdf">Least Squares Problems</a> </li></ol></div><p>Source: <a href="https://habr.com/ru/post/308626/">https://habr.com/ru/post/308626/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../308614/index.html">Developing FreePBX Modules</a></li>
<li><a href="../308616/index.html">Everything has its time</a></li>
<li><a href="../308618/index.html">Security Week 34: Vulnerability in iOS, Powershell Trojan, Collisions against 3DES and Blowfish</a></li>
<li><a href="../308620/index.html">Top developers can have a life outside of programming.</a></li>
<li><a href="../308622/index.html">Hobby projects: lets-meet.ru - where to go on Friday</a></li>
<li><a href="../308628/index.html">Caution for Kubernetes Users</a></li>
<li><a href="../308630/index.html">What are the reasons Alphabet claims to be the most expensive IT company?</a></li>
<li><a href="../308632/index.html">GitLab 8.11: Kanban board and conflict resolution with one click</a></li>
<li><a href="../308634/index.html">Functional Security - Older Sister Information Security, Part 1 of 7</a></li>
<li><a href="../308636/index.html">Basics of computer networks. Subject number 3. Protocols of lower levels (transport, network and channel)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>