<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Launch of RabbitMQ cluster in Kubernetes</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the case of microservice organization of the application, substantial work rests on the mechanisms of integration communication of microservices. M...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Launch of RabbitMQ cluster in Kubernetes</h1><div class="post__text post__text-html js-mediator-article">  In the case of microservice organization of the application, substantial work rests on the mechanisms of integration communication of microservices.  Moreover, this integration should be fault tolerant, with a high degree of availability. <br><br>  In our solutions, we use integration using Kafka, using gRPC, and using RabbitMQ. <br><br>  In this article we will share our experience in RabbitMQ clustering, whose nodes are located in Kubernetes. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/dx/ll/-h/dxll-hzomoco0zcfp7esju8pena.jpeg" alt="image"><br><br>  Prior to RabbitMQ version 3.7, its clustering in K8S was not a very trivial task, with a lot of hacks and not so beautiful solutions.  In version 3.6, the autocluster plugin from the RabbitMQ Community was used.  And in 3.7 Kubernetes Peer Discovery Backend appeared.  It is built into the RabbitMQ basic distribution and does not require separate assembly and installation. <br><br>  We will describe the final configuration in its entirety, simultaneously commenting on what is happening. <br><a name="habracut"></a><br><h2>  In theory </h2><br>  The plugin has a <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-k8s">repository on github</a> , which has <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-k8s/tree/master/examples/k8s_statefulsets">an example of basic use</a> . <br>  This example is not intended for Production, which is clearly indicated in its description, and moreover, some of the settings in it are set against the logic of use in the sale.  Also in the example, the storage persistence is not mentioned in any way, so in any abnormal situation our cluster will turn into zilch. <br><br><h2>  On practice </h2><br>  Now let us tell you what we faced and how RabbitMQ was installed and configured. <br><br>  We describe the configuration of all parts of RabbitMQ as a service in K8s.  Immediately clarify that we installed RabbitMQ in K8s as a StatefulSet.  On each node of the K8s cluster, one instance of RabbitMQ will always function (one node in the classical cluster configuration).  We will also install a RabbitMQ control panel in K8s and give access to this panel outside the cluster. <br><br><h3>  Rights and Roles: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_rbac.yaml</b> <div class="spoiler_text"><pre><code class="plaintext hljs">--- apiVersion: v1 kind: ServiceAccount metadata: name: rabbitmq --- kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader rules: - apiGroups: [""] resources: ["endpoints"] verbs: ["get"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: endpoint-reader subjects: - kind: ServiceAccount name: rabbitmq roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: endpoint-reader</code> </pre> </div></div><br>  Access rights for RabbitMQ are taken entirely from the example, no changes are required in them.  Create a ServiceAccount for our cluster and grant it read rights to Endpoints K8s. <br><br><h3>  Persistent storage: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pv.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolume apiVersion: v1 metadata: name: rabbitmq-data-sigma labels: type: local annotations: volume.alpha.kubernetes.io/storage-class: rabbitmq-data-sigma spec: storageClassName: rabbitmq-data-sigma capacity: storage: 10Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Recycle hostPath: path: "/opt/rabbitmq-data-sigma"</code> </pre> </div></div><br>  Here, we took the simplest case as the persistent storage ‚Äî hostPath (the usual folder on each K8s node), but you can use any of the many types of persistent volumes that are supported in K8s. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_pvc.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: PersistentVolumeClaim apiVersion: v1 metadata: name: rabbitmq-data spec: storageClassName: rabbitmq-data-sigma accessModes: - ReadWriteMany resources: requests: storage: 10Gi</code> </pre> </div></div><br>  Create Volume Claim on the volume created in the previous step.  This Claim will then be used in the StatefulSet as a persistent data store. <br><br><h3>  Services: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq-internal labels: app: rabbitmq spec: clusterIP: None ports: - name: http protocol: TCP port: 15672 - name: amqp protocol: TCP port: 5672 selector: app: rabbitmq</code> </pre> </div></div><br>  Create an internal headless service through which the Peer Discovery plugin will work. <br><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_service_ext.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">kind: Service apiVersion: v1 metadata: name: rabbitmq labels: app: rabbitmq type: LoadBalancer spec: type: NodePort ports: - name: http protocol: TCP port: 15672 targetPort: 15672 nodePort: 31673 - name: amqp protocol: TCP port: 5672 targetPort: 5672 nodePort: 30673 selector: app: rabbitmq</code> </pre> </div></div><br>  To run applications in K8s with our cluster, we create a balancer service. <br><br>  Since we need access to the RabbitMQ cluster outside of K8s, we push NodePort.  RabbitMQ will be available when accessing any K8s cluster node on ports 31673 and 30673. There is no great need for this in actual operation.  The question of the convenience of using the RabbitMQ admin panel. <br><br>  When creating a service with the NodePort type in K8s, a service with the ClusterIP type is also implicitly created for its maintenance.  Therefore, applications in K8s that need to work with our RabbitMQ will be able to access the cluster at <i>amqp: // rabbitmq: 5672</i> <br><br><h3>  Configuration: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_configmap.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: v1 kind: ConfigMap metadata: name: rabbitmq-config data: enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s]. rabbitmq.conf: | cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443 ### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true cluster_partition_handling = autoheal queue_master_locator=min-masters cluster_formation.randomized_startup_delay_range.min = 0 cluster_formation.randomized_startup_delay_range.max = 2 cluster_formation.k8s.service_name = rabbitmq-internal cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> </div></div><br>  Create RabbitMQ configuration files.  The main magic. <br><br><pre> <code class="plaintext hljs">enabled_plugins: | [rabbitmq_management,rabbitmq_peer_discovery_k8s].</code> </pre><br>  Add the necessary plugins to the allowed downloads.  Now we can use automatic Peer Discovery in the K8S. <br><br><pre> <code class="plaintext hljs">cluster_formation.peer_discovery_backend = rabbit_peer_discovery_k8s</code> </pre><br>  We expose the necessary plugin as a backend for peer discovery. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.host = kubernetes.default.svc.cluster.local cluster_formation.k8s.port = 443</code> </pre><br>  Specify the address and port through which you can reach the kubernetes apiserver.  Here you can directly specify the ip-address, but it will be more beautiful to do so. <br><br>  In the namespace default, a service named kubernetes is usually created, leading to k8-apiserver.  In different ways of installing K8S namespace, the service name and port may be different.  If something in a particular installation is different, you need, respectively, to correct here. <br><br>  For example, we are faced with the fact that in some clusters the service is on port 443, and in some on 6443. To understand that something is wrong, it will be possible in the start logs of RabbitMQ, where the connection point to the address specified here is clearly highlighted. <br><br><pre> <code class="plaintext hljs">### cluster_formation.k8s.address_type = ip cluster_formation.k8s.address_type = hostname</code> </pre><br>  By default, the example specified the addressing type of RabbitMQ cluster nodes by ip-address.  But when you restart the pod, it gets a new IP each time.  Surprise!  The cluster dies in agony. <br><br>  Change addressing to hostname.  StatefulSet guarantees the immutability of the hostname within the life cycle of the entire StatefulSet that we are completely satisfied. <br><br><pre> <code class="plaintext hljs">cluster_formation.node_cleanup.interval = 10 cluster_formation.node_cleanup.only_log_warning = true</code> </pre><br>  Since if one of the nodes is lost, we assume that it will recover sooner or later, we disable self-deletion of inaccessible nodes by the cluster.  In this case, as soon as the node returns to online, it will enter the cluster without losing its previous state. <br><br><pre> <code class="plaintext hljs">cluster_partition_handling = autoheal</code> </pre> <br>  This parameter determines the actions of the cluster with the loss of a quorum.  Here you should just read the <a href="http://www.rabbitmq.com/partitions.html">documentation on this topic</a> and understand for yourself what is closer to a specific use case. <br><br><pre> <code class="plaintext hljs">queue_master_locator=min-masters</code> </pre> <br>  Determine the choice of wizard for new queues.  With this setting, the wizard will select the node with the least number of queues, so the queues will be distributed evenly across the nodes of the cluster. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.service_name = rabbitmq-internal</code> </pre> <br>  We set the name of the K8s headless service (created by us earlier), through which the RabbitMQ nodes will communicate with each other. <br><br><pre> <code class="plaintext hljs">cluster_formation.k8s.hostname_suffix = .rabbitmq-internal.our-namespace.svc.cluster.local</code> </pre> <br>  An important thing to work addressing in a cluster by hostname.  FQDN hearth K8s is formed as a short name (rabbitmq-0, rabbitmq-1) + suffix (domain part).  Here we specify this suffix.  In K8S, it looks like <b>. &lt;Service name&gt;. &lt;Namespace name&gt; .svc.cluster.local</b> <br><br>  kube-dns without any additional configuration resolves the names of the type rabbitmq-0.rabbitmq-internal.our-namespace.svc.cluster.local to the ip-address of a specific submission, which makes all the clustering magic by the hostname possible. <br><br><h3>  Configuration StatefulSet RabbitMQ: </h3><br><div class="spoiler">  <b class="spoiler_title">rabbitmq_statefulset.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: rabbitmq spec: serviceName: rabbitmq-internal replicas: 3 template: metadata: labels: app: rabbitmq annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } } spec: serviceAccountName: rabbitmq terminationGracePeriodSeconds: 10 containers: - name: rabbitmq-k8s image: rabbitmq:3.7 volumeMounts: - name: config-volume mountPath: /etc/rabbitmq - name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia ports: - name: http protocol: TCP containerPort: 15672 - name: amqp protocol: TCP containerPort: 5672 livenessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 60 periodSeconds: 10 timeoutSeconds: 10 readinessProbe: exec: command: ["rabbitmqctl", "status"] initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 10 imagePullPolicy: Always env: - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local" - name: K8S_SERVICE_NAME value: "rabbitmq-internal" - name: RABBITMQ_ERLANG_COOKIE value: "mycookie" volumes: - name: config-volume configMap: name: rabbitmq-config items: - key: rabbitmq.conf path: rabbitmq.conf - key: enabled_plugins path: enabled_plugins - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> </div></div><br>  Actually, StatefulSet itself.  Note the interesting moments. <br><br><pre> <code class="plaintext hljs">serviceName: rabbitmq-internal</code> </pre> <br>  We prescribe the name of a headless service, through which pods communicate in the StatefulSet. <br><br><pre> <code class="plaintext hljs">replicas: 3</code> </pre> <br>  We set the number of replicas in the cluster.  We have it is equal to the number of working nodes K8s. <br><br><pre> <code class="plaintext hljs">annotations: scheduler.alpha.kubernetes.io/affinity: &gt; { "podAntiAffinity": { "requiredDuringSchedulingIgnoredDuringExecution": [{ "labelSelector": { "matchExpressions": [{ "key": "app", "operator": "In", "values": ["rabbitmq"] }] }, "topologyKey": "kubernetes.io/hostname" }] } }</code> </pre> <br>  When one of the K8s nodes falls, the statefulset tends to keep the number of instances in the set, so it creates several sub-streams on the same K8s node.  This behavior is completely undesirable and, in principle, meaningless.  Therefore, we prescribe an anti-affinity rule for hearths from a statefulset.  The rule is made tough (Required) so that kube-scheduler cannot break it when scheduling pods. <br><br>  The essence is simple: the scheduler is prohibited to place (within the namespace) more than one poda with the <i>app: rabbitmq tag</i> on each node.  Nodes are distinguished by the value of the label <i>kubernetes.io/hostname</i> .  Now, if for some reason the number of working K8S nodes is less than the required number of replicas in StatefulSet, new replicas will not be created until the free node reappears. <br><br><pre> <code class="plaintext hljs">serviceAccountName: rabbitmq</code> </pre> <br>  We register ServiceAccount under which our scams work. <br><br><pre> <code class="plaintext hljs">image: rabbitmq:3.7</code> </pre> <br>  The image of RabbitMQ is completely standard and is taken from the docker hub, does not require any reassembly and modification by the file. <br><br><pre> <code class="plaintext hljs">- name: rabbitmq-data mountPath: /var/lib/rabbitmq/mnesia</code> </pre><br>  The persistent data of RabbitMQ is stored in / var / lib / rabbitmq / mnesia.  Here we mount our Persistent Volume Claim into this folder, so that when restarting the sub / node or even the entire StatefulSet data (both official, including the collected cluster, and user data) remain safe and sound.  There are some examples where the entire folder / var / lib / rabbitmq / is persistent.  We came to the conclusion that this is not the best idea, since it also begins to remember all the information specified by the Rabbit configs.  That is, in order to change something in the configuration file, it is required to clean the persistent storage, which is very inconvenient in operation. <br><br><pre> <code class="plaintext hljs"> - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name - name: NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: RABBITMQ_USE_LONGNAME value: "true" - name: RABBITMQ_NODENAME value: "rabbit@$(HOSTNAME).rabbitmq-internal.$(NAMESPACE).svc.cluster.local"</code> </pre><br>  With this set of environment variables, we first tell RabbitMQ to use the FQDN name as the identifier of the cluster members, and second, we set the format of this name.  The format was described earlier when parsing the config. <br><br><pre> <code class="plaintext hljs">- name: K8S_SERVICE_NAME value: "rabbitmq-internal"</code> </pre> <br>  The name of a headless service for communicating cluster members. <br><br><pre> <code class="plaintext hljs">- name: RABBITMQ_ERLANG_COOKIE value: "mycookie"</code> </pre> <br>  The contents of the Erlang Cookie should be the same on all nodes of the cluster, you need to register your own value.  A node with a different cookie will not be able to enter the cluster. <br><br><pre> <code class="plaintext hljs">volumes: - name: rabbitmq-data persistentVolumeClaim: claimName: rabbitmq-data</code> </pre> <br>  We define a plug-in volume from the previously created Persistent Volume Claim. <br><br>  At this point, we are done with customization in K8s.  The result is a RabbitMQ cluster, which evenly distributes queues among the nodes and is resistant to problems in the runtime environment. <br><br><img src="https://habrastorage.org/webt/_j/ky/mw/_jkymwmxe7syyjfa7h2idbcxosc.png" alt="image"><br><br>  When one of the cluster nodes is unavailable, the queues contained in it will no longer be available, everything else will continue to work.  As soon as the node returns to the system, it will return to the cluster, and the queues for which she was the Master will become operational again, saving all the data contained in them (if the persistent storage is not broken, of course).  All these processes are fully automatic and do not require intervention. <br><br><h2>  Bonus: set up HA </h2><br>  In one of the projects was a nuance.  The requirements sounded complete mirroring of all data contained in the cluster.  It is necessary that in a situation where at least one cluster node is operational, from the point of view of the application, everything continues to work.  This moment is not connected with K8s, we describe it simply as a mini how-to. <br><br>  To enable full HA, you need to create a Policy in the RabbitMQ dashboard on the <i>Admin -&gt; Policies</i> tab.  The name is arbitrary, Pattern is empty (all queues), in Definitions add two parameters: <i>ha-mode: all</i> , <i>ha-sync-mode: automatic</i> . <br><br><img src="https://habrastorage.org/webt/jz/tn/vu/jztnvu5zygtv56hurbyss1w9ljm.png" alt="image"><br><br><img src="https://habrastorage.org/webt/_6/in/om/_6inoma38lvluhpaet1g66uus_u.png" alt="image"><br><br>  After that, all the queues created in the cluster will be in High Availability mode: when the Master node is unavailable, the new master will automatically select one of the Slaves.  And the data arriving in the queue will be mirrored to all the nodes of the cluster.  What, actually, was required to receive. <br><br><img src="https://habrastorage.org/webt/0v/m3/je/0vm3jem0bi4fqckj8ucmiy5zcxe.png" alt="image"><br><br>  Read more about HA in RabbitMQ <a href="http://www.rabbitmq.com/ha.html">here.</a> <br><br><h2>  Useful literature: </h2><br><ul><li>  <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-k8s/">Repository RabbitMQ Peer Discovery Kubernetes Plugin</a> </li><li>  <a href="https://github.com/rabbitmq/rabbitmq-peer-discovery-k8s/tree/master/examples/k8s_statefulsets">Configuration example for RabbitMQ deployment in K8S</a> </li><li>  <a href="http://www.rabbitmq.com/cluster-formation.html">Description of the principles of cluster formation, the mechanism of Peer Discovery and plug-ins for it</a> </li><li>  <a href="https://groups.google.com/forum/">An epic discussion about how to properly configure hostname-based discovery</a> </li><li>  <a href="http://www.rabbitmq.com/clustering.html">RabbitMQ Clustering Guide</a> </li><li>  <a href="http://www.rabbitmq.com/partitions.html">Description of split-brain clustering problems and solutions</a> </li><li>  <a href="http://www.rabbitmq.com/ha.html">High Availability Queues in RabbitMQ</a> </li><li>  <a href="http://www.rabbitmq.com/parameters.html">Setting Policies</a> </li></ul><br>  Successes! </div><p>Source: <a href="https://habr.com/ru/post/419817/">https://habr.com/ru/post/419817/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../419805/index.html">The Super Tiny Compiler - now in Russian</a></li>
<li><a href="../419807/index.html">Glaucoma - how not to go blind: let's talk about the treatment ...</a></li>
<li><a href="../419811/index.html">Evolution of flexible displays</a></li>
<li><a href="../419813/index.html">Skillbox Webinars: Friday Collection</a></li>
<li><a href="../419815/index.html">Secrets of resiliency of our front office</a></li>
<li><a href="../419819/index.html">Biomarkers of aging. Frailty panel. Part 2</a></li>
<li><a href="../419823/index.html">Unusual Duet - Passwords and Mnemonic Images</a></li>
<li><a href="../419825/index.html">Performance testing of several types of drives in a virtual environment</a></li>
<li><a href="../419829/index.html">The default key encryption in OpenSSH is worse than its absence</a></li>
<li><a href="../419831/index.html">How JS works: custom elements</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>