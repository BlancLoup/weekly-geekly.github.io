<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Naive Bayes classifier in 25 lines of code</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A naive Bayes classifier is one of the simplest of the classification algorithms. However, very often it works as good as, if not better, for more com...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Naive Bayes classifier in 25 lines of code</h1><div class="post__text post__text-html js-mediator-article">  A naive Bayes classifier is one of the simplest of the classification algorithms.  However, very often it works as good as, if not better, for more complex algorithms.  Here I want to share the code and description of how it all works. <br><br>  And so, for example, take the task of determining gender by name.  Of course, to determine the gender you can create a large list of names with gender labels.  But this list will in any case be incomplete.  In order to solve this problem, you can ‚Äútrain‚Äù the model by tagged names. <br>  If interested, please <a name="habracut"></a>  . <br><br><h2>  A bit of theory </h2><br>  Suppose we have a string of text O. In addition, there are classes C, to one of which we must assign a string.  We need to find a class c, in which its probability for a given line would be maximum.  Mathematically, this is written like this: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/91f/747/2c5/91f7472c5d78cd4ac9d038ee807f5632.png" alt="image"><br><br>  Calculate P (C | O) is difficult.  But you can use the Bayes theorem and go to the indirect probabilities: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5f0/ba1/6fa/5f0ba16fae1e4613485d29e1fa62a4e4.png" alt="image"><br><br>  Since we are looking for the maximum of the function, the denominator does not interest us (it is a constant in this case).  In addition, you need to look at the O line. Usually, there is no point in working with the entire line.  It is much more effective to select certain features from it.  Thus, the formula will take the form: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e92/4d2/63b/e924d263b59e77a4026a48dab0cad9ae.png" alt="image"><br><br>  The denominator does not interest us.  The numerator can be rewritten as follows. <br><br><img src="http://mathurl.com/3vdesp4.png" alt="image"><br><br>  But it is again difficult.  Here we include the ‚Äúnaive‚Äù assumption that the variables O depend only on the class C, and do not depend on each other.  This is greatly simplified, but it often works.  The numerator will look like: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7c0/af6/944/7c0af694411c5f3dc7174d9a027759b3.png" alt="image"><br><br>  The final formula will look like: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f60/17a/e3b/f6017ae3be1985b9e9900920c05659d3.png" alt="image">  (one) <br><br>  Those.  all you need to do is calculate the probabilities P (C) and P (O | C).  The calculation of these parameters is called classifier training. <br><br><h2>  Code </h2><br>  Below is the code on python.  It contains only two functions: one for training (calculating the parameters of the formula), the other for classification (direct calculation of the formula). <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> division <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> defaultdict <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> math <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> log <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(samples)</span></span></span><span class="hljs-function">:</span></span> classes, freq = defaultdict(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span>:<span class="hljs-number"><span class="hljs-number">0</span></span>), defaultdict(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span>:<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> feats, label <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> samples: classes[label] += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-comment"><span class="hljs-comment"># count classes frequencies for feat in feats: freq[label, feat] += 1 # count features frequencies for label, feat in freq: # normalize features frequencies freq[label, feat] /= classes[label] for c in classes: # normalize classes frequencies classes[c] /= len(samples) return classes, freq # return P(C) and P(O|C) def classify(classifier, feats): classes, prob = classifier return min(classes.keys(), # calculate argmin(-log(C|O)) key = lambda cl: -log(classes[cl]) + \ sum(-log(prob.get((cl,feat), 10**(-7))) for feat in feats))</span></span></code> </pre> <br><br>  In the <i>train</i> function, the first five lines count the number of classes C, as well as the frequency of O and C features in one sample.  The second part of the method simply normalizes these frequencies.  In this way, the probabilities P P and P (O | C) are obtained at the output. <br><br>  The <i>classify</i> function searches for the most likely class.  The only difference from formula (1) is that I replace the product of probabilities by the sum of logarithms taken with a negative sign, and I calculate not argmax, but argmin.  The transition to logarithms is a common technique to avoid too small numbers, which could be obtained by multiplying probabilities. <br>  The number 10 (^ - 7), which is substituted into the logarithm, is a way to avoid zero in the argument of the logarithm (because it will be otherwise it will be undefined). <br><br>  To train the classifier, take the marked list of male and female names and use this code: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_features</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(sample)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (sample[<span class="hljs-number"><span class="hljs-number">-1</span></span>],) <span class="hljs-comment"><span class="hljs-comment"># get last letter samples = (line.decode('utf-8').split() for line in open('names.txt')) features = [(get_features(feat), label) for feat, label in samples] classifier = train(features) print 'gender: ', classify(classifier, get_features(u''))</span></span></code> </pre><br><br>  The file 'names.txt' can be downloaded <a href="http://dl.dropbox.com/u/27900527/names.txt">here</a> . <br><br>  As a feature, I chose the last letter of the name (see the get_features function).  It works well, but for the working version it is better to use the scheme more complicated.  For example, select the first letter of the name and the last two.  For example, like this: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_features</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(sample)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ( <span class="hljs-string"><span class="hljs-string">'ll: %s'</span></span> % sample[<span class="hljs-number"><span class="hljs-number">-1</span></span>], <span class="hljs-comment"><span class="hljs-comment"># get last letter 'fl: %s' % sample[0], # get first letter 'sl: %s' % sample[1], # get second letter )</span></span></code> </pre><br><br>  The algorithm can be used for an arbitrary number of classes.  For example, you can try to build a classifier of texts on emotional coloring. <br><br><h2>  Tests </h2><br>  I tested the classifier on the part of the original case with the names.  Accuracy was 96%.  This is not a brilliant result, but for many tasks it is enough. </div><p>Source: <a href="https://habr.com/ru/post/120194/">https://habr.com/ru/post/120194/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../120188/index.html">RubyGems - detail</a></li>
<li><a href="../120189/index.html">On Friday, WordPress turned 8 years old. WordPress history in screenshots</a></li>
<li><a href="../120191/index.html">Proprietary provider modems 1/3</a></li>
<li><a href="../120192/index.html">JavaScript FAQ: Part 1</a></li>
<li><a href="../120193/index.html">Basics and Misconceptions About JavaScript</a></li>
<li><a href="../120197/index.html">The developer has created a tool for returning RSS to Twitter</a></li>
<li><a href="../120198/index.html">We learn bash-scripts, we write Sokoban</a></li>
<li><a href="../120200/index.html">Harvey Fineberg: Are you ready for neo-evolution?</a></li>
<li><a href="../120204/index.html">Virtual Machine Console</a></li>
<li><a href="../120206/index.html">Samsung lawyers want to show them iPhone 5 and iPad 3</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>