<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>LipNet neural network reads lips with 93.4% accuracy</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Commander Dave Bowman and co-pilot Frank Poole, not trusting the computer, decided to disconnect it from controlling the ship. To do this, they confer...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>LipNet neural network reads lips with 93.4% accuracy</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/f5b/c98/b66/f5bc98b66e6b4c54a8b456037259caab.jpg"><br>  <i>Commander Dave Bowman and co-pilot Frank Poole, not trusting the computer, decided to disconnect it from controlling the ship.</i>  <i>To do this, they confer in a soundproofed room, but HAL 9000 reads their lips-talk.</i>  <i><font color="gray">Frame from the film "Space Odyssey 2001"</font></i> <br><br>  Reading lips plays an important role in communication.  The experiments of 1976 showed that people ‚Äúhear‚Äù completely <i>different</i> phonemes if you put the wrong sound on the lip movement (see <a href="http://www.nature.com/nature/journal/v264/n5588/abs/264746a0.html">‚ÄúHearing lips and seeing voices‚Äù</a> , Nature 264, 746-748, 23 December 1976, doi: 10.1038 / 264746a0) . <br><br>  From a practical point of view, lip reading is an important and useful skill.  You can understand the interlocutor without turning off the music in the headphones, read the conversations of all people in sight (for example, all passengers in the waiting room), listen to people with binoculars or a telescope.  The scope of the skill is very wide.  The professional who mastered it will easily find a highly paid job.  For example, in the field of security or competitive intelligence. <br><a name="habracut"></a><br>  Automatic lip reading systems also have a rich practical potential.  These are medical hearing aids of a new generation with speech recognition, systems for silent lectures in public places, biometric identification, systems of hidden information transmission for espionage, speech recognition by video from surveillance cameras, etc.  In the end, the computers of the future, too, will read lips, like <a href="https://ru.wikipedia.org/wiki/HAL_9000">HAL 9000</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Therefore, scientists for many years trying to develop automatic lip reading, but without much success.  Even for relatively simple English, in which the number of phonemes is much less than in Russian, the recognition accuracy is low. <br><br>  To understand speech on the basis of a person‚Äôs mimicry is a most difficult task.  People who have mastered this skill are trying to recognize dozens of consonant phonemes, many of which are very similar in appearance.  It is especially difficult for an unprepared person to distinguish between <a href="https://www.ncbi.nlm.nih.gov/pubmed/5719234">five categories of visual phonemes</a> (that is, a visa) of English.  In other words, it is almost impossible to distinguish the pronunciation of certain consonant sounds in the lips.  It is not surprising that people do very poorly with accurate lip reading.  Even the best among hearing impaired people demonstrate an accuracy of only <a href="http://link.springer.com/article/10.3758/BF03204211">17 ¬± 12% of 30 monosyllabic words or 21 ¬± 11% of polysyllabic words</a> (hereinafter, the results for the English language). <br><br>  Automatic lip reading is one of the tasks of machine vision, which comes down to the frame-by-frame processing of a video sequence.  The task is greatly complicated by the low quality of most practical video materials, which do not allow accurate reading of the spatiotemporal, that is, spatial-temporal characteristics of a person during a conversation.  Persons move and turn in different directions.  Recent developments in computer vision are trying to track the movement of faces in the frame to solve this problem.  Despite successes, until recently they were able to recognize only single words, but not sentences. <br><br>  Significant breakthroughs in this area have been made by the developers at Oxford University.  <a href="http://openreview.net/forum%3Fid%3DBkjLkSqxg">The LipNet neural network trained by them</a> became the first in the world to successfully recognize lip-level speech at the level of entire sentences while processing the video sequence. <br><br><img src="https://habrastorage.org/files/c91/f89/dae/c91f89daeefd4e0da9a55c41073a5285.jpg"><br>  <i>Single- <a href="http://compression.ru/video/seminar/slides/2012_saliency_map_generation.pdf">card salinity cards</a> for the English words "please" (above) and "lay" (below) when processed by a neural network that reads lips, highlighting the most attention-grabbing (salient) features</i> <br><br>  LipNet - recurrent neural network of type LSTM (long short-term memory).  The architecture is shown in the illustration.  The neural network was trained using the neural network temporal classification method (Connectionist Temporal Classification, CTC), which is widely used in modern speech recognition systems, since it eliminates the need for training in input data that is synchronized with the correct result. <br><br><img src="https://habrastorage.org/files/856/4ac/a35/8564aca35bea4280bce7c669cce37fe2.png"><br>  <i>LipNet neural network architecture.</i>  <i>The input is a sequence of frames T, which are then processed by three layers of the space-time (spatiotemporal) convolutional neural network (STCNN), each of which is accompanied by a layer of spatial sampling.</i>  <i>For extracted features, the sampling rate on the timeline increases (upsampling), and then they are processed with dual LTSM.</i>  <i>Each time step at the LTSM output is processed by a two-layer network of direct distribution and the last layer of SoftMax</i> <br><br>  On a special package of GRID sentences, the neural network shows recognition accuracy of 93.4%.  This not only exceeds the recognition accuracy of other software developments (which are listed in the table below), but also exceeds the lip-reading performance of specially trained people. <br><br><table><tbody><tr><th>  Method </th><th>  Data set </th><th>  The size </th><th>  Extradition </th><th>  Accuracy </th></tr><tr><td>  Fu et al.  (2008) </td><td>  AVICAR </td><td>  851 </td><td>  Numbers </td><td>  37.9% </td></tr><tr><td>  Zhao et al.  (2009) </td><td>  AVLetter </td><td>  78 </td><td>  Alphabet </td><td>  43.5% </td></tr><tr><td>  Papandreou et al.  (2009) </td><td>  CUAVE </td><td>  1800 </td><td>  Numbers </td><td>  83.0% </td></tr><tr><td>  Chung &amp; Zisserman (2016a) </td><td>  OuluVS1 </td><td>  200 </td><td>  Phrases </td><td>  91.4% </td></tr><tr><td>  Chung &amp; Zisserman (2016b) </td><td>  OuluVS2 </td><td>  520 </td><td>  Phrases </td><td>  94.1% </td></tr><tr><td>  Chung &amp; Zisserman (2016a) </td><td>  BBC TV </td><td>  &gt; 400,000 </td><td>  The words </td><td>  65.4% </td></tr><tr><td>  Wand et al.  (2016) </td><td>  GRID </td><td>  9000 </td><td>  The words </td><td>  79.6% </td></tr><tr><td>  Lipnet </td><td>  GRID </td><td>  28853 </td><td>  <b>suggestions</b> </td><td>  <b>93.4%</b> </td></tr></tbody></table><br>  The special GRID package is composed of the following pattern: <br><br>  <i>command (4) + color (4) + preposition (4) + letter (25) + digit (10) + adverb (4),</i> <br><br>  where the number corresponds to the number of variants of words for each of the six verbal categories. <br><br>  In other words, the accuracy of 93.4% is still a result obtained in greenhouse laboratory conditions.  Of course, when recognizing arbitrary human speech, the result will be much worse.  Not to mention the analysis of data from a real video, where a person's face is not taken close-up in excellent lighting and with high resolution. <br><br>  The operation of the LipNet neural network is shown in a demo video. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/fa5QGremQf8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  The scientific article was prepared for the ICLR 2017 conference and was <a href="http://www.oxml.co.uk/publications/2016-Assael_Shillingford_LipNet.pdf">published</a> on November 4, 2016 in open access. </div><p>Source: <a href="https://habr.com/ru/post/398901/">https://habr.com/ru/post/398901/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../398889/index.html">NASA Report on EmDrive Success Tests Published</a></li>
<li><a href="../398893/index.html">GSM base station hid in an office printer</a></li>
<li><a href="../398895/index.html">How I reprogrammed my brain to begin to understand math</a></li>
<li><a href="../398897/index.html">Yandex launched the service "Health"</a></li>
<li><a href="../398899/index.html">Masterkeys Pro L: customize all fields</a></li>
<li><a href="../398903/index.html">The everlasting beauty of the cosmos</a></li>
<li><a href="../398905/index.html">TP-LINK HS110 - a home assistant or another outlet with Wi-Fi?</a></li>
<li><a href="../398907/index.html">Symphonic music - problems of quality of reproduction, choice of format and equipment</a></li>
<li><a href="../398909/index.html">The National Bank of Ukraine recalled the ban on electronic payment systems Webmoney, Yandex.Money, QIWI Wallet and Wallet One</a></li>
<li><a href="../398913/index.html">How Valve tries to teach courtesy players</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>