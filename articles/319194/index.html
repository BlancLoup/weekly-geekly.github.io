<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Increase code performance: think first about data</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Being engaged in graphics rendering programming, we live in a world in which low-level optimizations are required in order to achieve GPU frames with ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Increase code performance: think first about data</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/496/282/08e/49628208ec59442e8a9bee85b4cb1d6a.png"><br><br>  Being engaged in graphics rendering programming, we live in a world in which low-level optimizations are required in order to achieve GPU frames with a length of 30 ms.  To do this, we use various techniques and developed from scratch new rendering passes with enhanced performance (geometry attributes, texture cache, export, etc.), GPR compression, latency hiding, ROP ... <br><br>  In the area of ‚Äã‚Äãimproving CPU performance, various tricks were used at one time, and it is noteworthy that today they are used for modern video cards to speed up <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D1%2580%25D0%25B8%25D1%2584%25D0%25BC%25D0%25B5%25D1%2582%25D0%25B8%25D0%25BA%25D0%25BE-%25D0%25BB%25D0%25BE%25D0%25B3%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B5_%25D1%2583%25D1%2581%25D1%2582%25D1%2580%25D0%25BE%25D0%25B9%25D1%2581%25D1%2582%25D0%25B2%25D0%25BE">ALU</a> computations ( <a href="https://michaldrobot.files.wordpress.com/2014/05/gcn_alu_opt_digitaldragons2014.pdf">Low Level Optimization for AMD GCN</a> , <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D1%258B%25D1%2581%25D1%2582%25D1%2580%25D1%258B%25D0%25B9_%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D1%2582%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BA%25D0%25B2%25D0%25B0%25D0%25B4%25D1%2580%25D0%25B0%25D1%2582%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BA%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BD%25D1%258C">Fast Backward Square Root in Quake</a> ). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/bb4/93f/153/bb493f153d3346d6941cfc478bf50c12.png"><br>  <i>Quick square root in quake</i> <br><br>  But recently, especially in the light of the transition to 64 bits, I have noticed an increase in the number of non-optimized code, as if the industry is rapidly losing all the previously accumulated knowledge.  Yes, old tricks like fast reverse square root on modern processors are counterproductive.  But programmers should not forget about low-level optimizations and hope that the compilers will solve all their problems.  <b>They will not decide.</b> <br><br>  This article is not an exhaustive hardcore iron guide.  This is just an introduction, a reminder, a set of basic principles for writing effective code for the CPU.  I want to ‚Äú <a href="http://www.humus.name/Articles/Persson_LowLevelThinking.pdf">show that low-level thinking is still useful today,</a> ‚Äù <b>even if it‚Äôs about processors that I could add.</b> <br><br>  In this article, we will look at caching, vector programming, reading and understanding assembly code, and writing code that is convenient for the compiler. <br><a name="habracut"></a><br><h1>  Why bother? </h1><br><h3>  Do not forget about breaking </h3><br>  In the 1980s, the memory bus frequency was equal to the CPU frequency, and the delay was almost zero.  But processor performance grew logarithmically in accordance with Moore's law, and the performance of RAM chips increased disproportionately, so that memory soon became a bottleneck.  And the point is not that it is impossible to create a faster memory: it is possible, but <b>economically unprofitable.</b> <br><br><img src="https://habrastorage.org/files/e6f/73e/a41/e6f73ea416dc446b8a2b71934a1de723.png"><br>  <i>Change processor speed and memory</i> <br><br>  To reduce the impact of memory performance, CPU developers have added a tiny amount of this very expensive memory between the processor and the main memory - this is how the <b>processor cache</b> appeared. <br><br><img src="https://habrastorage.org/files/f23/d28/2e2/f23d282e2bcf4718a16f7ca13195ed66.png"><br><br>  The idea is this: there is a good chance that the same code or data may again be required in a short period of time. <br><br><ul><li>  <b>Spatial locality:</b> loops in code, so that the same code is executed time after time. </li><li>  <b>Temporal locality:</b> even if the memory areas used for short periods of time are not adjacent to each other, there is still a high probability that the same data will soon be used again. </li></ul><br>  CPU cache is a complex method of increasing productivity, but without the help of a programmer, it will not work correctly.  Unfortunately, many developers do not realize the cost of using memory and CPU cache structure. <br><br><h3>  Data-oriented architecture </h3><br>  We are interested in game engines.  They process increasing amounts of data, transform them and display them in real time.  Considering this, as well as the need to solve problems with efficiency, the programmer must understand what data he processes and know the equipment with which his code will work.  <b>Therefore, he must be aware of the need to implement a data-oriented architecture (DoD).</b> <br><br><h3>  Or maybe the compiler will do it for me? </h3><br><img src="https://habrastorage.org/files/791/41a/c04/79141ac0416a45e08d67e3f777482885.png"><br>  <i>Simple addition.</i>  <i>On the left - C ++, on the right - the resulting code in assembler</i> <br><br>  Let's look at the above example for an AMD Jaguar processor (similar to those used in game consoles) (useful links: <a href="http://www.realworldtech.com/jaguar/6/">AMD's Jaguar Microarchitecture: Memory Hierarchy</a> , <a href="http://www.guru3d.com/articles_pages/amd_athlon_5350_apu_and_am1_platform_review,14.html">AMD Athlon 5350 APU and AM1 Platform Review - Performance - System Memory</a> ): <br><br><ul><li>  Load operation (about 200 cycles <b>without caching</b> ) </li><li>  Actual work: <i>inc eax</i> (1 cycle) </li><li>  Storage operation (~ 3 cycles, same cache line) </li></ul><br>  Even in such a simple example, most of the processor time is spent waiting for data, and in more complex programs, the situation does not get better until the programmer pays attention to the basic architecture. <br><br>  In short, compilers: <br><br><ul><li>  They do not see the whole picture; it is very difficult for them to predict how the data will be organized and how they will be accessed. </li><li>  Arithmetic operations can be well optimized, but sometimes these operations are just the tip of the iceberg. </li></ul><br>  The compiler has quite a bit of room for maneuver when it comes to optimizing memory access.  The context is known only to the programmer, and only he knows what code he wants to write.  Therefore, you need to understand the flow of <b>information flows</b> and first of all <b>proceed from data processing in</b> order to squeeze everything possible from modern CPUs. <br><br><h3>  Brutal Truth: OOP vs. DoD </h3><br><img src="https://habrastorage.org/files/1ea/ee2/977/1eaee2977f8746418acafc1d1539db2b.png"><br><br>  <i>Impact of memory access on performance (Mike Acton GDC15)</i> <br><br>  Object-oriented programming (OOP) today is the dominant paradigm, it is her who is first of all studied by future programmers.  It makes you think in terms of real-world objects and their relationships. <br><br>  In a class, code and data are usually encapsulated, therefore an object contains all its information.  Forcing to use arrays of structures (array of structures) and arrays * of pointers to * structures / objects, OOP violates the principle of <b>spatial locality</b> , on which cache memory acceleration is based.  Remember the gap between processor and memory performance? <br><br><img src="https://habrastorage.org/files/97c/79e/b0e/97c79eb0e3c54a69beec8a32a01b7104.png"><br><br>  <i><u>Excessive encapsulation is detrimental when working on modern hardware.</u></i> <br><br>  I want to tell you that when developing software, you need to shift the focus from the code itself to understanding data transformations, as well as respond to the current programming culture and state of affairs imposed by OO supporters. <br><br>  In conclusion, I want to quote three big lies told by Mike Acton (Mike Acton) ( <a href="https://www.youtube.com/watch%3Fv%3DrX0ItVEVjHc">CppCon 2014: Mike Acton, "Data-Oriented Design and C ++"</a> ) <br><br><ul><li>  <s>Software is a platform</s> <br><ul><li>  You need to understand the hardware you are working with. </li></ul></li><li>  <s>The code architecture is formed according to the model of the world.</s> <br><ul><li>  The code architecture must conform to the data model. </li></ul></li><li>  <s>Code is more important than data</s> <br><ul><li>  Memory is a bottleneck, data is by far the most important thing. </li></ul></li></ul><br><h1>  Study iron </h1><br><h3>  Microprocessor cache </h3><br>  The processor is not physically connected directly to the main memory.  All operations with RAM (loading and storage) on modern processors are performed through the cache. <br><br>  When the processor is busy with a call (load) command, the <b>memory controller</b> first searches the cache for an entry with a <b>tag</b> corresponding to the memory address at which it needs to read.  If such a record is found ‚Äî that is, <b>a cache hit</b> occurs ‚Äî then the data can be loaded directly from the cache.  If not - <b>cache miss</b> , - the controller will try to extract data from lower cache levels (for example, first L1D, then L2, then L3) and, finally, from RAM.  Then the data will be stored in L1, L2 and L3 ( <b>inclusive cache</b> ). <br><br><img src="https://habrastorage.org/files/53b/c1d/50b/53bc1d50badd4dbf8a85a3ceefa2d8d7.png"><br>  <i>Memory Console Delay - Jason Gregory</i> <br><br>  In this simplified illustration, the processor (AMD Jaguar, used in PS4 and XB1) has two cache levels - L1 and L2.  As you can see, not just data is cached, L1 is divided into code instruction cache (code instruction) (L1I) and data cache (L1D).  The memory areas needed for code and data are independent of each other.  In general, L1I creates far fewer problems than L1D. <br><br>  In terms of latency, L1 is orders of magnitude faster than L2, which <b>is 10 times faster than</b> main memory.  It looks sad in numbers, but not for every <b>slip of the cache</b> you have to pay the full price.  You can reduce costs by hiding latency, dispatching, and so on, but this is beyond the scope of the post. <br><br><img src="https://habrastorage.org/files/9c8/614/376/9c8614376cb54394adb98c1b0f584de6.gif"><br>  <i>Memory Delay - Andreas Fredriksson</i> <br><br>  Each entry in the cache - the <b>cache line</b> - contains several adjacent words (64 bytes for AMD Jaguar or Core i7).  When the CPU executes the instruction that retrieves or saves the value, the entire cache line is transferred to L1D.  In the case of saving, the cache line to which an entry is made is marked as <b>dirty</b> until it is written back to RAM. <br><br><img src="https://habrastorage.org/files/0fb/247/8e2/0fb2478e22974149859c4c5866b7c097.png"><br>  <i>Write from register to memory</i> <br><br>  To be able to load new data into the cache, it is almost always necessary to first free up space by <b>evicting the (evict) cache line</b> . <br><br><ul><li>  <b>Exclusive cache:</b> when retrieving, the cache line is moved from L1D to L2.  This means that space must be allocated in L2, which may lead to the transfer of data back to main memory.  Moving a retrieved string from L1D to L2 affects the latency of a cache miss. </li><li>  <b>Inclusive cache:</b> each cache line in L1D is also represented in L2.  Extraction from L1D is much faster and requires no further action. </li></ul><br>  Fresh processors from Intel and AMD use <b>inclusive cache</b> .  At first, this may seem like a wrong decision, but it has two advantages: <br><br><ul><li>  It reduces latency when a cache misses, since there is no need to move the cache line to another level when retrieving. </li><li>  If one kernel needs data with which another kernel works, then it can extract the latest version from the upper levels of the cache without interrupting the work of another kernel.  Therefore, inclusive cache has become very popular with the development of multicore architectures. </li></ul><br>  <b>Cash line collisions:</b> although several cores can efficiently read cache lines, write operations can lead to poor performance.  False sharing means that different cores can change independent data in the same cache line.  According to cache coherence protocols, if the kernel writes to the cache line, then a line in another kernel referring to the same memory is considered invalid ( <b>cache slip</b> , cache trashing).  As a result, at each write operation memory locks occur.  False separation can be avoided by making different kernels work with different strings (using extra space - extra padding, aligning the structures with 64 bytes, and so on). <br><br><img src="https://habrastorage.org/files/ac4/f3f/6be/ac4f3f6be1994624b0574678b89b84ea.png"><br>  <i>We avoid false division by writing data to different cache lines in each thread.</i> <br><br>  As you can see, understanding the hardware architecture is the key to finding and fixing problems that might otherwise go unnoticed. <br><br>  Coreinfo is a command-line utility.  It provides detailed information about all instruction sets stored in the processor, as well as reports which caches are assigned to each logical processor.  Here is an example for a Core i5-3570K: <br><br><pre><code class="html hljs xml">*--- Data Cache 0, Level 1, 32 KB, Assoc 8, LineSize 64 *--- Instruction Cache 0, Level 1, 32 KB, Assoc 8, LineSize 64 *--- Unified Cache 0, Level 2, 256 KB, Assoc 8, LineSize 64 **** Unified Cache 1, Level 3, 6 MB, Assoc 12, LineSize 64 -*-- Data Cache 1, Level 1, 32 KB, Assoc 8, LineSize 64 -*-- Instruction Cache 1, Level 1, 32 KB, Assoc 8, LineSize 64 -*-- Unified Cache 2, Level 2, 256 KB, Assoc 8, LineSize 64 --*- Data Cache 2, Level 1, 32 KB, Assoc 8, LineSize 64 --*- Instruction Cache 2, Level 1, 32 KB, Assoc 8, LineSize 64 --*- Unified Cache 3, Level 2, 256 KB, Assoc 8, LineSize 64 ---* Data Cache 3, Level 1, 32 KB, Assoc 8, LineSize 64 ---* Instruction Cache 3, Level 1, 32 KB, Assoc 8, LineSize 64 ---* Unified Cache 4, Level 2, 256 KB, Assoc 8, LineSize 64</code> </pre> <br>  Here the 32 KB L1 cache, 32 KB L1 cache, 256 KB L2 cache, and 6 MB L3 cache.  In this architecture, L1 and L2 are assigned to each core, and L3 is shared by all cores. <br><br>  In the case of AMD Jaguar CPU, each core has a dedicated L1 cache, and L2 is shared by groups of 4 cores in clusters (there is no L3 in the Jaguar). <br><br><img src="https://habrastorage.org/files/522/af2/6ce/522af26ce4994ddebcd3c252f710b0c1.png"><br>  <i>4-core cluster (AMD Jaguar)</i> <br><br>  When working with such clusters, special care should be taken.  When a kernel writes to a cache line, it may become invalid in other kernels, which reduces performance.  Moreover, with such an architecture, everything can become even worse: the kernel extracting data from the nearest L2 located in the same cluster <a href="http://www.agner.org/optimize/microarchitecture.pdf">takes about 26 cycles</a> , and retrieving another cluster from L2 can <a href="https://www.youtube.com/watch%3Fv%3Df8XdvIO8JxE">take up to 190 cycles</a> .  Comparable to the extraction of data from RAM! <br><br><img src="https://habrastorage.org/files/e0a/12f/f41/e0a12ff410d149b581c886653f64c85f.png"><br>  <i>L2 latency in clusters in AMD Jaguar - Jason Gregory</i> <br><br>  For more information on cache consistency, see the <a href="https://fgiesen.wordpress.com/2014/07/07/cache-coherency/">Cache Coherency Primer</a> article. <br><br><h1>  Assembly Basics </h1><br><h3>  x86-64 bits, x64, IA-64, AMD64 ... or the birth of the x64 architecture </h3><br>  Intel and AMD have developed their own 64-bit architectures: AMD64 and IA-64.  IA-64 is very different from x86-32 processors in the sense that it does not inherit anything from the x86 architecture.  Applications for x86 should work on IA-64 through the emulation layer, therefore, they have poor performance on this architecture.  Due to lack of compatibility with x86, IA-64 never took off, except for the commercial sphere.  On the other hand, AMD has created a more conservative architecture, expanding its existing x86 with a new set of 64-bit instructions.  Intel, who <a href="http://www.pcmag.com/article.aspx/curl/2339629">lost the 64-bit war</a> , was forced to introduce the same extensions into <a href="https://en.wikipedia.org/wiki/Itanium">its x86 processors</a> .  In this section, we look at x86-64 bits, also known as x64 architecture, or AMD64. <br><br>  For many years, PC programmers have used x86 assembler to write high-performance code: <a href="http://www.drdobbs.com/parallel/graphics-programming-black-book/184404919">mode'X '</a> , CPU-Skinning, collisions, software rasterizers (software rasterizers) ... But 32-bit computers were slowly replaced by 64-bit ones, and the assembler code also changed . <br><br>  Know the assembler is necessary if you want to understand why some things work slowly and others quickly.  It will also help you understand how to use intrinsic- <b>functions</b> to optimize critical parts of the code, and how to debug optimized (for example, -O3) code when debugging at the source code level no longer makes sense. <br><br><h3>  Registers </h3><br>  Registers are small fragments of very fast memory with an almost zero delay (usually one processor cycle).  They are used as internal processor memory.  They store data directly processed by processor instructions. <br><br>  The x64 processor has 16 general-purpose registers (GPR).  They are not used to store specific data types, and at the time of execution they contain operands and addresses. <br><br>  In x64, eight x86 registers are expanded to 64 bits, and 8 new 64-bit registers are added.  The names of 64-bit registers begin with r.  For example, the 64-bit extension <b>eax</b> (32-bit) is called <b>rax</b> .  New registers are named from <b>r8</b> to <b>r15</b> . <br><br><img src="https://habrastorage.org/files/ca9/839/131/ca98391311f84f989960a1253d24431a.jpg"><br>  <i>General architecture (software.intel.com)</i> <br><br>  The x64 registers include: <br><br><ul><li>  16 64-bit general purpose registers (GPR), of which the first eight are called rax, rbx, rcx, rdx, rbp, rsi, rdi and rsp.  The second eight: r8 ‚Äî r15. </li><li>  8 64-bit MMX registers (a set of MMX instructions) covering the floating-point registers fpr (x87 FPU). </li><li>  16 128-bit vector XMM-registers (set of SSE instructions). </li></ul><br>  In newer processors: <br><br><ul><li>  256-bit YMM registers (set of AVX instructions) that extend XMM registers. </li><li>  512-bit ZMM-registers (set of AVX-512 instructions), extending XMM-registers and increasing their number to 32. </li></ul><br><img src="https://habrastorage.org/files/80e/76c/2f5/80e76c2f57fa45a69bc0cea90975f6f8.png"><br>  <i>Relationships between ZMM, YMM, and XMM Registers</i> <br><br>  For historical reasons, several GPRs are called differently.  For example, <b>ax</b> was an Accumulator register, <b>cx</b> - Counter, <b>dx</b> - Data.  Today, most of them have lost their specific purpose, with the exception of <b>rsp</b> (Stack Pointer) and <b>rbp</b> (Base Pointer), which are reserved for managing the <b>hardware stack</b> (hardware stack) (although <b>rbp</b> can often be ‚Äúoptimized‚Äù and used as a GRP - omit frame pointer in Clang). <br><br>  The low bits of the x86 registers can be accessed using <b>subregisters</b> .  In the case of the first eight x86 registers, legacy names are used.  Newer registers (r8 ‚Äî r15) use the same, only simplified approach: <br><br><img src="https://habrastorage.org/files/4b8/4f5/2dd/4b84f52dd9104eafa10e080873d39c41.png"><br>  <i>Named scalar registers</i> <br><br><h3>  Addressing </h3><br>  When assembler instructions require two operands, usually the first is the destination, and the second is the source.  Each of them contains the data to be processed, or the address of the data.  There are three main addressing modes: <br><br><ul><li>  <i>Immediate</i> <br><ul><li>  <b>mov eax, 4</b> ;  moves 4 to eax </li></ul></li><li>  <i>From register to register</i> <br><ul><li>  <b>mov eax, ecx</b> ;  moves the contents of ecx to eax </li></ul></li><li>  <i>Indirect:</i> <br><ul><li>  <b>mov eax, [ebx]</b> ;  moves 4 bytes (eax size) to ebx address in eax </li><li>  <b>mov byte ptr [rcx], 5</b> ;  moves 5 to <i>byte</i> at rcx </li><li>  <b>mov rdx, dword ptr [rcx + 4 * rax]</b> ;  moves dword to rcx + 4 * rax to rdx </li></ul></li></ul><br>  dword ptr is called a size directive.  It tells the assembler what size to take if there is an uncertainty in the size of the memory area to which it is referenced (for example: <b>mov [rcx]</b> , 5: should write byte? Dword?). <br>  This can mean: byte (8-bit), word (16-bit), dword (32-bit), qword (64-bit), xmmword (128-bit), ymmword (256-bit), zmmword (512- bit). <br><br><h3>  SIMD instruction sets </h3><br>  Scalar implementation refers to operations with one pair of operands at a time.  <b>Vectorization</b> is the process of transforming an algorithm, when instead of working with single pieces of data at a time, it begins to process several portions at a time (below we will see how it does it). <br><br>  Modern processors can take advantage of a <a href="https://ru.wikipedia.org/wiki/SIMD">set of SIMD instructions</a> (vector instructions) for parallel data processing. <br><br><img src="https://habrastorage.org/files/c1e/4c0/0f0/c1e4c00f0d3941198fb25cf9e9086c35.png"><br>  <i>SIMD processing</i> <br><br>  SIMD instruction sets, which are available in x86 processors: <br><br><ul><li>  Multimedia eXtension (MMX) <br><ul><li>  Legacy  It supports arithmetic operations on integer values ‚Äã‚Äãpacked in 64-bit vector registers. </li></ul></li><li>  Streaming SIMD Extensions (SSE) <br><ul><li>  Arithmetic operations on floating-point numbers packed in 128-bit vector registers.  Integer and double precision support has been added to SSE2. </li></ul></li><li>  Advanced Vector Extensions (AVX) - x64 only <br><ul><li>  Added support for 256-bit vector registers. </li></ul></li><li>  AVX-512 - x64 only <br><ul><li>  Added support for 512-bit vector registers. </li></ul></li></ul><br><img src="https://habrastorage.org/files/d88/0af/1ce/d880af1ce0b5444fa89837bde91891ee.png"><br><br>  <i>Vector registers in x64 processors</i> <br><br>  Game engines typically spend 90% of their execution time on running small portions of the codebase, mainly iterating and processing data.  In such scenarios, SIMD can make a big difference.  SSE instructions are usually used for parallel processing of sets of four floating point values ‚Äã‚Äãpacked into 128-bit vector registers. <br><br>  SSE is mainly focused on the vertical representation (structure of arrays - Structure of Arrays, SoA) of data and their processing.  But in general, the performance of <a href="https://en.wikipedia.org/wiki/AOS_and_SOA">SoA compared to Array of Structures (AoS)</a> depends on the memory access patterns. <br><br><ul><li>  <b>AoS</b> is probably the most natural option, easy to write.  Satisfies the OOP paradigm. </li><li>  <b>AoS has</b> better data locality if all members are accessed together. </li><li>  <b>SoA</b> offers more possibilities for vectorization (vertical processing). </li><li>  <b>SoA</b> often uses less memory due to padding only between arrays. </li></ul><br><pre> <code class="html hljs xml"> // Array Of Structures struct Sphere { float x; float y; float z; double r; }; Sphere* AoS;    (   8 ): ------------------------------------------------------------------ | x | y | z | r | pad | x | y | z | r | pad | x | y | z | r | pad ------------------------------------------------------------------ // Structure Of Arrays struct SoA { float* x; float* y; float* z; double* r; size_t size; };   : ------------------------------------------------------------------ | x | x | x ..| pad | y | y | y ..| pad | z | z | z ..| pad | r.. ------------------------------------------------------------------</code> </pre><br>  AVX is a natural extension of SSE.  The size of vector registers is increased to 256 bits, which means that up to 8 floating point numbers can be packed and processed in parallel.  Intel processors initially support 256-bit registers, and there may be problems with AMD.  AMD's early AVX processors, such as Bulldozer and Jaguar, decompose 256-bit operations on 128-bit pairs, which increases latency compared to SSE. <br><br>  In conclusion, I‚Äôll say that it‚Äôs not so easy to rely solely on AVX (maybe for internal tools, if your computers run on Intel), and AMD processors for the most part do not support them natively.  On the other hand, on any x64 processors, you can a priori rely on SSE2 (this is part of the specification). <br><br><h3>  Extraordinary performance </h3><br>  If the pipeline (pipeline) of the processor is operating in <a href="https://ru.wikipedia.org/wiki/%25D0%2592%25D0%25BD%25D0%25B5%25D0%25BE%25D1%2587%25D0%25B5%25D1%2580%25D0%25B5%25D0%25B4%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25B8%25D1%2581%25D0%25BF%25D0%25BE%25D0%25BB%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">an extraordinary execution</a> mode (Out-of-Order, OoO) and the execution of instructions is delayed due to the unavailability of the necessary input data, then the processor tries to find later instructions whose input data are ready to perform first out of turn. <br><br>  The instruction cycle (or the cycle ‚Äúreceiving - decoding - execution‚Äù) is the process during which the processor receives instructions from the memory, determines what needs to be done with it, and executes it.  The execution cycle of the command in the extraordinary execution mode looks like this: <br><br><ul><li>  <b>Receive /</b> Decode <b>: The</b> instruction is retrieved from L1I (instruction cache).  It is then transformed into smaller operations called <b>microoperations</b> , or ¬µops. </li><li>  <b>Renaming:</b> due to existing dependencies between the register and the data, execution lock may occur.  To solve this problem and eliminate false dependencies, the processor provides a set of unnamed internal registers used for current calculations.  Register renaming is the process of converting references to <b>architectural registers</b> (logical) into references to <b>unnamed registers</b> (physical). </li><li>  <b>Reorder Buffer:</b> it contains pending micro-operations, stored in the order of receipt, as well as already completed, but not yet <b>retired</b> . </li><li>  <b>Dispatching:</b> micro-operations stored in the reordering buffer can be transferred in any order to <b>parallel execution modules</b> , taking into account dependencies and data availability.  The result of the micro-operation is written back to the reordering buffer along with the micro-operation itself. </li><li>  <b>Firing:</b> a retirement unit constantly checks the status of micro-operations in the buffer, writes the results of the executed micro-operations back to the architectural registers (accessible to the user), and then removes the micro-operations from the buffer. </li></ul><br><img src="https://habrastorage.org/files/def/2e9/b0f/def2e9b0fb6a47ecb2cfa6f256f68082.png"><br>  <i>AMD Jaguar processor architecture</i> <br><br>  In the AMD Jaguar processor architecture, we can detect all the above blocks.  For integer conveyor: <br><br><ul><li>  "Decode and Microcode ROMs" <br><ul><li>  = receive / decode module </li></ul></li><li>  "Int Rename" and "Int PRF" (physical register file) <br><ul><li>  = rename module </li><li>  The Retirement Control Unit (RCU), not shown here, controls the renaming of registers and the retirement of micro-operations. </li></ul></li><li>  Dispatchers <br><ul><li>  Internal dispatcher (Int Scheduler, ALU) <br><ul><li>  It can transfer one micro-operation to the conveyor (two ALU-modules of execution I0 and I1) in an extraordinary order. </li></ul></li><li>  AGU dispatcher (loading / storage) <br><ul><li>  It can transfer one micro-operation to the conveyor (two AGU-modules of the LAGU b SAGU version) in an extraordinary order. </li></ul><br></li></ul></li></ul><br>  Examples of micro-operations: <br><br><pre> <code class="html hljs xml"> ¬µops add reg, reg 1: add add reg, [mem] 2: load, add addpd xmm, xmm 1: addpd addpd xmm, [mem] 2: load, addpd</code> </pre> <br>  Looking at the AMD Jaguar section in the wonderful <a href="http://www.agner.org/optimize/instruction_tables.pdf">instruction table on the Agner website</a> , we can understand what the execution pipeline for this code looks like: <br><br><pre> <code class="html hljs xml">  mov eax, [mem1] ; 1 - load imul eax, 5 ; 2 - mul add eax, [mem2] ; 3 - load, add mov [mem3], eax ; 4 - store   (Jaguar) I0 | I1 | LAGU | SAGU | FP0 | FP1 | | 1-load | | | 2-mul | | 3-load | | | | 3-add | | | | | | | 4-store | |</code> </pre> <br>  Here, breaking instructions in micro-operations allow the processor to take advantage of parallel execution modules, partially or completely ‚Äúhiding‚Äù the delay in executing the instruction ( <code>3-load</code> and <code>2-mul</code> are executed in parallel, in two different modules). <br><br>  But this is not always possible.  The chain of dependencies between <code>2-mul</code> , <code>3-add</code> and <code>4-store</code> does not allow the processor to reorganize these micro-operations ( <code>4-store</code> needs the result <code>3-add</code> , and <code>3-add</code> needs the result <code>2-mul</code> ).  So for efficient use of parallel execution modules, avoid long chains of dependencies. <br><br><h3>  Visual Studio Options </h3><br>  To illustrate the compiler-generated assembler, I will use msvc ++ 14.0 (VS2015) and Clang.  I strongly recommend that you do the same and get used to comparing different compilers.  This will help to better understand how all the components of the system interact with each other, and form their own opinion about the quality of the generated code. <br><br>  <u>A few goodies:</u> <br><br><ul><li>  The Show Symbol Names option can show the names of local variables and functions in disassembled form, instead of instructions or stack addresses. <br><br><img src="https://habrastorage.org/files/3fe/775/2aa/3fe7752aa5df402584c23fed3c27cea3.png"><br></li><li>  Make the assembler more readable: <br><ul><li>  <i>Project settings&gt; C / C ++&gt; Code Generation&gt; Basic Runtime Checks</i> , change the value to <b>Default</b> . </li></ul></li><li>  Write the result to the .asm file: <br><ul><li>  <i>Project settings&gt; C / C ++&gt; Output Files&gt; Assembler Output</i> , make the value <b>Assembly With Source Code</b> . </li></ul></li><li>  Lowering the frame pointer (Frame-Pointer omission) tells the compiler that <b>ebp</b> should not be used to control the stack: <br><ul><li>  <i>/ Oy (x86 only, in Clang: -fomit-frame-pointer, works in x64)</i> </li></ul></li></ul><br><h3>  Basic examples of disassembling </h3><br>  Here we look at very simple C ++ code samples and their disassembly.  All assembler code is reorganized and fully documented to make it easier for beginners, but I recommend <a href="http://www.intel.ca/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-instruction-set-reference-manual-325383.pdf">checking</a> if you have any doubts about what the instructions are doing. <br><br>  For ease of perception, prologues and epilogues of functions have been removed; we will not discuss them here. <br><br>  <u><b>Note:</b></u> local variables are declared on the stack.  For example, <b><i>mov dword ptr [rbp + 4], 0Ah;</i></b>  <b><i>int b = 10</i></b> means that the local variable 'b' is pushed onto the stack (referenced by rbp) at a relative address (offset) 4 and initialized as 0Ah, or 10 in decimal expression. <br><br>  <u><b>Floating point arithmetic with simple precision</b></u> <br><br>  Floating-point arithmetic can be performed using x87 FPU (80-bit precision, scalar) or SSE (32-bit or 64-bit precision, vectorized).  In x64, the SSE2 instruction set is always supported, and <a href="http://stackoverflow.com/questions/22710272/difference-in-floating-point-arithmetics-between-x86-and-x64">by default</a> it is used for floating-point arithmetic. <br><br><img src="https://habrastorage.org/files/cc2/807/0c3/cc28070c39d34b33b17081e7176a0ef3.png"><br><br>  <i>Simple floating point arithmetic using SSE.</i>  <i>msvc ++</i> <br><br>  <i>Initialization</i> <br><br><ul><li>  <b>movss xmm0</b> , dword ptr [adr];  loads floating point value located at adr address in xmm0 </li><li>  <b>movss</b> dword ptr <b>[rbp], xmm0</b> ;  saves it to the stack (float x) </li><li>  ...;  same with y and z </li></ul><br>  <i>Calculates x * x</i> <br><br><ul><li>  <b>movss xmm0</b> , dword ptr <b>[rbp]</b> ;  loads scalar x to xmm0 </li><li>  <b>mulss xmm0</b> , dword ptr <b>[rbp]</b> ;  multiplies xmm0 (= x) by x </li></ul><br>  <i>Calculates y * y and adds to x * x</i> <br><br><ul><li>  <b>movss xmm1</b> , dword ptr <b>[rbp + 4]</b> ;  loads scalar y to xmm1 </li><li>  <b>mulss xmm1</b> , dword ptr <b>[rbp + 4]</b> ;  multiplies xmm1 (= y) by y </li><li>  <b>addss xmm0, xmm1</b> ;  adds xmm1 (y * y) to xmm0 (x * x) </li></ul><br>  <i>Calculates z * z and adds to x * x + y * y</i> <br><br><ul><li>  <b>movss xmm1</b> , dword ptr [rbp + 8];  loads scalar z into xmm1 </li><li>  <b>mulss xmm1</b> , dword ptr [rbp + 8];  multiplies xmm1 (= z) by z </li><li>  <b>addss xmm0, xmm1</b> ;  adds xmm1 (z * z) to xmm0 (x * x + y * y) </li></ul><br>  <i>Saves the final result</i> <br><br><ul><li>  <b>movss</b> dword ptr [ <b>rbp</b> + 0Ch], <b>xmm0</b> ;  saves xmm0 to the result </li><li>  <b>xor eax, eax</b> ;  eax = 0. eax contains the return value of main () </li></ul><br>  In this example, XMM registers are used to store a single floating point value.  SSE allows you to work with both single and multiple values, with different data types.  Look at the SSE addition instruction: <br><br><ul><li>  <b>addss xmm0, xmm1</b> ;  each register as 1 scalar floating point value with single precision ( <u>s</u> calar ingle precision floating-point value) </li><li>  <b>addps xmm0, xmm1</b> ;  each register as 4 packed floating point values ‚Äã‚Äãwith single precision ( <u>p</u> acked <u>s</u> ingle precision floating-point values) </li><li>  <b>addsd xmm0, xmm1</b> ;  each register as a double-precision scalar value with double precision ( <u>s</u> calar <u>d</u> ouble precision floating-point value) </li><li>  <b>addpd xmm0, xmm1</b> ;  each register as 2 packed double-precision floating-point values ‚Äã‚Äã( <u>p</u> acked <u>d</u> double precision floating-point values) </li><li>  <b>paddd xmm0, xmm1</b> ;  each register as 4 packed dword-values ‚Äã‚Äã( <u>p</u> acked <u>d</u> ouble word (32-bit integer) values) </li></ul><br>  <u><b>Branching</b></u> <br><img src="https://habrastorage.org/files/2dd/4f4/e24/2dd4f4e2409e4429b3c1cdd7bb2241e6.png"><br>  <i>An example of branching.</i>  <i>msvc ++</i> <br><br>  <i>Initialization</i> <br><br><ul><li>  <b>mov</b> dword ptr [ <b>rbp</b> ], 5;  saves 5 to the stack (integer a) </li><li>  <b>mov</b> dword ptr [ <b>rbp</b> +4], 0Ah;  saves 10 to the stack (integer b) </li><li>  <b>mov</b> dword ptr [ <b>rbp</b> +8], 0;  saves 0 to the stack (integer result) </li></ul><br>  <i>Condition</i> <br><br><ul><li>  <b>mov eax</b> , dword ptr [ <b>rbp</b> +4];  loads b into eax </li><li>  <b>cmp</b> dword ptr [ <b>rbp</b> ], <b>eax</b> ;  compares a with eax (b) </li><li>  <b>jge</b> @ ECF81536;  makes the transition if a is greater than or equal to b </li></ul><br>  <i>'then' result = a</i> <br><br><ul><li>  <b>mov eax</b> , dword ptr [ <b>rbp</b> ];  loads a into eax </li><li>  <b>mov</b> dword ptr [ <b>rbp</b> +8], <b>eax</b> ;  saves eax to stack (result) </li><li>  <b>jmp</b> @ ECF8153C;  moves to ECF8153C </li></ul><br>  <i>'else' result = b</i> <br><br><ul><li>  <b>(ECF81536) mov eax</b> , dword ptr [ <b>rbp</b> +4];  loads b into eax </li><li>  mov dword ptr [rbp + 8], eax;  saves eax to stack (result) </li><li>  <b>(ECF8153C)</b> xor eax, eax;  eax = 0. eax contains the return value of main () </li></ul><br>  The <b>cmp</b> instruction compares the operand of the first source with the second, <a href="http://www.godevtool.com/GoasmHelp/usflags.htm">sets the status flags</a> in the <b>RFLAGS</b> register according to the result.  Register <b>¬ÆFLAGS</b> is a register of the status of x86-processors, containing the current state of the processor.  The <b>cmp</b> instruction is usually used in conjunction with a conditional <b>branch</b> (for example, <b>jge</b> ).  The condition codes used by the transitions depend on the result of the <b>cmp</b> instruction ( <b>RFLAGS</b> condition codes). <br><br>  <u><b>Arithmetic operations with integer and 'for' loop</b></u> <br><br>  In assembly language, cycles are represented mainly as a series of conditional jumps (= if ... goto). <br><br><img src="https://habrastorage.org/files/47f/67f/79a/47f67f79af654e63b8e2721495f98ffa.png"><br><br>  <i>Arithmetic operations with integer and 'for' loop.</i>  <i>msvc ++</i> <br><br>  <i>Initialization</i> <br><br><ul><li>  <b>mov</b> dword ptr [ <b>rbp</b> ], 0;  saves 0 to the stack (integer sum) </li><li>  <b>mov</b> dword ptr [k], 0Ah;  saves 10 to the stack (integer k) </li><li>  <b>mov</b> dword ptr [ <b>rbp</b> +8], 0;  saves 0 to the stack (integer i) for iteration in a loop </li><li>  <b>jmp</b> main + 30h;  moves to <b>main + 30h</b> </li></ul><br>  <i>The part of the code responsible for incrementing i</i> <br><br><ul><li>  <b>(main + 28h) mov eax</b> , dword ptr [ <b>rbp</b> +8];  loads i into eax </li><li>  <b>inc eax</b> ;  increment </li><li>  <b>mov</b> dword ptr [ <b>rbp</b> +8], <b>eax</b> ;  saves back to the stack </li></ul><br>  <i>The part of the code responsible for testing the exit condition (i&gt; = k)</i> <br><br><ul><li>  <b>(main + 30h) mov eax</b> , dword ptr [k];  loads k from the stack to eax </li><li>  <b>cmp</b> dword ptr [ <b>rbp</b> +8], <b>eax</b> ;  compare i with eax (= k) </li><li>  <b>jge</b> main + 47h;  makes the transition (ends the cycle) if i is greater than or equal to k </li></ul><br>  <i>‚ÄúReal Work‚Äù: sum + = i</i> <br><br><ul><li>  <b>mov eax</b> , dword ptr [ <b>rbp</b> +8];  loads i into eax </li><li>  <b>mov ecx</b> , dword ptr [ <b>rbp</b> ];  loads the amount into ecx </li><li>  <b>add ecx, eax</b> ;  add eax with ecx (ecx = sum + i) </li><li>  <b>mov eax, ecx</b> ;  transfers ecx to eax </li><li>  <b>mov</b> dword ptr <b>[rbp], eax</b> ;  saves eax (amount) back to stack </li><li>  <b>jmp</b> main + 28h;  makes the transition and processes the next iteration of the loop </li><li>  <b>(main + 47h) xor eax, eax</b> ;  eax = 0. eax contains the return value of main () </li></ul><br><br>  <u><b>Built-in functions (intrinsics) SSE</b></u> <br><br>      ,   SSE        (   ‚Äî  ).  ,  <b> </b>      : <br><br><ul><li> <b>_mm_mul_ps</b>  <b>mulps</b> </li><li> <b>_mm_load_ps</b>  <b>movaps</b> </li><li> <b>_mm_add_ps</b>  <b>addps</b> </li><li> <b>_mm_store_ps</b>  <b>movaps</b> </li></ul><br><img src="https://habrastorage.org/files/f6f/7a9/153/f6f7a91536bd45fe96a4297c5158ffd2.jpg"><br><br> <i>  SSE, msvc++</i> <br><br> <i> (xmmword   128     dword)</i> <br><br><ul><li> <b>(main+340h) movaps xmm1</b> , xmmword ptr <b>[rdx+rax]</b> ;  128- xmmword (    )   xs+i  xmm1 </li><li> <b>movaps xmm3</b> , xmmword ptr <b>[rax]</b> ;  4       ys+i  xmm3 </li><li> <b>movaps xmm0</b> , xmmword ptr <b>[r8+rax]</b> ;  4       zs+i  xmm0 </li><li> <b>movaps xmm2</b> , xmmword ptr <b>[r9+rax]</b> ;  4       ws+i  xmm2 </li></ul><br> <i> <b>dot(v[i], A) = xi * Ax + yi * Ay + zi * Az + wi * Aw</b> ,   (vertices)  :</i> <br><br><ul><li> <b>mulps xmm1, xmm4</b> ; xmm1 *= xmm4 xn.Ax, n [0..3] </li><li> <b>mulps xmm3, xmm5</b> ; xmm3 *= xmm5 yn.Ay, n [0..3] </li><li> <b>mulps xmm0, xmm6</b> ; xmm0 *= xmm6 zn.Az, n [0..3] </li><li> <b>mulps xmm2, xmm7</b> ; xmm2 *= xmm7 wn.Aw, n [0..3] </li><li> <b>addps xmm3, xmm1</b> ; xmm3 += xmm1 xn.Ax + yn.Ay </li><li> <b>addps xmm2, xmm0</b> ; xmm2 += xmm0 zn.Az + wn.Aw </li><li> <b>addps xmm2, xmm3</b> ; xmm2 += xmm3 xn.Ax + yn.Ay + zn.Az + wn.Aw </li></ul><br> <i>     ( + )    </i> <br><br><ul><li> <b>movaps</b> xmmword ptr <b>[r10 + rax], xmm2</b> ;  128- xmmword (4    )  ,    r10+rax </li><li> <b>add rax</b> , 10h;  16  rax (  =  4    ) </li><li> <b>sub r11,1</b> ; r11‚Äì,    </li><li> <b>jne</b> main+34h;        </li></ul><br>        AVX (256-,  8       ): <br><br><pre> <code class="html hljs xml">_m256 Ax = _mm256_broadcast_ss(A); ... for (int i = 0; i <span class="hljs-tag"><span class="hljs-tag">&lt; </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">vertexCount</span></span></span><span class="hljs-tag">; </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">i</span></span></span><span class="hljs-tag">+=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">8)</span></span></span><span class="hljs-tag"> // </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">8</span></span></span><span class="hljs-tag">     (</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">256-</span></span></span><span class="hljs-tag">) { </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">__m256</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">x4</span></span></span><span class="hljs-tag"> = </span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">_mm256_load_ps(xs</span></span></span><span class="hljs-tag"> + </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">i</span></span></span><span class="hljs-tag">); </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">..</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">__m256</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">dx</span></span></span><span class="hljs-tag"> = </span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">_mm256_mul_ps(Ax,</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">x4</span></span></span><span class="hljs-tag">); </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">..</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">__m256</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">a0</span></span></span><span class="hljs-tag"> = </span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">_mm256_add_ps(dx,</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">dy</span></span></span><span class="hljs-tag">); </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">..</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">_mm256_store_ps</span></span></span><span class="hljs-tag">(</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">results</span></span></span><span class="hljs-tag"> + </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">i</span></span></span><span class="hljs-tag">, </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">dots</span></span></span><span class="hljs-tag">); }</span></span></code> </pre> <br><br> <u><b>   (switch)</b></u> <br><br><img src="https://habrastorage.org/files/c89/858/5bf/c898585bfe104ece8168c99f0306c826.png"><br><br> <i> . msvc++</i> <br><br> <i></i> <br><br><ul><li> <b>mov</b> dword ptr [ <b>rbp</b> ], 0;  0   ( ) </li><li> <b>mov eax</b> , dword ptr [argc];  argc  eax </li><li> <b>mov</b> dword ptr [ <b>rbp</b> +44h], <b>eax</b> ;     </li></ul><br>  <i>Conditions</i> <br><br><ul><li> <b>cmp</b> dword ptr [ <b>rbp</b> +44h], 0;  argc to 0 </li><li> <b>je</b> main+38h; if argc == 0,   main+38h (case 0) </li><li> <b>cmp</b> dword ptr [ <b>rbp</b> +44h], 1;  argc  1 </li><li> <b>je</b> main+41h; if argc == 1,   main+41h (case 1) </li><li> <b>cmp</b> dword ptr [ <b>rbp</b> +44h], 2;  argc  0 </li><li> <b>je</b> main+4Ah; if argc == 2,   main+4Ah (case 2) </li><li> <b>cmp</b> dword ptr [ <b>rbp</b> +44h], 3;  argc  3 </li><li> <b>je</b> main+53h; if argc == 3,   main+53h (case 3) </li><li> <b>jmp</b> main+5Ch;   main+5Ch ( ) </li></ul><br> <i>Case 0</i> <br><br><ul><li> <b>(main+38h) mov</b> dword ptr [ <b>rbp</b> ], 1;  1   (val) </li><li> <b>jmp</b> main+63h;   main+63h,     </li></ul><br> <i>Case 1</i> <br><br><ul><li> <b>(main+41h) mov</b> dword ptr [ <b>rbp</b> ], 3;  3   (val) </li><li> <b>jmp</b> main+63h;   main+63h,     </li></ul><br>  ... <br><ul><li> <b>(main+63h) xor eax, eax</b> ; eax = 0. eax    main() </li></ul><br>        .   ++-      if-else,     .                 . <br><br><h3>  useful links </h3><br><ul><li> <a href="https://people.freebsd.org/~lstewart/articles/cpumemory.pdf">What Every Programmer Should Know About Memory</a> </li><li> <a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/">Intel Instrinsics Guide</a> </li><li> <a href="http://www.realworldtech.com/jaguar/4/">Jaguar Out-of-Order Scheduling</a> </li><li> <a href="http://users.utcluj.ro/~baruch/book_ssce/SSCE-Intel-Pipeline.pdf">The Intel Architecture Processors Pipeline</a> </li><li> <a href="http://ref.x86asm.net/geek64.html">x86-64bit Opcode and Instruction Reference</a> </li><li> <a href="https://fgiesen.wordpress.com/2016/08/07/why-do-cpus-have-multiple-cache-levels/">Why do CPUs have multiple cache levels?</a> </li></ul><br><div style="text-align:center;"><img src="https://i.memecaptain.com/gend_images/av_TbA.gif"></div></div><p>Source: <a href="https://habr.com/ru/post/319194/">https://habr.com/ru/post/319194/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../319178/index.html">The vk library for working with the VK API in Python</a></li>
<li><a href="../319180/index.html">Handling button presses for Arduino. Cross OOP and ICA. Part 1</a></li>
<li><a href="../319184/index.html">Replacing delay () for non-blocking delays in the Arduino IDE</a></li>
<li><a href="../319186/index.html">Hackers attack MongoDB: the number of compromised systems has exceeded 27,000</a></li>
<li><a href="../319190/index.html">Atlassian buys Trello for $ 425 million</a></li>
<li><a href="../319196/index.html">How does the language support service for 1237 Russian IT engineers work, and how does this differ from conventional translation?</a></li>
<li><a href="../319198/index.html">The history of the development of Tower Defense for VK and Android</a></li>
<li><a href="../319200/index.html">Python: collections, part 2/4: indexing, slicing, sorting</a></li>
<li><a href="../319202/index.html">Pure Python Architecture: A Walkthrough. Part 2</a></li>
<li><a href="../319204/index.html">VulnHub: Parse the shortest quest DC416 Fortress</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>