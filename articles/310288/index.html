<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ClusterR clustering, part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article focuses on clustering, more precisely, my recently added ClusterR package to CRAN. The details and examples below are mostly based on the...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>ClusterR clustering, part 1</h1><div class="post__text post__text-html js-mediator-article">  This article focuses on clustering, more precisely, my recently added <b>ClusterR</b> package to CRAN.  The details and examples below are mostly based on the Vignette package. <br><br>  <a href="https://en.wikipedia.org/wiki/Cluster_analysis">Cluster analysis</a> or clustering is the task of grouping a set of objects so that the objects within one group (called a cluster) are more similar (in one sense or another) to each other than to objects in other groups (clusters).  This is one of the main tasks of the research analysis of data and the standard technique of statistical analysis used in various fields, including  machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics. <br><br>  The most well-known examples of clustering algorithms are <i>clustering based on connectivity</i> (hierarchical clustering), <i>clustering based on centers</i> (k-means method, k-medoid method), <i>distribution-based clustering</i> (GMM - Gaussian mixture models) and <i>clustering based on density</i> (DBSCAN - Density-based spatial clustering of applications with noise - spatial clustering of applications with noise based on density, OPTICS - ordering points to identify the clustering structure - ordering points to determine the clustering structure, etc.). <br><a name="habracut"></a><br>  The <b>ClusterR</b> package consists of clustering algorithms based on centers (k-means method, k-means method in mini-groups, k-medoids) and distributions (GMM).  The package also offers features for: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  validating output with truth labels; </li><li>  outputting results on a contour or two-dimensional graph; </li><li>  prediction of new observations; </li><li>  estimates of the optimal number of clusters for each individual algorithm. </li></ul><br><h4>  Gaussian mixture of distributions (GMM - Gaussian mixture models) </h4><br>  <a href="https://brilliant.org/wiki/gaussian-mixture-model/">Gaussian distribution mix</a> is a statistical model for representing normally distributed subpopulations within a general population.  The Gaussian mixture of distributions is parameterized by two types of values ‚Äã‚Äã‚Äî a mixture of the <i>weights of the components</i> and the <i>average components</i> or <i>covariances</i> (for the <i>multidimensional</i> case).  If the number of components is known, the technique most often used for estimating the parameters of a mixture of distributions is the EM algorithm. <br><br>  The <b>GMM</b> function in the <i>ClusterR</i> package is an implementation on the R class for modeling data as a Gaussian distribution mixture (GMM) from <a href="http://arma.sourceforge.net/docs.html">the Armadillo library</a> with the assumption of <i>diagonal</i> covariance matrices.  You can configure the parameters of the function, including <i>gaussian_comps</i> , <i>dist_mode</i> (eucl_dist, maha_dist), <i>seed_mode</i> (static_subset, random_subset, static_spread, random_spread), <i>km_iter,</i> and <i>em_iter</i> (more information about the parameters in the package documentation).  I illustrate the <i>GMM</i> function on synthetic data <i>dietary_survey_IBS</i> . <br><br><pre><code class="python hljs">library(ClusterR) data(dietary_survey_IBS) dim(dietary_survey_IBS)</code> </pre> <br><pre> <code class="diff hljs">## [1] 400 43</code> </pre><br><pre> <code class="python hljs">X = dietary_survey_IBS[, -ncol(dietary_survey_IBS)] <span class="hljs-comment"><span class="hljs-comment">#  (  ) y = dietary_survey_IBS[, ncol(dietary_survey_IBS)] #   dat = center_scale(X, mean_center = T, sd_scale = T) #     gmm = GMM(dat, 2, dist_mode = "maha_dist", seed_mode = "random_subset", km_iter = 10, em_iter = 10, verbose = F) #  ,     pr = predict_GMM(dat, gmm$centroids, gmm$covariance_matrices, gmm$weights)</span></span></code> </pre><br>  Initially, <i>GMM</i> returns <i>centroids</i> , <i>a covariance matrix</i> (where each row represents a diagonal covariance matrix), <i>weights,</i> and <i>log likelihood functions</i> for each Gaussian component.  The <i>predict_GMM</i> function <i>then</i> takes the output of the <i>GMM</i> model and returns possible clusters. <br><br>  In addition to the already mentioned functions, you can use <b>Optimal_Clusters_GMM</b> to estimate the number of data clusters using either the Akaike information criterion (AIC, Akaike information), or the Bayesian information Bayesian information criterion. <br><br><pre> <code class="python hljs">opt_gmm = Optimal_Clusters_GMM(dat, max_clusters = <span class="hljs-number"><span class="hljs-number">10</span></span>, criterion = <span class="hljs-string"><span class="hljs-string">"BIC"</span></span>, dist_mode = <span class="hljs-string"><span class="hljs-string">"maha_dist"</span></span>, seed_mode = <span class="hljs-string"><span class="hljs-string">"random_subset"</span></span>, km_iter = <span class="hljs-number"><span class="hljs-number">10</span></span>, em_iter = <span class="hljs-number"><span class="hljs-number">10</span></span>, var_floor = <span class="hljs-number"><span class="hljs-number">1e-10</span></span>, plot_data = T)</code> </pre> <br><img src="https://habrastorage.org/files/a36/e1c/dff/a36e1cdff6dd47018f36540ea8c66cb9.png"><br>  When choosing a model from a predefined set, it is preferable to choose the one with the lowest <a href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">BIC</a> , here it is true for the number of clusters equal to 2. <br><br>  Assuming that truth labels are available, you can use the <b>external_validation</b> methods ( <i>rand_index, adjusted_rand_index, jaccard_index, fowlkes_Mallows_index, mirkin_metric, purity, entropy, nmi</i> (normalized mutual information) and <i>var_info</i> (variation of information) to validate output clusters. <br><br><pre> <code class="python hljs">res = external_validation(dietary_survey_IBS$<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">pr</span></span></span><span class="hljs-class">$</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">cluster_labels</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">method</span></span></span><span class="hljs-class"> = "</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">adjusted_rand_index</span></span></span><span class="hljs-class">", </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">summary_stats</span></span></span><span class="hljs-class"> = </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">T</span></span></span><span class="hljs-class">) </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">res</span></span></span></span></code> </pre><br><pre> <code class="diff hljs">## ## ---------------------------------------- ## purity : 1 ## entropy : 0 ## normalized mutual information : 1 ## variation of information : 0 ## ---------------------------------------- ## specificity : 1 ## sensitivity : 1 ## precision : 1 ## recall : 1 ## F-measure : 1 ## ---------------------------------------- ## accuracy OR rand-index : 1 ## adjusted-rand-index : 1 ## jaccard-index : 1 ## fowlkes-mallows-index : 1 ## mirkin-metric : 0 ## ----------------------------------------</code> </pre><br>  And if the parameter <i>summary_stats is</i> set to TRUE, then metrics of specificity, sensitivity, accuracy, completeness, F-measures ( <i>specificity, sensitivity, precision, recall, F-measure,</i> respectively) are also returned. <br><br><h4>  K-medium method </h4><br>  <a href="https://en.wikipedia.org/wiki/K-means_clustering">K-means</a> clustering is a vector quantization method originally used in signal processing that is often used for cluster analysis of data.  The goal of k-means clustering is to divide <i>n</i> values ‚Äã‚Äãinto <i>k</i> clusters, in which each value belongs to a cluster with the closest average, which appears as a cluster prototype.  This leads to the division of the data area into <a href="https://en.wikipedia.org/wiki/Voronoi_diagram">Voronoi</a> cells.  The most commonly used algorithm uses an iterative refinement technique.  Due to its widespread usage, it is called the k-means algorithm;  in particular, among specialists in the field of computer science, it is also known as <a href="https://en.wikipedia.org/wiki/Lloyd%2527s_algorithm">the Lloyd's algorithm</a> . <br><br>  The ClusterR package provides two different k-means functions, <i>KMeans_arma</i> , an R-implementation of the k-means method from the armadillo library, and <i>KMeans_rcpp</i> , which uses the RcppArmadillo package.  Both functions lead to the same results, however, they return different signs (the code below illustrates this). <br><br><h5>  KMeans_arma </h5><br>  <i>KMeans_arma is</i> faster than the KMeans_rcpp function, however, it initially displays the centroids of only some clusters.  Moreover, the number of columns in the data must exceed the number of clusters, otherwise the function will return an error.  Clustering will run faster on multi-core machines with OpenMP enabled (for example, -fopenmp in GCC).  The algorithm is initialized once, and usually 10 iterations are enough for convergence.  The original centroids are distributed using one of the algorithms - <i>keep_existing, static_subset, random_subset, static_spread</i> or <i>random_spread</i> .  If seed_mode is equal to keep_existing, the user must pass in the matrix of the centroids. <br><br>  I will reduce the number of measurements in dietary_survey_IBS data using principal component analysis (PCA), namely, the <i>princomp</i> function from the <i>stats</i> package so that you can display a two-dimensional graph of the clusters built as a result. <br><br><pre> <code class="python hljs">pca_dat = stats::princomp(dat)$scores[, <span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">2</span></span>] km = KMeans_arma(pca_dat, clusters = <span class="hljs-number"><span class="hljs-number">2</span></span>, n_iter = <span class="hljs-number"><span class="hljs-number">10</span></span>, seed_mode = <span class="hljs-string"><span class="hljs-string">"random_subset"</span></span>, verbose = T, CENTROIDS = NULL) pr = predict_KMeans(pca_dat, km) table(dietary_survey_IBS$<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">pr</span></span></span><span class="hljs-class">) </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(km)</span></span></span><span class="hljs-class"> = '</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">matrix</span></span></span><span class="hljs-class">' </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">plot_2d</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(data = pca_dat, clusters = as.vector</span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params"><span class="hljs-params">(pr)</span></span></span></span><span class="hljs-class"><span class="hljs-params">, centroids_medoids = as.matrix</span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params"><span class="hljs-params">(km)</span></span></span></span><span class="hljs-class"><span class="hljs-params">)</span></span></span></span></code> </pre><br><img src="https://habrastorage.org/files/d11/802/78b/d1180278bdcf440bb60639726fe10f23.png"><br><h5>  KMeans_rcpp </h5><br>  As stated above, the <i>KMeans_rcpp</i> function offers some additional features compared to the <i>KMeans_arma</i> function: <br><br><ul><li>  it allows more than one initialization (can be parallelized with OpenMP) </li><li>  In addition to initialization with <i>optimal_init, quantile_init, random</i> or <i>kmeans ++</i> , centroids can be set in the CENTROIDS parameter </li><li>  the running time of the algorithm and its convergence can be adjusted by the parameters <i>num_init, max_iters</i> and <i>tol</i> </li><li>  if num_init&gt; 1, KMeans_rcpp returns the attributes of better initialization using the within-cluster-sum-of-squared-error criterion (sum-squared-errors-within-cluster) </li><li>  The algorithm returns the following attributes: <i>clusters, fuzzy_clusters</i> (if fuzzy = TRUE), <i>centroids, total_SSE, best_initialization, WCSS_per_cluster, obs_per_cluster, between.SS_DIV_total.SS</i> </li></ul><br>  More details about KMeans_rcpp are in the package documentation.  I will illustrate the different parameters of <i>KMeans_rcpp</i> using the example of <a href="https://en.wikipedia.org/wiki/Vector_quantization">vector quantization</a> and the OpenImageR package. <br><br><pre> <code class="python hljs">library(OpenImageR) path = <span class="hljs-string"><span class="hljs-string">'elephant.jpg'</span></span> im = readImage(path) <span class="hljs-comment"><span class="hljs-comment">#    ,     im = resizeImage(im, 75, 75, method = 'bilinear') imageShow(im) #    im2 = apply(im, 3, as.vector) #  RGB</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/files/02c/d25/415/02cd25415d8e4713baafb8d57bd3405e.png"></div><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   KMeans_rcpp km_rc = KMeans_rcpp(im2, clusters = 5, num_init = 5, max_iters = 100, initializer = 'optimal_init', threads = 1, verbose = F) km_rc$between.SS_DIV_total.SS</span></span></code> </pre><br><pre> <code class="diff hljs">## [1] 0.9873009</code> </pre><br>  The attribute <i>between.SS_DIV_total.SS</i> is <i>(total_SSE - sum (WCSS_per_cluster)) / total_SSE</i> .  If there is no pattern in clustering, the intermediate sum of squares will be a very small part of the total sum of squares, and if the attribute <i>between.SS_DIV_total.SS</i> is close to 1.0, the values ‚Äã‚Äãare clustered well enough. <br><br><pre> <code class="python hljs">getcent = km_rc$centroids getclust = km_rc$clusters new_im = getcent[getclust, ] <span class="hljs-comment"><span class="hljs-comment">#       dim(new_im) = c(nrow(im), ncol(im), 3) #      imageShow(new_im)</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a30/f8a/f3d/a30f8af3ded549e982cd45d2ec07aac3.png"></div><br>  In addition, you can use the <b>Optimal_Clusters_KMeans</b> feature (which implicitly uses KMeans_rcpp) to determine the optimal number of clusters.  The following criteria are available: <i>variance_explained, WCSSE</i> (within-cluster-sum-of-squared-error), <i>dissimilarity, silhouette, distortion_fK, AIC, BIC</i> and <i>Adjusted_Rsquared</i> .  The package documentation has more information on each criterion. <br><br>  The following code example uses the <i>distortion_fK</i> criterion, which is fully described in the article ‚ÄúSelection of K in K-means clustering, Pham., Dimov., Nguyen., (2004)‚Äù (‚ÄúChoice of K in K-means clustering‚Äù). <br><br><pre> <code class="python hljs">opt = Optimal_Clusters_KMeans(im2, max_clusters = <span class="hljs-number"><span class="hljs-number">10</span></span>, plot_clusters = T, verbose = F, criterion = <span class="hljs-string"><span class="hljs-string">'distortion_fK'</span></span>, fK_threshold = <span class="hljs-number"><span class="hljs-number">0.85</span></span>)</code> </pre> <br><img src="https://habrastorage.org/files/224/856/89a/22485689a6b94f01a3f20d7863cad08b.png"><br>  Values ‚Äã‚Äãless than a fixed limit (here fK_threshold = 0.85) can be recommended for clustering.  However, there is more than one optimal partitioning into clusters, so f (K) should be used only to assume their number, but the final decision on whether to accept this value or not is up to the user. <br><br><h4>  The k-means method in mini-groups </h4><br>  <a href="http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf">The k-means method in mini-groups</a> is a variation of the classic k-means algorithm.  It is especially useful for large data sets, because instead of using the entire set (as in k-means), mini-groups are taken from random data values ‚Äã‚Äãin order to optimize the objective function. <br><br>  The parameters of the <b>MiniBatchKmeans</b> algorithm <b>are</b> almost the same as the KMeans_rcpp functions from the ClusterR package.  The most important difference is the <i>batch_size</i> parameter (size of mini-groups) and <i>init_fraction</i> (the percentage of data for determining the initial centroids, which is used if the initializer is 'kmeans ++' or 'quantile_init'). <br><br>  I will use the example of <i>vector quantization</i> to show the difference in computation time and quality of the output of the KMeans_rcpp and MiniBatchKmeans functions. <br><br><pre> <code class="python hljs">path_d = <span class="hljs-string"><span class="hljs-string">'dog.jpg'</span></span> im_d = readImage(path_d) <span class="hljs-comment"><span class="hljs-comment">#    ,     im_d = resizeImage(im_d, 350, 350, method = 'bilinear') imageShow(im_d) #   </span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/files/9a8/bb6/c94/9a8bb6c949eb4fa3b889eff34e5c75fb.png"></div><br><pre> <code class="python hljs">im3 = apply(im_d, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">as</span></span>.vector) <span class="hljs-comment"><span class="hljs-comment">#  RGB dim(im3) #   </span></span></code> </pre><br><pre> <code class="diff hljs"># 122500 3</code> </pre><br>  First, perform <i>k-means</i> clustering. <br><br><pre> <code class="python hljs">start = Sys.time() km_init = KMeans_rcpp(im3, clusters = <span class="hljs-number"><span class="hljs-number">5</span></span>, num_init = <span class="hljs-number"><span class="hljs-number">5</span></span>, max_iters = <span class="hljs-number"><span class="hljs-number">100</span></span>, initializer = <span class="hljs-string"><span class="hljs-string">'kmeans++'</span></span>, threads = <span class="hljs-number"><span class="hljs-number">1</span></span>, verbose = F) end = Sys.time() t = end - start cat(<span class="hljs-string"><span class="hljs-string">'time to complete :'</span></span>, t, attributes(t)$units, <span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   : 2.44029 secs getcent_init = km_init$centroids getclust_init = km_init$clusters new_im_init = getcent_init[getclust_init, ] #       dim(new_im_init) = c(nrow(im_d), ncol(im_d), 3) #      imageShow(new_im_init)</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/files/5a0/fea/cb3/5a0feacb300a4cfc9157e7da1577d37d.png"></div><br>  Now <i>k-means</i> clustering <i>in mini-groups</i> . <br><br><pre> <code class="python hljs">start = Sys.time() km_mb = MiniBatchKmeans(im3, clusters = <span class="hljs-number"><span class="hljs-number">5</span></span>, batch_size = <span class="hljs-number"><span class="hljs-number">20</span></span>, num_init = <span class="hljs-number"><span class="hljs-number">5</span></span>, max_iters = <span class="hljs-number"><span class="hljs-number">100</span></span>, init_fraction = <span class="hljs-number"><span class="hljs-number">0.2</span></span>, initializer = <span class="hljs-string"><span class="hljs-string">'kmeans++'</span></span>, early_stop_iter = <span class="hljs-number"><span class="hljs-number">10</span></span>, verbose = F) pr_mb = predict_MBatchKMeans(im3, km_mb$centroids) end = Sys.time() t = end - start cat(<span class="hljs-string"><span class="hljs-string">'time to complete :'</span></span>, t, attributes(t)$units, <span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   : 0.8346727 secs getcent_mb = km_mb$centroids new_im_mb = getcent_mb[pr_mb, ] #       dim(new_im_mb) = c(nrow(im_d), ncol(im_d), 3) #      imageShow(new_im_mb)</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/files/0f2/31e/ef0/0f231eef017d4aa29934f1121bb271df.png"></div><br>  Despite the slight difference in output quality, k-means in mini-groups returns an average of two times faster than the classic k-means method for this data set. <br><br>  To implement the k-means method in mini-groups, I used the following resources: <br><br><ul><li>  <a href="http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf">www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf</a> </li><li>  <a href="https://github.com/siddharth-agrawal/Mini-Batch-K-Means">github.com/siddharth-agrawal/Mini-Batch-K-Means</a> </li><li>  <a href="https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/cluster/k_means_.py">github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/cluster/k_means_.py #L1113</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/310288/">https://habr.com/ru/post/310288/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../310276/index.html">Embedding functional objects, functions and lambdas through templates and unification using virtual in C ++</a></li>
<li><a href="../310278/index.html">Gitlab-CI and Ansible-lint syntax checking</a></li>
<li><a href="../310282/index.html">The second wave of fateful changes GitHub</a></li>
<li><a href="../310284/index.html">React.js: build an isomorphic / universal application from scratch. Part 2: add bootstrap, pages and routing</a></li>
<li><a href="../310286/index.html">What links to use: absolute or relative?</a></li>
<li><a href="../310292/index.html">Bug work</a></li>
<li><a href="../310294/index.html">The digest of interesting materials for the mobile developer # 171 (September 12-18)</a></li>
<li><a href="../310296/index.html">Voice and video for programmers. How we create a realtime communications conference</a></li>
<li><a href="../310298/index.html">Chinese government may ban high-PUE data centers</a></li>
<li><a href="../310300/index.html">How to officially accept foreign currency payments from abroad in Russia (PI)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>