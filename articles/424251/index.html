<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We consider the statistics on the experiments on hh.ru</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 

 Today I will tell you how we at hh.ru consider manual statistics on experiments. We'll see where the data comes from, how we process it, and...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We consider the statistics on the experiments on hh.ru</h1><div class="post__text post__text-html js-mediator-article">  Hello! <br><br>  Today I will tell you how we at <a href="http://hh.ru/">hh.ru</a> consider manual statistics on experiments.  We'll see where the data comes from, how we process it, and what pitfalls we encounter.  In the article I will share the general architecture and approach, the actual scripts and code will be at a minimum.  The main audience is novice analysts who are interested in how the data analysis infrastructure in hh.ru is arranged.  If this topic is interesting - write in the comments, we can delve into the code in the following articles. <br><br>  How automatic metrics for A / B experiments are considered can be found in our <a href="https://habr.com/company/hh/blog/321386/">other article</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/2t/ni/z7/2tniz7lywqs_e1q19yafq1n6vss.jpeg" alt="image"><br><a name="habracut"></a><br><h2>  What data we analyze and where they come from </h2><br>  We analyze access logs and any custom logs that we write ourselves. <br><br><blockquote>  95.108.213.12 - - [13 / Aug / 2018: 04: 00: 02 +0300] 200 "GET / employer / 2574971 HTTP / 1.1" 12012 "-" Mozilla / 5.0 (compatible; YandexBot / 3.0; + http: / /yandex.com/bots) "" - "" gardabani.headhunter.ge "" 0.063 "" - "" 1534122002.858 "" - "" 192.168.2.38:1500 "" [0.064] "{15341220027959c8c01c51a6e01b682f 200 https 1 -" - "- - [35827] [0.000 0] <br>  178.23.230.16 - - [13 / Aug / 2018: 04: 00: 02 +0300] 200 "GET / vacancy / 24266672 HTTP / 1.1" 24229 " <a href="https://hh.ru/vacancy/24007186%3Fquery%3Dbmw">hh.ru/vacancy/24007186?query=bmw</a> " "Mozilla / 5.0 ( Macintosh; Intel Mac OS X 10_10_5) AppleWebKit / 603.3.8 (KHTML, like Gecko) Version / 10.1.2 Safari / 603.3.8 "" - "" hh.ru "" 0.210 "" last_visit = 1534111115966 :: 1534121915966;  hhrole = anonymous;  regions = 1;  tmr_detect = 0% 7C1534121918520;  total_searches = 3;  unique_banner_user = 1534121429.273825242076558 "" 1534122002.859 "" - "" 192.168.2.239:1500 "" [0.208] "{1534122002649b7eef2e901d8c9c0469} 200 https 1 -" - "- - [35927] [0.001] </blockquote><br>  In our architecture, each service writes logs locally, and then through a samopisny client-server logs (including the nginx access logs) are collected at the central repository (further logging).  Developers have access to this machine and can manually log logs if necessary.  But how in a reasonable time a few hundred gigabytes of logs?  Of course, pour them into hadoop! <br><br><h3>  Where does the data appear in hadoop? </h3><br>  Not only service logs are stored in hadoop, but also prod-database unloading.  Every day in hadoop, we unload some of the tables that are needed for analytics. <br><br>  Logs of services get into hadoop in three ways. <br><br><ol><li>  <b>The path to the forehead</b> - cron is started from the log repository at night, and rsync uploads raw logs to hdfs. </li><li>  <b>The path is fashionable</b> - logs from services flow not only into the general storage, but also into kafka, from where they are read by flume, preprocessing and stored in hdfs. </li><li>  <b>The path is old-fashioned</b> - in the days before kafka we wrote our service, which reads raw logs from the repository, makes preprocessing and fills in hdfs. </li></ol><br>  Consider each approach in more detail. <br><br><h4>  Way to the forehead </h4><br>  Cron runs a regular bash script. <br><br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash LOGGING_DATE_PATH_PART=$(date -d yesterday +\%Y/\%m/\%d) HADOOP_DATE_PATH_PART=$(date -d yesterday +year=\%Y/month=\%m/day=\%d) ls /logging/java/${LOGGING_DATE_PATH_PART}/hh-banner-sync/banner-versions*.log | while read source_filename; do dest_filename=$(basename "$source_filename") /usr/bin/rsync --no-relative --no-implied-dirs --bwlimit=12288 ${source_filename} rsync://hadoop2.hhnet.ru/hdfs-raw/banner-versions/${HADOOP_DATE_PATH_PART}/${dest_filename}; done</span></span></code> </pre> <br>  As we remember, in the log repository all the logs are in the form of regular files, the folder structure is something like this: /logging/java/2018/08/10/ Your_server_name_by/*.log <br><br>  Hadoop stores its files in roughly the same folder structure hdfs-raw / banner-versions / year = 2018 / month = 08 / day = 10 <br>  year, month, day we use as partitions. <br><br>  Thus, we only need to form the correct paths (lines 3-4), then select all the necessary logs (line 6) and use rsync to fill them in hadoop (line 8). <br><br>  <b>Advantages of this approach:</b> <br><br><ul><li>  Rapid development </li><li>  Everything is transparent and clear </li></ul><br>  <b>Minuses:</b> <br><br><ul><li>  No preprocessing </li></ul><br><h4>  Fashionable way </h4><br>  Since we upload logs to the repository with a self-written script, it was logical to fasten the ability to upload them not only to the server, but also to kafka. <br><br>  <b>pros</b> <br><br><ul><li>  Online logs (logs in hadoop appear as fills in kafka) </li><li>  You can do preprocessing </li><li>  Well holds the load and you can upload large logs </li></ul><br>  <b>Minuses</b> <br><br><ul><li>  More difficult setup </li><li>  You need to write code </li><li>  More components of the pouring process </li><li>  More difficult to monitor and analyze incidents </li></ul><br><h4>  The path is old fashioned </h4><br>  It differs from the fashionable only in the absence of kafka.  Therefore, it inherits all the minuses and only some advantages of the previous approach.  A separate service (ustats-uploader) on java periodically reads the necessary files, makes them preprocessing and uploads into hadoop. <br><br>  <b>pros</b> <br><br><ul><li>  You can do preprocessing </li></ul><br>  <b>Minuses</b> <br><br><ul><li>  More difficult setup </li><li>  You need to write code </li></ul><br>  And the data got into hadoop and is ready for analysis.  Let's stop a bit and remember what hadoop is and why hundreds of gigabytes can be stuck on it much faster than the usual grep. <br><br><h3>  Hadoop </h3><br>  Hadoop is a distributed data repository.  The data does not lie on any separate server, but is distributed among several machines, and is also stored not in one instance, but in several - this is done to ensure reliability.  The basis of the speed of data processing lies in the change in approach in comparison with conventional databases. <br><br>  In the case of a normal database, we retrieve data from it and send it to the client, who does some analysis and returns the result to the analytics.  Thus, in order to read more quickly, we need to have many clients and parallel requests (for example, divide the data by months - and each client can read the data for his month). <br><br>  Hadoop is the opposite.  We send the code (what exactly we want to count) to the data, and this code is executed on the cluster.  As we know, data rests on many machines, so each machine executes code only on its data and returns the result to the client. <br><br>  Many have probably heard about <a href="https://ru.wikipedia.org/wiki/MapReduce">map-reduce</a> , but writing code for analytics is not very convenient and fast, while writing in SQL is much easier.  Therefore, there are services that can turn SQL into map-reduce transparently for the user, and the analyst may not even be aware of how his request is actually considered. <br><br>  In hh.ru for this we use hive and presto.  Hive was the first, but we are gradually moving to presto, since it is much faster for our requests.  We use hue and zeppelin as GUI. <br><br>  It is more convenient for me to consider analytics on python in jupyter, this allows you to read it with one click and to get properly styled excel tables at the output, which saves time.  Write in the comments, this topic pulls on a separate article. <br><br>  Let's return to the analyst itself. <br><br><h2>  How to understand what we want to count? </h2><br><h3>  A product manager came in with the task to calculate the results of an experiment. </h3><br>  We send an email-newsletter, in which we send suitable vacancies for the applicant (do everyone like such newsletters?).  We decided to slightly change the design of the letter and we want to understand whether it became better.  For this we will consider: <br><br><ul><li>  the number of transitions to vacancies from the letter; </li><li>  post-transition feedback </li></ul><br>  Let me remind you that all that we have is access log and base.  We need to formulate our metrics in terms of linking. <br><br><h4>  Number of transitions to a job from a letter </h4><br>  Transition is a GET request on <a href="https://hh.ru/vacancy/26646861">hh.ru/vacancy/26646861</a> .  To understand where the transition came from, we add utm tags of the form? Utm_source = email_campaign_123.  For GET requests in access log, there will be information about the parameters, and we can filter the transitions only from our mailing list. <br><br><h4>  The number of responses after the transition </h4><br>  Here we could just count the number of responses to vacancies from the newsletter, but then the statistics would be wrong, because the responses could have been affected by something else besides our letter, for example, an advertisement in ClickMe was bought on the vacancy, and therefore the number of responses has grown a lot. <br><br>  We have two options for how to formulate the number of responses: <br><br><ol><li>  The response is POST on <a href="https://hh.ru/applicant/vacancy_response/popup%3Fvacancy_id%3D26646861">hh.ru/applicant/vacancy_response/popup?vacancy_id=26646861</a> , which has referer <a href="https://hh.ru/vacancy/26646861%3Futm_source%3Demail_campaign_123">hh.ru/vacancy/26646861?utm_source=email_campaign_123</a> . </li><li>  The nuance of this approach is that if the user switched to a vacancy, and then walked around the site a bit and then responded to the vacancy, we will not deduct it. </li><li>  We can remember the id of the user who switched to <a href="https://hh.ru/vacancy/26646861">hh.ru/vacancy/26646861</a> , and calculate the number of reviews per vacancy during the day. </li></ol><br>  The choice of approach is determined by business requirements, usually the first option is enough, but it all depends on what the product manager is waiting for. <br><br><h2>  Pitfalls that may occur </h2><br><ol><li>  Not all data is in hadoop, you need to add data from the prod-database.  For example, in logs usually only id, and if you need a name, then it is in the database.  Sometimes you need to search for user by resume_id, and this is also stored in the database.  To do this, we unload the part of the base in hadoop, so that join is simpler. </li><li>  Data may be curves.  This is generally the trouble of hadoop and how we load data into it.  Depending on the data, an empty value can be null, None, none, an empty string, etc. You need to be careful in each case, because the data is really different, loaded in different ways and for different purposes. </li><li>  Long count for the entire period.  For example, we need to count our transitions and responses for the month.  These are about 3 terabytes of logs.  Even hadoop will take it for quite a while.  Usually it is quite difficult to write a 100% work request the first time, so we write it by trial and error.  Each time to wait for 20 minutes is very long.  Solutions: <br><br><ul><li>  Debugging request for logs for 1 day.  Since the data in hadoop is partitioned by us, it‚Äôs pretty quick to calculate something for 1 day of logs. </li><li>  Download the necessary logs to the temporary table.  As a rule, we understand which URLs are interesting to us, and we can make a temporary table for the logs from these URLs. </li></ul><br>  Personally, I prefer the first option, but sometimes it‚Äôs necessary to make a temporary table, depending on the situation. </li><li>  Distortions in summary metrics <br><ul><li>  It is better to filter the logs.  You need to pay attention, for example, to the response code, redirect, etc. Better is less data, but more accurate, of which you are sure. </li><li>  As few as possible intermediate steps in the metric.  For example, a transition to a job is one step (GET request to / vacancy / 123).  Response - two (go to the job + POST).  The shorter the chain, the less errors and the more accurate the metric.  Sometimes it happens that the data between the transitions are lost and count something is impossible at all.  To solve this problem, it is necessary before the development of the experiment to think about what we will consider and how.  Extremely helps your individual log of necessary events.  We are able to shoot the necessary events, and thus the chain of events will be more accurate, and it will be easier to count. </li><li>  Bots can generate a bunch of transitions.  You need to understand where bots can go (for example, on pages that require authorization, they should not be), and filter this data. </li><li>  Big bumps - for example, in one of the groups there may be one applicant who generates 50% of all responses.  There will be a bias of statistics, such data also needs to be filtered. </li></ul></li><li>  It is difficult to formulate what to consider in terms of access log.  Here helps knowledge of the code base, experience and chrome dev tools.  We read the description of the metric from the product, we repeat with our hands on the site and see which transitions are generated. </li></ol><br>  Finally, let's talk about how the result of calculations should look. <br><br><h2>  Counting result </h2><br>  In our example, there are 2 groups and 2 metrics that form a funnel. <br><img src="https://habrastorage.org/webt/mn/fx/cm/mnfxcmiwrbhfvxnqomoy_v_zztw.png" alt="image"><br>  Recommendations for the design of the results: <br><br><ol><li>  Do not overload with parts until it is required.  Simply and less is better (for example, here we could show each vacancy separately or clicks by day).  Focus on one thing. </li><li>  Details may be needed in the process of demo results, so think about what questions you may ask, and prepare the details.  (In our example, the detailing can be on the speed of transition after sending an email - 1 day, 3 days, week, grouping of vacancies by trade area) </li><li>  Remember about statistical significance.  For example, a change of 1% with the number of transitions of 100 and clicks of 15 is insignificant and could be accidental.  Use <a href="http://www.evanmiller.org/ab-testing/sample-size.html">calculators</a> </li><li>  Automate as much as possible, because you‚Äôll have to count several times.  Usually in the middle of an experiment you already want to understand how things are going.  After the experiment, questions may arise and something will have to be clarified.  Thus, it will be necessary to count 3-4 times, and if each calculation is a sequence of 10 requests and then manual copying to excel, it will be painful and spend a lot of time.  Learn python, it will save a lot of time. </li><li>  Use a graphical representation of the results when justified.  The built-in tools hive and zeppelin allow out of the box to build simple graphics. </li></ol><br>  It is necessary to count various metrics quite often, because we are releasing practically every task in the framework of the A / B experiment.  There is nothing complicated in the calculations, after 2-3 experiments there comes an understanding of how to do it.  Remember that access logs store a lot of useful information that can save a company money, help you promote your idea and prove which change option is better.  The main thing - to be able to get this information. </div><p>Source: <a href="https://habr.com/ru/post/424251/">https://habr.com/ru/post/424251/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../424241/index.html">Print your world</a></li>
<li><a href="../424243/index.html">5 easy ways to improve communication with customers</a></li>
<li><a href="../424245/index.html">Write a Telegram client - easy</a></li>
<li><a href="../424247/index.html">KotlinConf 2018 Live - see the broadcast on October 4-5</a></li>
<li><a href="../424249/index.html">Materials from the #RuPostgres meeting - videos, presentations, quiz screening and photo report</a></li>
<li><a href="../424255/index.html">How to use static analysis correctly</a></li>
<li><a href="../424257/index.html">Hexagon Cards in Unity: Parts 1-3</a></li>
<li><a href="../424259/index.html">Security Week 36: Telnet should be closed</a></li>
<li><a href="../424261/index.html">How to solve any programming problem</a></li>
<li><a href="../424263/index.html">IDA Pro Upgrade. We fix the jambs of processor modules</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>