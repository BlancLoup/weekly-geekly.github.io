<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>SSD + raid0 - not so simple</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 Colleagues from the neighboring department ( UCDN ) addressed a rather interesting and unexpected problem: when testing raid0 on a larg...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>SSD + raid0 - not so simple</h1><div class="post__text post__text-html js-mediator-article"><h1>  Introduction </h1><br>  Colleagues from the neighboring department ( <a href="http://ucdn.com/">UCDN</a> ) addressed a rather interesting and unexpected problem: when testing raid0 on a large number of SSDs, the performance changed in just such a sad way: <br><img src="https://habrastorage.org/getpro/habr/post_images/69b/3ee/fb1/69b3eefb1d549a3763d692009cd4490a.png"><br>  X axis - the number of disks in the array, Y axis - megabytes per second. <br><br>  I began to study the problem.  The initial diagnosis was simple - a hardware raid did not cope with a large number of SSDs and rested on its own performance ceiling. <br><br>  After the hardware raid was thrown out and HBA was put in its place, and the disks were assembled into raid0 using linux-raid (it is often called 'mdadm' by the command line utility name), the situation improved.  But it did not pass completely - the numbers increased, but they were still lower than the calculated ones.  In this case, the key parameter was not IOPS, but multi-threaded linear recording (that is, large pieces of data written to random places). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The situation for me was unusual - I never chased the net bandwidth raids.  IOPS are our everything.  And here - it is necessary many-sided in a second and more. <br><br><h1>  Hellish graphics </h1><br>  I began by defining the baseline, that is, the performance of a single disk.  I did it, rather, to clear my conscience. <br><br>  Here is a linear reading schedule with a single SSD. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d71/3c1/604/d713c16043383178da7b8f73059968db.png"><br><br>  Seeing the result, I really hoisted.  Because it very much resembled the tricks that manufacturers of cheap USB flash drives go to.  They place the fast memory in the areas where the FAT (tables) are located in FAT32 (the file system) and slower in the data storage area.  This allows a slight gain in performance when working with small operations with metadata, while assuming that users who copy large files are first ready to wait, and secondly the operations themselves will occur in large blocks.  Learn more about this heartbreaking phenomenon: <a href="https://lwn.net/Articles/428584/">lwn.net/Articles/428584</a> <br><a name="habracut"></a><br>  I was sure that I had found the cause and the root of all the problems and had already prepared a stinging message (see the captions in the picture) explaining what a dull, poor-quality equipment of the ‚Äúfertilizer‚Äù class turned out to be on the test, and many other words that are better not to repeat. <br><br>  Although I was embarrassed by the version of the kernel on the stand - 3.2.  According to my previous experience, knowing the regrettable features of LSI that change everything in the drivers (mpt2sas) from version to version, literally everything, I thought, ‚Äúwhat if‚Äù? <br><br>  A little background.  mpt2sas - LSI driver for HBA.  He lives an incredibly stormy life, starting from version v00.100.11.15 through version 01.100.0x.00, reaching as much as version 16.100.00.00 (I wonder what the number ‚Äú100‚Äù means?).  During this time, the driver distinguished itself by rearranging the drive letter names when updating the minor kernel version, which differs from the BIOS-BIOS disk ordering, crashes on the ‚Äúunexpected‚Äù LUN configurations, on the backplane timeout, on the unexpected number of drives, logging errors in dmesg with infinite loop speed the kernel itself (de facto, this is the equivalent of a system hangup), and similar gay things. <br><br>  Updated, run the test.  And this "suddenly" happened.  This is how the same chart looks at 3.14.  But I barely rejected innocent SSD's. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/182/e35/a9c/182e35a9cde07bed1998af0347592e34.png"><br><br>  After the disk performance stabilized, a second test was conducted: independent tests were run on all disks in parallel.  The goal was simple - to check if there is a bottleneck somewhere on the bus or HBA.  The performance of the disks turned out to be quite decent, there was no ‚Äúplug‚Äù on the bus.  The main task was solved.  However, the performance schedule was still different.  Not much, but obviously with a hint of worse than linear recording speed. <br><br>  Why does the recording behave this way as the number of disks in the array increases?  The graph (at the beginning of the article) very much resembled the graph of the performance of multi-threaded applications as the number of threads grow, to which programmers and Intels usually show when they talk about problems with thread locks ... <br><br>  During the test, something strange was observed in the <a href="https://github.com/amarao/blktop">blktop</a> : some of the disks were loaded into the ceiling, some were almost idle.  Moreover, those who show low performance are loaded into the ceiling, and the ‚Äúfast‚Äù disks are idle.  Moreover, disks sometimes change places - that is, before a 100% loaded disk suddenly shows a higher speed and lower loading, and vice versa, a disk that was loaded by 50% suddenly turns out to be loaded by 100% and at the same time shows a lower speed.  Why? <br><br>  And then it hit me. <br><br><h1>  raid0 depends on latency worst of disks </h1><br>  If we write a lot of data, the recording usually goes in large chunks.  These pieces are divided into smaller pieces by the raid0 driver, which writes them to all disks from raid0 at the same time.  Due to this, we get an N-fold increase in performance.  (In raid0 on N disks). <br><br>  But let's look at the record in more detail ... <br><br>  Suppose we have a raid using chunk'i and the size of 512k.  In the array of 8 disks.  The application wants to write a lot of data, and we write on raid in 4MB chunks. <br><br>  Now watch your hands: <br><ol><li>  raid0 receives a write request, divides the data into 8 pieces of 512kb each </li><li>  raid0 sends (in parallel) 8 requests for 8 devices by writing 512kb (each its own) </li><li>  raid0 is waiting for confirmation from all 8 devices to complete the recording </li><li>  raid0 responds to the ‚Äúwritten‚Äù application (that is, it returns control from the write () call) </li></ol><br>  Imagine now that the disks had a recording in such a time (in milliseconds): <br><table><tbody><tr><td>  Disk 1 </td><td>  Disk 2 </td><td>  Disk 3 </td><td>  Disk 4 </td><td>  Disk 5 </td><td>  Disk 6 </td><td>  Disk 7 </td><td>  Disk 8 </td></tr><tr><td>  4.1 </td><td>  2.2 </td><td>  1.9 </td><td>  1.4 </td><td>  1.0 </td><td>  9.7 </td><td>  5.4 </td><td>  8.6 </td></tr></tbody></table><br>  Question: how long will it take to write a 4Mb block to this array?  Answer: in 9.7 ms.  Question: What will be the recycling of disk number 4 at this time?  Answer: about 10%.  And the disk number 6?  100%.  Note, for example, I chose the most extreme values ‚Äã‚Äãfrom the log of operations, but with a smaller discrepancy, the problem will remain.  Compare the reading and writing schedule (I give the same picture again): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/182/e35/a9c/182e35a9cde07bed1998af0347592e34.png"><br>  See how uneven writing is in comparison with reading? <br><br>  The latency SSD drives are very uneven.  It is connected with their internal device (when a block of large size is recorded at a time, if necessary, by moving and transferring data from place to place).  The larger this block, the stronger the latency peaks (i.e., momentary dips in performance).  Regular magnetic disks have completely different graphics - they resemble a flat line with almost no deviations.  In the case of linear sequential IO, this line is high, in the case of constant random IO - always low, but the key one - constantly.  Latency in hard drives is predictable, latency in SSD is not.  Note that all drives have this property.  The most expensive latency is shifted (either very quickly or very, very quickly) - but the discrepancy still remains. <br><br>  With such latency fluctuations, the performance of an SSD is, on average, excellent, but at certain points in time, the recording may take slightly more than at other times.  For the tested disks, it fell at this moment to shameful values ‚Äã‚Äãof about 50 Mb / s (which is lower than the linear recording in modern HDDs by a factor of two). <br><br>  When device requests go stacked and independent, it does not affect.  Well, yes, one request was executed quickly, the other slowly, on average, everything is fine. <br><br>  But if the recording depends on all the drives in the array?  In this case, any "braked" disc slows down the entire operation.  As a result, the more disks in the array, the greater the likelihood that at least one disk will work slowly.  The more disks, the larger the performance curve of their sum in raid0 begins to approach the sum of the performance of their minima (and not the average values, as we would like). <br><br>  Here is a graph of real performance versus number of disks.  Pink line - predictions based on average disk performance, blue line - actual results. <br><br><img src="https://habrastorage.org/files/257/410/6d5/2574106d5b6a469d981c143be3b511e9.png"><br>  In the case of 7 discs, the differences were about 10%. <br><br>  A simple mathematical simulation (with data on the latency of a real disk for a situation of multiple disks in an array) made it possible to predict that as the number of disks increases, degradation can go up to 20-25%. <br><br>  Unlike replacing the HBA or the driver version, in this case nothing could have been changed significantly, and the information was simply taken into account. <br><br><h1>  What is better - HDD or SSD? </h1><br>  I will say right away: the worst expectation from SSD is better than the constant from HDD (if it sounded too difficult: SSD is better than HDD). <br><br>  Another thing is that an array of 20-30 HDD - this is normal.  30 SSD in raid0 will cause salivation in geeks and an attack of hepatic colic in the finance department.  That is, many HDDs are usually compared to several SSDs.  If we normalize the numbers by IOPS (ohoho), that is, we get the same parrots from the HDD as SSDs, then the numbers will suddenly, by others - an array of HDDs will significantly overtake the array of several SSDs by recording speed. <br><br>  Another thing is that a large array of HDDs is already another kind of extreme, and surprises await there because of the overall use of the bus, the performance of the HBA, and the behavior of the backplanes. <br><br><h1>  And raid1 / 5/6? </h1><br>  It is easy to understand that for all these arrays the problem of waiting for the ‚Äúslowest‚Äù is preserved, and even slightly intensified (that is, the problem occurs with a smaller block size and lower load intensity). <br><br><h1>  Conclusion </h1><br>  Admin: I do not like LSI.  If any discrepancies are detected in the operation of disks with the participation of LSI in the system, debugging should be started by comparing the behavior of different versions of the mpt2sas driver.  This is the case when a version change can affect performance and stability in the most dramatic way. <br><br>  Academic: When planning for high-load systems using SSD in raid0, it should be borne in mind that the more SSD in the array, the stronger the effect of uneven latency.  As the number of devices grows in raid0, device performance begins to strive to multiply the number of devices by the minimum disk performance (and not the average, as one would expect). <br><br>  Recommendations: in the case of this type of load, you should try to choose devices with the smallest variation in latency on the record, if possible use devices with a larger capacity (to reduce the number of devices). <br><br>  Special attention should be paid to configurations in which part or all of the disks are connected over the network with an uneven delay, such a configuration will cause much greater difficulties and degradation than local disks. </div><p>Source: <a href="https://habr.com/ru/post/227927/">https://habr.com/ru/post/227927/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../227913/index.html">Setting up a Mikrotik router for various tasks in SOHO</a></li>
<li><a href="../227915/index.html">Algorithm Visualization</a></li>
<li><a href="../227917/index.html">Do not deny yourself anything: Facebook has added 20 more gender options. Now there are 70</a></li>
<li><a href="../227921/index.html">Three new items in MongoDB 2.8</a></li>
<li><a href="../227925/index.html">Forecast World Cup 2014 in the language of Wolfram</a></li>
<li><a href="../227931/index.html">Marketing for financial institutions. Part Two: Campaigning</a></li>
<li><a href="../227933/index.html">As I walked on campus: notes of a bystander</a></li>
<li><a href="../227935/index.html">Another Pattern Matching in C # - now with context building</a></li>
<li><a href="../227937/index.html">History of one logo</a></li>
<li><a href="../227939/index.html">Samsung and Google will increase the level of information protection in Android</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>