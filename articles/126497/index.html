<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neural network data compression</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article I want to talk about another class of problems solved by neural networks - data compression. The algorithm described in the article do...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neural network data compression</h1><div class="post__text post__text-html js-mediator-article"><img src="http://picquick.ru/uploads/13082011/5c93309304909ffb24b0526a66814a22.jpeg" alt="image" align="left">  In this article I want to talk about another class of problems solved by neural networks - data compression.  The algorithm described in the article does not claim to be used in real combat conditions due to the existence of more efficient algorithms.  At once I will make a reservation that the discussion will deal only <a href="http://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B6%25D0%25B0%25D1%2582%25D0%25B8%25D0%25B5_%25D0%25B1%25D0%25B5%25D0%25B7_%25D0%25BF%25D0%25BE%25D1%2582%25D0%25B5%25D1%2580%25D1%258C">with lossless compression</a> . <br>  Most of the sources on the Internet claim that there are 3 main popular neural network architectures that solve the problem of data compression. <br><br>  1. <b>Kohonen network and its variations.</b>  Often used to compress images with loss of quality, it is not a lossless compression algorithm. <br><br>  2. <b>Associative memory</b> (Hopfield network, Bidirectional associative memory, etc.).  ‚ÄúData compression‚Äù for this class of networks is a ‚Äúfeature‚Äù, a side effect, since their main purpose is to restore the original signal / image from noisy / damaged input data.  Most often, a noisy image of the same dimension arrives at the input of these networks, so data is not about compression. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      3. <b>The method of "bottleneck".</b> <br><br>  The last method will be discussed in the article. <br><a name="habracut"></a><br><h4>  The essence of the method </h4><br><img src="http://picquick.ru/uploads/13082011/0a1a77be990474dd81d0dce547aec596.jpeg" alt="image" align="right">  The network architecture and learning algorithm are such that a large dimension vector is required to be transmitted from the input of the neural network to its outputs through a relatively small channel size.  As an analogy - an hourglass, a watering can, a bottleneck, which is closer to whom.  To implement this kind of compression, you can use a multilayer perceptron of the following architecture: the number of neurons in the input and output layers are the same, between them are several hidden layers of a smaller size. <br>  For learning, the back error propagation algorithm is used, and the sigmoid (sigmoid function, bipolar sigmoid function) is used as the activation function. <br>  The network consists of two parts - one part works on data compression, the other - on recovery.  The time spent on both procedures is about the same.  In practical use, the resulting network is divided into two.  The output of the first network is transmitted via the communication channel and is fed to the input of the second one, which performs decompression. <br><br><h4>  What influences learning? </h4><br>  An important lever of influence on the speed of learning network is skillful adjustment of the rate of learning.  The simplest option ‚Äî when the network error changes to a relatively small value (say, less than 0.0000001) compared to a network error in the previous era ‚Äî should decrease the factor.  There are more intelligent algorithms for changing the speed of learning. <br>  And another very important element in learning is the all-powerful rand.  It is adopted when creating a neuron, its weights set small random values.  Sometimes it turns out that the values ‚Äã‚Äãare randomly set so that the network becomes unteachable, or the error convergence is very small. <br><br><h4>  Implementation </h4><br>  When implementing the library used <a href="http://www.aforgenet.com/">AForge.NET</a> .  I carried out basic experiments on vectors of length 10, then on vectors of length 20, this did not change the essence, therefore I did not increase the dimension, but there is one more reason.  Why haven't I experimented with longer vectors?  Why not generate a large training sample with long vectors?  This is caused by <s>laziness</s> that causes some problems and inconveniences.  There may be contradictions in such a sample.  For example, in the input set of vectors there may be two or more identical or similar vectors, the desired output to which will be absolutely different.  Then the network on such a training set will be unteachable, and it is rather difficult to find ‚Äúconflicting‚Äù vectors. <br><br><h4>  Results and conclusions </h4><br>  For me, the most interesting question was how to build this network effectively?  How many hidden layers should she have?  How fast is the training?  How fast is compression and recovery? <br>  Experiments were conducted on two main possible options for building this network. <br><br><h5>  ‚ÄúFast Compression‚Äù </h5><br>  A network with ‚Äúfast compression‚Äù will be called a network where the number of neurons in hidden layers is much less (3, 5, etc. times) than in the input / output layer, and the network consists of 4 layers - input, output and two hidden (for example, 10-3-3-10).  This network topology has proven very effective in some cases.  First, such a network learns very quickly.  This is due to the fact that the learning algorithm needs to adjust the weights of a relatively small number of connections between adjacent layers.  Secondly, the speed of issuing results in the operation of the network is also very high (for the same reason).  The reverse side - the network is trained only on a small sample relative to the number of neurons in the input (and, accordingly, output) layer (an acceptable number of vectors is approximately equal to half of the neurons in the first / last layer of the network).  Also, do not bend the stick and make too much difference between the layers, the best option - the number of neurons in the hidden layer should be about 3 times less than in the input / output layer. <br><br><h5>  "Sequential compression" </h5><br>  A network with a ‚Äúsequential‚Äù or ‚Äúunhurried‚Äù compression will be called a network where the number of neurons in adjacent layers is not very different (up to 25%), and the network contains 4 or more hidden layers (for example, 10-8-5-3-3- 5-8-10).  Such networks did not meet my expectations.  Even with a small training set and the length of the vector, they learn very long.  Well, and function, of course, also slower.  Although they say that in real combat conditions they are more stable.  But because of the very long training time, it was not possible to verify this. <br><br><h5>  About efficiency and work time </h5><br>  From a practical point of view, two facts are interesting - the compression ratio and the compression / recovery time.  The network is almost 100% trained and functions perfectly, compressing data 3 times.  The speed of work also pleases.  For architecture (10-3-3-10), the full cycle (compression and recovery) execution time is 00: 00: 00.0000050.  For (20-6-6-20) - 00: 00: 00.0000065.  We can safely divide by 2 and get 00: 00: 00.0000025 and 00: 00: 00.0000033 to compress or restore one vector. <br>  The tests were carried out on an AMD Athlon II x240 2.80 Ghz processor. <br><br><h4>  Example </h4><br>  As an example, a small commented piece of code (in C #) that describes a primitive example of creating, training, and operating a network.  The network compresses the vector 3 times, it is trained in 98% of cases.  For operation, you must connect the library AForge.NET. <br><br><blockquote><code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black"><font color="#0000ff">using</font> System; <br> <font color="#0000ff">using</font> AForge.Neuro; <br> <font color="#0000ff">using</font> AForge.Neuro.Learning; <br> <br> <font color="#0000ff">namespace</font> FANN <br> { <br> <font color="#0000ff">class</font> Program <br> { <br> <font color="#0000ff">static</font> <font color="#0000ff">void</font> Main( <font color="#0000ff">string</font> [] args) <br> { <br> <font color="#008000">//        4  ( 10,3,3,10    , )</font> <br> ActivationNetwork net = <font color="#0000ff">new</font> ActivationNetwork((IActivationFunction) <font color="#0000ff">new</font> SigmoidFunction(), 10,3,3,10); <br> <font color="#008000">//  -     </font> <br> BackPropagationLearning trainer = <font color="#0000ff">new</font> BackPropagationLearning(net); <br> <br> <font color="#008000">//    </font> <br> <font color="#0000ff">double</font> [][] input = <font color="#0000ff">new</font> <font color="#0000ff">double</font> [][] { <br> <font color="#0000ff">new</font> <font color="#0000ff">double</font> []{1,1,1,1,1,1,1,1,1,1}, <br> <font color="#0000ff">new</font> <font color="#0000ff">double</font> []{0,0,0,0,0,0,0,0,0,0}, <br> <font color="#0000ff">new</font> <font color="#0000ff">double</font> []{1,1,1,1,1,0,0,0,0,0}, <br> <font color="#0000ff">new</font> <font color="#0000ff">double</font> []{0,0,0,0,0,1,1,1,1,1}, <br> }; <br> <br> <font color="#008000">//     </font> <br> <font color="#0000ff">double</font> [][] output= <font color="#0000ff">new</font> <font color="#0000ff">double</font> [][] { <br> <font color="#0000ff">new</font> <font color="#0000ff">double</font> []{1,1,1,1,1,1,1,1,1,1}, <br> <font color="#0000ff">new</font> <font color="#0000ff">double</font> []{0,0,0,0,0,0,0,0,0,0}, <br> <font color="#0000ff">new</font> <font color="#0000ff">double</font> []{1,1,1,1,1,0,0,0,0,0}, <br> <font color="#0000ff">new</font> <font color="#0000ff">double</font> []{0,0,0,0,0,1,1,1,1,1}, <br> }; <br> <br> <font color="#008000">// ,       </font> <br> <font color="#0000ff">double</font> prErr = 10000000; <br> <font color="#008000">//  </font> <br> <font color="#0000ff">double</font> error = 100; <br> <font color="#008000">//      </font> <br> trainer.LearningRate = 1; <br> <font color="#008000">//       </font> <br> <font color="#0000ff">while</font> (error &gt; 0.001) <br> { <br> <font color="#008000">//   </font> <br> error = trainer.RunEpoch(input, output); <br> <font color="#008000">//       ,     </font> <br> <font color="#0000ff">if</font> ( <font color="#2B91AF">Math</font> .Abs(error - prErr) &lt; 0.000000001) <br> { <br> <font color="#008000">//      2</font> <br> trainer.LearningRate /= 2; <br> <font color="#0000ff">if</font> (trainer.LearningRate &lt; 0.001) <br> trainer.LearningRate = 0.001; <br> } <br> <br> prErr = error; <br> } <br> <br> <font color="#008000">//   </font> <br> <font color="#0000ff">double</font> [] result; <br> result = net.Compute( <font color="#0000ff">new</font> <font color="#0000ff">double</font> [] { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 }); <br> result = net.Compute( <font color="#0000ff">new</font> <font color="#0000ff">double</font> [] { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }); <br> result = net.Compute( <font color="#0000ff">new</font> <font color="#0000ff">double</font> [] { 1, 1, 1, 1, 1, 0, 0, 0, 0, 0 }); <br> result = net.Compute( <font color="#0000ff">new</font> <font color="#0000ff">double</font> [] { 0, 0, 0, 0, 0, 1, 1, 1, 1, 1 }); <br> } <br> } <br> }</font> <br> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br><br><h4>  Literature and notes </h4><br><ul><li>  <a href="http://ru.wikipedia.org/wiki/%25D0%259D%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D1%2581%25D0%25B5%25D1%2582%25D0%25B5%25D0%25B2%25D0%25BE%25D0%25B5_%25D1%2581%25D0%25B6%25D0%25B0%25D1%2582%25D0%25B8%25D0%25B5_%25D0%25B4%25D0%25B0%25D0%25BD%25D0%25BD%25D1%258B%25D1%2585">Wikipedia</a> </li><li>  Library page - <a href="http://www.aforgenet.com/">www.aforgenet.com</a> </li><li>  Library Download Page - <a href="http://www.aforgenet.com/framework/downloads.html">www.aforgenet.com/framework/downloads.html</a> </li><li>  Of the entire AForge.NET library, only a part of it is needed - AForge.Neuro. </li><li>  The AForge.NET library works with double type, because the network output is fractional, but very close to the values ‚Äã‚Äãwe need - 0 and 1. Therefore, to get a full output, you can drive each value of the array through a ‚Äúthreshold‚Äù function that returns 0 if the value is less 0.5 and 1 otherwise. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/126497/">https://habr.com/ru/post/126497/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../126489/index.html">Competition for children habravchan</a></li>
<li><a href="../126492/index.html">In Yandex.Money, new limits and special commission</a></li>
<li><a href="../126493/index.html">Personal computers: in the prime of life!</a></li>
<li><a href="../126495/index.html">Threads in C # .NET first steps</a></li>
<li><a href="../126496/index.html">Backup in FreeNas</a></li>
<li><a href="../126498/index.html">Another project that gave way to MSTU. Bauman</a></li>
<li><a href="../126499/index.html">User rights in information systems through the lens of CMS Bitrix</a></li>
<li><a href="../126500/index.html">A group of 27,612 South Koreans sued Apple</a></li>
<li><a href="../126501/index.html">C ++: Leash Pattern</a></li>
<li><a href="../126503/index.html">Important nuances of "computer help"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>