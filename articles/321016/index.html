<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ML Boot Camp III opening soon</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="February 15 Machine Learning Boot Camp III starts - the third machine learning and data analysis contest from the Mail.Ru Group. Today we talk about t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>ML Boot Camp III opening soon</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/66b/652/926/66b652926dd34423bff5bf314febefde.png"><br><br>  February 15 Machine Learning Boot Camp III starts - the third machine learning and data analysis contest from the Mail.Ru Group.  Today we talk about the last contest and discover the secrets of the new!  So, in the course of the upcoming contest, you will need to guess whether the participant will remain in the online game or leave it.  Task samples are built on twelve game attributes for 25,000 users.  Naturally, all data is anonymized. <br><a name="habracut"></a><br>  Signs themselves: <br><br><ul><li>  <b>maxPlayerLevel</b> - the maximum level of the game that the player has passed; </li><li>  <b>numberOfAttemptedLevels</b> - the number of levels that the player tried to pass; </li><li>  <b>attemptsOnTheHighestLevel</b> - the number of attempts made at the highest level; </li><li>  <b>totalNumOfAttempts</b> - total number of attempts; </li><li>  <b>averageNumOfTurnsPerCompletedLevel</b> - the average number of moves performed on successfully completed levels; </li><li>  <b>doReturnOnLowerLevels</b> - whether the player made returns to the game at levels already completed; </li><li>  <b>numberOfBoostersUsed</b> - the number of boosters used; </li><li>  <b>fractionOfUsefullBoosters</b> - the number of boosters used during successful attempts (the player has passed the level); </li><li>  <b>totalScore</b> - total points scored; </li><li>  <b>totalBonusScore</b> - total bonus points earned; </li><li>  <b>totalStarsCount</b> - the total number of stars scored; </li><li>  <b>numberOfDaysActuallyPlayed</b> - the number of days the user played the game. </li></ul><br>  We split the test sample randomly in a 40/60 ratio.  The result of the first 40% will determine the position of participants in the rating table throughout the competition.  The result for the remaining 60% will be known after the end of the competition, and it is he who will determine the final placement of participants. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The winner of the best solution, we will give MacBook Air.  The second and third place will get the Apple iPad.  Fourth, fifth and sixth - Apple iPod Nano.  By tradition, the top 50 participants will receive memorable T-shirts with the symbols of the championship.  In addition, we will invite the best of the best to the Mail.Ru Group for interviews on positions related to data analysis.  Register for the championship <a href="http://mlbootcamp.ru/">here</a> . <br><br><h2>  Machine learning boot camp ii </h2><br>  To make the participants better understand what they have to do, we present the task of the last championship and the best solution from the winner. <br><br>  <b>Task.</b>  The participants of the second contest faced the task "Performance Evaluation".  We offered them to teach the computer to predict the multiplication time of two matrices of the size <code>mÔΩòk</code> and <code>kÔΩòn</code> on a test computing system.  Participants knew how much time this problem was solved on other systems, matrix sizes and system parameters. <br><br>  As a quality criterion for solving a problem, we used the smallest average relative error ( <a href="https://en.wikipedia.org/wiki/Mean_absolute_percentage_error">MAPE</a> , in some sources referred to as MRE) for implementations that work longer than one second: <br><br><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi><mi>A</mi><mi>P</mi><mi>E</mi><mo>=</mo><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>N</mi></mrow></msubsup><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;#x2212;</mo><msub><mi>g</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub></mrow><mo>,</mo><msub><mi>y</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>i</mi></mrow></msub><mo>&amp;gt;</mo><mn>1</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="49.416ex" height="3.021ex" viewBox="0 -883.9 21276.1 1300.8" role="img" focusable="false" style="vertical-align: -0.969ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-4D" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-41" x="1051" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-50" x="1802" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-45" x="2553" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMAIN-3D" x="3595" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-66" x="4902" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-72" x="5452" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-61" x="5904" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-63" x="6433" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMAIN-31" x="6867" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-4E" x="7367" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-73" x="8506" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-75" x="8975" y="0"></use><g transform="translate(9548,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-4E" x="1242" y="488"></use><g transform="translate(878,-308)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-69" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMAIN-3D" x="345" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMAIN-31" x="1124" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-66" x="11925" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-72" x="12475" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-61" x="12927" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-63" x="13456" y="0"></use><g transform="translate(13890,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMAIN-7C" x="0" y="0"></use><g transform="translate(278,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-79" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-69" x="693" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMAIN-2212" x="1335" y="0"></use><g transform="translate(2336,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-67" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-69" x="675" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMAIN-7C" x="3158" y="0"></use></g><g transform="translate(17326,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-79" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-69" x="693" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMAIN-2C" x="18161" y="0"></use><g transform="translate(18606,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-79" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMATHI-69" x="693" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMAIN-3E" x="19719" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/321016/&amp;xid=17259,15700002,15700022,15700186,15700191,15700248,15700253&amp;usg=ALkJrhiEUqcGV2OF4xVIQgDaHqExSq5CQQ#MJMAIN-31" x="20775" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi><mi>A</mi><mi>P</mi><mi>E</mi><mo>=</mo><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>N</mi></mrow><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>N</mi></mrow></msubsup><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow></msub><mo>‚àí</mo><msub><mi>g</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow></msub><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow></mrow><mrow class="MJX-TeXAtom-ORD"><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow></msub></mrow><mo>,</mo><msub><mi>y</mi><mrow class="MJX-TeXAtom-ORD"><mi>i</mi></mrow></msub><mo>&gt;</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-1"> MAPE = \ frac {1} {N} \ sum_ {i = 1} ^ {N} \ frac {| y_ {i} - g_ {i} |} {y_ {i}}, y_ {i}> 1 </script><br><br>  where N is the number of objects in the sample, y <sub>i</sub> is the true operation time in the i-th experiment, g <sub>i</sub> is the predicted time.  The first place in the competition was taken by Mikhail Karachun, about his method of solving - further with his words. <br><br><h2>  Winner history </h2><br>  The general methodology for solving such problems is described in the <a href="http://num-meth.srcc.msu.ru/zhurnal/tom_2014/pdf/v15r150.pdf">document</a> attached to the competition.  The basic idea is to predict not the time to calculate <b>time</b> , but the value <b>time / (m * n * k)</b> .  Moreover, this value within a single system fluctuates slightly, and when teaching a model on several systems, it is better to use nonlinear methods at once, for example, a <a href="https://ru.wikipedia.org/wiki/Random_forest">random forest</a> . <br><br>  <b>Step zero - common</b> <br><br>  Python was used for the solution, and the result was checked locally using the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html">kfold</a> method with 5 partitions.  When preparing the data, all categorical signs underwent ‚Äúone value sign - one column‚Äù transformation ( <a href="http://fastml.com/converting-categorical-data-into-numbers-with-pandas-and-scikit-learn/">one hot</a> ) <br><br>  <b>Step one - model selection</b> <br><br>  The <a href="https://github.com/hyperopt/hyperopt">hyperopt</a> module was used to select model parameters.  It works better than a simple <a href="http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html">search on the grid</a> , since it does not just go through the parameters, but tries to optimize them. <br><br><div class="spoiler">  <b class="spoiler_title">Example</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- import pandas as pd import numpy as np from sklearn.ensemble import GradientBoostingRegressor from sklearn.cross_validation import KFold from hyperopt import fmin, tpe, hp, STATUS_OK, Trials import random random.seed(1) def mean_absolute_percentage_error(y_true, y_pred): ind = y_true &gt; -1 return np.mean(np.abs((y_true[ind] - y_pred[ind]) / y_true[ind])) def loss_func(y_true, y_pred): return mean_absolute_percentage_error(y_true,y_pred) all_train = pd.read_csv('~/Projects/DataMining/Bimbo/data/train1.csv') all_target = pd.read_csv('~/Projects/DataMining/Bimbo/data/y_train.csv') all_train['TARGET'] = all_target['time'] cols_to_drop = ['ID','TARGET'] cols = list(set(all_train.columns)-set(cols_to_drop)) print(len(cols)) def hyperopt_train_test(hpparams): all_results = [] kf = KFold(len(all_train['TARGET'].values),n_folds=5,random_state=1, shuffle=True) for train_index, test_index in kf: train = all_train.ix[train_index,:] test = all_train.ix[test_index,:] X_train = train[cols].values y_train_c = train['n'].values*train['m'].values*train['k'].values y_train = train['TARGET'].values X_test = test[cols].values y_test_c = test['n'].values*test['m'].values*test['k'].values y_test = test['TARGET'].values params_est = {'n_estimators':int(hpparams['n_estimators']), 'learning_rate':hpparams['eta'], 'max_depth':hpparams['max_depth'], 'min_samples_split':hpparams['min_samples_split'], 'min_samples_leaf':hpparams['min_samples_leaf'], 'loss':hpparams['loss'], 'alpha':hpparams['alpha'], 'subsample':hpparams['subsample'], 'random_state':1} bst = GradientBoostingRegressor(**params_est) bst.fit(X_train, np.log(y_train/y_train_c)) y_test_pred = np.exp(bst.predict(X_test))*y_test_c current_res = loss_func(y_test, y_test_pred) all_results.append(current_res) return np.mean(all_results) space4dt = { 'min_samples_split': hp.quniform('min_samples_split', 3, 14, 1), 'min_samples_leaf': hp.quniform('min_samples_leaf', 1, 7, 1), 'subsample': hp.quniform('subsample', 0.6, 0.99, 0.001), 'eta': hp.quniform('eta', 0.07,0.2, 0.001), 'n_estimators': hp.quniform('n_estimators', 10, 1000, 10), 'max_depth': hp.choice('max_depth', (4,5,6,7,8,9,10)), 'alpha': hp.quniform('alpha', 0.01, 0.99, 0.01), 'loss':hp.choice('loss', ('ls', 'lad', 'huber', 'quantile')), } def f(params): acc = hyperopt_train_test(params) print(acc) print(params) return {'loss': acc, 'status': STATUS_OK} trials = Trials() best = fmin(f, space4dt, algo=tpe.suggest, max_evals=2000, trials=trials) print 'best:' print best</span></span></code> </pre> </div></div><br>  After the first run with a small number of trees, GradientBoostingRegressor (loss = 'lad') performed best. <br><br>  <b>Step two - feature engineering</b> <br><br>  At the second stage, the task was set to eliminate unnecessary signs, since there were ~ 1100 of them altogether. The <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html">recursive selection method</a> was used <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html">for</a> this.  It consists in the gradual elimination of N signs by evaluation based on cross-validation.  At the output, the algorithm in the ranking_ parameter stores the stage where the sign was deselected: 1 - it means that it remains to the end, the more - the worse.  The parameter support_ stores the mask of the selected features - that is, those that are in the ranking_ with a unit.  It should be noted that the final variant is not always the best; sometimes, for solutions, signs can be used that were eliminated near the end of the selection.  This procedure was carried out for a long time, for example, on my laptop with an average performance, it took more than 12 hours. <br><br><div class="spoiler">  <b class="spoiler_title">Example</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- from sklearn.feature_selection.rfe import RFECV from sklearn.ensemble import GradientBoostingRegressor import numpy as np bst = GradientBoostingRegressor(**params_est) selector = RFECV(bst, step=50, cv=5) selector.fit(all_train[cols], target) print(list(selector.ranking_ )) print(np.asarray(cols)[selector.support_ ])</span></span></code> </pre> </div></div><br>  As a result, the number of signs was reduced to about 10. Further, by trial and error, several more successful ones were found. <br><br><div class="spoiler">  <b class="spoiler_title">Example</b> <div class="spoiler_text"><pre> <code class="python hljs">df.ix[:, <span class="hljs-string"><span class="hljs-string">'cpuExtra1'</span></span>] = <span class="hljs-number"><span class="hljs-number">0</span></span> df.ix[df[<span class="hljs-string"><span class="hljs-string">'cpuFull'</span></span>] == <span class="hljs-string"><span class="hljs-string">'Intel(R) Core(TM) i3-2310M CPU @ 2.10GHz'</span></span>, <span class="hljs-string"><span class="hljs-string">'cpuExtra1'</span></span>] = <span class="hljs-number"><span class="hljs-number">1</span></span> df.ix[:, <span class="hljs-string"><span class="hljs-string">'cpuExtra2'</span></span>] = <span class="hljs-number"><span class="hljs-number">0</span></span> df.ix[df[<span class="hljs-string"><span class="hljs-string">'cpuFull'</span></span>] == <span class="hljs-string"><span class="hljs-string">'Intel(R) Atom(TM) CPU N550 @ 1.50GHz'</span></span>, <span class="hljs-string"><span class="hljs-string">'cpuExtra2'</span></span>] = <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-comment"><span class="hljs-comment">#     df.ix[:, 'm_div_n'] = df['m'] / df['n'] df.ix[:, 'magic'] = df['k'] * df['m'] * df['n'] / (df['cpuCount'] * df['cpuCount']) cols = [ 'n', 'Sequential_read_128B_by128', 'k', 'Random_write_3MB_by128', 'cpuCount', 'Sequential_write_32kB_by128', 'Random_read_9MB_by128', 'm', 'SeqRead_20kB_by256', 'cpuCores', 'Sequential_read_48MB_by128', 'Random_read_4MB_by32', 'Random_write_32MB_by128', 'Random_read_2MB_by32', 'SeqCopy10MB_by128', 'BMI', 'm_div_n', 'magic', 'cpuExtra1', 'cpuExtra2', 'Random_write_bypassing_cache_6kB_by128', 'Sequential_read_192kB_by32', ]</span></span></code> </pre> </div></div><br>  In total, there were 20 traits taken from the sample and two generated.  The first was obtained by enumerating various functions of the dimension of matrices.  The second sign deserves special attention, since, despite the apparent lack of logic, it contributes to the improvement of the result. <br><br>  <b>Step Three - ensembling</b> <br><br>  Ensembles  If we combine decision trees trained on different subsets of features, then the result is more efficient than a single tree ‚Äî this is how random forest works.  But if you take several different ensembles and combine their solutions, this can also help.  As the general practice and my personal experience shows, if you have several models with approximately the same result, then their average is almost always better.  And the more these models differ in the logic of construction - the better.  For example, if you decide to take two random forests with the same parameters, but different numbers of trees, this is unlikely to help.  And if you take a random forest and gradient boosting regressor - it almost always turns out better, sometimes it is exactly what you need, if we are talking about two or three decimal places. <br><br>  When solving this problem, I took the top models obtained by over-optimizing the parameters and went through their combinations.  Especially well considered were models that gave an equally good average result for cross-validation, with the best and worst folds being different.  As a result, three models remained, the average was used as the predicted values. <br><br>  Learning different models on subsets of rows and / or columns did not give a quick result. <br><br><div class="spoiler">  <b class="spoiler_title">Example</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     params_est = {'n_estimators': 370, 'subsample': 0.961, 'learning_rate': 0.076, 'min_samples_split': 18.0, 'max_depth': 6, 'min_samples_leaf': 8.0, 'random_state':1, 'loss':'lad',} bst1 = GradientBoostingRegressor(**params_est) bst1.fit(X_train, y_train/y_train_c1) params_est = {'n_estimators': 680, 'subsample': 0.902, 'learning_rate': 0.076, 'min_samples_split': 14.0, 'alpha': 0.29, 'max_depth': 9, 'min_samples_leaf': 5.0, 'loss':'quantile', 'random_state':1} bst2 = GradientBoostingRegressor(**params_est) bst2.fit(X_train, y_train/y_train_c1) params_est = {'n_estimators': 430, 'subsample': 0.978, 'learning_rate': 0.086, 'min_samples_split': 19.0, 'max_depth': 6, 'min_samples_leaf': 10.0, 'loss':'lad', 'random_state':1} bst3 = GradientBoostingRegressor(**params_est) bst3.fit(X_train, y_train/y_train_c1)</span></span></code> </pre> </div></div><br>  <b>Step Four - we need to go deeper</b> <br><br>  Then I decided to check how much the forecast error differs depending on the different parameters of the computing system.  A simple search showed that if the average error during cross-validation is ~ 0.05, then on one of the operating systems this error is ~ 0.30. <br><br>  The first idea was to adjust the weights of objects during training, increasing them for a given OS, but the result only got worse.  Since the data for this OS is quite small (&lt;100), it is also impossible to set up a separate model - it will be retrained.  An interim solution helped.  I took a separate model, trained it in the entire sample, but with weights that gave priority to this OS.  When calculating the final result, this model was used for only one OS.  Those.  the main models were trained on the whole sample without weights and predicted the result for all but one of the wasps.  One model was trained on the entire sample with weights, and predicted for only one wasp.  Here, for the first time, difficulties with cross-validation arose - a local improvement was not always confirmed by a public assessment.  This is due to the fact that the number of examples for one OS is quite small, and if they are still divided into several parts for verification, then a stable result will not have to wait. <br><br><div class="spoiler">  <b class="spoiler_title">Example</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      all_train['w'] = 1 all_train['w'][all_train['os'] == 15] = 4 #      os = 15 params_est = {'n_estimators': 480, 'subsample': 0.881, 'learning_rate': 0.197, 'min_samples_split': 3.0, 'max_depth': 7, 'min_samples_leaf': 2.0, 'loss':'lad', 'random_state':1} bst4 = GradientBoostingRegressor(**params_est) bst4.fit(X_train, np.log(y_train/y_train_c), sample_weight=train['w'])</span></span></code> </pre> </div></div><br>  <b>Step five - last step</b> <br><br>  Having a fairly good set of models, another assumption was made that brought a fairly good result.  If you need to minimize the relative deviation, why not predict it?  Here I will try more.  There are several options that serve the model as a reference example: <br><br><ol><li>  We teach the model, feeding it as an output directly <b>time</b> - the time of calculation.  This is something that we refused immediately, following the advice of the authors of the article cited at the beginning. </li><li>  We train the model by applying <b>time / (m * n * k)</b> to the output ‚Äî that is, the angular coefficient calculated for each computing system.  This is what we have been doing up to this point. </li><li>  We teach the model by <b>inputting the</b> value <b>time / reg_k * (m * n * k)</b> .  Those.  we assume that for each computing system the time dependence of <b>m * n * k is</b> linear with the angular coefficient <b>reg_k</b> , and we train the model to predict the relative deviation from this model. </li></ol><br>  <b>Medium</b> <b>time / (m * n * k)</b> was taken as <b>reg_k</b> , signs of os + cpuFull were used as the identifier of the computing system, since it was on this combination that the linear model with the median gave the best result. <br><br><div class="spoiler">  <b class="spoiler_title">Example</b> <div class="spoiler_text"><pre> <code class="python hljs">all_train.ix[:, <span class="hljs-string"><span class="hljs-string">'c1'</span></span>] = all_train[<span class="hljs-string"><span class="hljs-string">'TARGET'</span></span>] / (all_train[<span class="hljs-string"><span class="hljs-string">'m'</span></span>] * all_train[<span class="hljs-string"><span class="hljs-string">'n'</span></span>] * all_train[<span class="hljs-string"><span class="hljs-string">'k'</span></span>]) all_train_median = all_train[[<span class="hljs-string"><span class="hljs-string">'c1'</span></span>, <span class="hljs-string"><span class="hljs-string">'os'</span></span>, <span class="hljs-string"><span class="hljs-string">'cpuFull'</span></span>]].groupby([<span class="hljs-string"><span class="hljs-string">'os'</span></span>, <span class="hljs-string"><span class="hljs-string">'cpuFull'</span></span>], as_index=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>).median() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#    df = pd.merge(df, all_train_median, on=['os', 'cpuFull'], how='left', suffixes=('', '_med')) df.ix[:, 'test_mdeian'] = df['c1_med']*df['m']*df['n']*df['k'] return df</span></span></code> </pre> </div></div><br>  <b>Step Six - rules rule</b> <br><br>  Also I will tell about a small hack, which, by the way, had a tangible impact on the final result.  If you look closely at the evaluation formula, it is clear that measurements with a duration of less than a second are excluded from it.  This means that all predicted values ‚Äã‚Äãof less than one can be safely rounded up, since if they are really smaller, then the result is not included, and if not, you will reduce the error. <br><br>  <b>General impressions</b> <br><br>  It should be noted that in the course of all the improvements described in the article, the local assessment of the decision, the score in the public score, and as it turned out in the private score, were always changed in the direction of improvement.  Here you can see the <a href="https://github.com/KarachunMikhail/mlbootcamp_matrix/blob/master/mlbootcamp.py">final working script</a> . <br><br><h2>  Try yourself! </h2><br>  As always, we offer on the portal a training article for newbies and a serious task for experts.  By the way, on the portal you can practice in solving previous contests - all of them are open in sandbox mode.  Join us for <a href="http://mlbootcamp.ru/signup/">registration</a> ! </div><p>Source: <a href="https://habr.com/ru/post/321016/">https://habr.com/ru/post/321016/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../321004/index.html">Why are CPA networks no longer a cake? Some good reasons</a></li>
<li><a href="../321008/index.html">Budget version of TrueRMS measurement</a></li>
<li><a href="../321010/index.html">Content, metadata and context of open data</a></li>
<li><a href="../321012/index.html">Simple math for solving difficult problems</a></li>
<li><a href="../321014/index.html">The book ‚ÄúDecentralized Applications. Blockchain technology in action ¬ª</a></li>
<li><a href="../321018/index.html">What is crowd marketing?</a></li>
<li><a href="../321020/index.html">How IT professionals work. SearchInform development manager Dmitry Gatsura</a></li>
<li><a href="../321022/index.html">YouTube Video Text Search</a></li>
<li><a href="../321024/index.html">Class'y Class'y</a></li>
<li><a href="../321026/index.html">How we got smarter and learned how to open quests twice as fast</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>