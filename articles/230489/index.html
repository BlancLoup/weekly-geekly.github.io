<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Some methods for finding fuzzy duplicate video</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="There is a fairly wide range of tasks where analysis is required, audio-visual models of reality. This applies to both static images and video. 



 B...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Some methods for finding fuzzy duplicate video</h1><div class="post__text post__text-html js-mediator-article">  There is a fairly wide range of tasks where analysis is required, audio-visual models of reality.  This applies to both static images and video. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ba/3d5/64c/6ba3d564cc407d07e9ba0eac57db5c76.png" width="600" alt="image"></div><br><br>  Below is a brief overview of some of the existing methods for finding and identifying fuzzy duplicates of video, and their advantages and disadvantages.  Based on the structural presentation of the video, a combination of methods is built. <br>  Overview of a very small, for details, it is better to refer to the original sources. <br><a name="habracut"></a><br>  The term ‚Äúfuzzy duplicate‚Äù means an incomplete or partial coincidence of an object with another object of a similar class.  Duplicates are natural and artificial.  Natural duplicates are similar objects under similar conditions.  Artificial fuzzy duplicates - derived from the same original. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Finding fuzzy duplicates can be useful for optical navigation of unmanned aerial vehicles (unfortunately, not always for peaceful purposes), for determining the nature of the terrain landscape, compiling video directories, grouping snippets of search engines, filtering video ads, and searching for pirated video. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ac/d92/261/3acd92261a985ba346bd032d789cb9aa.png" width="600" alt="image"></div><br><br>  The problem of finding fuzzy duplicate video (NDV) is closely related to the problems of classification (HF) and video search (PV).  The task of searching for NDV can be reduced to classification, and that, in turn, to annotation and search by video.  | {search NDV} |  &lt;| {KV} |  &lt;| {PV} |.  But these tasks are independent. <br><br><div style="text-align:center;"><img src="http://img-fotki.yandex.ru/get/6739/15410753.10/0_bf306_8f1fe63b_orig" width="400" alt="image"></div><br><br><h4>  General algorithm </h4><br>  To search for NDV video is divided into segments.  Keyframes are extracted from each segment.  Keyframe characteristics are used to represent the entire video.  The similarity between the videos is calculated as the similarity of the sets of these characteristics. <br><br><h4>  Approaches </h4><br>  WFD search methods form two categories: methods using global characteristics (GC);  methods using local characteristics (LH).  The distinction between categories is conditional, very often mixed techniques are used.  GC methods highlight frame level signatures to model spatial, color, and temporal information.  GC summarizes global statistics for low-level traits.  The similarity between the video is defined as the correspondence of sequences of signatures (signatures).  They can be useful for searching for ‚Äúalmost identical‚Äù videos and can reveal minor edits in the space-time domain.  GC ineffective when working with artificial NDV, which were obtained as a result of cosmetic editing.  For such groups, methods using the low-level characteristics of a segment or frame are more useful.  Typically, these methods are affected by changes in temporal order and the insertion or deletion of frames.  Compared to global methods, segment level approaches are slower.  They are more demanding from memory, although they are capable of identifying copies that have undergone substantial editing. <br><br><h4>  Local characteristics </h4><br><h5>  Track Trajectories </h5><br>  LH methods, reduce the task of finding similar videos to the task of finding duplicate images.  The main steps when comparing images: the definition of singular points;  selection of neighborhoods of singular points;  construction of feature vectors;  selection of image descriptors;  comparison of descriptors for a pair of images.  In the work, they identify particular points of the frame (using the Harris detector) and track their position throughout the video.  Then form a set of trajectories of points. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/476/eb9/817/476eb9817e01cd8d69433e274ae09165.png" width="600" alt="image"></div><br><br>  Pattern matching takes place on the basis of a fuzzy search.  The approach facilitates the localization of fuzzy duplicates of fragments.  However, the method of roads due to the allocation of special points of frames.  And the fact that the trajectories of the points are sensitive to the movement of the camera, make the algorithm applicable only for finding exact copies of the video. <br><br><h5>  Average value </h5><br>  Authors of <a href="http://www.researchgate.net/publication/4267374_NON-IDENTICAL_DUPLICATE_VIDEO_DETECTION_USING_THE_SIFT_METHOD">Non-identical Duplicate Video Detection Using The SIFT method</a> highlight specific points of keyframes, and estimate the similarity of frames based on SIFT.  The similarity of frames is calculated as the arithmetic average of the number of matched singular points.  But to determine the similarity of a video, a full conformity assessment (PIC) is used as the mean value of the similarity of key frames throughout the video.  It is important that the average value is calculated, not for all possible pairs of frames, but only for some of them.  This saves computing resources. <br><br><h6>  Longitudinal frames </h6><br>  The work is also interesting because the selection of frames in it occurs along the time axis, and not across, as in a normal video.  Such a slice allows you to extract temporal information from the video, and apply spatial comparison methods to it, the same as for ordinary frames: SIFT is applied to the two video slices and PIC is calculated. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ac5/2bf/109/ac52bf1095781c8ed2ba9335d5176ac3.png" alt="image"></div><br><br>  According to the results of experiments, the method of longitudinal frame allocation is worse than usual.  This is especially evident if there are a lot of sharp camera movements in the video.  The disadvantages of the general approach include the use of SIFT.  The experiments were conducted on video with a small resolution (320 √ó 240).  If you increase the size of frames, singling out special points will be very expensive.  If you apply the PIC only to regular frames, the time information of the video will not be taken into account. <br><br><h5>  Visual words </h5><br>  Methods using visual words are an improved version of the direct comparison of special points of frames.  They are based on the quantization of singular points - the formation of "words".  Comparison of frames (and video) entirely occurs in frequency dictionaries, as for texts.  The work of the <a href="https://lear.inrialpes.fr/pubs/2008/DGJMS08a/trecvid_douze_jegou_schmid.pdf">Inria-Learar's video copy detection system</a> demonstrates excellent method performance.  Keyframes are represented by features that are obtained using SIFT.  Then these characteristics are quantized into visual words.  A visual signature is built from the visual words.  But, for the application of visual words, frequency dictionaries must be built for a previously known subject area. <br><br><h4>  Global characteristics </h4><br>  Using GC methods, video, color, spatial and temporal information is separated from the video, presented as a sequence of characters, and methods of searching for matching strings are used. <br><br><h5>  Local-Sensitive Hashing </h5><br>  <a href="http://www.cs.princeton.edu/cass/papers/mm08.pdf">Efficiently matching sets of features with random histograms</a> use locally sensitive hashing (LCH).  It is used to display the color histogram of each keyframe on a binary vector.  Characteristics of the frame stand out from the local features of the image.  These characteristics are represented as sets of points in the space of characteristics.  With the help of LCH points are mapped to discrete values.  The frame histogram is built according to the set of features.  Next, the histograms are compared as normal sequences. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/830/8ab/7cf/8308ab7cf26fbadfb41824142e631585.png" width="600" alt="image"></div><br><br>  Experimental results confirmed effectiveness.  But, as indicated in the <a href="http://vireo.cs.cityu.edu.hk/papers/icme09-wanlei.pdf">Large-scale work near-duplicate Web video search: challenge and opportunity</a> method suffers from the potential problem of large memory consumption.  Temporary information is not taken into account. <br><br><h5>  Ordinal signatures </h5><br>  <a href="http://www.researchgate.net/publication/224612969_Robust_video_signature_based_on_ordinal_measure">Robust video signature based on ordinal measure</a> uses ordinal signatures to simulate the relative intensity distribution in a frame.  The distance between two fragments is measured using the temporal similarity of signatures.  The approach allows you to search for fuzzy duplicates of video with different resolutions, frame rates, with minor spatial changes of frames.  The advantage of the algorithm is the ability to work in real time.  The disadvantages include instability to large inserts of extra frames.  The method is poorly applicable to the search for natural fuzzy duplicates, for example, if the object was filmed at different light levels. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3f6/7c5/81d/3f67c581dbdde264499ced8a6dd85262.png" width="600" alt="image"></div><br><br>  <a href="http://www.researchgate.net/publication/220979619_Comparison_of_sequence_matching_techniques_for_video_copy_detection">Comparison of</a> motion <a href="http://www.researchgate.net/publication/220979619_Comparison_of_sequence_matching_techniques_for_video_copy_detection">detection</a> describes the motion signature that captures the relative change in intensity over time.  Color signatures, movement signatures and ordinal signatures are compared.  Experiments show that ordinal signature is more efficient ... <br><br><h5>  Video as DNA </h5><br>  The approach proposed in the work <a href="http://www.academia.edu/2031926/Near-Duplicate_Video_Retrieval_Based_on_Clustering_by_Multiple_Sequence_Alignment">Near-duplicate Video Retrieval Based on Clustering by Multiple Sequence Alignment</a> reduces the task of finding fuzzy video duplicates to the task of classifying video.  The method is based on multiple sequence alignment (MSA).  A similar approach is used in bioinformatics to search for alignment of DNA sequences.  The authors use heuristic alignment and iterative methods proposed in the work of <a href="">Muscle: multiple sequence alignment</a> .  The method can be described in the following sequence of steps. <br><br><ol><li>  For the video is built DNA representation. </li><li>  Video from the database, each is compared with each, and a distance matrix is ‚Äã‚Äãconstructed. </li><li>  A guide tree is built on the basis of the distance matrix using the neighbor joining method (neighbor joining). </li><li>  Based on the guide tree, progressive video alignment is performed. </li><li>  Alignment results are used to form video clusters. </li><li>  When searching the database, the query is compared with the cluster centers.  If the similarity between the request and the cluster center exceeds a certain threshold, then all the clips in the cluster are considered fuzzy duplicates of the request. </li></ol><br><br>  For the presentation of the video as genomes, keyframes are selected, translated into a halftone color space, and the frames are divided into 2 √ó 2 blocks. Perhaps only 24 = (2 ‚ãÖ 2)!  spatial ordinal pattern.  Each such pattern can be assigned a letter of the Latin alphabet. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e17/c08/2d4/e17c082d41d5eced36eb3d9231318f1d.png" width="600" alt="image"></div><br><br>  The distance matrix is ‚Äã‚Äãconstructed using a sliding window of size n.  Thus, the video comparison is based on n-grams.  Comparison between two DNA representations of a video occurs only in a window. <br>  The window size and step size (how much to shift the window after comparison) are set as parameters. <br><br>  The guide tree is constructed by a greedy heuristic algorithm for joining neighbors.  According to the distance matrix, the two closest DNA views are distinguished and combined into one tree node as a video profile.  In this model, each video DNA is the leaves of the tree, profiles appear in the nodes of the tree.  Profiles are also compared with each other, and glued similarly to a sheet view.  The result is a decision tree. <br><br>  Below is a guide tree. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/54d/687/679/54d687679fb62a6385a5d8e850bf2454.png" width="600" alt="image"></div><br><br>  Advantages of the approach: it has high accuracy and completeness and does not require special computational costs.  Cons of the approach - the method does not take into account the temporal information of the video. <br><br><h5>  Scene change </h5><br>  A suffix array approach to socialnetworks introduced a signature based on the definition of the boundaries of scenes (shootings). <br>  There are three different concepts.  A frame or photographic frame is a static picture.  A scene or a scene is a multitude of frames connected by the unity of place and time.  Shooting or cinematic frame, a lot of frames related to the unity of shooting.  The scene may include several shots.  In the literature, filming is often called a ‚Äúscene.‚Äù  Next, we will consider shooting, but we will call it the same - ‚Äúscene.‚Äù <br><br>  The video contains more information than just a series of frames.  Events in a video uniquely define its temporal structure, which can be represented by a set of key frames. <br><br>  In this case, key frames are understood not as an I-frame, but just some special video frame.  By events is not a semantic component of the video plot, but only a change in the frame content.  Keyframing is based on finding differences in brightness histograms.  After extracting the keyframes, the length between the current keyframe and the previous one is calculated.  All lengths are recorded as a one-dimensional sequence.  This sequence of scene lengths is the caption of the video.  Experiments show that two unrelated video clips do not have a long set of consecutive key frames with the same scene lengths.  The paper proposes an effective way to map video signatures.  To do this, use the suffix array.  The problem comes down to finding common substrings.  The method does not work well if there are a lot of camera or object movements in the video and with smooth transitions between scenes.  The disadvantage is the inability to work in real time, for comparison, you must have the entire video. <br><br>  In this paper <a href="http://msu-issled.ru/index.php/kibernetika/20-kibernetika/866-duplikate">, the Duplicate Search Algorithm in the base of video sequences based on the comparison of the scene change hierarchy</a> suggests an algorithm for comparing the scene tree (shooting).  Based on the scene changes found, a binary tree is built.  Subtrees correspond to video fragments.  Root vertices of subtrees store the values ‚Äã‚Äãand position of the dominant scene change in the current fragment.  When comparing, in one tree (longer video) look for the vertices closest in size to the root of the second tree, and compare their coordinates.  The procedure is performed recursively for the remaining scene changes.  The advantages of the method are the resistance to most methods of creating artificial duplicates (because only temporary information is used), the low complexity of comparing two clips and the possibility of creating hierarchical film indexes.  This allows you to catch the mismatch in the initial stages of verification.  The disadvantages of the approach as the previous one is that the characteristics of the scenes themselves are not taken into account.  To determine fuzzy duplicates, it is required to have a video request completely, which is not always possible. <br><br><h4>  Conclusion </h4><br>  To search for NDV used a variety of methods.  At the moment, the most promising seem GC-methods.  They allow solving the problem approximately without special computational costs.  However, LH methods may be required to clarify.  As shown in the <a href="http://vireo.cs.cityu.edu.hk/papers/icme09-wanlei.pdf">Large-scale near-duplicate Web video search: challenge and opportunity, the</a> use of combined approaches gives higher accuracy than each of the methods separately. <br><br><h4>  Combination </h4><br>  Of interest are the works in which the structural model of the video is built.  Video can be viewed as a sequence of facts, developing in time.  Moreover, in different videos both the facts and their order can be different.  The properties of the facts form the spatial characteristic of the video, and the duration and order of the facts are temporary.  The easiest way to extract facts from a video is to use scene change points (filming).  It is important to note that the time in two different videos may go differently.  We propose to use the relationship of the lengths of scenes to the lengths of adjacent scenes ... The relative lengths of the scenes of two fuzzy duplicates will rarely coincide.  This is due, among other things, to the recognition errors of scene boundaries.  To solve this problem, you can apply sequence alignment algorithms.  But, so, we compare only the order of the video facts.  To compare the facts themselves, internal characteristics of the scenes are required, for example, characteristics of the initial and final frames.  Here it is convenient to use visual words as a technique from LH-methods.  So we got a scene descriptor. <br><br>  Formally, the scene as "shooting", cinematic frame - a set of multiple photographic frames inside the time domain, frames, which is significantly different from the frames of neighboring areas.  The picture below shows the first frames of some video scenes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/10c/95b/ba4/10c95bba4d92a41c3e891ce0cc87b4a5.png" width="600" alt="The first frames of some video scenes"></div><br><br>  If the original video is compressed with different codecs, we will get fuzzy duplicates of this video.  Selecting the scenes in each video, we see that the scene reversal points for these two files do not match. <br><br>  To solve this problem, it is proposed to use relative lengths of scenes.  The relative length of the scene is calculated as a vector of ratios of the absolute length of the scene to the absolute lengths of the remaining video scenes.  In practical problems, it is more convenient to calculate the length relations for the three previous scenes, and not for all.  This is convenient even if all the video is completely inaccessible to us and we are dealing with a video stream, for example, in real-time tasks. <br><br>  The relative length of the scenes of two fuzzy duplicates will rarely coincide.  Moreover, many scenes may simply not be recognized.  This is due, among other things, to the recognition errors of scene boundaries. <br><br>  If the relative length of the scene of one video is no more than twice the length of the scene of another video, and all previous scenes are aligned, then the current pair of scenes expresses the same phenomenon, provided that both videos are indistinct duplicates of each other ( <a href="http://en.wikipedia.org/wiki/Gale%25E2%2580%2593Church_alignment_algorithm">hypothesis Heila Church</a> ).  A similar approach is used in mathematical linguistics for the alignment of parallel bodies of texts in different languages.  The less the relative length of the scenes, the more likely the scenes are similar.  If the lengths differ more than twice, then the length of the smaller scene is added to the length of the next scene of the same video, and the combined scene is considered as one.  In case of coincidence of the relative lengths of the video scenes, a comparison of the internal properties of the scene is applied. <br><br>  Thus, the proposed scene descriptor can be formally described.  It consists of a vector of ratios of the length of the scene to the lengths of the other scenes and the characteristics of the initial and final frames.  It is convenient to store it immediately, with associations of neighboring scenes (the three previous ones), taking into account the Gale-Church hypothesis. <br><br><h4>  So what is next ‚Ä¶ </h4><br>  Thanks to this descriptor and various technical tricks (semantic hashing), a fairly obvious algorithm was built. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/223/203/b22/223203b22910540cb78f8830d9d973ca.png" width="600" alt="image"></div><br><br>  But the practical implementation of it is still in its infancy.  Some results were still obtained <a href="http://www.slideshare.net/w495/ss-36679261">*</a> , but for some external reasons, the work was temporarily stopped. <br><br>  I would be very happy with any comments and additions. <br><br>  UPD 1: <br><br>  Some material additions can be found in the presentation: <a href="http://www.slideshare.net/w-495/nkp-2015">Fuzzy Duplicate Video Search Elements</a> <br><br>  More on the topic: <br><ul><li>  <a href="http://www.slideshare.net/yandex/ss-42667172">Search for similar video clips based on the analysis of video content (Anatoly Borisov)</a> <br></li></ul><br><br><h4>  Thanks </h4><br>  Thank <a href="https://habrahabr.ru/users/goodok/" class="user_link">goodok</a> for correcting grammatical errors. </div><p>Source: <a href="https://habr.com/ru/post/230489/">https://habr.com/ru/post/230489/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../230477/index.html">WebCamp 2014 Live Webcast</a></li>
<li><a href="../230479/index.html">3 worst things you can do with Linq to Database</a></li>
<li><a href="../230481/index.html">The digest of interesting news and materials from the world of PHP No. 44 (July 1 - 20, 2014)</a></li>
<li><a href="../230483/index.html">Lightning bolts</a></li>
<li><a href="../230487/index.html">Macros in Vim are just</a></li>
<li><a href="../230491/index.html">Rosetta - the nucleus of the comet is close</a></li>
<li><a href="../230493/index.html">Python-digest # 33. News, interesting projects, articles and interviews [July 13, 2014 - July 20, 2014]</a></li>
<li><a href="../230495/index.html">How to discuss money at the interview, if you hire someone and a little about working with people</a></li>
<li><a href="../230497/index.html">Announcement of the second meeting of Java User Group Ekaterinburg</a></li>
<li><a href="../230499/index.html">Jibo: ‚Äúsocial‚Äù robot for the whole family</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>