<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Why self-learning artificial intelligence has problems with the real world</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The newest AI systems start learning without knowing anything about the game, and grow to the world level in a few hours. But researchers can hardly c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Why self-learning artificial intelligence has problems with the real world</h1><div class="post__text post__text-html js-mediator-article"><h2>  The newest AI systems start learning without knowing anything about the game, and grow to the world level in a few hours.  But researchers can hardly cope with the use of such systems outside the game world. </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/e80/132/bc8/e80132bc8376e0c783d9e07bd7447587.jpg"><br><br>  Until recently, machines capable of confounding human champions would at least have respect for using human experience to learn games. <br><br>  To win Garry Kasparov chess in 1997, IBM engineers used centuries of chess wisdom to create their computer Deep Blue.  In 2016, the Google DeepMind AlphaGo program <a href="https://www.quantamagazine.org/is-alphago-really-such-a-big-deal-20160329/">defeated</a> champion Lee Sedol in an ancient go board game, processing millions of game positions collected from tens of thousands of games between people. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      But now AI researchers are rethinking how their bots should absorb human knowledge.  The current trend can be described as ‚Äúyes and god with it‚Äù. <br><br>  Last October, the DeepMind team <a href="https://www.nature.com/articles/nature24270">published the</a> details of the go go system, AlphaGo Zero, which didn‚Äôt study people at all.  She started with the rules of the game and played with herself.  The first moves were completely random.  After each game she accepted new knowledge about what led to victory and what did not.  After these matches, AlphaGo Zero pitched against the already superhuman version of AlphaGo, which defeated Lee Sedol.  The first won the second with a score of 100: 0. <br><a name="habracut"></a><br><img src="https://habrastorage.org/getpro/habr/post_images/601/f9c/cf8/601f9ccf85217150722d2498d695750f.jpg"><br>  <i>Lee Cedol, the 18-time world champion in go, the match against AlphaGo in 2016.</i> <br><br>  The team continued the research and created the next genius player in the AlphaGo family, this time simply called AlphaZero.  In a <a href="https://arxiv.org/abs/1712.01815">paper</a> published on arxiv.org in December, DeepMind researchers uncovered how AlphaZero, starting from scratch, trained and defeated AlphaGo Zero ‚Äî that is, it defeated the bot, the winning bot, and the winning best player in the world.  And when she was given the rules for Japanese chess <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2591%25D0%25B3%25D0%25B8">shogi</a> , AlphaZero quickly learned and managed to beat the best of their specially created algorithms for this game.  Experts were amazed at the aggressive and unfamiliar style of the program.  ‚ÄúIt was always interesting to me what it would be like if our superior beings flew to Earth and showed us how they play chess,‚Äù said Danish grandmaster Peter Heine Nielsen in <a href="http://www.bbc.com/news/technology-42251535">an interview with the</a> BBC.  - Now I know". <br><br>  Last year, we also saw other bots from other worlds that showed themselves in areas as diverse as unlimited poker and Dota 2, a popular online game in which fantasy heroes fight for control of another world. <br><br>  Naturally, the ambitions of companies investing money in such systems extend beyond the dominance of gaming championships.  Research teams like DeepMind hope to apply similar methods to real-world tasks ‚Äî creating superconductors operating at room temperature, or understanding how origami will turn proteins into molecules useful for drugs.  And, of course, many practitioners hope to build a general-purpose artificial intelligence ‚Äî a poorly defined, but fascinating goal is to give the machine the opportunity to think like a person and be flexible in solving various problems. <br><br>  However, despite all the investments, it is not yet clear how far current technologies can go beyond the limits of the game board.  ‚ÄúI'm not sure that the ideas behind AlphaZero will be so easy to summarize,‚Äù <a href="https://homes.cs.washington.edu/~pedrod/">says</a> Pedro Domingos, a computer scientist at the University of Washington.  "Games are a very, very unusual topic." <br><br><h2>  Ideal goals for an imperfect world </h2><br>  One common feature of many games, including chess and go - players, is constantly visible all the chips on both sides of the board.  Each player has what is called "ideal information" about the state of the game.  No matter how difficult the game is, you just need to think about the current position. <br><br>  Many real world situations don't compare with this.  Imagine asking a computer to make a diagnosis or conduct business negotiations.  ‚ÄúMost of the strategic interactions in the real world are related to hidden information,‚Äù says <a href="https://www.cs.cmu.edu/~noamb/">Noam Brown</a> , a computer science graduate student at Carnegie Malone University.  "It seems to me that most of the AI ‚Äã‚Äãcommunity ignores this fact." <br><br>  Brown, on which Brown specializes, offers a different challenge.  You do not see the opponent's cards.  But here, too, machines learning through the game with themselves are already reaching superhuman heights.  In January 2017, the Libratus program, created by Brown and his curator <a href="http://www.cs.cmu.edu/~sandholm/">Thomas Sandholm</a> , <a href="https://www.cmu.edu/news/stories/archives/2017/january/AI-beats-poker-pros.html">beat</a> four professional players in no-limit <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D1%2585%25D0%25B0%25D1%2581%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9_%25D1%2585%25D0%25BE%25D0%25BB%25D0%25B4%25D0%25B5%25D0%25BC">Texas Hold'em</a> , winning $ 1.7 million at the end of the 20-day championship. <br><br>  An even more discouraging game with imperfect information - StarCraft II, another multiplayer online game with a huge number of fans.  Players choose a team, build an army and wage war on a sci-fi landscape.  But the landscape is surrounded by the fog of war, because of which players see only those parts of the territory in which their own troops or buildings are located.  Even in the decision to explore the territory of the opponent is full of uncertainty. <br><br>  This is the only game in which the AI ‚Äã‚Äãcan not yet win.  Obstacles are a huge number of options for moves in the game, which usually exceeds a thousand, and the speed of decision making.  Each player - a person or a car - has to worry about a huge number of likely development scenarios with each click of the mouse. <br><br>  So far, the AI ‚Äã‚Äãcannot compete on an equal footing with people in this area.  But this is the goal for the development of AI.  In August 2017, DeepMind <a href="https://deepmind.com/blog/deepmind-and-blizzard-open-starcraft-ii-ai-research-environment/">cooperated</a> with Blizzard Entertainment, the company that created StarCraft II, to create tools that they said would open this game for AI researchers. <br><br>  Despite all the complexity, the goal of StarCraft II is simple: to destroy the enemy.  This makes it related to chess, go, poker, Dota 2 and almost any other game.  In games you can win. <br><br>  From the point of view of the algorithm, tasks should have a ‚Äútarget function‚Äù, a goal to be pursued.  When AlphaZero played chess, it was easy.  The loss was estimated at -1, a draw at 0, a victory at +1.  The objective function of AlphaZero is to maximize points.  The objective function of a poker bot is just as simple: win a lot of money. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/986/b6d/744/986b6d744256e0777fa12030a3703737.gif"><br>  <i>Computer walkers can train complex behavior, like walking in unfamiliar terrain.</i> <br><br>  Situations in real life are not so simple.  For example, the robot requires a more subtle formation of the objective function ‚Äî something like a careful selection of words when describing your desire for a genie.  For example: quickly deliver a passenger to the correct address, submitting to all laws and weighing the cost of human life in dangerous and uncertain situations accordingly.  Domingos says that the formation of a target function by researchers is ‚Äúone of those things that distinguishes a great researcher in the field of machine learning from the average one‚Äù. <br><br>  Consider Tay, the chat bot for Twitter, which Microsoft released on March 23, 2016.  His goal was to engage people in conversation, which he did.  ‚ÄúWhat, unfortunately, Tay found,‚Äù said Domingos, ‚Äúis that the best way to maximize people's involvement would be to give racist insults.‚Äù  He was <a href="https://motherboard.vice.com/en_us/article/kb7zdw/microsoft-suspends-ai-chatbot-after-it-veers-into-white-supremacy-tay-and-you">turned off</a> just a day after starting work. <br><br><h2>  Your own main enemy </h2><br>  Some things do not change.  The strategies used today by the prevailing game bots were invented many decades ago.  ‚ÄúThis is such a blast from the past - they just give it more computing power,‚Äù says <a href="http://www.cs.toronto.edu/~duvenaud/">David Duveno</a> , an informatics specialist at Tokyo University. <br><br>  Strategies are often based on reinforced learning, technology with freedom of action.  Instead of engaging in micromanagement, tuning the smallest details of the algorithm‚Äôs work, engineers let the machine explore the environment to learn how to achieve goals on its own, through trial and error.  Before the release of AlphaGo and his heirs, the DeepMind team achieved the first big success that hit the headlines in 2013 when it used reinforcement training to create a bot that <a href="https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning/">learned to play</a> seven Atari 2600 games, and in three of them - at the expert level. <br><br>  This progress continued.  On February 5, DeepMind released <a href="https://deepmind.com/blog/impala-scalable-distributed-deeprl-dmlab-30/">IMPALA</a> , an AI system that can learn 57 games with the Atari 2600 and another 30 levels made by DeepMind in three dimensions.  On them, the player acts in various environments and achieves goals such as opening doors or collecting mushrooms.  IMPALA seemed to transfer knowledge between tasks ‚Äî the time spent on one game improved the results in the others. <br><br>  But in the wider category of training with reinforcements, board and multiplayer games, you can use a more specific approach.  Their study can go in the form of a game with itself, when the algorithm reaches strategic superiority, repeatedly competing with a close copy of itself. <br><br>  This idea has been around for decades.  In the 1950s, IBM engineer Arthur Samuel <a href="https://webdocs.cs.ualberta.ca/~chinook/project/legacy.html">created a</a> program for the game of checkers, which partially learned to play, competing with itself.  In the 1990s, Gerald Tesauro of IBM created a backgammon program that opposed the algorithm to itself.  The program has reached the level of expert people, at the same time inventing unusual but effective strategies of the game. <br><br>  In an ever-increasing number of games, algorithms for playing with themselves are provided with an equal opponent.  This means that a change in the strategy of a game leads to a different result, due to which the algorithm receives instant feedback.  ‚ÄúEvery time you learn something when you open up some little thing, your opponent immediately starts using it against you,‚Äù says <a href="http://www.cs.toronto.edu/~ilya/">Ilya Suckever</a> , research director at OpenAI, a non-profit organization that he founded with Ilon Mask, dedicated to the development and dissemination of AI-technologies and the direction of their development in a safe direction.  In August 2017, the organization <a href="https://geektimes.ru/post/292547/">released a bot</a> for Dota 2, which managed one of the characters of the game, Shadow Fiend, a necromantic demon, who won the best players in the world in one-on-one battles.  Another OpenAI project confronts simulations of people in a <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2583%25D0%25BC%25D0%25BE">sumo</a> match, as a result of which they learn how to fight and tricks.  While playing with oneself, ‚Äúthere is no time to rest, you need to constantly improve,‚Äù said Suckever. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/wpa5wyutpGc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  Openai </h2><br>  But the old idea of ‚Äã‚Äãplaying with oneself is only one ingredient in the bots that prevail today, they still need a way to turn the gaming experience into a deeper understanding of the subject.  In chess, go, Dota 2 video games, permutations are larger than atoms in the universe.  Even if we wait for several human lives, while the AI ‚Äã‚Äãwill struggle with its shadow in the virtual arenas, the machine will not be able to realize each scenario, write it in a special table and access it when this situation happens again. <br><br>  To stay afloat in this sea of ‚Äã‚Äãpossibilities, ‚Äúit is necessary to generalize and highlight the essence,‚Äù says <a href="https://people.eecs.berkeley.edu/~pabbeel/">Peter Abbil</a> , an informatics specialist at the University of California at Berkeley.  IBM's Deep Blue did this with a built-in chess formula.  Armed with the ability to evaluate the power of game positions that she had not yet seen, the program was able to apply moves and strategies that increase its chances of winning.  In recent years, new technology makes it possible to completely abandon this formula.  ‚ÄúNow, all of a sudden, all this encompasses the 'deep network',‚Äù said Abbil. <br><br>  Deep neural networks, whose popularity has soared in recent years, are built from layers of artificial "neurons" <a href="https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/">layered on top of each other</a> , like a stack of pancakes.  When a neuron in one of the layers is activated, it sends signals to a higher level, and there they are sent even higher, and so on. <br><br>  By adjusting the connections between the levels, these networks surprisingly cope with the transformation of input data into the associated output, even if the connection between them seems abstract.  Give them a phrase in English, and they will be able to practice translating it into Turkish.  Give them images of animal shelters, and they can determine which one is for cats.  Show them the game poly, and they will be able to understand the probability of winning.  But usually such networks first need to provide lists of marked examples on which they can practice. <br><br>  That is why the game with yourself and the deep neural networks are so well combined with each other.  Independent games give out a huge number of scenarios, and the depth network has a virtually unlimited amount of training data.  And then the neural network offers a way to learn the experiences and patterns encountered during the game. <br><br>  But there is a catch.  For such systems to provide useful data, they need a realistic platform for games. <br><br>  "All of these games, all of these results, were achieved under conditions that allowed the world to simulate perfectly," said <a href="http://people.eecs.berkeley.edu/~cbfinn/">Chelsea Finn, a</a> Berkeley graduate student who uses AI to manipulate robotic hands and interpret data from sensors. Other areas are not easy to imitate. <br><br>  Robomobils, for example, hardly cope with bad weather or cyclists.  Or they may not perceive the unusual possibilities found in the real world - such as a bird flying directly into the camera.  In the case of robotic hands, as Finn says, the initial simulations gave basic physics, which allowed the arm to learn how to learn.  But they do not cope with the details of touching different surfaces, so tasks such as tightening the bottle cap - or carrying out complex surgery - require experience in reality. <br><br>  In the case of problems that are difficult to simulate, playing with yourself will no longer be so helpful.  ‚ÄúThere is a big difference between a truly ideal model of the environment, and an exemplary model that has been learned, especially when reality is really complicated,‚Äù <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Joshua Bengio</a> , a pioneer in-depth learning from the University of Montreal, wrote to me.  But AI researchers still have ways to move on. <br><br><h2>  Life after games </h2><br>  It is difficult to accurately indicate the beginning of the superiority of AI in games.  You can choose to lose Kasparov in chess, defeat Lee Sedol from AlphaGo's virtual hands.  Another popular option will be the day of 2011, when the legendary champion of the game <a href="https://ru.wikipedia.org/wiki/Jeopardy!">Jeopardy!</a>  Ken Jennings lost to IBM Watson.  Watson was able to handle clues and wordplay.  ‚ÄúI welcome the advent of our new computer masters,‚Äù wrote Jennings, under his last answer. <br><br>  It seemed that Watson has office skills similar to what people use to solve many real-world problems.  He could perceive input in English, process related documents in a blink of an eye, extract coherent pieces of information and choose the one best answer.  But after seven years, reality continues to put complex obstacles in front of AI.  Stat's health <a href="https://www.statnews.com/2017/09/05/watson-ibm-cancer/">report</a> in September indicated that the heir to Watson, specializing in cancer research and developing personalized recommendations for treating Watson for Oncology, ran into problems. <br><br>  ‚ÄúQuestions in the game Jeopardy!  It is easier to handle, because common sense is not required for this, ‚Äùwrote Bengio, who worked with the Watson team, in response to a request to compare these two cases from an AI point of view.  ‚ÄúTo understand a medical article is much more difficult.  A lot of basic research is required. ‚Äù <br><br>  But let the games and narrowly specialized, they resemble several real problems.  DeepMind researchers did not want to answer interview questions, indicating that their work on the AlphaZero is currently being studied by independent experts.  But the team suggested that such a technology would soon be able to help biomedicine researchers who wish to understand protein coagulation. <br><br>  To do this, they need to understand how the various amino acids that make up a protein <a href="http://folding.stanford.edu/diseases/">bend and fold</a> into a small three-dimensional machine, the functionality of which depends on its shape.  This complexity is similar to the complexity of chess: the chemists know the laws at such a level that it is rough enough to cheat certain scenarios, but there are so many possible configurations that it will not work to search all possible options.  But what if protein folding can be thought of as a game?  And this has already been undertaken.  Since 2008, hundreds of thousands of people have tried the online game <a href="https://fold.it/portal/">Foldit</a> , in which users are awarded points for the stability and reality of the collapsed protein structure.  The machine could train in a similar way, perhaps trying to surpass its previous best achievement with the help of reinforcement training. <br><br>  Learning with reinforcements and playing with yourself can help train and interactive systems, Satskever suggests.  This can give the robots, who have to talk to people, a chance to practice in this while talking to themselves.  Considering that specialized equipment for operating AI is becoming faster and more accessible, engineers have more and more incentives to design tasks in the form of games.  ‚ÄúI think that in the future the importance of playing with oneself and other ways of consuming a large amount of computing power will increase,‚Äù said Sackever. <br><br>  But if the final goal of machines is to repeat everything that a person is capable of, then even a generalized champion in the game of board games like AlphaZero still has room to grow.  ‚ÄúI need to pay attention, at least to me, this is obvious, to the huge gap between real thinking, creative research of ideas and today's abilities of AI,‚Äù says <a href="http://web.mit.edu/cocosci/josh.html">John Tenenbaum</a> , a cognitive scientist from MTI.  ‚ÄúSuch intelligence exists, but so far only in the minds of great AI researchers.‚Äù <br><br>  Many other researchers, sensing the hype around their field, offer their own criteria.  ‚ÄúI would recommend not to overestimate the importance of these games, for AI or for general purpose tasks.  People are not very good at playing the game, ‚Äùsaid Francois Cholet, a Google depth study researcher.  ‚ÄúBut keep in mind that even very simple and specialized tools can accomplish much.‚Äù </div><p>Source: <a href="https://habr.com/ru/post/410999/">https://habr.com/ru/post/410999/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../410989/index.html">The predecessors of fitness bracelets: pedometer, heart rate monitor, bike computer</a></li>
<li><a href="../410991/index.html">Ledger's hardware cryptocurrency wallet was hacked by a 15-year-old hacker</a></li>
<li><a href="../410993/index.html">Someone sends sex toys from Amazon to strangers. Amazon doesn't know how to stop them.</a></li>
<li><a href="../410995/index.html">Telegram complained of Russia to the European Court of Human Rights</a></li>
<li><a href="../410997/index.html">Medical lawyer commented on the current law on telemedicine</a></li>
<li><a href="../411001/index.html">Video filmed from a Uber unmanned vehicle that killed a man</a></li>
<li><a href="../411003/index.html">We turn the gateway of the smart home Xiaomi into a column</a></li>
<li><a href="../411005/index.html">President of the United States banned the merger of Broadcom and Qualcomm</a></li>
<li><a href="../411007/index.html">California is asking Apple to explain why the company does not allow users to repair their phones</a></li>
<li><a href="../411009/index.html">Heating system of an apartment building. Likbez with examples</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>