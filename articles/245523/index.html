<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Concurrent programming with CUDA. Part 2: GPU hardware and parallel communication patterns</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Content 
 Part 1: Introduction. 
 Part 2: GPU hardware and parallel communication patterns. 
 Part 3: GPU fundamental algorithms: convolution (reduce)...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Concurrent programming with CUDA. Part 2: GPU hardware and parallel communication patterns</h1><div class="post__text post__text-html js-mediator-article"><h4>  Content </h4><br>  <a href="http://habrahabr.ru/company/epam_systems/blog/245503/">Part 1: Introduction.</a> <br>  <b>Part 2: GPU hardware and parallel communication patterns.</b> <br>  <a href="http://habrahabr.ru/company/epam_systems/blog/247805/">Part 3: GPU fundamental algorithms: convolution (reduce), scan (scan) and histogram (histogram).</a> <br>  Part 4: Fundamental GPU algorithms: compaction (compact), segmented scan (segmented scan), sorting.  Practical application of some algorithms. <br>  Part 5: Optimization of GPU programs. <br>  Part 6: Examples of parallelization of sequential algorithms. <br>  Part 7: Additional Parallel Programming Topics, Dynamic Concurrency. <br><a name="habracut"></a><br><h4>  Parallel communication patterns </h4><br><img src="https://habrastorage.org/files/b72/ccf/351/b72ccf3519e64260a9bef3261e4bac44.png"><br>  What is parallel computing?  Nothing else but a multitude of flows that solve a specific task <b>cooperatively</b> .  The key word here is ‚Äúcooperative‚Äù - to achieve cooperation, it is necessary to apply certain communication mechanisms between the streams.  When using CUDA, communication takes place through memory: streams can read input data, change output data, or exchange ‚Äúintermediate‚Äù results. <br>  Depending on the way in which the threads communicate through memory, different <b>patterns of parallel communication</b> are distinguished. <br>  In the previous section, the task of converting a color image to shades of gray was considered as a simple example of using CUDA.  For this, the intensity of each pixel of the output image in shades of gray was calculated by the formula <i>I = A * pix.R + B * pix.G + C * pix.B</i> , where <i>A, B, C</i> are constants, <i>pix</i> is the corresponding pixel of the original image.  Graphically, this process looks like this: <br><img src="https://habrastorage.org/files/754/3ce/dfd/7543cedfde1d42feb1e4405b52888239.png"><br>  If we abstract from the very method of calculating the output value, we get the first parallel communication pattern - <b>map</b> : the same function is performed on each input data element, with index <i>i</i> , and the result is stored in the output data array under the same index <i>i</i> .  The <b>map</b> template is very effective on the GPU, and moreover, it is simply expressed in the framework of CUDA - just run one thread per input element (as was done in the previous part for the task of converting an image).  However, only a small part of the tasks can be solved using only this template. <br>  If, however, several input elements are used to calculate the output value with the index <i>i</i> , then this pattern is called the <b>gather</b> , and it can look like this: <br><img src="https://habrastorage.org/files/004/144/3b3/0041443b3d474e28859e13afbeda8d6c.png"><br>  Or so: <br><img src="https://habrastorage.org/files/5d5/f68/9cd/5d5f689cde0148da9fbfa2a579f3e577.png"><br>  The effectiveness of the implementation of this pattern on CUDA depends on what kind of input values ‚Äã‚Äãare used when calculating the output and their number - it is best when a small number of consecutive elements are used. <br>  The reverse pattern, <b>scatter</b> - each input element affects several (or one) output elements, graphically looks like the <b>gather</b> , but the meaning changes: now we are ‚Äúrepelling‚Äù not from the output elements, for which we calculate their value, but from the input, which affect the values ‚Äã‚Äãof certain output elements.  Quite often, the same problem can be solved within the framework of both the <b>gather</b> pattern and the <b>scatter</b> .  For example, if we want to average 3 neighboring input elements and write them into the output array, then we can: <br><ul><li>  run along the stream to each output element, where each stream will average the values ‚Äã‚Äãof 3 neighboring input elements - <b>gather</b> ; </li><li>  or run downstream to each input element, where each stream will add 1/3 of the value of its input element to the value of the corresponding output element - <b>scatter</b> . </li></ul><br>  As you most likely guessed, when using the <b>scatter</b> approach, a synchronization problem arises, since several streams may attempt to modify the same output element at the same time. <br>  It is also worth highlighting the subspecies of the <b>gather</b> - <b>stencil</b> pattern: in this pattern, a restriction is imposed on the input elements that are involved in the calculation of the output element - namely, it can only be neighboring elements.  In the case of 2D / 3D images, various types of this pattern can be used, for example, a two-dimensional von Neumann stencil: <br><img src="https://habrastorage.org/files/86c/0d1/2bb/86c0d12bbb1e4128a9cccadf8cc23ea1.png"><br>  or two-dimensional Moore stencil: <br><img src="https://habrastorage.org/files/441/4fe/28c/4414fe28c92f47209289e8af962f0d3c.png"><br>  In connection with this limitation, the <b>stencil</b> pattern is usually quite effectively implemented within CUDA: it is enough to run one thread per output element, and the flow will read the input elements it needs.  With this organization of computation, efficiency is ensured by two factors: <br><ol><li>  All data needed for one stream is grouped in memory (in the case of a one-dimensional array, a solid ‚Äúpiece‚Äù of memory, in the 2D case, several pieces of memory that are at the same distance from each other). </li><li>  The value of some input element is read several times from neighboring streams (the specific number of readings depends on the selected mask) - it becomes possible to "reuse" the data provided to us by CUDA - this will be discussed later in the article. </li></ol><br><h4>  GPU hardware </h4><br>  Consider the high-level structure of the GPU hardware: <br><img src="https://habrastorage.org/files/380/a5d/bea/380a5dbea458456db5e2b02934096921.png"><br><ul><li>  A CUDA-compatible GPU consists of several (usually dozens) <b>streaming multiprocessors</b> (streaming multiprocessors), followed by <b>SM</b> . </li><li>  Each <b>SM</b> , in turn, consists of several dozen <b>simple / streaming processors (SP)</b> (regular / stream processors), or to put it more precisely, <b>CUDA cores</b> ( <b>CUDA cores</b> ).  These guys are more like the usual CPU - they have their own registers, cache, etc.  Each <b>SM</b> also has its own <b>shared memory</b> (shared memory) ‚Äîa kind of additional cache that is accessible to all SPs, and can be used both as a cache for frequently used data and for ‚Äúcommunication‚Äù between threads of a single CUDA block. </li><li>  The GPU also has its own memory, called <b>device memory</b> , which is common to all CUDA streams - the <i>cudaMalloc</i> and <i>cudaMemcpy</i> functions work with it <s>(it is also the size used by schoolchildren and GPU manufacturers to</s> <i>compare</i> <s>with its size)</s> . </li></ul><br><h4>  Compliance with CUDA model and GPU hardware.  CUDA Warranties </h4><br>  According to the CUDA model, the programmer divides the task into blocks, and blocks into threads.  How does the mapping of these software entities with the above described GPU hardware blocks performed? <br><img src="https://habrastorage.org/files/ae5/bd0/a4f/ae5bd0a4f6b845069d29c1fc13c9c7e3.png"><br><ul><li>  Each block will be completely executed on the <b>SM</b> allocated to it. </li><li>  The distribution of blocks on the <b>SM</b> is engaged in the GPU, not the programmer. </li><li>  All streams of block <i>X</i> will be divided into groups, called <b>warps</b> (usually say so - warps), and executed on the <b>SM</b> .  The size of these groups depends on the GPU model, for example, for models with the <a href="http://en.wikipedia.org/wiki/Fermi_(microarchitecture)">Fermi</a> microarchitecture, it is equal to 32. All threads from the same warp are executed at the same time, occupying a certain part of the <b>SM</b> resources.  And they either perform the same instruction (but on different data), or idle. </li></ul><br>  For all these reasons, CUDA provides the following guarantees: <br><ul><li>  All threads in a particular block will be executed on any one <b>SM</b> . </li><li>  All threads of a specific <b>kernel</b> will be executed before the next kernel starts. </li></ul><br>  CUDA <b>does not guarantee</b> that: <br><ul><li>  Any block <i>X</i> will be executed before / after / simultaneously with some block <i>Y.</i> </li><li>  Some block <i>X</i> will be executed on some particular <b>SM</b> <i>Z.</i> </li></ul><br><h4>  Synchronization </h4><br>  So, let's list the main synchronization mechanisms provided by CUDA: <br><ul><li>  <b>A barrier</b> is a point in the code of the kernel, upon reaching which a thread can ‚Äúpass‚Äù further only if all the threads <b>from its block</b> have reached this point.  Once again: the barrier allows you to synchronize only the threads of <b>one block</b> , and not all the threads in principle!  The restriction is quite natural, because the number of blocks set by the programmer can significantly exceed the number of available <b>SMs</b> . </li><li>  <b>Atomic operations</b> are similar to atomic operations of the CPU, the full list of available operations can be found <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/">here</a> . </li><li>  <b>__threadfence</b> is not exactly a synchronization primitive: when this instruction is reached, a thread can continue execution only after all its memory manipulations become visible to other threads - in effect, it forces the thread to flush the cache. </li></ul><br><h4>  Basic principles for the effective use of CUDA </h4><br><ul><li>  The principle of increasing the ratio (useful time) / (time of memory operations) - was discussed in the previous article.  The fraction value can be increased in two ways - to increase the numerator, to reduce the denominator: that is, you need to either do more work, or spend less time on memory operations.  In addition to the obvious solution - to reduce the number of memory accesses as far as possible, the following principles of efficient work with memory are used: <br><ul><li>  Moving frequently used data to faster memory: local stream memory <i>&gt;</i> shared block memory <i>&gt;&gt;</i> shared device memory <i>&gt;&gt;</i> host memory.  Thus, if several streams in the same block use the same data, it is most likely that it makes sense to move them to the common memory of the block. </li><li>  Sequential memory access: since the streams in the blocks are actually running in warp groups, provided that the streams in the same warp work with data located in memory sequentially, CUDA can read one large chunk of memory as one instruction.  Otherwise, if the streams in the warp will access the data scattered in the memory, the number of memory accesses will increase. </li></ul><br></li><li>  Reducing divergence of flows: a feature of the work of CUDA is that the threads in the same warp <b>always</b> either execute the same instruction or idle.  Thus, if in the kernel code there is code of the form <br><pre><code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (threadIdx.x % <span class="hljs-number"><span class="hljs-number">2</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>) { ... } ...</code> </pre> <br>  , then half of the warp threads (those with an odd index) will wait for the other half to execute the code inside the if-a.  Therefore, you should avoid such situations. </li></ul><br><h4>  Writing a second program on CUDA </h4><br>  We proceed to practice.  As an example illustrating the theory presented, we will write a program that performs Gaussian image blurring.  The principle of operation is the following: the value of the <i>R, G, B</i> channels of a pixel in the output blurred image is calculated as the weighted sum of the values ‚Äã‚Äãof the <i>R, G, B</i> channels of all pixels of the original image in a stencil: <br><img src="https://habrastorage.org/files/d9a/f65/f48/d9af65f480a549e3abd9357eca847f89.png"><br>  The weights are calculated using the 2D Gaussian distribution, but exactly how this is done is not too important for our task. <br>  As you can see from the description of the task, it is quite natural to choose a <b>stencil</b> pattern for the implementation of this algorithm, because each pixel of the output image is calculated from the corresponding neighboring pixels of the original image. <br>  Let's start with the skeleton of the program: <br><div class="spoiler">  <b class="spoiler_title">main.cpp</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;chrono&gt; #include &lt;iostream&gt; #include &lt;cstring&gt; #include &lt;string&gt; #include &lt;opencv2/core/core.hpp&gt; #include &lt;opencv2/highgui/highgui.hpp&gt; #include &lt;opencv2/opencv.hpp&gt; #include &lt;vector_types.h&gt; #include "openMP.hpp" #include "CUDA_wrappers.hpp" #include "common/image_helpers.hpp" void prepareFilter(float **filter, int *filterWidth, float *filterSigma) { static const int blurFilterWidth = 9; static const float blurFilterSigma = 2.; *filter = new float[blurFilterWidth * blurFilterWidth]; *filterWidth = blurFilterWidth; *filterSigma = blurFilterSigma; float filterSum = 0.f; const int halfWidth = blurFilterWidth/2; for (int r = -halfWidth; r &lt;= halfWidth; ++r) { for (int c = -halfWidth; c &lt;= halfWidth; ++c) { float filterValue = expf( -(float)(c * c + r * r) / (2.f * blurFilterSigma * blurFilterSigma)); (*filter)[(r + halfWidth) * blurFilterWidth + c + halfWidth] = filterValue; filterSum += filterValue; } } float normalizationFactor = 1.f / filterSum; for (int r = -halfWidth; r &lt;= halfWidth; ++r) { for (int c = -halfWidth; c &lt;= halfWidth; ++c) { (*filter)[(r + halfWidth) * blurFilterWidth + c + halfWidth] *= normalizationFactor; } } } void freeFilter(float *filter) { delete[] filter; } int main( int argc, char** argv ) { using namespace cv; using namespace std; using namespace std::chrono; if( argc != 2) { cout &lt;&lt;" Usage: blur_image imagefile" &lt;&lt; endl; return -1; } Mat image, blurredImage, referenceBlurredImage; uchar4 *imageArray, *blurredImageArray; prepareImagePointers(argv[1], image, &amp;imageArray, blurredImage, &amp;blurredImageArray, CV_8UC4); int numRows = image.rows, numCols = image.cols; float *filter, filterSigma; int filterWidth; prepareFilter(&amp;filter, &amp;filterWidth, &amp;filterSigma); cv::Size filterSize(filterWidth, filterWidth); auto start = system_clock::now(); cv::GaussianBlur(image, referenceBlurredImage, filterSize, filterSigma, filterSigma, BORDER_REPLICATE); auto duration = duration_cast&lt;milliseconds&gt;(system_clock::now() - start); cout&lt;&lt;"OpenCV time (ms):" &lt;&lt; duration.count() &lt;&lt; endl; start = system_clock::now(); BlurImageOpenMP(imageArray, blurredImageArray, numRows, numCols, filter, filterWidth); duration = duration_cast&lt;milliseconds&gt;(system_clock::now() - start); cout&lt;&lt;"OpenMP time (ms):" &lt;&lt; duration.count() &lt;&lt; endl; cout&lt;&lt;"OpenMP similarity:" &lt;&lt; getEuclidianSimilarity(referenceBlurredImage, blurredImage) &lt;&lt; endl; for (int i=0; i&lt;4; ++i) { memset(blurredImageArray, 0, sizeof(uchar4)*numRows*numCols); start = system_clock::now(); BlurImageCUDA(imageArray, blurredImageArray, numRows, numCols, filter, filterWidth); duration = duration_cast&lt;milliseconds&gt;(system_clock::now() - start); cout&lt;&lt;"CUDA time full (ms):" &lt;&lt; duration.count() &lt;&lt; endl; cout&lt;&lt;"CUDA similarity:" &lt;&lt; getEuclidianSimilarity(referenceBlurredImage, blurredImage) &lt;&lt; endl; } freeFilter(filter); return 0; }</span></span></span></span></code> </pre><br></div></div><br>  The points: <br><ol><li>  We read the image file, prepare pointers to the original image and the resulting blurred image.  The <i>prepareImagePointers</i> function remains the same; if necessary, you can view its source code on bitbucket. </li><li>  Prepare a Gaussian filter - that is, a set of our scales.  We also remember the used filter parameters in order to transfer them to OpenCV and get a sample of the blurred image to verify the correctness of our algorithms. </li><li>  Call the Gauss blur function from OpenCV, save the resulting sample, measure the time spent. </li><li>  Call the Gauss blur function written using OpenMP, measure the time spent, check the result obtained with the sample.  The image similarity calculation function of <i>getEuclidianSimilarity is</i> as follows: <br><div class="spoiler">  <b class="spoiler_title">getEuclidianSimilarity</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">double</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getEuclidianSimilarity</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">const</span></span></span></span><span class="hljs-function"><span class="hljs-params"> cv::Mat&amp; a, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">const</span></span></span></span><span class="hljs-function"><span class="hljs-params"> cv::Mat&amp; b)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> errorL2 = cv::norm(a, b, cv::NORM_L2); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> similarity = errorL2 / (<span class="hljs-keyword"><span class="hljs-keyword">double</span></span>) (a.rows * a.cols); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> similarity; }</code> </pre><br></div></div><br>  In fact, it finds the average sum of squares of differences in the values ‚Äã‚Äãof all channels of all pixels of two images. </li><li>  Call the CUDA-version of the blur according to Gauss 4 times, each time measuring the time spent and checking the obtained result with the sample.  Why call 4 times?  The fact is that with the very first call, some time will be spent on initialization - therefore, it is better to run several times and measure the time spent on subsequent calls. </li></ol><br>  OpenMP implementation of the algorithm: <br><div class="spoiler">  <b class="spoiler_title">openMP.hpp</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;stdio.h&gt; #include &lt;omp.h&gt; #include &lt;vector_types.h&gt; #include &lt;vector_functions.h&gt; void BlurImageOpenMP(const uchar4 * const imageArray, uchar4 * const blurredImageArray, const long numRows, const long numCols, const float * const filter, const size_t filterWidth) { using namespace std; const long halfWidth = filterWidth/2; #pragma omp parallel for collapse(2) for (long row = 0; row &lt; numRows; ++row) { for (long col = 0; col &lt; numCols; ++col) { float resR=0.0f, resG=0.0f, resB=0.0f; for (long filterRow = -halfWidth; filterRow &lt;= halfWidth; ++filterRow) { for (long filterCol = -halfWidth; filterCol &lt;= halfWidth; ++filterCol) { //Find the global image position for this filter position //clamp to boundary of the image const long imageRow = min(max(row + filterRow, static_cast&lt;long&gt;(0)), numRows - 1); const long imageCol = min(max(col + filterCol, static_cast&lt;long&gt;(0)), numCols - 1); const uchar4 imagePixel = imageArray[imageRow*numCols+imageCol]; const float filterValue = filter[(filterRow+halfWidth)*filterWidth+filterCol+halfWidth]; resR += imagePixel.x*filterValue; resG += imagePixel.y*filterValue; resB += imagePixel.z*filterValue; } } blurredImageArray[row*numCols+col] = make_uchar4(resR, resG, resB, 255); } } }</span></span></span></span></code> </pre><br></div></div><br>  For all 3 channels of each pixel of the original image, we consider the described weighted sum, write the result to the corresponding position of the output image. <br>  CUDA option: <br><div class="spoiler">  <b class="spoiler_title">CUDA.cu</b> <div class="spoiler_text"><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include &lt;cuda.h&gt; #include "CUDA_wrappers.hpp" #include "common/CUDA_common.hpp" __global__ void gaussian_blur(const uchar4* const d_image, uchar4* const d_blurredImage, const int numRows, const int numCols, const float * const d_filter, const int filterWidth) { const int row = blockIdx.y*blockDim.y+threadIdx.y; const int col = blockIdx.x*blockDim.x+threadIdx.x; if (col &gt;= numCols || row &gt;= numRows) return; const int halfWidth = filterWidth/2; extern __shared__ float shared_filter[]; if (threadIdx.y &lt; filterWidth &amp;&amp; threadIdx.x &lt; filterWidth) { const int filterOff = threadIdx.y*filterWidth+threadIdx.x; shared_filter[filterOff] = d_filter[filterOff]; } __syncthreads(); float resR=0.0f, resG=0.0f, resB=0.0f; for (int filterRow = -halfWidth; filterRow &lt;= halfWidth; ++filterRow) { for (int filterCol = -halfWidth; filterCol &lt;= halfWidth; ++filterCol) { //Find the global image position for this filter position //clamp to boundary of the image const int imageRow = min(max(row + filterRow, 0), numRows - 1); const int imageCol = min(max(col + filterCol, 0), numCols - 1); const uchar4 imagePixel = d_image[imageRow*numCols+imageCol]; const float filterValue = shared_filter[(filterRow+halfWidth)*filterWidth+filterCol+halfWidth]; resR += imagePixel.x * filterValue; resG += imagePixel.y * filterValue; resB += imagePixel.z * filterValue; } } d_blurredImage[row*numCols+col] = make_uchar4(resR, resG, resB, 255); } void BlurImageCUDA(const uchar4 * const h_image, uchar4 * const h_blurredImage, const size_t numRows, const size_t numCols, const float * const h_filter, const size_t filterWidth) { uchar4 *d_image, *d_blurredImage; cudaSetDevice(0); checkCudaErrors(cudaGetLastError()); const size_t numPixels = numRows * numCols; const size_t imageSize = sizeof(uchar4) * numPixels; //allocate memory on the device for both input and output checkCudaErrors(cudaMalloc(&amp;d_image, imageSize)); checkCudaErrors(cudaMalloc(&amp;d_blurredImage, imageSize)); //copy input array to the GPU checkCudaErrors(cudaMemcpy(d_image, h_image, imageSize, cudaMemcpyHostToDevice)); float *d_filter; const size_t filterSize = sizeof(float) * filterWidth * filterWidth; checkCudaErrors(cudaMalloc(&amp;d_filter, filterSize)); checkCudaErrors(cudaMemcpy(d_filter, h_filter, filterSize, cudaMemcpyHostToDevice)); dim3 blockSize; dim3 gridSize; int threadNum; cudaEvent_t start, stop; cudaEventCreate(&amp;start); cudaEventCreate(&amp;stop); threadNum = 32; blockSize = dim3(threadNum, threadNum, 1); gridSize = dim3(numCols/threadNum+1, numRows/threadNum+1, 1); cudaEventRecord(start); gaussian_blur&lt;&lt;&lt;gridSize, blockSize, filterSize&gt;&gt;&gt;(d_image, d_blurredImage, numRows, numCols, d_filter, filterWidth); cudaEventRecord(stop); cudaEventSynchronize(stop); cudaDeviceSynchronize(); checkCudaErrors(cudaGetLastError()); float milliseconds = 0; cudaEventElapsedTime(&amp;milliseconds, start, stop); std::cout &lt;&lt; "CUDA time kernel (ms): " &lt;&lt; milliseconds &lt;&lt; std::endl; checkCudaErrors(cudaMemcpy(h_blurredImage, d_blurredImage, sizeof(uchar4) * numPixels, cudaMemcpyDeviceToHost)); checkCudaErrors(cudaFree(d_filter)); checkCudaErrors(cudaFree(d_image)); checkCudaErrors(cudaFree(d_blurredImage)); }</span></span></span></span></code> </pre><br></div></div><br><ol><li>  Allocate memory on the device for the original image, output image, filter.  Copy the relevant data from the host memory to the allocated device memory. </li><li>  Call the core.  Pay attention to the new, 3rd parameter when you call the kernel: <i>&lt;&lt;&lt; gridSize, blockSize, <b>filterSize</b> &gt;&gt;&gt;</i> - it sets the size of the total memory that is needed for each block.  In this task, it was possible to realize the use of shared memory in two ways: either move the filter data to the common memory of the block, or move the ‚Äúpiece‚Äù of the image that only this block needs, since it is these data that are needed by several threads of the block at once.  However, the second option is somewhat more complicated - you need to take into account that the piece of the input image that is needed for each block is slightly larger than the block itself - because we perform the <b>gather</b> operation, which means each stream calculates the values ‚Äã‚Äãof one pixel of the output image using several neighboring pixels of the original image: <br><img src="https://habrastorage.org/files/18c/c52/197/18cc52197c724898a346ded0e04a35de.png"><br>  Therefore, I stopped at the first option, which means that each block needs exactly <b>sizeof (float) * filterWidth * filterWidth</b> memory to store all filter values.  Moving the filter weights from the device‚Äôs memory to the block‚Äôs common memory is as follows: <br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><pre> <code class="cpp hljs"> <span class="hljs-keyword"><span class="hljs-keyword">extern</span></span> __shared__ <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> shared_filter[]; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (threadIdx.y &lt; filterWidth &amp;&amp; threadIdx.x &lt; filterWidth) { <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> filterOff = threadIdx.y*filterWidth+threadIdx.x; shared_filter[filterOff] = d_filter[filterOff]; } __syncthreads();</code> </pre><br></div></div><br>  Here <i>__shared__</i> in the declaration of the filter weights array says that this data should be placed in the shared memory of the block;  <i>extern</i> means that the size of the allocated memory will be set at the location of the kernel call;  <i>__syncthreads</i> is a barrier that guarantees that all filter weights will be transferred to the common memory of the block before any of the threads of this block continues its execution.  Further, all readings of the filter weights are made from the total memory of the block. </li><li>  Copy the output image from the device memory to the host memory, freeing the allocated memory. </li></ol><br>  Compile, run (input image size - 557x313): <br><pre> <code class="bash hljs">OpenCV time (ms):2 OpenMP time (ms):11 OpenMP similarity:0.00287131 CUDA time kernel (ms): 2.93245 CUDA time full (ms):32 CUDA similarity:0.00287131 CUDA time kernel (ms): 2.93402 CUDA time full (ms):4 CUDA similarity:0.00287131 CUDA time kernel (ms): 2.93267 CUDA time full (ms):4 CUDA similarity:0.00287131 CUDA time kernel (ms): 2.93312 CUDA time full (ms):4 CUDA similarity:0.00287131</code> </pre><br>  As you can see, if you do not take into account the very first launch of the CUDA version, we received almost 3-fold time gain in comparison with the OpenMP option, although we did not catch up with the OpenCV option - which, by the way, uses <a href="http://en.wikipedia.org/wiki/OpenCL">OpenCL</a> . <br>  Configuration of the machine on which the tests were conducted: <br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text">  Processor: Intel¬Æ Core (TM) i7-3615QM CPU @ 2.30GHz. <br>  GPU: NVIDIA GeForce GT 650M, 1024 MB, 900 MHz. <br>  RAM: DD3,2x4GB, 1600 MHz. <br>  OS: OS X 10.9.5. <br>  Compiler: g ++ (GCC) 4.9.2 20141029. <br>  CUDA compiler: Cuda compilation tools, release 6.0, V6.0.1. <br>  Supported OpenMP Version: OpenMP 4.0. <br></div></div><br>  Today, everything, in the next part, will look at some of the fundamental algorithms of the GPU. <br>  All source code is available on <a href="https://bitbucket.org/VladyslavGorbatiuk/cuda-parallel-programming-code">bitbucket</a> . </div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/245523/">https://habr.com/ru/post/245523/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../245511/index.html">Analog video capture with STM32F4-DISCOVERY</a></li>
<li><a href="../245513/index.html">Read SVG in C ++</a></li>
<li><a href="../245515/index.html">Time Series, metrics and statistics: familiarity with InfluxDB</a></li>
<li><a href="../245517/index.html">A simple built-in woofer amplifier on a chip with an FM receiver based on Arduino</a></li>
<li><a href="../245521/index.html">Qt 5.4 and Qt Creator 3.3 Release</a></li>
<li><a href="../245525/index.html">Peter Thiel: How to build a monopoly?</a></li>
<li><a href="../245527/index.html">Qt Creator 3.3 IDE Release</a></li>
<li><a href="../245529/index.html">Payments via WebMoney and other benefits of the Timeweb affiliate program</a></li>
<li><a href="../245531/index.html">Code Review Process with Atlassian Stash</a></li>
<li><a href="../245533/index.html">Containers for Windows: 10 years before Microsoft</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>