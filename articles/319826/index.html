<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to distinguish between British and American literature using machine learning</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Once I wondered if the British and American literature was different in terms of the choice of words, and if it was different, would I be able to trai...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to distinguish between British and American literature using machine learning</h1><div class="post__text post__text-html js-mediator-article"><p>  Once I wondered if the British and American literature was different in terms of the choice of words, and if it was different, would I be able to train a classifier who could distinguish literary texts in terms of the frequency of the words used.  It is quite easy to distinguish texts written in different languages, the power of intersection of a set of words is small relative to the set of words in the sample.  Text classification by categories ‚Äúscience‚Äù, ‚ÄúChristianity‚Äù, ‚Äúcomputer graphics‚Äù, ‚Äúatheism‚Äù is a well-known <a href="http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html">hello world</a> among the tasks of working with text frequency.  I was faced with a more difficult task, since I compared two dialects of the same language, and the texts did not have a common semantic orientation. </p><br><p><img src="https://habrastorage.org/files/ff8/ce9/fe7/ff8ce9fe774e44fcb971d7e101a48c9e.jpg" alt="image"></p><a name="habracut"></a><br><p>  The longest stage of machine learning is data extraction.  For the training sample, I used texts from <a href="http://www.gutenberg.org/">Project Gutenberg</a> , they can be freely downloaded.  I downloaded a list of American and British authors from Wikipedia.  The difficulty was in finding a match by the name of the author.  A good search by names is implemented on the project site, but parsing the site is prohibited; instead, it is proposed to use an archive with metadata.  This meant that I needed to solve a non-trivial name matching problem (Sir Arthur Ignatius Conan Doyle and Doyle, C. ‚Äî the same people, but Doyle, ME anymore) and do it with very high accuracy.  Instead, I, donating the sample size in favor of accuracy and saving my time, chose as a unique identifier a link to the author's Wikipedia, which was included in some metadata files.  So I received about 1600 British texts and 2500 American and began to train the classifier. </p><br><p>  For all operations, I used the sklearn package.  The first step after collecting and analyzing data is preprocessing, for which I took CountVectorizer.  It takes an array of text data as input and returns a feature vector.  Further, it is required to present the signs in a numeric format, since the classifier works with numeric data.  To do this, you need to calculate tf-idf, <a href="https://ru.wikipedia.org/wiki/TF-IDF">term frequency - inverted document frequency</a> , using TfidfTransformer. </p><br><p>  A short example of how this is done and why: </p><br><p>  Take the word ‚Äúthe‚Äù and calculate the number of occurrences of this word in the text A. Let us have 100 occurrences, and the total number of words in the document is 1000, </p><br><pre><code class="hljs lisp">tf(‚Äúthe‚Äù) = <span class="hljs-number"><span class="hljs-number">100/1000</span></span> = <span class="hljs-number"><span class="hljs-number">0.1</span></span></code> </pre> <br><p>  Next, take the word ‚Äúsepal‚Äù (sepal), which has occurred 50 times, </p><br><pre> <code class="hljs lisp">tf(‚Äúsepal‚Äù) = <span class="hljs-number"><span class="hljs-number">50/1000</span></span> = <span class="hljs-number"><span class="hljs-number">0.05</span></span></code> </pre> <br><p>  To calculate the inverted document frequency for these words, you need to take the logarithm of the ratio of the number of texts, in which there is at least one occurrence of this word, to the total number of texts.  If there are 10,000 texts in all, and each has the word ‚Äúthe‚Äù, </p><br><pre> <code class="hljs lisp">idf(‚Äúthe‚Äù) = log(<span class="hljs-number"><span class="hljs-number">10000/10000</span></span>) = <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br><pre> <code class="hljs lisp">tf-idf(‚Äúthe‚Äù) = idf(‚Äúthe‚Äù) * tf(‚Äúthe‚Äù) = <span class="hljs-number"><span class="hljs-number">0</span></span> * <span class="hljs-number"><span class="hljs-number">0.1</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br><p>  The word ‚Äúsepal‚Äù is much rarer, and is found only in 5 texts, therefore </p><br><pre> <code class="hljs lisp">idf(‚Äúsepal‚Äù) = log(<span class="hljs-number"><span class="hljs-number">10000/5</span></span>) = <span class="hljs-number"><span class="hljs-number">7.6</span></span></code> </pre> <br><pre> <code class="hljs lisp">tf-idf(‚Äúsepal‚Äù) = <span class="hljs-number"><span class="hljs-number">7.6</span></span> * <span class="hljs-number"><span class="hljs-number">0.05</span></span> = <span class="hljs-number"><span class="hljs-number">0.38</span></span></code> </pre> <br><p>  Thus, frequent words have minimal weight, and specific rare ones are large, and due to the large occurrence of the word ‚Äúsepal‚Äù in the text, it can be assumed that it is somehow related to botany. </p><br><p>  Now that the data is presented as a set of features, you need to train the classifier.  I work with text that is presented as sparse data, so the linear classifier, which copes well with classification tasks with a large number of features, is the best option.  I trained with CountVectorizer, TF-IDFTransformer, and SGD with default parameters.  You can work separately with each stage, but it is more convenient to use the pipeline: </p><br><pre> <code class="python hljs">pipeline = Pipeline([ (<span class="hljs-string"><span class="hljs-string">'vect'</span></span>, CountVectorizer()), (<span class="hljs-string"><span class="hljs-string">'tfidf'</span></span>, TfidfTransformer()), (<span class="hljs-string"><span class="hljs-string">'clf'</span></span>, SGDClassifier()), ])</code> </pre> <br><p>  After analyzing the graph of accuracy on the sample size, I noticed strong fluctuations in accuracy even on large sample sizes, which indicated that the classifier is very dependent on a particular sample, and therefore not very effective, and significant improvements are required.  Having received the list of classifier weights, I noticed a part of the problem: the algorithm was retrained on frequent words like ‚Äúof‚Äù and ‚Äúhe‚Äù, which are in fact noise.  This problem is easily solved by removing similar words from features, and is set by the CountVectorizer parameter stop_words = 'english' or by your own word list.  A little improved accuracy, the removal of some popular common words.  By removing the stop words, I got an accuracy improvement of 0.85. </p><br><p>  Next, I started setting parameters using GridSearchCV.  This method reveals the best combination of parameters for the CountVectorizer, TfidfTransformer and SGDClassifier, so this is a very long process, I have considered it for about a day.  As a result, I received such a pipeline: </p><br><pre> <code class="python hljs">pipeline = Pipeline([ (<span class="hljs-string"><span class="hljs-string">'vect'</span></span>, CountVectorizer(stop_words = modifyStopWords(), ngram_range = (<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>))), (<span class="hljs-string"><span class="hljs-string">'tfidf'</span></span>, TfidfTransformer(use_idf = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, norm = <span class="hljs-string"><span class="hljs-string">'l2'</span></span>, smooth_idf = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)), (<span class="hljs-string"><span class="hljs-string">'clf'</span></span>, SGDClassifier(alpha=<span class="hljs-number"><span class="hljs-number">0.001</span></span>, fit_intercept = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, n_iter = <span class="hljs-number"><span class="hljs-number">10</span></span>, penalty = <span class="hljs-string"><span class="hljs-string">'l2'</span></span>, loss = <span class="hljs-string"><span class="hljs-string">'epsilon_insensitive'</span></span>)), ])</code> </pre> <br><p>  Total accuracy - 0.89. </p><br><p>  Now the most interesting for me: what words indicate the origin of the text.  Here is a list of words, sorted by descending module of weight in the classifier: </p><br><p>  <strong>American text</strong> : dollars, new, york, girl, gray, american, carvel, color, city, ain, long, just, parlor, boston, honor, washington, home, labor, got, finally, maybe, hodder, forever, dorothy dr </p><br><p>  <strong>British text</strong> : round, sir, lady, london, quite, mr, shall, lord, gray, dear, honor, having, philip, poor, pounds, scrooge, soames, things, sea, man, end, come, color, illustration , english, learnt </p><br><p>  Having fun with the classifier, I got the most "British" authors from among the Americans and the most "American" British (an elegant way to tell about how much my classifier can make mistakes): </p><br><p>  <strong>The most "British" Americans:</strong> </p><br><ul><li>  Burnett, Francis Eliza (born in England, moved to the USA at the age of 17, therefore I attribute her to the Americans) </li><li>  Henry James (born in the USA, emigrated to the UK at the age of 33) </li><li>  Wister, Owen </li><li>  Mary Roberts Reinhart (as we see, called the American Agatha Christie for good reason) </li><li>  William McFee (like Burnett, moved to the USA at a young age) </li></ul><br><p>  <strong>The most "American" British:</strong> </p><br><ul><li>  Ridyard Kipling (lived for several years in the USA, made the list thanks to ‚ÄúAmerican Notes‚Äù) </li><li>  Anthony Trollop (again American place names are to blame in his ‚ÄúNorth America‚Äù) </li><li>  Frederic Marriet (perhaps one of the names ‚ÄúTraveling and Adventures of Monsieur Violet in California, Sonara, and Western Texas‚Äù would be enough to confuse the classifier) </li><li>  Arnold Bennett (thanks to his ‚ÄúYour United States: Impressions of a first visit‚Äù) </li><li>  Phillips Oppenheim </li></ul><br><p>  And also the most "British" British and "American" Americans (because the classifier is still good). </p><br><p>  <strong>Americans:</strong> </p><br><ul><li>  Francis Hopkinson Smith </li><li>  Gamlin garland </li><li>  George Aide </li><li>  Charles Dudley Warner </li><li>  Mark Twain </li></ul><br><p>  <strong>British:</strong> </p><br><ul><li>  George Meredith </li><li>  Samuel richardson </li><li>  John galsworthy </li><li>  Gilbert Keith Chesterton </li><li>  Anthony Trollop </li></ul><br><p>  On an attempt to make such a classifier, I was pushed by the @TragicAllyHere <a href="https://twitter.com/TragicAllyHere/status/779403865099599872%3Fref_src%3Dtwsrc%255Etfw">tweet</a> : </p><br><blockquote>  I would love to be British.  The phone booth, adding letters to wourds. </blockquote><p>  The code can be taken <a href="https://github.com/SonechkaGodovykh/BritishAmericanClassifier">here</a> , the already trained classifier is also available there. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/319826/">https://habr.com/ru/post/319826/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../319812/index.html">International Student School Recent Advances in Algorithms: St. Petersburg, May 22‚Äì26, 2017</a></li>
<li><a href="../319814/index.html">What to catch in the career of an IT architect: expectations VS reality</a></li>
<li><a href="../319820/index.html">How to avoid self-excitation of cellular amplifier</a></li>
<li><a href="../319822/index.html">IT outsourcing in Russia - current realities and perspectives, opinions and experience of experts</a></li>
<li><a href="../319824/index.html">Old men have a place here: forgotten IT technologies in the ranks</a></li>
<li><a href="../319830/index.html">[Translation] Report: The most popular Android devices in the US, Britain, Germany and Canada for the 4th quarter of 2016</a></li>
<li><a href="../319832/index.html">Hey request! Are you alive? How to easily handle locks in PostgreSQL</a></li>
<li><a href="../319834/index.html">WebAssembly - the path to new performance horizons</a></li>
<li><a href="../319836/index.html">ExBB - PHP File Forum</a></li>
<li><a href="../319838/index.html">As I in one team did a redesign of a web product (cases!)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>