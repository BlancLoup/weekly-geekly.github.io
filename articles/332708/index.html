<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How does JVM allocate objects?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="How does the JVM create new objects? What exactly happens when you write new Object() ? 


 At conferences, they periodically say that TLABs (thread-l...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How does JVM allocate objects?</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/web/9af/3d2/c90/9af3d2c90c0c4f678821b4d9c66ab0a3.png" align="left" width="265" height="320" title="I have never been completely satisfied with the explanation of how Java allocates objects.">  How does the JVM create new objects?  What exactly happens when you write <code>new Object()</code> ? </p><br><p>  At conferences, they periodically say that TLABs (thread-local allocation buffer) are used to allocate objects: memory areas allocated exclusively to each thread, the creation of objects in which is very fast due to the lack of synchronization. </p><br><p>  But how to choose the right size TLAB'a?  What to do if you need to allocate 10% of the size of TLAB, and only 9% is free?  Can an object be allocated outside TLAB?  When (if) the allocated memory is reset? <br>  After asking these questions and not finding all the answers, I decided to write an article to correct the situation. </p><br><p>  Before reading it is useful to remember how some garbage collector works (for example, after reading <a href="https://habrahabr.ru/post/269621">this</a> series of articles). </p><a name="habracut"></a><br><h2 id="vvedenie">  Introduction </h2><br><p>  What steps are needed to create a new object? </p><br><p>  First of all, you need to find an unallocated memory area of ‚Äã‚Äãthe required size, then you need to initialize the object: reset the memory, initialize some internal structures (information that is used when <code>getClass()</code> called and synchronized on the object, etc.) and finally you need to call the constructor. </p><br><p>  The article is structured like this: first, let's try to understand what should happen in theory, then somehow climb into the inside of the JVM and see how it all happens, and in the end we will write some benchmarks to make sure for sure. </p><br><blockquote>  <strong>Disclaimer:</strong> Some parts are deliberately simplified without loss of generality.  Speaking about garbage collection, I mean any compacting collector, and speaking of address space - eden of the younger generation.  For other [standard or widely known] garbage collectors, parts may vary, but not too much. <br></blockquote><br><h2 id="tlab-101">  TLAB 101 </h2><br><br>  The first part is to allocate free memory for our object. <br>  In general, effective allocation of memory is a non-trivial task, full of pain, suffering, and dragons.  For example, coherent lists are created for sizes multiple of powers of two, they are searched and, if necessary, memory areas are cut and moved from one list to another (aka <a href="https://en.wikipedia.org/wiki/Buddy_memory_allocation">buddy allocator</a> ). <p>  Fortunately, there is a garbage collector in the Java machine that takes the hard part of the work.  During the young generation process, all living objects are moved to the survivor space, leaving one large continuous region of free memory in eden. </p><br><p>  Since the memory in the JVM frees up the GC, the allocator only needs to know where to look for this free memory, to actually control access to one pointer to this free memory.  That is, the allocation should be very simple. <del>  and consist of ponies and rainbows </del>  : you need to add to the pointer to the free eden object size, and our memory (this technique is called <strong>bump-the-pointer</strong> ). </p><br><p>  Memory can be allocated multiple threads, so you need some form of synchronization.  If you make it the easiest way (blocking on a heap region or atomic pointer increment), memory allocation can easily become a bottleneck, so JVM developers developed the previous idea with bump-the-pointer: each thread is allocated a large chunk of memory that belongs only to it .  Allocations within such a buffer occur with the same pointer increment (but local, without synchronization) as long as possible, and a new area is requested each time the current one ends.  This area is called the <strong>thread-local allocation buffer</strong> .  It turns out a kind of hierarchical bump-the-pointer, where the heap region is on the first level, and the current stream is on the second level.  Some <a href="https://arise.or.at/pubpdf/Hierarchical__PLABs____CLABs____TLABs__in__Hotspot_.pdf">cannot</a> stop at this and go even further, <a href="">hierarchically</a> putting buffers into buffers. </p><br><img src="https://habrastorage.org/web/907/fe5/40a/907fe540a674490483b64081766f517b.png"><br><p>  It turns out that in most cases the allocation should be very fast, run in just a couple of instructions and look something like this: </p><br><pre> <code class="cpp hljs">start = currentThread.tlabTop; end = start + <span class="hljs-keyword"><span class="hljs-keyword">sizeof</span></span>(Object.class); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (end &gt; currentThread.tlabEnd) { <span class="hljs-keyword"><span class="hljs-keyword">goto</span></span> slow_path; } currentThread.setTlabTop(end); callConstructor(start, end);</code> </pre> <br><p>  It looks too good to be true, so we will use <a href="http://psy-lob-saw.blogspot.ru/2013/01/java-print-assembly.html">PrintAssembly</a> and see what the method that <code>java.lang.Object</code> creates is compiled: </p><br><pre> <code class="hljs sql">; Hotspot machinery skipped mov 0x60(%r15),%rax ; <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> = tlabTop lea <span class="hljs-number"><span class="hljs-number">0x10</span></span>(%rax),%rdi ; <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> + sizeof(<span class="hljs-keyword"><span class="hljs-keyword">Object</span></span>) cmp <span class="hljs-number"><span class="hljs-number">0x70</span></span>(%r15),%rdi ; if (<span class="hljs-keyword"><span class="hljs-keyword">end</span></span> &gt; tlabEnd) ja <span class="hljs-number"><span class="hljs-number">0x00000001032b22b5</span></span> ; goto slow_path mov %rdi,0x60(%r15) ; tlabTop = <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> ; Object initialization skipped</code> </pre> <br><p>  Possessing the <a href="">secret knowledge</a> that in the register <code>%r15</code> there is always a pointer to the VM flow (lyrical digression: due to such an invariant thread-local and <code>Thread.currentThread()</code> work <em>very</em> quickly), we understand that this is the code which we expected to see.  At the same time, we note that the JIT compiler injects the allocation directly into the calling method. </p><br><p>  In this way, the JVM is almost free (without recalling garbage collection) creating new objects for a dozen instructions, shifting the responsibility for clearing the memory and defragmentation to the GC.  A nice bonus is the locality of the allocated data in a row, which classical allocators may not guarantee.  There is a whole <a href="http://www.cs.utexas.edu/~mckinley/papers/mmtk-sigmetrics-2004.pdf">study</a> about the effect of such locality on the performance of typical applications.  <strong>Spoiler alert</strong> : does everything a bit faster even though the GC is heavily loaded. <br></p><br><h2 id="vliyanie-razmera-tlab-na-proishodyaschee">  The effect of TLAB size on what is happening </h2><br><p>  What should be the size of a TLAB?  In the first approximation, it is reasonable to assume that the smaller the buffer size, the more memory allocation will pass through a slow branch, and, therefore, TLAB needs to be done more: less often we go to a relatively slow overall heap for memory and quickly create new objects. </p><br><p>  But there is another problem: <strong>internal fragmentation</strong> . <br>  Consider a situation where TLAB has a size of 2 megabytes, eden region (from which TLABs are allocated) is 500 megabytes, and an application has 50 streams.  As soon as the space for new TLABs in the heap is over, the very first thread that runs out of its TLAB will trigger garbage collection.  If we assume that TLABs are filled with ¬± uniformly (in real applications this may not be the case), then on average, the remaining TLABs will be filled in about half.  That is, if there is another <code>0.5 * 50 * 2 == 50</code> megabytes of unallocated memory (as much as 10%), garbage collection begins.  It turns out not very well: a significant part of the memory is still free, but the GC is still called. </p><br><img src="https://habrastorage.org/web/6bc/108/466/6bc108466fe748f993062528321685e8.png" title="'Screw this' - as if Duke tells us"><br><p>  If you continue to increase the size of TLAB or the number of threads, then the memory loss will grow linearly, and it turns out that TLAB accelerates allocations, but slows down the application as a whole, straining the garbage collector once again. </p><br><p>  And if there is still a place in TLAB, but the new object is too big?  If you throw away the old buffer and allocate a new one, then fragmentation will only increase, and if in such situations you always create an object directly in eden, then the application will start to work slower than it could? </p><br><p>  In general, what to do is not very clear.  You can hard-code the <a href="http://lesswrong.ru/w/%25D0%259C%25D0%25B8%25D1%2581%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B5_%25D0%25BE%25D1%2582%25D0%25B2%25D0%25B5%25D1%2582%25D1%258B">mystical</a> constant (as it was done for inlinating heuristics), you can give the size to the developer and tune it for each application individually (incredibly convenient), you can teach the JVM how to guess the correct answer. </p><br><h2 id="chto-delat-to">  What to do? </h2><br><p>  Choosing a constant is a thankless task, but Sun engineers did not despair and went the other way: instead of specifying the size, the percentage of fragmentation is indicated - part of the heap, which we are willing to sacrifice for quick allocations, and the JVM will somehow figure it out.  Parameter <code>TLABWasteTargetPercent</code> responsible for this and defaults to 1%. </p><br><p>  Using all the same hypothesis about the uniformity of memory allocation by threads, we get a simple equation: <code>tlab_size * threads_count * 1/2 = eden_size * waste_percent</code> . <br>  If we are ready to donate 10% of eden, we have 50 threads, and eden occupies 500 megabytes, then at the beginning of garbage collection 50 megabytes can be free in half-empty TLABs, that is, in our example, the size of TLAB will be 2 megabytes. </p><br><p>  There is a serious omission in this approach: the assumption is used that all flows are allocated in the same way, which is almost always not true.  It is undesirable to adjust the number to the speed of allocation of the most intensive streams, I do not want to offend their less fast colleagues (for example, scheduled-workers).  Moreover, in a typical application there are hundreds of streams (for example, in the trades of your favorite app server), and there will be only a few new objects created without serious workload, this also needs to be somehow taken into account.  And if you recall the question "What to do if you need to allocate 10% of the size of the TLAB, and only 9% is free?", Then it becomes completely unobvious. </p><br><p>  There are too many details to just guess or spy on a blog, so it's time to figure out how things <em>really work out</em> ‚Ñ¢: look at the hotspot sources. <br>  I used the jdk9 wizard, here is <a href="https://gist.github.com/qwwdfsad/12d54867a70d84a155cf129ac923bbe0">CMakeLists.txt</a> , which CLion starts working with if you want to repeat the journey. </p><br><h2 id="tumbling-down-the-rabbit-hole">  Tumbling down the rabbit hole </h2><br><p>  The file of interest is located on the first grep and is called <a href="">threadLocalAllocBuffer.cpp</a> , which describes the structure of the buffer.  Despite the fact that the class describes the buffer, it is created once for each stream and reused when allocating new TLABs, at the same time it also stores various statistics on the use of TLABs. </p><br><p>  To understand the JIT compiler, you need to think like a JIT compiler.  Therefore, we immediately skip the initial initialization, creating a buffer for the new stream and calculating default values ‚Äã‚Äãand look at the <a href=""><code>resize</code></a> method, which is called for all streams at the end of each assembly: </p><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">void</span></span> ThreadLocalAllocBuffer::resize() { <span class="hljs-comment"><span class="hljs-comment">// ... size_t alloc =_allocation_fraction.average() * (Universe::heap()-&gt;tlab_capacity(myThread()) / HeapWordSize); size_t new_size = alloc / _target_refills; // ... }</span></span></code> </pre> <br><p>  Aha  For each stream, the intensity of its allocations is monitored, and depending on it and the <code>_target_refills</code> constant (which is carefully signed as "the number of TLABs that the stream would like to request between two assemblies" would be calculated), the new size is calculated. </p><br><p>  <code>_target_refills</code> initialized once: </p><br><pre> <code class="cpp hljs"> <span class="hljs-comment"><span class="hljs-comment">// Assuming each thread's active tlab is, on average, 1/2 full at a GC _target_refills = 100 / (2 * TLABWasteTargetPercent);</span></span></code> </pre> <br><p>  This is exactly the hypothesis that we assumed above, but instead of the size of TLAB, the number of requests for a new TLAB for the stream is calculated.  In order for all threads to have at most <code>x%</code> free memory at the time of assembly, it is necessary that the TLAB size of each thread be <code>2x%</code> of the entire memory, which it usually allocates between assemblies.  Dividing <code>1</code> by <code>2x</code> is just the desired number of requests. </p><br><p>  Share share allocations need to be updated sometime.  At the beginning of each garbage collection, statistics of all streams is updated, which is in the <a href=""><code>accumulate_statistics</code></a> method: </p><br><ul><li>  Check if the thread has updated its TLAB at least once.  There is no need to recalculate the size for a stream that does nothing (or, at least, does not allocate). </li><li>  We check if half of eden was used to avoid the influence of full GC or pathological cases (for example, an explicit call to <code>System.gc()</code> ) on the calculations. </li><li>  In the end, we consider what percentage of eden'a spent the flow, and update its share of allocations. </li><li>  We update the statistics of how the thread used its TLABs, how much it allocated and how much memory it wasted. </li></ul><br><p>  To avoid various unstable effects due to the frequency of assemblies and different allocation patterns associated with inconsistency of the garbage collector and flow desires, the share of allocations is not just a number, but an <a href="https://en.wikipedia.org/wiki/Moving_average">exponentially weighted moving average</a> that maintains the average value for the last N assemblies.  The JVM has its own key for everything, and this place is no exception, the <code>TLABAllocationWeight</code> flag controls how quickly the average ‚Äúforgets‚Äù old values ‚Äã‚Äã(not that someone wanted to change the value of this flag). </p><br><h2 id="rezultat">  Result </h2><br><p>  The information received is enough to answer the question of interest to us about the size of a TLAB: </p><br><ul><li>  The JVM knows how much memory it can spend on fragmentation.  From this value, the number of TLABs that the stream should request between garbage collections is calculated. </li><li>  The JVM keeps track of how much memory each thread uses and smooths out these values. </li><li>  Each thread receives the size of the TLAB in <strong>proportion</strong> to the memory it uses.  This solves the problem of non-uniform allocation between threads and, on average, all allocate quickly and waste little memory. </li></ul><br><img src="https://habrastorage.org/web/5a2/f27/4a0/5a2f274a04de42fe8d33f64898d72cf2.png"><br><p>  If an application has one hundred threads, 3 of which are in full servicing user requests, 2 have a timer on some auxiliary activity, and all the others are idle, then the first group of threads will receive large TLABs, the second will be very small, and all the others will have default values .  Best of all, the number of "slow" allocations (TLAB requests) will be the same for all threads. <br></p><br><h2 id="allokaciya-v-c1">  Allocation in C1 </h2><br><p>  With the size TLAB'ov figured out.  In order not to go far, we dig the source further and see how exactly TLABs stand out when it's fast, when it's slow, and when it's very slow. </p><br><p>  Here you will not manage with one class and you need to look into what the <code>new</code> operator is compiling.  In order to avoid traumatic brain injury, we will look at the client compiler code (C1): it is much simpler and clearer than the server compiler, well describes the overall picture of the world, and since the <code>new</code> thing in Java is quite popular, there are plenty of interesting optimizations for us. </p><br><p>  We are interested in two methods: <code>C1_MacroAssembler::allocate_object</code> , which describes the allocation of an object in TLAB and initialization, and <code>Runtime1::generate_code_for</code> , which is executed when the memory cannot be quickly allocated. </p><br><p>  It is interesting to see if an object can always be created quickly, and the "find usages" chain leads us to the following comment in <a href="">instanceKlass.hpp</a> : </p><br><pre> <code class="cpp hljs"> <span class="hljs-comment"><span class="hljs-comment">// This bit is initialized in classFileParser.cpp. // It is false under any of the following conditions: // - the class is abstract (including any interface) // - the class has a finalizer (if !RegisterFinalizersAtInit) // - the class size is larger than FastAllocateSizeLimit // - the class is java/lang/Class, which cannot be allocated directly bool can_be_fastpath_allocated() const { return !layout_helper_needs_slow_path(layout_helper()); }</span></span></code> </pre> <br><p>  From it it becomes clear that very large objects (more than 128 kilobytes by default) and finalizeable classes always go through a slow call to the JVM.  (Riddle - where does the abstract classes come from?) <br>  Let's take note of this and go back to the allocation process: </p><br><ol><li><p>  <strong>tlab_allocate</strong> - an attempt to quickly allocate an object, exactly the code we saw when we looked at PrintAssembly.  If it worked, then we finish the allocation and proceed to initializing the object. </p><br></li><li><p>  <strong>tlab_refill</strong> - an attempt to allocate a new TLAB.  Using an interesting test, the method decides whether to allocate a new TLAB (by discarding the old one) or allocate an object directly in eden, leaving the old TLAB: </p><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">// Retain tlab and allocate object in shared space if // the amount free in the tlab is too large to discard. cmpptr(t1, Address(thread_reg, in_bytes(JavaThread::tlab_refill_waste_limit_offset()))); jcc(Assembler::lessEqual, discard_tlab);</span></span></code> </pre> <br><p>  <code>tlab_refill_waste_limit</code> is just responsible for the size of the TLAB, which we are not ready to sacrifice for the sake of allocating one object.  The default value is <code>1.5%</code> of the current TLAB size (for this, of course, there is a parameter - <code>TLABRefillWasteFraction</code> , which <em>suddenly</em> has a value of 64, and the value itself is considered as the current size of TLAB'a divided by the value of this parameter).  This limit is raised at each slow allocation to avoid degradation in unsuccessful cases, and is reset at the end of each GC cycle.  Another issue less. </p><br></li><li>  <strong>eden_allocate</strong> - an attempt to allocate memory (object or TLAB) in eden.  This place is very similar to the allocation in TLAB: we check if there is a place, and if so, then atomically, using the <code>lock cmpxchg</code> instruction, we take away our memory, and if not, then we leave in slow path.  Selection in eden is not wait-free: if two threads try to allocate something in eden at the same time, then with some probability one of them will fail and have to repeat everything anew. </li></ol><br><h3 id="jvm-upcall">  Jvm upcall </h3><br><p>  If it did not work out to allocate memory in eden, then a call is made to the JVM, which leads us to the <code>InstanceKlass::allocate_instance</code> .  Before the call itself, a lot of auxiliary work is carried out - special structures are set up for the GC and the necessary frames are created to meet the <a href="https://en.wikipedia.org/wiki/X86_calling_conventions">calling conventions</a> , so the operation is not fast. <br>  There is a lot of code and you will not get by with one superficial description, so in order not to bore anyone, I‚Äôll give you only an approximate scheme of work: </p><br><ol><li>  First, the JVM tries to allocate memory through an interface specific to the current garbage collector.  There the same call chain occurs as it was above: first an attempt to allocate from TLAB, then an attempt to allocate TLAB from the heap and the creation of an object. </li><li>  In case of failure, garbage collection is called.  There is also a GC error overhead limit exceeded, various GC notifications, logs and other checks that are not related to allocation. </li><li>  If garbage collection did not help, then an allocation attempt is made directly to Old Generation (here the behavior depends on the selected GC algorithm), and in case of failure another assembly occurs and an attempt to create an object, and if it did not work here, then in the end it rushes <code>OutOfMemoryError</code> . </li><li>  When the object has been successfully created, it is checked whether it is an hour finalizable and if so, it is registered, which consists in calling the <code>Finalizer#register</code> method (you, too, always wondered why this class is in the standard library, but never by anyone not used explicitly?).  The method itself was clearly written a long time ago: the Finalizer object is created and under the global (sic!) Lock it is added to the linked list (with the help of which the objects will then be finalized and assembled).  This fully justifies the unconditional challenge in the JVM and (in part) the advice "do not use the finalize method, even if you really want to." </li></ol><br><p>  As a result, we now know almost everything about allocation: objects are allocated quickly, TLABs are filled quickly, objects in some cases are allocated immediately in eden, and in some cases they go through unhurried calls in the JVM. <br></p><br><h2 id="monitoring-medlennyh-allokaciy">  Monitoring slow allocations </h2><br><p>  How the memory is allocated, we found out, but what to do with this information is not yet. <br>  Somewhere above, I wrote that all statistics (slow allocations, the average number of refills, the number of allocating flows, the loss to internal fragmentation) are recorded somewhere. </p><br><p><img src="https://habrastorage.org/web/00f/db4/c71/00fdb4c7101c4c34a3e4e76eabc97ace.png" align="left" width="216" height="207" title="Statistics CLion'a, which I enjoyed while writing this article">  - ‚Äî perf data,        hsperfdata,        jcmd     <code>sun.jvmstat.monitor</code> API. </p><br><p>          ,     Oracle JDK,  JFR    (  API,   OpenJDK),     -. <br>  Is this important?      ,      <a href="https://www.youtube.com/watch%3Fv%3DM9o1LVfGp2A%26t%3D2400"></a>  Twitter JVM team,        ,          . <br></p><br><h2 id="prefetch"> Prefetch </h2><br><p>     ,    -      prefetch',    . </p><br><blockquote> <strong>Prefetch</strong> ‚Äî    ,   ,   , ,  (   ) ,    . Prefetch  ,    ,         ,  ,   (,  )   ,    ,           . <br></blockquote><br>   prefetch  C2- ,         C1.    :    TLAB  ,    ,      .   Java-     ,           :         ,       . <br><img src="https://habrastorage.org/web/256/582/def/256582defc394205b3ee7edbf2a8ce7b.png"><br><p>  prefetch'   ,    <code>AllocatePrefetchStyle</code> :   prefetch   ,  ,    ,     .   <code>AllocatePrefetchInstr</code>   ,   prefetch :      L1- (,   -    ),   L3    :      ,          <a href="">.ad </a>   . </p><br><p>          ,     <del> JVM-,      SPECjbb- </del>   Java -  ,        ( , ,     ,      ). <br></p><br><h2 id="inicalizaciya">  </h2><br><p>     ,   ,        .       C1-,      ARM ‚Äî    ,    . <br></p><br>    <code>C1_MacroAssembler::initialize_object</code>     : <br><ol><li><p>    .      ‚Äî <a href="">mark word</a> , <br>       , identity hashcode ( biased locking)   ,  klass pointer,      ‚Äî      ,    metaspace,      <code>java.lang.Class</code> . </p><br><img src="https://habrastorage.org/web/e2d/a4b/301/e2da4b30184a495c8e02dd1912539409.png"><br><p>        32   64. ,       12  (   ,      16). </p><br></li><li><p>   ,     <code>ZeroTLAB</code> .     : <br>        ,      ,    .     C2-         ,       .     . </p><br></li><li>    StoreStore  (      <a href="https://habrahabr.ru/post/209128/"></a> <a href="https://habrahabr.ru/users/gvsmirnov/" class="user_link">gvsmirnov</a> ),  (, )   ,    . <br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">// StoreStore barrier required after complete initialization // (headers + content zeroing), before the object may escape. membar(MacroAssembler::StoreStore, tmp1);</span></span></code> </pre> <br><p>      :     ,  -    ,       (     )       ,  ,   ,     (out of thin air) ,       .  x86    ,      ,      ARM. <br></p><br><br></li></ol><br><h3 id="proveryaem-na-praktike">    </h3><br><blockquote> Beware of bugs in the above code; I have only proved it correct, not tried it. <br></blockquote><br>     :   ,    ,   ,     <em>  </em> ,          . <p>     <code>PrintAssembly</code>         <code>new Long(1023)</code> : </p><br><pre> <code class="hljs perl"> <span class="hljs-number"><span class="hljs-number">0x0000000105eb7b3e</span></span>: mov <span class="hljs-number"><span class="hljs-number">0x60</span></span>(%r15),%rax <span class="hljs-number"><span class="hljs-number">0x0000000105eb7b42</span></span>: mov %rax,%r10 <span class="hljs-number"><span class="hljs-number">0x0000000105eb7b45</span></span>: add $0x18,%r10 ;  <span class="hljs-number"><span class="hljs-number">24</span></span> : <span class="hljs-number"><span class="hljs-number">8</span></span>  , ; <span class="hljs-number"><span class="hljs-number">4</span></span>    , ; <span class="hljs-number"><span class="hljs-number">4</span></span>   , ; <span class="hljs-number"><span class="hljs-number">8</span></span>   long  <span class="hljs-number"><span class="hljs-number">0x0000000105eb7b49</span></span>: cmp <span class="hljs-number"><span class="hljs-number">0x70</span></span>(%r15),%r10 <span class="hljs-number"><span class="hljs-number">0x0000000105eb7b4d</span></span>: jae <span class="hljs-number"><span class="hljs-number">0x0000000105eb7bb5</span></span> <span class="hljs-number"><span class="hljs-number">0x0000000105eb7b4f</span></span>: mov %r10,<span class="hljs-number"><span class="hljs-number">0x60</span></span>(%r15) <span class="hljs-number"><span class="hljs-number">0x0000000105eb7b53</span></span>: prefetchnta <span class="hljs-number"><span class="hljs-number">0xc0</span></span>(%r10) ; prefetch <span class="hljs-number"><span class="hljs-number">0x0000000105eb7b5b</span></span>: movq $0x1,(%rax) ;   <span class="hljs-number"><span class="hljs-number">0x0000000105eb7b62</span></span>: movl $0xf80022ab,<span class="hljs-number"><span class="hljs-number">0x8</span></span>(%rax) ;     Long <span class="hljs-number"><span class="hljs-number">0x0000000105eb7b69</span></span>: mov %r12d,<span class="hljs-number"><span class="hljs-number">0xc</span></span>(%rax) <span class="hljs-number"><span class="hljs-number">0x0000000105eb7b6d</span></span>: movq $0x3ff,<span class="hljs-number"><span class="hljs-number">0x10</span></span>(%rax) ;  <span class="hljs-number"><span class="hljs-number">1023</span></span>   </code> </pre> <br><p>    ,    ,    . <br>  ,       : </p><br><ol><li>      TLAB'. </li><li>    TLAB' ,    eden'   TLAB,      eden',      . </li><li>    eden'  ,    . </li><li>      ,       . </li><li>   ,   OOM. </li><li>      . </li></ol><br><p>          :    ,   prefetch     TLAB'  -. </p><br><h2 id="eksperimenty">  </h2><br><p>   ,          ,      .   ,    <code>java.lang.Object</code>   ,    JVM. <br>    Java 1.8.0_121, Debian 3.16, Intel Xeon X5675.    ‚Äî  ,    ‚Äî    . </p><br><img src="https://habrastorage.org/web/894/5d9/bd8/8945d9bd80824726bab330150e295f68.png" title="Hero this benchmark deserves: https://habrastorage.org/web/d9b/e8f/3b3/d9be8f3b3a7f4389b7ea3ec87894f621.png"><br><p>   : </p><br><ul><li>            ,     ,     <code>new</code> .       ,    :      -   (,  <code>Blackhole#consumeCPU</code> ),      ,      . </li><li>  prefetch    .       JVM ,         -,          .    ,              . </li><li>     TLAB'    :          ‚Äî    JIT -&gt; JVM,            ,        . </li></ul><br><p>      <code>finalize</code> ,   <strong> eden'</strong>   finalizable-: </p><br><img src="https://habrastorage.org/web/204/518/006/2045180060b3458fad754e004ca295a3.png"><br><p>             ! </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p> JVM      ,           ,  TLAB' ‚Äî  ,    .   TLAB'        :       ,    . <br>    ?  ,       ,  []        . <br></p><br>   <a href="https://habrahabr.ru/users/apangin/" class="user_link">apangin</a>  <a href="https://habrahabr.ru/users/gvsmirnov/" class="user_link">gvsmirnov</a>  ,       ,      ,   ,    . </div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/332708/">https://habr.com/ru/post/332708/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../332694/index.html">Configuring BGP Looking glass based on OpenBSD 6.1</a></li>
<li><a href="../332698/index.html">The digest of interesting materials for the mobile developer # 211 (July 03 - July 09)</a></li>
<li><a href="../332700/index.html">Quotes program data collection</a></li>
<li><a href="../332704/index.html">Announcement Heisenbag 2017 Moscow: Double the Benefit</a></li>
<li><a href="../332706/index.html">Translation of the Appium Essentials book. Chapter 4</a></li>
<li><a href="../332710/index.html">Quick removal of spaces from lines on ARM processors</a></li>
<li><a href="../332712/index.html">GitLab CI for continuous integration and delivery in production. Part 1: our pipeline</a></li>
<li><a href="../332714/index.html">Cipher Hila. Detailed analysis</a></li>
<li><a href="../332718/index.html">Android Architecture Components. Part 2. Lifecycle</a></li>
<li><a href="../332722/index.html">PLC from manufacturers Aries, Segnetics and Schneider Electric for HVAC</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>