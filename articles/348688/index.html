<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Run a full-fledged cluster on Kubernetes from scratch on Ubuntu 16.04</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Already quite a lot of articles have been written on installing and launching Kubernetes , however, not everything is so smooth (I spent several days ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Run a full-fledged cluster on Kubernetes from scratch on Ubuntu 16.04</h1><div class="post__text post__text-html js-mediator-article">  Already quite a lot of articles have been written on installing and launching <a href="https://ru.wikipedia.org/wiki/Kubernetes">Kubernetes</a> , however, not everything is so smooth (I spent several days launching my cluster). <br><br>  This article is intended to provide comprehensive information not only on installing k8s, but also to explain each step: why and why we are doing exactly as it is written (this is very important for a successful launch). <br><br><h3>  What you need to know </h3><br>  <b>Servers:</b> <br>  A cluster means that you have more than one physical server between which resources will be distributed.  Servers are called nodes. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>Disks:</b> <br>  Normal hards in k8s are not supported.  Work with disks occurs by means of distributed file storages.  This is necessary so that k8s can ‚Äúmove‚Äù docker containers to other nodes, if necessary, without losing data (files). <br><br>  You need to start creating a cluster by creating your own distributed file storage.  If you are sure that you will never need disks, you can skip this step. <br>  I chose <a href="https://ru.wikipedia.org/wiki/Ceph">Ceph</a> .  And I recommend reading this <a href="https://habrahabr.ru/post/313644/">wonderful article</a> . <br><br>  The minimum reasonable number of servers for Ceph is 3 (you can build on one, but this makes little sense because of the high probability of losing data). <br><br>  <b>Network:</b> <br>  We need Flannel - it allows you to organize a software defined network (Software Defined Network, SDN).  It is SDN that allows all our containers to communicate with each other within the cluster (the Flannel installation is done with k8s and described below). <br><br><h3>  Server Preparation </h3><br>  In our example, we use 3 physical servers.  Install Ubuntu 16.04 on all servers.  Do not create <b>swap</b> partitions (k8s requirement). <br><br>  Provide at least one disk (or partition) for Ceph in each server. <br><br>  Do not enable SELinux support (it is turned off by default in Ubuntu 16.04). <br><br>  We called the server like this: kub01 kub02 kub03.  The sda2 part on each server is created for Ceph (formatting is optional). <br><a name="habracut"></a><br><h3>  Install and configure Ceph </h3><br>  I will describe the Ceph installation fairly briefly.  There are many examples on the web and the <a href="https://ceph.com/">Ceph</a> site itself has pretty good documentation. <br><br>  We perform all operations from under the privileged root user. <br><br>  Create a temporary directory: <br><br><pre><code class="bash hljs">mkdir ~/ceph-admin <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> ~/ceph-admin</code> </pre> <br>  Install Ceph: <br><br><pre> <code class="bash hljs">apt install ceph-deploy ceph-common</code> </pre><br>  You need to create a key and decompose it across all servers.  This is needed for the ceph-deploy utility: <br><br><pre> <code class="bash hljs">ssh-keygen ssh-copy-id kub01 ssh-copy-id kub02 ssh-copy-id kub03</code> </pre><br><br>  (you may need to correct the ssh config to allow it to log in as root). <br><br>  Check that kub01 is not registered in your / etc / hosts as 127.0.0.1 (if registered, delete this line). <br><br>  Create a disk cluster and initialize it: <br><br><pre> <code class="bash hljs">ceph-deploy new kub01 kub02 kub03 ceph-deploy install kub01 kub02 kub03 ceph-deploy mon create-initial ceph-deploy osd prepare kub01:sda2 kub02:sda2 kub03:sda2 ceph-deploy osd activate kub01:sda2 kub02:sda2 kub03:sda2</code> </pre><br>  Check our disk cluster: <br><br><pre> <code class="bash hljs">ceph -s cluster 363a4cd8-4cb3-4955-96b2-73da72b63cf5 health HEALTH_OK</code> </pre><br>  You can use the following useful commands: <br><br><pre> <code class="bash hljs">ceph -s ceph df ceph osd tree</code> </pre><br>  Now that we have verified that Ceph is working, we will create a separate pool for k8s: <br><br><pre> <code class="bash hljs">ceph osd pool create kube 100 100</code> </pre><br>  (you can view all existing pools with: ceph df) <br><br>  Now we will create a separate user for our <b>kube</b> pool and save the keys: <br><br><pre> <code class="bash hljs">ceph auth get-or-create client.kube mon <span class="hljs-string"><span class="hljs-string">'allow r'</span></span> osd <span class="hljs-string"><span class="hljs-string">'allow rwx pool=kube'</span></span> ceph auth get-key client.admin &gt; /etc/ceph/client.admin ceph auth get-key client.kube &gt; /etc/ceph/client.kube</code> </pre><br>  (you will need the keys to access the k8s storage) <br><br><h3>  Install Kubernetes </h3><br>  Add the k8s repository to our system: <br><br><pre> <code class="bash hljs">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF</code> </pre><br>  Now install the main packages: <br><br><pre> <code class="bash hljs">apt update apt install -y docker.io kubelet kubeadm kubernetes-cni</code> </pre><br>  Initialize and run k8s <br><br><pre> <code class="bash hljs">kubeadm init --pod-network-cidr=10.244.0.0/16</code> </pre><br>  (such a network 10.244.0.0/16 is necessary for the flannel to work - do not change it) <br><br>  Save the script printed command to attach the nodes to the cluster. <br><br>  It is convenient to use a separate unprivileged user for working with k8s.  Create it and copy the k8s configuration file into it: <br><br><pre> <code class="bash hljs">useradd -s /bin/bash -m kube mkdir ~kube/.kube cp /etc/kubernetes/admin.conf ~kube/.kube/config chown kube: ~kube/.kube/config</code> </pre><br>  The utility is used to work with k8s: <b>kubectl</b> .  We use it only from under our user <b>kube</b> .  To go under the user run: <br><br><pre> <code class="bash hljs">su - kube</code> </pre><br>  Allow containers to run on the wizard: <br><br><pre> <code class="bash hljs">kubectl taint nodes --all node-role.kubernetes.io/master-</code> </pre><br>  Configuring rights: <br><br><pre> <code class="bash hljs">kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default</code> </pre><br>  Install the flannel (network subsystem): <br><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</code> </pre><br><div class="spoiler">  <b class="spoiler_title">How to check that everything works</b> <div class="spoiler_text">  Run the command: <br><br><pre> <code class="bash hljs">kubectl -n kube-system get pods</code> </pre><br>  The output should be something like: <br><br><pre> <code class="plaintext hljs">NAME READY STATUS RESTARTS AGE etcd-kub01.domain.com 1/1 Running 1 4d kube-apiserver-kub01.domain.com 1/1 Running 1 4d kube-controller-manager-kub01.domain.com 1/1 Running 0 4d kube-dns-7c6d8859cb-dmqrn 3/3 Running 0 1d kube-flannel-ds-j948h 1/1 Running 0 1d kube-proxy-rmbqq 1/1 Running 0 1d kube-scheduler-kub01.domain.com 1/1 Running 1 4d</code> </pre><br></div></div><br><h4>  Install and configure the web interface </h4><br><pre> <code class="bash hljs">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</code> </pre><br>  <b>Create a user to access the web interface:</b> <br><pre> <code class="plaintext hljs">cat &lt;&lt; EOF &gt; account.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kube-system EOF kubectl -n kube-system create -f account.yaml</code> </pre><br>  Run kube-proxy, you can do it like this: <br><br><pre> <code class="bash hljs">kubectl proxy &amp;</code> </pre><br>  And forward port 8001 from your working machine to kub01 server: <br><br><pre> <code class="bash hljs">ssh -L 8001:127.0.0.1:8001 -N kub01 &amp;</code> </pre><br>  Now we can access the web interface from our working machine at: <br><br>  <a href="http://127.0.0.1:8001/ui">http://127.0.0.1:8001/ui</a> <a href="http://127.0.0.1:8001/ui"><br></a>  (a web interface will open where you need to specify a token) <br><br>  You can get a token to access the web interface like this: <br><br><pre> <code class="bash hljs">kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk <span class="hljs-string"><span class="hljs-string">'{print $1}'</span></span>)</code> </pre><br><h3>  Customize Kubernetes and Ceph </h3><br>  The current <b>kube-controller-manager-amd64 controller: v1.9.2 is</b> missing the <b>rbd</b> binary required for working with <b>Ceph</b> , so we will create our own kube-controller-manager: <br><br><div class="spoiler">  <b class="spoiler_title">What is RBD, why is it needed and why is it missing?</b> <div class="spoiler_text">  <b>RBD</b> (Rados Block Device) is a block device that is used by k8s to create and mount Docker container partitions.  In this case, this is the binary included in the package: <b>ceph-common</b> . <br><br>  k8s does not include this package in its controller apparently because it depends on the distribution of the operating system you are using.  Therefore, when assembling your controller, be sure to specify your distribution exactly, so that RBD is relevant. <br></div></div><br>  To create our <b>kube-controller-manager,</b> do the following: <br>  (we execute all commands from under the root user) <br><br><pre> <code class="bash hljs">mkdir docker cat &lt;&lt; EOF &gt; docker/Dockerfile FROM ubuntu:16.04 ARG KUBERNETES_VERSION=v1.9.2 ENV DEBIAN_FRONTEND=noninteractive \ container=docker \ KUBERNETES_DOWNLOAD_ROOT=https://storage.googleapis.com/kubernetes-release/release/<span class="hljs-variable"><span class="hljs-variable">${KUBERNETES_VERSION}</span></span>/bin/linux/amd64 \ KUBERNETES_COMPONENT=kube-controller-manager RUN <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> -x \ &amp;&amp; apt-get update \ &amp;&amp; apt-get install -y \ ceph-common \ curl \ &amp;&amp; curl -L <span class="hljs-variable"><span class="hljs-variable">${KUBERNETES_DOWNLOAD_ROOT}</span></span>/<span class="hljs-variable"><span class="hljs-variable">${KUBERNETES_COMPONENT}</span></span> -o /usr/bin/<span class="hljs-variable"><span class="hljs-variable">${KUBERNETES_COMPONENT}</span></span> \ &amp;&amp; chmod +x /usr/bin/<span class="hljs-variable"><span class="hljs-variable">${KUBERNETES_COMPONENT}</span></span> \ &amp;&amp; apt-get purge -y --auto-remove \ curl \ &amp;&amp; rm -rf /var/lib/apt/lists/* EOF docker build -t <span class="hljs-string"><span class="hljs-string">"my-kube-controller-manager:v1.9.2"</span></span> docker/</code> </pre><br>  (Be sure to include current versions of the k8s and the OS distribution) <br><br>  We check that our controller is successfully created: <br><br><pre> <code class="bash hljs">docker images | grep my-kube-controller-manager</code> </pre><br>  Check that our image has rbd: <br><br><pre> <code class="bash hljs">docker run my-kube-controller-manager:v1.9.2 whereis rbd</code> </pre><br>  Should see something like this: rbd: / usr / bin / rbd /usr/share/man/man8/rbd.8.gz <br><br>  We replace the standard controller with ours, for this we edit the file: <br>  <b>/etc/kubernetes/manifests/kube-controller-manager.yaml</b> <br><br>  Replace the string: <br>  <b>image: gcr.io/google_containers/kube-controller-manager-amd64:v1.9.2</b> <b><br></b>  on: <br>  <b>image: my-kube-controller-manager: v1.9.2</b> <b><br></b>  <b>imagePullPolicy: IfNotPresent</b> <br>  (be sure to add the <b>imagePullPolicy</b> directive <b>so</b> that k8s does not try to download this image from the Internet) <br><br>  <b>We go under the user kube and wait until our controller starts up (nothing needs to be done).</b> <br><br><pre> <code class="bash hljs">kubectl -n kube-system describe pods | grep kube-controller</code> </pre><br>  <b>Must see that our image is used:</b> <br>  Image: my-kube-controller-manager: v1.9.2 <br><br>  Now that our controller with RBD support has been launched, we can start setting up a bunch of k8s and Ceph. <br><br><h4>  Setting up the disk subsystem bundle (k8s + Ceph) </h4><br>  <b>Add keys to k8s to access Ceph:</b> <br><br><pre> <code class="bash hljs">kubectl create secret generic ceph-secret --<span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=<span class="hljs-string"><span class="hljs-string">"kubernetes.io/rbd"</span></span> --from-file=/etc/ceph/client.admin --namespace=kube-system kubectl create secret generic ceph-secret-kube --<span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=<span class="hljs-string"><span class="hljs-string">"kubernetes.io/rbd"</span></span> --from-file=/etc/ceph/client.kube --namespace=default</code> </pre><br>  <b>Create StorageClass (default):</b> <br><br><pre> <code class="bash hljs">cat &lt;&lt; EOF &gt; ceph_storage.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-rbd annotations: {<span class="hljs-string"><span class="hljs-string">"storageclass.kubernetes.io/is-default-class"</span></span>:<span class="hljs-string"><span class="hljs-string">"true"</span></span>} provisioner: kubernetes.io/rbd parameters: monitors: kub01:6789,kub02:6789,kub03:6789 pool: kube adminId: admin adminSecretName: ceph-secret adminSecretNamespace: <span class="hljs-string"><span class="hljs-string">"kube-system"</span></span> userId: kube userSecretName: ceph-secret-kube fsType: ext4 imageFormat: <span class="hljs-string"><span class="hljs-string">"2"</span></span> imageFeatures: <span class="hljs-string"><span class="hljs-string">"layering"</span></span> EOF kubectl create -f ceph_storage.yaml</code> </pre><br><div class="spoiler">  <b class="spoiler_title">How to check the operation of the disk subsystem?</b> <div class="spoiler_text">  Checking the availability of StorageClass: <br><br><pre> <code class="bash hljs">kube@kub01:~$ kubectl get storageclass NAME PROVISIONER AGE ceph-rbd (default) kubernetes.io/rbd 4d</code> </pre><br>  Create a test pod with a disk: <br><br><pre> <code class="bash hljs">cat &lt;&lt; EOF &gt; test_pod.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: claim1 spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi --- apiVersion: v1 kind: Pod metadata: name: <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-pod-with-pvc spec: volumes: - name: <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-pvc-storage persistentVolumeClaim: claimName: claim1 containers: - name: <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-container image: kubernetes/pause volumeMounts: - name: <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-pvc-storage mountPath: /var/lib/www/html EOF kubectl create -f test_pod.yaml</code> </pre><br>  Check that the Pod was created (you need to wait for the creation and launch): <br><br><pre> <code class="bash hljs">kube@kub01:~$ kubectl get pods NAME READY STATUS RESTARTS AGE <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>-pod-with-pvc 1/1 Running 0 15m</code> </pre><br>  Check that Claim was created (request for a disc): <br><br><pre> <code class="bash hljs">kube@kub01:~$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE claim1 Bound pvc-076df6ee-0ce9-11e8-8b93-901b0e8fc39b 1Gi RWO ceph-rbd 12m</code> </pre><br>  Check that the disk itself is created: <br><br><pre> <code class="bash hljs">kube@kub01:~$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-076df6ee-0ce9-11e8-8b93-901b0e8fc39b 1Gi RWO Delete Bound default/claim1 ceph-rbd</code> </pre><br>  Finally, check that the drive is mounted on the system: <br><br><pre> <code class="bash hljs">root@kub01:~$ mount | grep pvc-076df6ee-0ce9-11e8-8b93-901b0e8fc39b /dev/rbd0 on /var/lib/kubelet/pods/076fff13-0ce9-11e8-8b93-901b0e8fc39b/volumes/kubernetes.io~rbd/pvc-076df6ee-0ce9-11e8-8b93-901b0e8fc39b <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> ext4 (rw,relatime,stripe=1024,data=ordered)</code> </pre><br></div></div><br><h3>  Adding new (additional) nodes to the k8s cluster </h3><br>  On the new server, run the following commands: <br><br><pre> <code class="bash hljs">curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt update apt install -y docker.io kubelet kubeadm kubernetes-cni ceph-common python</code> </pre><br>  <b>We attach the node to the master:</b> <br>  We need a key.  You can get it on the wizard by running the command: <br><br><pre> <code class="bash hljs">kubeadm token list</code> </pre><br>  Or, create it: <br><pre> <code class="bash hljs">kubeadm token create --<span class="hljs-built_in"><span class="hljs-built_in">print</span></span>-join-command</code> </pre><br><br>  <b>An example of a command to join (execute on a new node):</b> <br><br><pre> <code class="bash hljs">kubeadm join --token cb9141.6a912d1dd7f66ff5 8.8.8.8:6443 --discovery-token-ca-cert-hash sha256:f0ec6d8f9699169089c89112e0e6b5905b4e1b42db22815186240777970dc6fd</code> </pre><br><h3>  Install and configure <a href="https://docs.helm.sh/">Helm</a> </h3><br>  For quick and easy installation of applications in k8s Helm was invented. <br><br>  A list of available applications can be found <a href="https://hub.kubeapps.com/">here</a> . <br><br>  <b>Install and initialize Helm:</b> <br><br><pre> <code class="bash hljs">curl https://storage.googleapis.com/kubernetes-helm/helm-v2.8.0-linux-amd64.tar.gz | tar -xz ./linux-amd64/helm init</code> </pre><br>  PS: it took 6 hours to write this article.  Do not judge strictly if there are typos somewhere.  Ask questions, gladly answer and help. </div><p>Source: <a href="https://habr.com/ru/post/348688/">https://habr.com/ru/post/348688/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../348678/index.html">IT events digest for February and March</a></li>
<li><a href="../348680/index.html">Learning to intercept unprocessed messages or an example of how SObjectizer is cluttered with new features ...</a></li>
<li><a href="../348682/index.html">Deploy your Java environment with Ansible</a></li>
<li><a href="../348684/index.html">BLE400 board and development for nRF51822</a></li>
<li><a href="../348686/index.html">Journal of the work with the network. Part 1</a></li>
<li><a href="../348690/index.html">Five problems and trends in information security: what to expect in 2018</a></li>
<li><a href="../348692/index.html">Wireless network protection: WIPS. Part 1: Mojo AirTight</a></li>
<li><a href="../348694/index.html">AgileDays Conference March 22 and 23, 2018</a></li>
<li><a href="../348696/index.html">Datalore: open beta application for analyzing data in Python</a></li>
<li><a href="../348698/index.html">Your 5 kopecks: Wi-Fi today and tomorrow</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>