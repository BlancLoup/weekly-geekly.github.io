<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Scale Kubernetes to 2500 nodes</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="All good! 

 Well. The first course of the DevOps course has been released, the second one is being trained with might and main and the third one is c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Scale Kubernetes to 2500 nodes</h1><div class="post__text post__text-html js-mediator-article"> All good! <br><br>  Well.  The first course of the <a href="https://otus.pw/erdR/">DevOps</a> course has <a href="https://otus.pw/erdR/">been</a> released, the second one is being trained with might and main and the third one is <a href="https://otus.pw/erdR/">coming up</a> .  The course will be improved, the project too, remains unchanged so far one thing: interesting articles that we are only translating for you, but on the nose there are already disruptions of cover from those things that we have been asked for :) <br><br>  Go! 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We have been using Kubernetes for deep learning research for more than two years now.  While our most massive loads manage cloud VMs directly, Kubernetes provides a fast iterative cycle and scalability, which makes it ideal for our experiments.  Now we manage several Kubernetes clusters (both cloud and physical equipment), the largest of which consists of more than 2500 nodes - this is a cluster in Azure on a combination of D15v2 and NC24 virtual machines. <br><br>  Many system components have failed in the scaling process, including etcd, Kube masters, loading Docker images, networks, KubeDNS, and even ARP caches of our machines.  Therefore, we decided that it would be useful to share what problems we encountered and how we coped with them. <br><br><img src="https://habrastorage.org/webt/1_/y9/ny/1_y9ny-icxxpffq6lz9ud4snj-w.jpeg"><a name="habracut"></a><br>  <b>etcd</b> <br><br>  After expanding the cluster to 500 nodes, our researchers began to receive regular timeouts in <a href="https://kubernetes.io/docs/reference/kubectl/overview/">kubectl</a> .  We tried to solve the problem by adding more Kube Masters ( <a href="https://kubernetes.io/docs/reference/generated/kube-apiserver/">VMs</a> running <a href="https://kubernetes.io/docs/reference/generated/kube-apiserver/">kube-apiserver</a> ).  At first glance, this helped temporarily solve the problem, but after 10 iterations, we realized that we were trying to cure a symptom, and not the original cause (for comparison, <a href="https://cloud.google.com/kubernetes-engine/">GKE</a> uses one 32-core VM for 500 nodes). <br><br>  The main suspect was our cluster <a href="https://github.com/coreos/etcd">etcd</a> , which is the central repository of Kube Masters' states.  Looking at the <a href="https://www.datadoghq.com/">Datadog</a> charts, we found recording delays of up to hundreds of milliseconds on our DS15v2 machines, where copies of etcd were running, despite the fact that each server used a P30 SSD with a capacity of 5000 IOPS. <br><br><img src="https://habrastorage.org/webt/6r/1s/44/6r1s441phlod4tsl38lek6bsi_0.png"><br>  <i>These bursts of delays block the entire cluster!</i> <br><br>  The performance test with <a href="https://github.com/axboe/fio">fio</a> showed that etcd can only use 10% IOPS due to 2ms write delays - etcd makes serial I / O, so delays block it. <br><br>  For each node, we moved the etcd directory to a local temporary disk that is connected to the SSD directly, and not over the network.  Moving to a local disk reduced recording latency to 200us and cured etcd! <br><br>  The problem arose again when the cluster increased to 1000 nodes - we again encountered large delays etcd.  This time, kube-apiservers read more than 500MB / s from etcd.  We set up <a href="https://github.com/coreos/kube-prometheus">Prometheus</a> to monitor apiservers and add the flags <code>--audit-log-path</code> and <code>--audit-log-maxbackup</code> for better logging of apiservers.  This has revealed a series of slow queries and excessive calls to the LIST API for Events. <br><br>  The main problem: the default settings for the <a href="https://www.fluentd.org/">Fluentd</a> and Datadog monitoring processes were to create requests for apiservers from each node in the cluster (for example, this <a href="https://github.com/DataDog/dd-agent/issues/3381">problem</a> is now solved).  We made the survey process less aggressive, and the download of apisevers again became stable: <br><br><img src="https://habrastorage.org/webt/q6/hl/4e/q6hl4ewkgkklfik-xhc5kmkb7q4.png"><br><br>  <i>Egress etcd dropped from over 500MB / s to almost zero values ‚Äã‚Äã(negative numbers in the image above show egress)</i> <br><br>  Another useful editing was storing Kubernetes Events in a separate etcd cluster - in this case, bursts in event creation will not affect the activities of core etcd instances.  To do this, we simply set the <code>--etcd-servers-overrides</code> flag like this: <br><br><pre> <code class="bash hljs">--etcd-servers-overrides=/events<span class="hljs-comment"><span class="hljs-comment">#https://0.example.com:2381;https://1.example.com:2381;https://2.example.com:2381</span></span></code> </pre> <br>  Another problem after reaching 1000 nodes was exceeding the storage limit etcd (default 2GB), after which it stopped accepting records.  This caused an avalanche-like problem - all Kube nodes failed the performance check, and so our <a href="https://github.com/openai/kubernetes-ec2-autoscaler">autoscaler</a> decided to destroy all workers.  We increased the maximum storage size of etcd using the <code>--quota-backend-bytes</code> flag.  And they added a check to autoscaler - if the autoscaler action destroys more than 50% of the cluster, then it does not perform it. <br><br><h3>  Kube masters </h3><br>  We host the kube-apiserver, <a href="https://kubernetes.io/docs/reference/generated/kube-controller-manager/">kube-controller-manager</a> and <a href="https://kubernetes.io/docs/reference/generated/kube-scheduler/">kube-scheduler processes</a> on the same machines.  For <a href="https://ru.wikipedia.org/wiki/%25D0%259E%25D1%2582%25D0%25BA%25D0%25B0%25D0%25B7%25D0%25BE%25D1%2583%25D1%2581%25D1%2582%25D0%25BE%25D0%25B9%25D1%2587%25D0%25B8%25D0%25B2%25D1%258B%25D0%25B9_%25D0%25BA%25D0%25BB%25D0%25B0%25D1%2581%25D1%2582%25D0%25B5%25D1%2580">fault tolerance</a> , we always have at least two wizards, and the <code>--apiserver-count</code> flag is set to the number of running apiservers (otherwise Prometheus might get confused in instances). <br><br>  Basically, we use Kubernetes as a task scheduling system, and the <a href="https://github.com/openai/kubernetes-ec2-autoscaler">autoscaler</a> dynamically scales the cluster - this can significantly reduce the cost of idle nodes, providing low delays during fast iterations.  By default, the kube-scheduler evenly distributes the load between the nodes, but we want the opposite to destroy unused nodes and quickly plan large heaps.  Therefore, we have moved to the following policy: <br><br><pre> <code class="bash hljs">{ <span class="hljs-string"><span class="hljs-string">"kind"</span></span> : <span class="hljs-string"><span class="hljs-string">"Policy"</span></span>, <span class="hljs-string"><span class="hljs-string">"apiVersion"</span></span> : <span class="hljs-string"><span class="hljs-string">"v1"</span></span>, <span class="hljs-string"><span class="hljs-string">"predicates"</span></span> : [ {<span class="hljs-string"><span class="hljs-string">"name"</span></span> : <span class="hljs-string"><span class="hljs-string">"GeneralPredicates"</span></span>}, {<span class="hljs-string"><span class="hljs-string">"name"</span></span> : <span class="hljs-string"><span class="hljs-string">"MatchInterPodAffinity"</span></span>}, {<span class="hljs-string"><span class="hljs-string">"name"</span></span> : <span class="hljs-string"><span class="hljs-string">"NoDiskConflict"</span></span>}, {<span class="hljs-string"><span class="hljs-string">"name"</span></span> : <span class="hljs-string"><span class="hljs-string">"NoVolumeZoneConflict"</span></span>}, {<span class="hljs-string"><span class="hljs-string">"name"</span></span> : <span class="hljs-string"><span class="hljs-string">"PodToleratesNodeTaints"</span></span>} ], <span class="hljs-string"><span class="hljs-string">"priorities"</span></span> : [ {<span class="hljs-string"><span class="hljs-string">"name"</span></span> : <span class="hljs-string"><span class="hljs-string">"MostRequestedPriority"</span></span>, <span class="hljs-string"><span class="hljs-string">"weight"</span></span> : 1}, {<span class="hljs-string"><span class="hljs-string">"name"</span></span> : <span class="hljs-string"><span class="hljs-string">"InterPodAffinityPriority"</span></span>, <span class="hljs-string"><span class="hljs-string">"weight"</span></span> : 2} ] }</code> </pre><br>  We are using <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns">KubeDNS extensively</a> to discover services, but soon after deploying a new scheduling policy, it had problems with reliability.  It became clear that problems arise only on certain KubeDNS subs.  Due to the new scheduling policy, some machines started to run more than 10 copies of KubeDNS, create hotspot.  Because of this, we exceeded ~ 200QPS, which are allowed for each VM in Azure for external DNS queries. <br><br>  We fixed this by adding an <a href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">anti-affinity</a> rule to all KubeDNS files: <br><br><pre> <code class="bash hljs">affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - weight: 100 labelSelector: matchExpressions: - key: k8s-app operator: In values: - kube-dns topologyKey: kubernetes.io/hostname</code> </pre> <br><h3>  Download Docker Images </h3><br>  We started the <a href="https://blog.openai.com/more-on-dota-2/">Dota</a> project on Kubernetes, but as it grew, we began to notice that the pods in the Kubernetes nodes were often in the <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/">Pending</a> state for a long time.  The image of the game weighs approximately 17GB, it takes about 30 minutes to deploy it to the fresh node of the cluster, and to understand why the Dota container is in Pending.  It turned out that this applies to other containers.  We found that <a href="https://kubernetes.io/docs/admin/kubelet">kubelet</a> has a flag <code>--serialize-image-pulls</code> (true by default), which means that uploading the Dota image blocks all other images.  Changing the flag to false required switching Docker from AUFS to overlay2.  To make uploading even faster, we moved the Docker root to the local SSD instance, as is the case with etcd machines. <br><br>  But even after optimizing the download speed, the sums could not start, giving an error message: <code>rpc error: code = 2 desc = net/http: request canceled</code> .  In the kubelet and Docker logs there were error messages indicating interruption of the image upload due to lack of progress.  The root of the problem was excessively long unloading of large images and cases where the backlog of unloading images turned out to be too long. <br><br>  To solve this problem, we set the kubelet <code>--image-pull-progress-deadline</code> flag to 30 minutes, and the Docker max-concurrent-downloads daemon option to 10. (The second edit did not speed up unloading of large images, but allowed parallelizing the unloading of the image queue. ) <br><br>  Our latest Docker issue was related to the Google Container Registry.  By default, a cublet unloads a specific image with gcr.io (controlled by the --pod-infra-container-image flag), which is used at the start of any new container.  If this unloading is unsuccessful for some reason (for example, exceeding the quota), it will not allow the node to launch containers.  Since our nodes do not have their own public IP and go through NAT to achieve gcr.io, we most likely exceed the IP quota limit.  To solve the problem, we simply preload the Docker image into the image of the machine for our Kubernetes workers using <br><br><pre> <code class="plaintext hljs">docker image save -o /opt/preloaded_docker_images.tar and docker image load -i /opt/preloaded_docker_images.tar</code> </pre>  . <br><br>  We do the same with the whitelist of common internal OpenAI images, such as the Dota image, to improve performance. <br><br><h3>  Network </h3><br>  As development progressed, our experiments grew to complex distributed systems, the performance of which depended heavily on the network.  Already at the beginning of the launch of distributed experiments, it became clear that our network was poorly configured.  The bandwidth between the machines was at the level of 10-15Gbit / s, but Kube, using Flannel, reached the limit by ~ 2Gbit / s.  Machine Zone's <a href="http://machinezone.github.io/research/networking-solutions-for-kubernetes/">publicly available tests</a> showed similar values, which means the problem is not in a poor setup, but in our environment.  (At the same time, Flannel does not create an overhead for our physical machines.) <br><br>  To get around this problem, users can add two different settings to disable Flannel in the sub-sites: hostNetwork: true and dnsPolicy: ClusterFirstWithHostNet.  (But read the Kubernetes documentation warnings before doing so.) <br><br><h3>  ARP Cache </h3><br>  Despite DNS tuning, we still faced DNS problems.  One day, the engineer reported that the nc -v command before the Redis server had been running for more than 30 seconds before the connection setup message appeared.  We traced the problem in the ARP kernel stack.  The Redis host exploration revealed a problem with the network: communication on any port hung for a few seconds, but no DNS names were resolved by the local <a href="https://ru.wikipedia.org/wiki/Dnsmasq">dnsmasq</a> daemon, while <a href="https://ru.wikipedia.org/wiki/Dig">dig</a> just printed a cryptic error message: socket.c: 1915: internal_send: 127.0. 0.1 # 53: Invalid argument.  The <a href="https://ru.wikipedia.org/wiki/Dmesg">dmesg</a> log was more informative: neighbor table overflow!  This meant that the ARP cache ran out of space.  ARP is used to map a network address, such as an IPv4 address, to a physical address, such as a MAC address.  Fortunately, it is easy to fix by configuring several parameters in /etc/sysctl.conf: <br><pre> <code class="bash hljs">net.ipv4.neigh.default.gc_thresh1 = 80000 net.ipv4.neigh.default.gc_thresh2 = 90000 net.ipv4.neigh.default.gc_thresh3 = 100000</code> </pre> <br><br>  Usually, these parameters are configured in HPC clusters, which is especially true for Kubernetes clusters, since each of them has its own IP address, which takes up space in the ARP cache. <br><br>  Our Kubernetes clusters have been operating without incident for 3 months now, and we plan to expand it even more in 2018.  We recently updated to version 1.8.4 and are pleased to see that now it officially supports 5000. <br><br>  THE END <br><br>  We are waiting for questions, comments here or at <a href="https://otus.pw/MlQc/">the Open Day</a> .  Today, it is led, by the way, by <a href="https://otus.pw/aGTo/">Alexander Titov</a> - a man and a steamer. </div><p>Source: <a href="https://habr.com/ru/post/348640/">https://habr.com/ru/post/348640/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../348630/index.html">Android development news to subscribe to</a></li>
<li><a href="../348632/index.html">Canceled Promises in EcmaScript6</a></li>
<li><a href="../348634/index.html">Why is it so difficult in St. Petersburg to build a career of VP of engineering?</a></li>
<li><a href="../348636/index.html">The market of cooling systems for data centers on the verge of significant changes</a></li>
<li><a href="../348638/index.html">What is crowd marketing? The most complete guide and 9 specific tips for effective use</a></li>
<li><a href="../348642/index.html">Creating parametric database objects in nanoCAD Mechanics (Part 3)</a></li>
<li><a href="../348644/index.html">How we make video content in Puzzle English: from the chromakey on the wall to a full-fledged video recording studio</a></li>
<li><a href="../348646/index.html">We invite you to MosCode Festival and analyze the tasks of past years.</a></li>
<li><a href="../348648/index.html">Seminar "Data Storage Systems for the Cloud and in the Cloud", February 15, Moscow</a></li>
<li><a href="../348650/index.html">Who to contact for design: freelancers, design studios, branding agencies</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>