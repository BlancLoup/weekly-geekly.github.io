<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Continuous integration in Yandex. Part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the previous article, we talked about transferring development to a single repository with a trunk-based approach to development, with common syste...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Continuous integration in Yandex. Part 2</h1><div class="post__text post__text-html js-mediator-article"><p>  In the previous <a href="https://habr.com/company/yandex/blog/428972/">article,</a> we talked about transferring development to a single repository with a trunk-based approach to development, with common systems for building, testing, deploying and monitoring, which tasks the continuous integration system should perform to work effectively in such conditions. </p><br><p>  Today we will tell Habr's readers about the device of the system of continuous integration. </p><br><p><img src="https://habrastorage.org/webt/wb/mt/xc/wbmtxcvurtd6cdv1aomjrtcyfw8.png" alt="image"></p><br><p>  A continuous integration system must operate reliably and quickly.  The system should respond quickly to incoming events and should not introduce additional delays in the delivery of test run results to the user.  Build and test results must be delivered to the user in real time. </p><a name="habracut"></a><br><p>  The continuous integration system is a stream processing system with minimal delays. </p><br><p>  After sending all the results at a certain stage (configure, build, style, small tests, medium tests, etc.) the build system signals this to the continuous integration system ("closes" the stage), and the user sees that for a given check and All results are known at this stage.  Each stage closes independently.  The user quickly receives a useful signal.  After all stages are closed, the check is considered complete. </p><br><p>  To implement the system, we chose <a href="https://www.oreilly.com/ideas/questioning-the-lambda-architecture">Kappa</a> architecture.  The system consists of 2 subsystems: </p><br><ul><li>  Event and data processing takes place in a realtime loop.  Any input data is processed as data streams (streams).  First, the events are recorded in the stream and only then they are processed. </li><li>  The results of data processing are continuously recorded in the database, which are then accessed via the API.  In Kappa architecture, this is called the serving layer. </li></ul><br><p>  All requests for data modification should go through a realtime loop, since there you should always have the current state of the system.  Requests for reading go only to the database. </p><br><img src="https://habrastorage.org/webt/fb/bu/ps/fbbups7bcp0zgwvigxv5un9reek.png"><br><br><p>  Wherever possible, we follow the "append-only" rule.  No modifications or deletions of objects, except for deleting old, unnecessary data. </p><br><p>  During the day more than 2 TB of raw data passes through the service. </p><br><p>  Benefits: </p><br><ul><li>  Streams contain all events and messages.  We can always understand what happened when.  Stream can be perceived as a big log. </li><li>  High efficiency and minimal overhead.  It turns out completely event-oriented system, without any loss on polling'e.  No event - do not do anything extra. </li><li>  The application code practically does not deal with stream synchronization primitives and memory shared between threads.  This makes the system more reliable. </li><li>  Processors are well isolated from each other, because  do not interact directly, only through the stream.  You can provide a good test coverage. </li></ul><br><p>  But streaming data processing is not so simple: </p><br><ul><li>  A good understanding of the computational model is required.  You will have to rethink existing data processing algorithms.  Not all algorithms on the move effectively fall on the model of streams and will have to break a little head. </li><li>  It is necessary to ensure that the order of receipt and processing of events </li><li>  You must be able to handle interrelated events, i.e.  have quick access to all the necessary data during the processing of a new message. </li><li>  You also need to be able to handle duplicate events. </li></ul><br><h3 id="potokovaya-obrabotka-dannyh-stream-processing">  Stream processing </h3><br><p>  While working on the project, the Stream Processor library was written, which helped us to implement and launch streaming data processing algorithms in production in a short time. </p><br><p>  Stream Processor is a library for building streaming data processing systems.  Stream is a potentially infinite sequence of data (messages) to which it is possible only to add new messages, already recorded messages are not changed or deleted from the stream.  Converters from one stream to another (stream processors) functionally consist of three parts: an incoming message provider, which usually reads messages from one or several streams and puts them into a processing queue, a message processor, which converts incoming messages into outgoing messages and puts them into a queue on the record, and the writer, where outgoing messages grouped within a time window fall into the weekend streams.  The message data generated by one processor of the streams may later be used by others.  Thus, streams and processors form a directed graph in which cycles are possible, in particular, a stream processor can even generate messages in the same stream from which it takes data. </p><br><p>  It is guaranteed that each message of the input stream will be processed by each processor associated with it at least once (at least once semantics).  It is also guaranteed that all messages will be processed in the order in which they arrived in this stream.  To do this, the stream processors are distributed across all service nodes, so that no more than one instance of each of the registered processors runs at a time. </p><br><p>  Handling interconnected events is one of the main problems in building streaming data processing systems.  As a rule, when streaming messages, the stream processors incrementally create a certain state that is valid at the time of processing the current message.  Such state objects are usually associated not with the whole stream as a whole, but with some of its subsets of messages, which is determined by the key value in this stream.  Efficient state storage is the key to success.  When processing the next message, it is important for the processor to be able to quickly receive this state and, based on it and the current message, generate outgoing messages.  These state objects are available to processors in L1 (please do not confuse with the CPU cache) LRU cache, which is located in memory.  If there is no status in the L1-cache, it is restored from the L2-cache located in the same storage where the streams are stored and where it is periodically stored while the processor is running.  If there is no status in the L2 cache, it is restored from the original stream messages, as if the processor had processed all the original messages associated with the key of the current message.  The caching technique also allows you to deal with the problem of high storage latency, since often sequential processing rests not on server performance, but on delays in requests and responses when communicating with the data storage. </p><br><img width="400" src="https://habrastorage.org/webt/_o/pk/sj/_opksjvyut5cirxrnbjerswkt78.png"><br><br><p>  In order to efficiently store in memory data in L1-caches and message data, in addition to memory-efficient structures, we use object pools that allow you to have only one copy of an object (or even parts of it) in memory.  This technique is already used in the JDK for <a href="https://en.wikipedia.org/wiki/String_interning">string interning</a> and in a similar way extends to other types of objects, which in this case should be immutable. </p><br><p>  For compact data storage in the stream storage, some data is normalized before writing to the stream, i.e.  turn into numbers.  Numbers (object identifiers) can then be applied to efficient compression algorithms.  Numbers are sorted, deltas are considered, then encoding using <a href="https://en.wikipedia.org/wiki/Variable-length_quantity">ZigZag Encoding</a> and then compression by the archiver.  Normalization is not quite standard technique for streaming data processing systems.  But this compression technique is very effective and the amount of data in the most loaded stream is reduced by about 1,000 times. </p><br><img width="600" src="https://habrastorage.org/getpro/habr/post_images/b1b/6f4/fa9/b1b6f4fa9d551a96fd4069b44435354f.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>  For each stream and processor, we monitor the message processing life cycle: the appearance of new messages in the input stream, the size of the queue of unprocessed messages, the size of the write queue in the resulting stream, the processing time of messages and the time distribution by message processing stages: </p><br><img src="https://habrastorage.org/webt/jg/0q/rp/jg0qrpqbojreaugzrf66cikffwa.png"><br><br><h3 id="hranilische-dannyh">  Data store </h3><br><p>  The results of stream processing should be available to the user as soon as possible.  The processed data from the streams should be continuously recorded in the database, which can then be accessed for data (for example, show a report with the results of the test, show the history of the test). </p><br><p>  Characteristics of stored data and queries. <br>  Most of the data are test runs.  More than 1.5 billion launches of assemblies and tests occur in a month.  For each launch, quite a large amount of information is stored: the result and type of error, a brief description of the error (snippet), several links to logs, test duration, a set of numeric values, metrics, in the format name = value, etc.  Some of this data ‚Äî for example, metrics and duration ‚Äî is very difficult to compress, since it actually represents random values.  The other part - for example, the result, the type of error, the logs - can be saved more efficiently, since the same test almost does not change from run to run. </p><br><p>  Previously, we used MySQL to store processed data.  We gradually began to run into database capabilities: </p><br><ul><li>  The volume of data processed doubles every six months. </li><li>  We could only store data for the last 2 months, and wanted to store data for at least a year. </li><li>  Problems with the speed of some heavy (close to analytical) requests. </li><li>  Complicated database schema.  Many tables (normalization), which complicates the entry in the database.  The base scheme is very different from the objects used in the realtime contour. </li><li>  Not experiencing server shutdown.  Failure of a separate server or disabling the data center can lead to system failure. </li><li>  Rather difficult operation. </li></ul><br><p>  We considered several options as candidates for the new data warehouse: PostgreSQL, MongoDB, and several internal solutions, including <a href="https://habr.com/company/yandex/blog/303282/">ClickHouse</a> . </p><br><p>  Some solutions do not allow us to store our data more efficiently than the old solution based on MySQL.  Others do not allow for the implementation of fast and complex (almost analytical) queries.  For example, we have a rather heavy query that shows commits that affect a certain project (some set of tests).  In all cases where we cannot perform fast SQL queries, we would have to force the user to wait a long time or do some calculations in advance with a loss of flexibility.  If you count something in advance, then you need to write more code and at the same time flexibility is lost - there is no way to quickly change behavior and recalculate something.  It is much more convenient and faster to write a SQL query that will return the data you need to the user and be able to quickly modify it if you want to change the behavior of the system. </p><br><h3 id="clickhouse">  Clickhouse </h3><br><p>  We opted for <a href="https://habr.com/company/yandex/blog/303282/">ClickHouse</a> .  ClickHouse is a column database management system (DBMS) for online processing of analytical queries (OLAP). </p><br><p>  Turning to ClickHouse, we deliberately abandoned some of the opportunities offered by other DBMS, having received more than adequate compensation for this in the form of very fast analytical queries and a compact data warehouse. </p><br><p>  In relational DBMS, values ‚Äã‚Äãrelated to one line are physically stored side by side.  In ClickHouse, values ‚Äã‚Äãfrom different columns are stored separately, and data from one column is stored together.  This order of data storage allows for a high degree of data compression with the right choice of primary key.  This also affects which scenarios the DBMS will work well.  ClickHouse works better with queries, where a small number of columns are read and one big table is used in the query, while the rest of the tables are small.  But even in non-analytical queries, ClickHouse can show good results. </p><br><p>  The data in the tables are sorted by primary key.  Sorting is done in the background.  This allows you to create a sparse index of a small amount, which allows you to quickly find data.  ClickHouse has no secondary indexes.  Strictly speaking, there is one secondary index - the partition key (ClickHouse cuts off the data on the partitions where the partition key is specified in the request).  <a href="https://clickhouse.yandex/docs/ru/operations/table_engines/mergetree/">More details</a> . </p><br><p> Non-functional data scheme with normalization, on the contrary, it is preferable to denormalize the data depending on the requests to them.  It is preferable to create "wide" tables with a large number of columns.  This clause is also related to the previous one, because the lack of secondary indexes sometimes makes it necessary to create copies of tables using a different primary key. </p><br><p>  In ClickHouse, there is no UPDATE and DELETE in the classical sense, but there is a possibility of emulating them. </p><br><p>  The data must be inserted in large blocks and not too often (every few seconds).  Line-by-line data loading is practically unworkable on real data volumes. </p><br><p>  ClickHouse does not support transactions, the system becomes <a href="https://en.wikipedia.org/wiki/Eventual_consistency">eventually consistent</a> . </p><br><p>  Nevertheless, some features of ClickHouse, similar to other DBMS, make it easier to convert existing systems to it. </p><br><ul><li>  ClickHouse uses SQL, but with a few differences, useful for queries that are typical in OLAP systems.  There is a powerful system of aggregate functions, ALL / ANY JOIN, lambda-expressions in functions and other SQL extensions, allowing you to write almost any analytical query. </li><li>  ClickHouse supports replication, <a href="https://clickhouse.yandex/docs/ru/operations/settings/settings/">quorum record</a> , quorum reading.  A quorum record is necessary for safe data storage: INSERT succeeds only if ClickHouse was able to write data for a given number of replicas without an error. </li></ul><br><p>  More details about the features ClickHouse can be read in the <a href="https://clickhouse.yandex/docs/ru/">documentation</a> . </p><br><h4 id="osobennosti-raboty-s-clickhouse">  Features of working with ClickHouse </h4><br><p>  Selection of primary and partition keys. </p><br><p>  How to choose <a href="https://clickhouse.yandex/docs/ru/operations/table_engines/mergetree/">primary key</a> and partitioning key?  Perhaps this is the first question that arises when creating a new table.  The selection of a primary key and a partitioning key is usually dictated by the queries that will be performed on the data.  In this case, the most effective are requests that use both conditions: for the primary key and for the partitioning key. </p><br><p>  In our case, the main tables are test run matrices.  It is logical to assume that with such a data structure, the keys must be chosen so that the order of traversing one of them goes in the order of increasing the row number, and the order of traversing the other in the order of increasing the number of the column. </p><br><p>  It is also important to bear in mind that the choice of the primary key can dramatically affect the compactness of data storage, since the same values ‚Äã‚Äãin the order of bypassing the primary key in other columns almost do not occupy space in the table.  So in our case, for example, test states vary little from commit to commit.  This fact essentially predetermined the choice of the primary key - the test ID and the commit number pairs.  And in that order. </p><br><img width="600" src="https://habrastorage.org/webt/0t/gj/jo/0tgjjoefxhjgbxexx4gevfwte7y.png"><br><br><p>  The partitioning key has two destinations.  On the one hand, it allows partitions to become ‚Äúarchived‚Äù so that they can be permanently deleted from the storage, since the data in them is already out of date.  On the other hand, the partitioning key is a secondary index, which means that it allows you to speed up queries, if the expression on it is present in them. </p><br><p>  For our matrices, choosing the commit number as the key for partitioning seems quite natural.  But if you set the value of the revision in the expression for the partitioning key, then there will be unreasonably many partitions in such a table, which will lead to degradation of the performance of queries to it.  Therefore, in the expression for the partitioning key, the revision value can be divided into some large number to reduce the number of partitions, for example, PARTITION BY intDiv (revision, 2000).  This number should be large enough so that the number of partitions does not exceed the recommended values, while it should be small enough so that not a lot of data fall into one partition and the database would not have to read too much data. </p><br><p>  How to implement UPDATE and DELETE? </p><br><p>  In the usual sense, UPDATE and DELETE are not supported in ClickHouse.  However, instead of UPDATE and DELETE, you can add a column with a version to the table and use the special <a href="https://clickhouse.yandex/docs/ru/operations/table_engines/replacingmergetree/">ReplacingMergeTree</a> engine (deletes duplicate records with the same primary key value).  In some cases, the version will naturally be present in the table from the very beginning: for example, if we want to create a table for the current state of the test, the version in this table will be the commit number. </p><br><pre><code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> current_tests ( test_id UInt64, <span class="hljs-keyword"><span class="hljs-keyword">value</span></span> Nullable(<span class="hljs-keyword"><span class="hljs-keyword">String</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">version</span></span> UInt64 ) <span class="hljs-keyword"><span class="hljs-keyword">ENGINE</span></span> = ReplacingMergeTree(<span class="hljs-keyword"><span class="hljs-keyword">version</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">ORDER</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> test_id</code> </pre> <br><p>  In the case of a record change, we add a version with a new value, in the case of deletion, with a NULL value (or some other special value that cannot be found in the data). </p><br><p>  What happened to achieve with the new repository? </p><br><p>  One of the main goals of the transition to ClickHouse was the ability to store test history for a long period of time (several years or at least a year in the worst case).  Already at the prototype stage, it became clear that we would be able to manage the SSDs existing in our servers for storing at least three years of history.  Analytical queries have significantly accelerated, now we can extract much more useful information from our data.  Increased safety margin over RPS.  Moreover, this value is almost linearly scaled by adding new servers to the ClickHouse cluster.  Creating a new data warehouse database ClickHouse - this is just a barely noticeable end-user step towards a more important goal - adding new features, speeding up and simplifying development, thanks to the ability to store and process large amounts of data. </p><br><h4 id="prihodite-k-nam">  Come to us </h4><br><p>  Our department is constantly expanding.  <a href="https://ya.cc/4Yv1q">Come to us</a> if you want to work on complex and interesting tasks and algorithms.  If you have questions, you can ask me directly in PM. </p><br><h3 id="poleznye-ssylki">  useful links </h3><br><p>  Stream processing </p><br><ul><li>  <a href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">The Real-Time Data's Unifying Abstraction</a> . </li><li>  <a href="https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101">The world beyond batch: Streaming 101</a> . </li><li>  The book <a href="http://shop.oreilly.com/product/0636920032175.do">Designing Data-Intensive Applications</a> - O'Reilly Media. </li></ul><br><p>  Kappa architecture </p><br><ul><li><a href="https://www.oreilly.com/ideas/questioning-the-lambda-architecture"></a>  <a href="https://www.oreilly.com/ideas/questioning-the-lambda-architecture">https://www.oreilly.com/ideas/questioning-the-lambda-architecture</a> . </li><li>  <a href="http://milinda.pathirage.org/kappa-architecture.com/">kappa-architecture</a> . </li></ul><br><p>  ClickHouse: </p><br><ul><li>  <a href="https://habr.com/company/yandex/blog/303282/">Yandex opens ClickHouse</a> . </li><li><a href="https://clickhouse.yandex/"></a>  <a href="https://clickhouse.yandex/">https://clickhouse.yandex</a> </li><li>  <a href="https://clickhouse.yandex/docs/ru/">Documentation</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/429956/">https://habr.com/ru/post/429956/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../429946/index.html">The Madness and Success of Oracle Database Code</a></li>
<li><a href="../429948/index.html">Why do we need product managers in fintech</a></li>
<li><a href="../429950/index.html">How to maintain healthy communication habits of remote teams</a></li>
<li><a href="../429952/index.html">Past, Present, and Future of Docker and Other Container Executables in Kubernetes</a></li>
<li><a href="../429954/index.html">Irish Bookmakers Programmer</a></li>
<li><a href="../429958/index.html">Five simple debug rules for novice programmers</a></li>
<li><a href="../429960/index.html">10 reasons why customers unsubscribe from the product</a></li>
<li><a href="../429964/index.html">U> X> I> P ... or ‚ÄúHow the names of professions play leap-frog‚Äù</a></li>
<li><a href="../429966/index.html">Overview of Deep Domain Adaptation Basic Methods (Part 2)</a></li>
<li><a href="../429968/index.html">The largest courier company from China begins to use unmanned "maize" for the transport of goods</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>