<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The ghost of a locomotive or the stock market through the prism of correlations</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article will demonstrate the technique of processing information on stock quotes using the pandas (python) package, as well as studying some of t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The ghost of a locomotive or the stock market through the prism of correlations</h1><div class="post__text post__text-html js-mediator-article">  This article will demonstrate the technique of processing information on stock quotes using the <a href="http://pandas.pydata.org/">pandas</a> (python) package, as well as studying some of the ‚Äúmyths and legends‚Äù of stock trading through the use of mathematical statistics methods.  Along the way, let's briefly review the features of using the <a href="https://plot.ly/">plotly</a> library. <br><br>  One of the legends of traders is the concept of "locomotive".  It can be described as follows: there are ‚Äúlead‚Äù papers and there are ‚Äúfollower‚Äù papers.  If we believe in the existence of such a pattern, then we can ‚Äúpredict‚Äù the future movements of a financial instrument along the movement of ‚Äúlocomotives‚Äù (‚Äúleading‚Äù securities).  Is it so?  Is there a reason for this? <br><img src="https://habrastorage.org/getpro/habr/post_images/6f3/be9/3e6/6f3be93e64dbeb2736153a1e12c382a4.jpg" alt="image"><br><a name="habracut"></a><br>  We formulate the problem.  There are financial instruments: A, B, C, D;  there is a time characteristic - t.  Are there any links between the movements of these tools: <br><br>  A <sub>t</sub> and B <sub>t-1</sub> ;  A <sub>t</sub> and C <sub>t-1</sub> ;  A <sub>t</sub> and D <sub>t-1</sub> <br>  B <sub>t</sub> and C <sub>t-1</sub> ;  B <sub>t</sub> and D <sub>t-1</sub> ;  B <sub>t</sub> and A <sub>t-1</sub> <br>  C <sub>t</sub> and D <sub>t-1</sub> ;  C <sub>t</sub> and A <sub>t-1</sub> ;  C <sub>t</sub> and B <sub>t-1</sub> <br>  D <sub>t</sub> and A <sub>t-1</sub> ;  D <sub>t</sub> and B <sub>t-1</sub> ;  D <sub>t</sub> and B <sub>t-1</sub> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>How to get data for the study of this issue?</b>  <b>How strong, stable are the links mentioned?</b>  <b>How can they be measured?</b>  <b>What tools?</b> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dfd/f0e/326/dfdf0e326accfdf8a4f732ea7007ca16.jpg" alt="image" align="left">  We first note that today there are a significant number of predictive models.  Some sources say that their number has exceeded <a href="http://www.mbureau.ru/articles/dissertaciya-model-prognozirovaniya-vremennyh-ryadov-glava-1">one hundred</a> .  By the way - the main joke of reality is that ... the more complex the model, the harder the interpretation, the understanding of each individual component of this model itself.  I emphasize that the <u>purpose of this article is to answer the above questions</u> , and not to use one of the existing forecasting models. <br><br>  The <a href="http://pandas.pydata.org/">pandas</a> package is a powerful data analysis tool that has a rich arsenal of tools.  We use its capabilities to study the questions raised by us. <br><br>  Previously, we get quotes from the server of the company "FINAM".  We will take the "watch" for the period from <b>01/01/2017</b> to <b>13/07/2017</b> .  By slightly modifying the function mentioned <a href="https://habrahabr.ru/post/332700/">here</a> , we get: <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- """ @author: optimusqp """ import os import urllib import pandas as pd import time import codecs from datetime import datetime, date from pandas.io.common import EmptyDataError e='.csv'; p='7'; yf='2017'; yt='2017'; month_start='01'; day_start='01'; month_end='07'; day_end='13'; year_start=yf[2:]; year_end=yt[2:]; mf=(int(month_start.replace('0','')))-1; mt=(int(month_end.replace('0','')))-1; df=(int(day_start.replace('0',''))); dt=(int(day_end.replace('0',''))); dtf='1'; tmf='1'; MSOR='1'; mstimever='0' sep='1'; sep2='1'; datf='5'; at='1'; def quotes_finam_optimusqp(data,year_start,month_start,day_start,year_end,month_end,day_end,e,df,mf,yf,dt,mt,yt,p,dtf,tmf,MSOR,mstimever,sep,sep2,datf,at): temp_name_file='id,company\n'; incrim=1; for index, row in data.iterrows(): page = urllib.urlopen('http://export.finam.ru/'+str(row['code'])+'_'+str(year_start)+str(month_start)+str(day_start)+'_'+str(year_end)+str(month_end)+str(day_end)+str(e)+'?market='+str(row['id_exchange_2'])+'&amp;em='+str(row['em'])+'&amp;code='+str(row['code'])+'&amp;apply=0&amp;df='+str(df)+'&amp;mf='+str(mf)+'&amp;yf='+str(yf)+'&amp;from='+str(day_start)+'.'+str(month_start)+'.'+str(yf)+'&amp;dt='+str(dt)+'&amp;mt='+str(mt)+'&amp;yt='+str(yt)+'&amp;to='+str(day_end)+'.'+str(month_end)+'.'+str(yt)+'&amp;p='+str(p)+'&amp;f='+str(row['code'])+'_'+str(year_start)+str(month_start)+str(day_start)+'_'+str(year_end)+str(month_end)+str(day_end)+'&amp;e='+str(e)+'&amp;cn='+str(row['code'])+'&amp;dtf='+str(dtf)+'&amp;tmf='+str(tmf)+'&amp;MSOR='+str(MSOR)+'&amp;mstimever='+str(mstimever)+'&amp;sep='+str(sep)+'&amp;sep2='+str(sep2)+'&amp;datf='+str(datf)+'&amp;at='+str(at)) print('http://export.finam.ru/'+str(row['code'])+'_'+str(year_start)+str(month_start)+str(day_start)+'_'+str(year_end)+str(month_end)+str(day_end)+str(e)+'?market='+str(row['id_exchange_2'])+'&amp;em='+str(row['em'])+'&amp;code='+str(row['code'])+'&amp;apply=0&amp;df='+str(df)+'&amp;mf='+str(mf)+'&amp;yf='+str(yf)+'&amp;from='+str(day_start)+'.'+str(month_start)+'.'+str(yf)+'&amp;dt='+str(dt)+'&amp;mt='+str(mt)+'&amp;yt='+str(yt)+'&amp;to='+str(day_end)+'.'+str(month_end)+'.'+str(yt)+'&amp;p='+str(p)+'&amp;f='+str(row['code'])+'_'+str(year_start)+str(month_start)+str(day_start)+'_'+str(year_end)+str(month_end)+str(day_end)+'&amp;e='+str(e)+'&amp;cn='+str(row['code'])+'&amp;dtf='+str(dtf)+'&amp;tmf='+str(tmf)+'&amp;MSOR='+str(MSOR)+'&amp;mstimever='+str(mstimever)+'&amp;sep='+str(sep)+'&amp;sep2='+str(sep2)+'&amp;datf='+str(datf)+'&amp;at='+str(at)) print('code: '+str(row['code'])) #       . #  -    file = codecs.open(str(row['code'])+"_"+"0"+".csv", "w", "utf-8") content = page.read() file.write(content) file.close() temp_name_file = temp_name_file + (str(incrim) + "," + str(row['code'])+"\n") incrim+=1 time.sleep(2) #     code   , #    -  . write_file = "name_file_data.csv" with open(write_file, "w") as output: for line in temp_name_file: output.write(line) #  quotes_finam_optimusqp     #  function_parameters.csv #___http://optimusqp.ru/articles/articles_1/function_parameters.csv data_all = pd.read_csv('function_parameters.csv', index_col='id') #      ,   #  id_exchange_2 == 1, ..   data = data_all[data_all['id_exchange_2']==1] quotes_finam_optimusqp(data,year_start,month_start,day_start,year_end,month_end,day_end,e,df,mf,yf,dt,mt,yt,p,dtf,tmf,MSOR,mstimever,sep,sep2,datf,at)</span></span></code> </pre> <br>  As a result, we have a list of files of type A_0.csv: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7d1/783/3e4/7d17833e4e71b6895898d25a394e5149.jpg" alt="image"><br><br>  Next, we determine the movements of financial instruments A <sub>t</sub> -A <sub>t-1</sub> , remove the columns OPEN, HIGH, LOW, VOL, form a single column DATETIME.  We will eliminate those financial instruments that have too little data for analysis (they are traded recently, unstable or have low liquidity). <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   ,      ? #  -   . name_file_data = pd.read_csv('name_file_data.csv', index_col='id') incrim=1; #  how_work_days -    ,    # ,       #  temp_string_in_file='id,how_work_days\n'; for index, row1 in name_file_data.iterrows(): how_string_in_file = 0 #     ,      name_file=row1['company']+"_"+"0"+".csv" #    ?     if os.path.exists(name_file): folder_size = os.path.getsize(name_file) #      -   ,       if folder_size&gt;0: temp_quotes_data=pd.read_csv(name_file, delimiter=',') #   ,      EmptyDataError #   try: #     (CLOSE); #     quotes_data = temp_quotes_data.drop(['&lt;OPEN&gt;', '&lt;HIGH&gt;', '&lt;LOW&gt;', '&lt;VOL&gt;'], axis=1) # -       how_string_in_file = len(quotes_data.index) #        1 100, # ;     if how_string_in_file&gt;1100: #     days_data.csv,   #       #  temp_string_in_file = temp_string_in_file + (str(incrim) + "," + str(how_string_in_file)+"\n") incrim+=1 quotes_data['DATE_str']=quotes_data['&lt;DATE&gt;'].astype(basestring) quotes_data['TIME_str']=quotes_data['&lt;TIME&gt;'].astype(basestring) #""       DATETIME quotes_data['DATETIME'] = quotes_data.apply(lambda x:'%s%s' % (x['DATE_str'],x['TIME_str']),axis=1) quotes_data = quotes_data.drop(['&lt;DATE&gt;','&lt;TIME&gt;','DATE_str','TIME_str'], axis=1) quotes_data['DATETIME'].apply(lambda d: datetime.strptime(d, '%Y%m%d%H%M%S')) quotes_data [row1['company']] = quotes_data['&lt;CLOSE&gt;'] - quotes_data['&lt;CLOSE&gt;'].shift(1) quotes_data = quotes_data.drop(['&lt;CLOSE&gt;'], axis=1) quotes_data.to_csv(row1['company']+"_"+"1"+".csv", sep=',', encoding='utf-8') os.unlink(row1['company']+"_"+"0"+".csv") else: os.unlink(row1['company']+"_"+"0"+".csv") except pd.io.common.EmptyDataError: os.unlink(row1['company']+"_"+"0"+".csv") else: os.unlink(row1['company']+"_"+"0"+".csv") else: continue write_file = "days_data.csv" with open(write_file, "w") as output: for line in temp_string_in_file: output.write(line)</span></span></code> </pre><br>  As a result, we get a list of files of type A_1.csv.  Total 91 files: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c39/b8f/379/c39b8f3791bbb8946eaa6784e19d35ce.jpg" alt="image"><br><br>  We merge all movements of all financial instruments into one file <a href="">securities.csv by</a> deleting the first empty line. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> glob allFiles = glob.glob(<span class="hljs-string"><span class="hljs-string">"*_1.csv"</span></span>) frame = pd.DataFrame() list_ = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file_ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> allFiles: df = pd.read_csv(file_,index_col=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, header=<span class="hljs-number"><span class="hljs-number">0</span></span>) list_.append(df) dfff = reduce(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> df1,df2: pd.merge(df1,df2,on=<span class="hljs-string"><span class="hljs-string">'DATETIME'</span></span>), list_) quotes_data = dfff.drop([<span class="hljs-string"><span class="hljs-string">'Unnamed: 0_x'</span></span>, <span class="hljs-string"><span class="hljs-string">'Unnamed: 0_y'</span></span>, <span class="hljs-string"><span class="hljs-string">'Unnamed: 0'</span></span>], axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) quotes_data.to_csv(<span class="hljs-string"><span class="hljs-string">"securities.csv"</span></span>, sep=<span class="hljs-string"><span class="hljs-string">','</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) quotes_data = quotes_data.drop([<span class="hljs-string"><span class="hljs-string">'DATETIME'</span></span>], axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) number_columns=len(quotes_data.columns) columns_name_0 = quotes_data.columns columns_name_1 = quotes_data.columns</code> </pre><br>  At this stage, there is a rather interesting operation of merging records by the DATETIME column (pd.merge).  This merger procedure discards those dates on which at least one of the 91 securities did not trade.  That is, the union is based on the complete elimination of empty data.  As a result: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/12b/e97/1fe/12be971fe8cb6b34735af9f497ed3cf4.jpg" alt="image"><br><br>  In the file <a href="">securities.csv</a> , operating with data in a cycle, we shift all lines except the current one.  Thus, on the contrary A <sub>t</sub> are the values ‚Äã‚ÄãB <sub>t-1</sub> , C <sub>t-1</sub> , D <sub>t-1</sub> . <br><br><pre> <code class="python hljs">incrim=<span class="hljs-number"><span class="hljs-number">0</span></span> quotes_data_w=quotes_data.shift(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> column <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> columns_name_0: quotes_data_w[column]=quotes_data_w[column].shift(<span class="hljs-number"><span class="hljs-number">-1</span></span>) quotes_data_w.to_csv(<span class="hljs-string"><span class="hljs-string">"securities_"</span></span>+column+<span class="hljs-string"><span class="hljs-string">".csv"</span></span>, sep=<span class="hljs-string"><span class="hljs-string">','</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#     quotes_data_w[column]=quotes_data_w[column].shift(1) incrim+=1</span></span></code> </pre><br>  The data will look like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ec3/9a8/027/ec39a8027c60353618149cd7c8c5e622.jpg" alt="image"><br><br>  And yes, you need to delete the first row with empty data.  Now you can build correlations between columns.  They will then reveal the existence or absence of papers, ‚Äúlocomotives‚Äù ... or they will make it possible to make sure that ‚Äúlocomotives‚Äù are nothing more than a myth. <br><br>  The fact of the absence of a normal (Gaussian) distribution in the movement of financial instruments is mentioned relatively recently.  Nevertheless, the majority of financial models are based on his assumption.  Is the Gaussian distribution present in our data?  The question is not idle, since the existence of normality will allow the use of the Pearson correlation, and the absence will oblige the use of a nonparametric form of correlation.  With this question we turn to <a href="https://plot.ly/">plotly's</a> wonderful service. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e2c/cd7/060/e2ccd7060ff250b93e12740fdfa72b83.jpg" alt="image" align="left">  What is interesting this service?  Firstly, the ability to graphically interpret the data.  Secondly, a set of statistical test methods;  in particular, the possibility of conducting tests for the conformity of the sample to the normal (Gaussian) distribution.  We will use the following tests: the Shapiro-Wilk criterion (Shapiro-Wilk), the Kolmogorov-Smirnov criterion (Kolmogorov-Smirnov), see the rules of work <a href="https://plot.ly/python/normality-test/">here</a> . <br><br>  The service related to <a href="https://plot.ly/">plotly</a> is worthy of the highest praise.  Tutorial on setting up <a href="https://plot.ly/">plotly</a> on Linux can be viewed on plot.ly, and under Windows, for example, <a href="http://www.instructables.com/id/Plotly-with-Python/">here</a> .  But there are <a href="https://plot.ly/">plotly</a> and weirdness.  And the question here is nothing more than a description of the logic of the test.  In the examples for the application table is given: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/342/d1a/de6/342d1ade6b853f7d26c8ba62ef72be78.jpg" alt="image"><br><br>  The developer gives the following comment: <br><br>  <b><i>Since our p-value is less than our test status, the numerical hypothesis at the 0.05 significance level.</i></b> <br><br>  Transfer: <br><br>  <b><i>Since our p-value is much smaller than our test statistics, we have good evidence that we do not reject the null hypothesis at the 0.05 significance level.</i></b> <br><br>  Thus, according to this recommendation, we are <b>not entitled to abandon the hypothesis of normal distribution for the sample under consideration!</b>  <b>But ... this advice is not correct</b> . <br><br>  So, remember - what is p-value?  This value is necessary for testing statistical hypotheses.  It can be understood as the probability of error if we reject the null hypothesis.  Under the null hypothesis in the Shapiro-Wilk criterion H <sub>0</sub> , I recall, it means that "the random variable X is distributed normally."  If we reject H <sub>0</sub> for an extremely small p-value (close to zero), then we will not be mistaken.  We are not mistaken in eliminating the assumption of normal distribution.  In general, the level of significance in the <a href="https://plot.ly/">plotly</a> tests for normality is 0.05, <b>and acceptance or non-acceptance of the null hypothesis should be based on the comparison of this value to the p-value</b> .  Exceeding the threshold of the significance level by the p-value indicates that the hypothesis that the distribution of the test sample is normal cannot be rejected. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a2a/b6a/344/a2ab6a34479c9fde7678d8faf33a287f.jpg" alt="image"><br><br>  And ... suddenly, and ... the tests themselves for the normal distribution on the <a href="https://plot.ly/">plotly are</a> not correct?  Looking ahead to say - everything is in order.  I have generated two types of random samples - Gaussian and Pareto;  These arrays are sequentially sent to plot.ly.  We are testing.  The nature of the distributions is very different and it is obvious that Pareto sampling does not have to pass the ‚Äúnormality‚Äù test. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0f6/fd5/752/0f6fd575246e2d98c65ef2744e5ded3c.jpg" alt="image"><br><br>  Test code: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> plotly.plotly <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> py <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> plotly.graph_objs <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> go <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> plotly.tools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FigureFactory <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> FF <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> stats, optimize, interpolate <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Normality_Test</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(L)</span></span></span><span class="hljs-function">:</span></span> x = L shapiro_results = scipy.stats.shapiro(x) matrix_sw = [ [<span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">'DF'</span></span>, <span class="hljs-string"><span class="hljs-string">'Test Statistic'</span></span>, <span class="hljs-string"><span class="hljs-string">'p-value'</span></span>], [<span class="hljs-string"><span class="hljs-string">'Sample Data'</span></span>, len(x) - <span class="hljs-number"><span class="hljs-number">1</span></span>, shapiro_results[<span class="hljs-number"><span class="hljs-number">0</span></span>], shapiro_results[<span class="hljs-number"><span class="hljs-number">1</span></span>]] ] shapiro_table = FF.create_table(matrix_sw, index=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) py.iplot(shapiro_table, filename=<span class="hljs-string"><span class="hljs-string">'pareto_file'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#py.iplot(shapiro_table, filename='normal_file') #L =np.random.normal(115.0, 10, 860) L =np.random.pareto(3,50) Normality_Test(L)</span></span></code> </pre><br>  The processing results can be viewed in your profile at <a href="https://plot.ly/organize/home/">plot.ly/organize/home</a> <br>  So, here are some results of the Shapiro-Wilk tests: <br><br>  <b>For Pareto distribution</b> <br><br>  First test <br><br><img src="https://habrastorage.org/getpro/habr/post_images/79e/fea/488/79efea48825ba09a71b6809c8d11165e.jpg" alt="image"><br><br>  Second test <br><br><img src="https://habrastorage.org/getpro/habr/post_images/035/5f2/405/0355f2405ce106905b3a0fb4092cd6ec.jpg" alt="image"><br><br>  <b>For normal (Gaussian) distribution</b> <br><br>  First test <br><br><img src="https://habrastorage.org/getpro/habr/post_images/312/228/b82/312228b82836594c43bc2575393f3d99.jpg" alt="image"><br><br>  Second test <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8f0/809/aa9/8f0809aa99d4dfbe5dcac16d1eb6fcbc.jpg" alt="image"><br><br>  So, the test algorithm works correctly.  However, the tips for using the test are not quite correct, to say the least.  The moral of the following: beware!  <b>Near the correctly written tool does not always lie correctly written instructions!</b> <br>  Let us proceed to testing the movement of financial instruments for normality (Gauss) distribution using the <a href="https://plot.ly/">plotly</a> library.  I obtained the following results: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/042/924/307/0429243076e73751974fad5fc07d4dfd.jpg" alt="image"><br><br>  The rest of the financial instruments have a similar picture.  Therefore, we exclude the assumption that the distribution of the considered financial instruments is normal in motion.  The test code itself: <br><br><pre> <code class="python hljs">allFiles = glob.glob(<span class="hljs-string"><span class="hljs-string">"*_1.csv"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Shapiro</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df,temp_header)</span></span></span><span class="hljs-function">:</span></span> df=df.drop(df.index[<span class="hljs-number"><span class="hljs-number">0</span></span>]) x = df[temp_header].tolist() shapiro_results = scipy.stats.shapiro(x) matrix_sw = [ [<span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">'DF'</span></span>, <span class="hljs-string"><span class="hljs-string">'Test Statistic'</span></span>, <span class="hljs-string"><span class="hljs-string">'p-value'</span></span>], [<span class="hljs-string"><span class="hljs-string">'Sample Data'</span></span>, len(x) - <span class="hljs-number"><span class="hljs-number">1</span></span>, shapiro_results[<span class="hljs-number"><span class="hljs-number">0</span></span>], shapiro_results[<span class="hljs-number"><span class="hljs-number">1</span></span>]] ] shapiro_table = FF.create_table(matrix_sw, index=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) py.iplot(shapiro_table, filename=<span class="hljs-string"><span class="hljs-string">'shapiro-table_'</span></span>+temp_header) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Kolmogorov_Smirnov</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df,temp_header)</span></span></span><span class="hljs-function">:</span></span> df=df.drop(df.index[<span class="hljs-number"><span class="hljs-number">0</span></span>]) x = df[temp_header].tolist() ks_results = scipy.stats.kstest(x, cdf=<span class="hljs-string"><span class="hljs-string">'norm'</span></span>) matrix_ks = [ [<span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">'DF'</span></span>, <span class="hljs-string"><span class="hljs-string">'Test Statistic'</span></span>, <span class="hljs-string"><span class="hljs-string">'p-value'</span></span>], [<span class="hljs-string"><span class="hljs-string">'Sample Data'</span></span>, len(x) - <span class="hljs-number"><span class="hljs-number">1</span></span>, ks_results[<span class="hljs-number"><span class="hljs-number">0</span></span>], ks_results[<span class="hljs-number"><span class="hljs-number">1</span></span>]] ] ks_table = FF.create_table(matrix_ks, index=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) py.iplot(ks_table, filename=<span class="hljs-string"><span class="hljs-string">'ks-table_'</span></span>+temp_header) frame = pd.DataFrame() list_ = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file_ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> allFiles: df = pd.read_csv(file_,index_col=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, header=<span class="hljs-number"><span class="hljs-number">0</span></span>) print(file_) columns = df.columns temp_header = columns[<span class="hljs-number"><span class="hljs-number">2</span></span>] Shapiro(df,temp_header) time.sleep(<span class="hljs-number"><span class="hljs-number">3</span></span>) Kolmogorov_Smirnov(df,temp_header) time.sleep(<span class="hljs-number"><span class="hljs-number">3</span></span>)</code> </pre><br>  <b>Since we cannot rely on the normal (Gaussian) distribution - therefore, when calculating the correlations, it is necessary to choose a nonparametric tool, namely the Spearman correlation coefficient (Spearman rank correlation coefficient)</b> .  Once you have decided on the type of correlation, you can go directly to its calculations: <br><br><pre> <code class="python hljs">incrim=<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> column0 <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> columns_name_1: df000 = pd.read_csv(<span class="hljs-string"><span class="hljs-string">'securities_'</span></span>+column0+<span class="hljs-string"><span class="hljs-string">".csv"</span></span>,index_col=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, header=<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment">#     df000=df000.drop(df000.index[0]) df000 = df000.drop(['Unnamed: 0'], axis=1) #      #         corr_spr=df000.corr('spearman') #       #    corr_spr=corr_spr.sort_values([column0], ascending=False) #   DataFrame corr_spr_temp=corr_spr[column0] corr_spr_temp.to_csv("corr_"+column0+".csv", sep=',', encoding='utf-8') incrim+=1</span></span></code> </pre><br>  We get a file with correlations for the current paper (of the type corr_A.csv) and the previous period for other securities (B, C, D, there are only 90), for this we delete the first line with empty values ‚Äã‚Äãin the file of the type securities_A.csv;  We calculate the correlations of other securities in relation to the current one.  Sort a column of correlations and naming to them.  Save the correlation column for the current security as a separate DataFrame. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/02d/2a4/c03/02d2a4c037ac03ccc84e65e36e927e80.jpg" alt="image"><br><br>  Alternately, each of the files with correlations of the type corr_A.csv is ‚Äúmerged‚Äù into one common file - <a href="">_quotes_data_end.csv</a> .csv.  The lines in this file are impersonal.  One can observe only the values ‚Äã‚Äãof sorted correlations. <br><br><pre> <code class="python hljs">incrim=<span class="hljs-number"><span class="hljs-number">0</span></span> all_corr_Files = glob.glob(<span class="hljs-string"><span class="hljs-string">"corr_*.csv"</span></span>) list_corr = [] quotes_data_end = pd.DataFrame() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file_corr <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> all_corr_Files: df_corr = pd.read_csv(file_corr,index_col=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, header=<span class="hljs-number"><span class="hljs-number">0</span></span>) columns_corr = df_corr.columns temp_header = columns_corr[<span class="hljs-number"><span class="hljs-number">0</span></span>] quotes_data_end[str(temp_header)]=df_corr.iloc[:,<span class="hljs-number"><span class="hljs-number">1</span></span>] incrim+=<span class="hljs-number"><span class="hljs-number">1</span></span> quotes_data_end.to_csv(<span class="hljs-string"><span class="hljs-string">"_quotes_data_end.csv"</span></span>, sep=<span class="hljs-string"><span class="hljs-string">','</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) plt.figure(); quotes_data_end.plot();</code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/a96/77c/e48/a9677ce48b19450bae4d1a87ff5ca089.jpg" alt="image"><br><br>  According to the data obtained <a href="">_quotes_data_end.csv</a> we build a graph: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1bb/59c/10a/1bb59c10a14ad3a7881432259e6e6f02.jpg" alt="image"><br><br>  The level of correlation even in the extreme regions is not high.  The bulk of the correlation values ‚Äã‚Äãis in the range of -0.15; 0.15.  There are no such securities as such that ‚Äúconducted‚Äù any other financial instruments within the period under review (7.5 months) and on this timeframe (‚Äúwatch‚Äù).  Let me remind you that we have at our disposal data on 91 securities.  But ... if you try to process the same "watch" for a shorter period?  For a sample of 1 month duration, we obtain the following graph: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dce/1e5/2d4/dce1e52d4c8794d85657144b3b57382e.jpg" alt="image"><br><br>  Reducing the timeframe and reducing the size of the samples under consideration yields higher correlations.  The myth of ‚Äúlocomotive‚Äù movements (when one paper ‚Äúpulls‚Äù behind itself another, or acts as a ‚Äúcounterweight‚Äù) ... <b>turns into reality.</b>  <b>This effect is observed as the sample size decreases</b> .  However, as the flip side of the coin - an increase in the values ‚Äã‚Äãof correlations at the same time, is <b>accompanied by their increasingly unstable behavior</b> .  Paper from the "locomotive" can turn into a "slave" in a relatively short period of time.  We can state that the methods of data processing were covered by us, the answers to the above questions were received. <br><br>  What is the nature of the dynamics of changes in correlations;  How does this happen and how is it accompanied?  But ... this is a topic to continue. <br><br>  Thanks for attention! </div><p>Source: <a href="https://habr.com/ru/post/334288/">https://habr.com/ru/post/334288/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../334278/index.html">A bit about SSL certificates: How to choose and how to get</a></li>
<li><a href="../334280/index.html">Cryptocurrency in terms of civil law</a></li>
<li><a href="../334282/index.html">What are chemists and biologists doing at EPAM?</a></li>
<li><a href="../334284/index.html">Metaclasses in C ++</a></li>
<li><a href="../334286/index.html">Internet travel. Analysis of foreign SIM-cards</a></li>
<li><a href="../334290/index.html">SOAP and REST services using the Spyne Python library</a></li>
<li><a href="../334292/index.html">Anatomy of Google Analytics</a></li>
<li><a href="../334294/index.html">About Agile, Scrum and teamwork. How are the processes of product development in the Alpha Laboratory</a></li>
<li><a href="../334296/index.html">Monitoring of actors in Akka.Net, but on F #</a></li>
<li><a href="../334298/index.html">Here you are not DevOps: the fate of a sysadmin in a small business</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>