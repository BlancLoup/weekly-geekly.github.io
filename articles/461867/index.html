<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to recognize pictures and texts on your phone using ML Kit</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Two years ago, Sundar Pichai, the head of Google, said that the company from mobile-first becomes AI-first and focuses on machine learning. A year lat...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to recognize pictures and texts on your phone using ML Kit</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/6u/ek/co/6uekco-kgxxahafq0864aq6rmsw.png"></p><br><p>  Two years ago, Sundar Pichai, the head of Google, said that the company from mobile-first becomes AI-first and focuses on machine learning.  A year later, the Machine Learning Kit was released - a set of tools with which you can effectively use ML on iOS and Android. </p><br><p>  They say a lot about the ML Kit in the USA, but there is almost no information in Russian.  And since we use it for some tasks in Yandex.Money, I decided to share my experience and show with examples how to use it to do interesting things. </p><br><p>  My name is Yura. Last year I have been working in the Yandex.Money team on a mobile wallet.  We‚Äôll talk about machine learning in mobile. </p><a name="habracut"></a><br><hr><br><p>  Note  Editorial staff: this post is a retelling of the report by Yuri Chechetkin ‚ÄúFrom mobile first to AI first‚Äù from the Yandex.Money metap <a href="https://habr.com/ru/company/yamoney/blog/457690/">Android Paranoid</a> . </p><br><h2 id="chto-takoe-ml-kit">  What is the ML Kit? </h2><br><p>  This is Google‚Äôs mobile SDK that makes it easy to use machine learning on Android and iOS devices.  It is not necessary to be an expert in ML or in artificial intelligence, because in a few lines of code you can implement very complex things.  Moreover, it is not necessary to know how neural networks or model optimization work. </p><br><h2 id="chto-zhe-mozhet-ml-kit">  What can the ML Kit do? </h2><br><p>  The basic features are quite wide.  For example, you can recognize text, faces, find and track objects, create labels for images and your own classification models, scan barcodes and QR tags. </p><br><p>  We already used QR code recognition in the Yandex.Money application. </p><br><p>  There is also a ML Kit </p><br><ol><li>  Landmark recognition; </li><li>  Definition of the language in which the text is written; </li><li>  Translation of texts on the device; </li><li>  Quick reply to a letter or message. </li></ol><br><p>  In addition to a huge number of methods out of the box, there is support for custom models, which practically gives unlimited possibilities - for example, you can colorize black and white photographs and make them color. </p><br><p>  It is important that you do not need to use any services, API or backend for this.  Everything can be done directly on the device, so we don‚Äôt load user traffic, don‚Äôt get a bunch of network errors, we don‚Äôt have to process a bunch of cases, for example, lack of Internet, loss of connection, and so on.  Moreover, on the device it works much faster than through a network. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/024/fb0/78d/024fb078d1cd1b8f8b81f8be44122aad.png" alt="one"></p><br><h2 id="raspoznavanie-teksta">  Text recognising </h2><br><p>  <strong>Task: given a photograph, you need to get the text circled in a rectangle.</strong> </p><br><p>  We start with the dependency in Gradle.  It is enough to connect one dependency, and we are ready to work. </p><br><pre><code class="kotlin hljs">dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation'com.google.firebase:firebase-ml-vision:20.0.0' }</span></span></code> </pre> <br><p>  It is worth specifying metadata that says that the model will be downloaded to the device while downloading the application from the Play Market.  If you do not do this and access the API without a model, we will get an error, and the model will have to be downloaded in the background.  If you need to use several models, it is advisable to specify them separated by commas.  In our example, we use the OCR model, and the name of the rest can be found in the <a href="https://firebase.google.com/docs/ml-kit">documentation</a> . </p><br><pre> <code class="kotlin hljs">&lt;application ...&gt; ... &lt;meta-<span class="hljs-keyword"><span class="hljs-keyword">data</span></span> android:name=<span class="hljs-string"><span class="hljs-string">"com.google.firebase.ml.vision.DEPENDENCIES"</span></span> android:value=<span class="hljs-string"><span class="hljs-string">"ocr"</span></span> /&gt; &lt;!-- To use multiple models: android:value=<span class="hljs-string"><span class="hljs-string">"ocr,model2,model3"</span></span> --&gt; &lt;/application&gt;</code> </pre> <br><p>  After the project configuration, you need to set the input values.  ML Kit works with the FirebaseVisionImage type, we have five methods, the signature of which I wrote out below.  They convert the usual types of Android and Java into the types of ML Kit, with which it is convenient to work with. </p><br><pre> <code class="kotlin hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromMediaImage</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(image: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Image</span></span></span></span><span class="hljs-function"><span class="hljs-params">, rotation: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Int</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromBitmap</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(bitmap: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Bitmap</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromFilePath</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(context: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Context</span></span></span></span><span class="hljs-function"><span class="hljs-params">, uri: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">Uri</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteBuffer</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( byteBuffer: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteBuffer</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fromByteArray</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( bytes: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">ByteArray</span></span></span></span><span class="hljs-function"><span class="hljs-params">, metadata: </span></span><span class="hljs-type"><span class="hljs-function"><span class="hljs-params"><span class="hljs-type">FirebaseVisionImageMetadata</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span></span>: FirebaseVisionImage</code> </pre> <br><p>  Pay attention to the last two - they work with an array of bytes and with a byte buffer, and we need to specify metadata so that ML Kit understands how to handle it all.  Metadata, in fact, describes the format, in this case, the width and height, the default format, IMAGE_FORMAT_NV21 and rotation. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> metadata = FirebaseVisionImageMetadata.Builder() .setWidth(<span class="hljs-number"><span class="hljs-number">480</span></span>) .setHeight(<span class="hljs-number"><span class="hljs-number">360</span></span>) .setFormat(FirebaseVisionImageMetadata.IMAGE_FORMAT_NV21) .setRotation(rotation) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> image = FirebaseVisionImage.fromByteBuffer(buffer, metadata)</code> </pre> <br><p>  When the input data is collected, create a detector that will recognize the text. </p><br><p>  There are two types of detectors, on the device and in the cloud, they are created literally in one line.  It is worth noting that the detector on the device only works with English.  The cloud detector supports more than 20 languages; they must be specified in the special setLanguageHints method. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">//  onDevice val detector = FirebaseVision.getInstance().getOnDeviceTextRecognizer() // onCloud with options val options = FirebaseVisionCloudTextRecognizerOptions.Builder() .setLanguageHints(arrayOf("en", "ru")) .build() val detector = FirebaseVision.getInstance().getCloudTextRecognizer(options)</span></span></code> </pre> <br><p>  The number of supported languages ‚Äã‚Äãis more than 20, they are all on the official website.  In our example, only English and Russian. </p><br><p>  After you have input and a detector, just call the processImage method on this detector.  We get the result in the form of a task, on which we hang two callbacks - for success and for error.  The standard exception comes to an error, and the type of FirebaseVisionText comes to success from onSuccessListener. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> result: Task&lt;FirebaseVisionText&gt; = detector.processImage(image) .addOnSuccessListener { result: FirebaseVisionText -&gt; <span class="hljs-comment"><span class="hljs-comment">// Task completed successfully // ... } .addOnFailureListener { exception: Exception -&gt; // Task failed with an exception // ... }</span></span></code> </pre> <br><h2 id="kak-rabotat-s-tipom-firebasevisiontext">  How to work with type FirebaseVisionText? </h2><br><p>  It consists of text blocks (TextBlock), those in turn consist of lines (Line), and lines of elements (Element).  They are nested in each other. </p><br><p>  Moreover, each of these classes has five methods that return different data about the object.  A rectangle is the area where the text is located, confidence is the accuracy of the recognized text, corner points are the corner points clockwise, starting from the upper left corner, the recognized languages ‚Äã‚Äãand the text itself. </p><br><pre> <code class="kotlin hljs">FirebaseVisionText contains a list of FirebaseVisionText.TextBlock which contains a list of FirebaseVisionText.Line which <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> composed of a list of FirebaseVisionText.Element. <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">fun</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getBoundingBox</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span>: Rect <span class="hljs-comment"><span class="hljs-comment">// axis-aligned bounding rectangle of the detected text fun getConfidence(): Float // confidence of the recognized text fun getCornerPoints(): Array&lt;Point&gt; // four corner points in clockwise direction fun getRecognizedLanguages(): List&lt;RecognizedLanguage&gt; // a list of recognized languages fun getText(): String //recognized text as a string</span></span></code> </pre> <br><h2 id="dlya-chego-eto-nuzhno">  What is it for? </h2><br><p>  We can recognize both the entire text in the picture and its individual paragraphs, pieces, lines or just words.  And as an example, we can iterate over, at each stage take a text, take the borders of this text, and draw.  Very comfortably. </p><br><p>  We plan to use this tool in our application for recognizing bank cards, the inscriptions on which are located non-standard.  Not all card recognition libraries work well, and for custom cards the ML Kit would be very useful.  Since there is not much text, it is very easy to process in this way. </p><br><h2 id="raspoznavanie-obektov-na-foto">  Recognition of objects in the photo </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/766/526/977/766526977ec9b1a83419e2aa6bdac37f.png" alt="2"></p><br><p>  Using the following tool as an example, I would like to show that the principle of operation is approximately the same.  In this case, recognition of what is depicted on the object.  We also create two detectors, one on the device, the other on the cloud, we can specify the minimum accuracy as parameters.  The default is 0.5, indicated 0.7, and ready to go.  We also get the result in the form of FirebaseImageLabel, this is a list of labels, each of which contains ID, description and accuracy. </p><br><pre> <code class="kotlin hljs"><span class="hljs-comment"><span class="hljs-comment">// onDevice val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getOnDeviceImageLabeler() // onCloud with minimum confidence val options = FirebaseVisionCloudImageLabelerOptions.Builder() .setConfidenceThreshold(0.7f) .build() val detector: FirebaseVisionImageLabeler = FirebaseVision .getInstance() .getCloudImageLabeler(options)</span></span></code> </pre> <br><h2 id="garold-skryvayuschiy-schaste">  Harold hiding happiness </h2><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ec4/223/a1c/ec4223a1c8dcab56e489c6b0a4a4ca5b.jpg" alt="3"></p><br><p>  You can try to understand how well Harold hides the pain and whether he is happy at the same time.  We use a face recognition tool, which, in addition to recognizing facial features, can tell how happy a person is.  As it turned out, Harold is 93% happy.  Or he hides the pain very well. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/237/352/dee/237352dee824e16c69ba131fe814fc8b.png" alt="four"></p><br><h2 id="ot-legkogo-k-legkomu-no-chut-bolee-slozhnomu-kastomnye-modeli">  From easy to easy, but a little more complicated.  Custom models. </h2><br><p>  <strong>Task: classification of what is shown in the photo.</strong> </p><br><p>  I took a picture of the laptop and recognized the modem, desktop computer and keyboard.  Sounds like the truth.  There are a thousand classifiers, and he takes three of them that best describe this photo. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/5fb/0d1/cf1/5fb0d1cf1b1ae43bdb0bd2151937229a.png" alt="7"></p><br><p>  When working with custom models, we can also work with them both on the device and through the cloud. </p><br><p>  If we work through the cloud, you need to go to the Firebase Console, to the ML Kit tab, and to tap custom, where we can load our model into TensorFlow Lite, because ML Kit works with models with this resolution.  If we use it on a device, we can simply put the model in any part of the project as an asset. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/cd6/70e/aae/cd670eaae79abc02c358dc6583c9061e.png" alt="6"></p><br><p>  We point out the dependence on the interpreter, which can work with custom models, and do not forget about the permission to work with the Internet. </p><br><pre> <code class="kotlin hljs">&lt;uses-permission android:name=<span class="hljs-string"><span class="hljs-string">"android.permission.INTERNET"</span></span> /&gt; dependencies { <span class="hljs-comment"><span class="hljs-comment">// ... implementation 'com.google.firebase:firebase-ml-model-interpreter:19.0.0' }</span></span></code> </pre> <br><p>  For those models that are on the device, you must indicate in Gradle that the model should not be compressed, because it can be distorted. </p><br><pre> <code class="kotlin hljs">android { <span class="hljs-comment"><span class="hljs-comment">// ... aaptOptions { noCompress "tflite" // Your model's file extension: "tflite" } }</span></span></code> </pre> <br><p>  When we have configured everything in our environment, we must set special conditions, which include, for example, the use of Wi-Fi, also with Android N require charging and require device idle are available - these conditions indicate that the phone is charging or is in standby mode. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> conditionsBuilder: FirebaseModelDownloadConditions.Builder = FirebaseModelDownloadConditions.Builder().requireWifi() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.N) { <span class="hljs-comment"><span class="hljs-comment">// Enable advanced conditions on Android Nougat and newer. conditionsBuilder = conditionsBuilder .requireCharging() .requireDeviceIdle() } val conditions: FirebaseModelDownloadConditions = conditionsBuilder.build()</span></span></code> </pre> <br><p>  When we create a remote model, we set the initialization and update conditions, as well as the flag whether our model should be updated.  The model name should match the one we specified in the Firebase console.  When we created the remote model, we must register it in the Firebase Model Manager. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> cloudSource: FirebaseRemoteModel = FirebaseRemoteModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .enableModelUpdates(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setInitialDownloadConditions(conditions) .setUpdatesDownloadConditions(conditions) .build() FirebaseModelManager.getInstance().registerRemoteModel(cloudSource)</code> </pre> <br><p>  We do the same steps for the local model, specify its name, the path to the model, and register it in the Firebase Model Manager. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> localSource: FirebaseLocalModel = FirebaseLocalModel.Builder(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .setAssetFilePath(<span class="hljs-string"><span class="hljs-string">"my_model.tflite"</span></span>) .build() FirebaseModelManager.getInstance().registerLocalModel(localSource)</code> </pre> <br><p>  After that, you need to create such options where we specify the names of our models, install the remote model, install the local model and create an interpreter with these options.  We can specify either a remote model, or only a local one, and the interpreter will himself understand which one to work with. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> options: FirebaseModelOptions = FirebaseModelOptions.Builder() .setRemoteModelName(<span class="hljs-string"><span class="hljs-string">"my_cloud_model"</span></span>) .setLocalModelName(<span class="hljs-string"><span class="hljs-string">"my_local_model"</span></span>) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> interpreter = FirebaseModelInterpreter.getInstance(options)</code> </pre> <br><p>  ML Kit does not know anything about the format of input and output data of custom models, so you need to specify them. </p><br><p>  Input data is a multidimensional array, where 1 is the number of images, 224x224 is the resolution, and 3 is a three-channel RGB image.  Well, the data type is bytes. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> input = intArrayOf(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-comment"><span class="hljs-comment">//one 224x224 three-channel (RGB) image val output = intArrayOf(1, 1000) val inputOutputOptions = FirebaseModelInputOutputOptions.Builder() .setInputFormat(0, FirebaseModelDataType.BYTE, input) .setOutputFormat(0, FirebaseModelDataType.BYTE, output) .build()</span></span></code> </pre> <br><p>  The output values ‚Äã‚Äãare 1000 classifiers.  We set the format of the input and output values ‚Äã‚Äãin bytes with the specified multidimensional arrays.  In addition to bytes, float, long, int are also available. </p><br><p>  Now we set the input values.  We take Bitmap, compress it to 224 by 224, convert it to ByteBuffer and create input values ‚Äã‚Äãusing FirebaseModelInput using a special builder. </p><br><pre> <code class="kotlin hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> bitmap = Bitmap.createScaledBitmap(yourInputImage, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-literal"><span class="hljs-literal">true</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> imgData = convertBitmapToByteBuffer(bitmap) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> inputs: FirebaseModelInputs = FirebaseModelInputs.Builder() .add(imageData) .build()</code> </pre> <br><p>  And now, when there is an interpreter, the format of the input and output values ‚Äã‚Äãand the input values ‚Äã‚Äãthemselves, we can execute the request using the run method.  We transfer all of the above as parameters, and as a result we get FirebaseModelOutput, which inside contains a generic of the type we specified.  In this case, it was a Byte array, after which we can start processing.  This is exactly the thousand classifiers that we requested, and we display, for example, the top 3 most suitable. </p><br><pre> <code class="kotlin hljs">interpreter.run(inputs, inputOutputOptions) .addOnSuccessListener { result: FirebaseModelOutputs -&gt; <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> labelProbArray = result.getOutput&lt;Array&lt;ByteArray&gt;&gt;(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-comment"><span class="hljs-comment">//handle labelProbArray } .addOnFailureListener( object : OnFailureListener { override fun onFailure(e: Exception) { // Task failed with an exception } })</span></span></code> </pre> <br><h2 id="realizaciya-za-odin-den">  One day implementation </h2><br><p>  Everything is quite easy to implement, and recognition of objects with built-in tools can be realized in just one day.  The tool is available on iOS and Android, in addition, you can use the same TensorFlow model for both platforms. </p><br><p>  In addition, there are tons of methods available out of the box that can cover many cases.  Most APIs are available on the device, that is, recognition will work even without the Internet. </p><br><p>  And most importantly - support for custom models that can be used as you like for any task. </p><br><h2 id="poleznye-ssylki">  useful links </h2><br><p>  <a href="https://firebase.google.com/docs/ml-kit">ML Kit Documentation</a> <br>  <a href="https://github.com/firebase/quickstart-android/tree/master/mlkit">Github ML Kit Demo Project</a> <br>  <a href="https://www.youtube.com/watch%3Fv%3DQwHD36bhXZA">Machine Learning for mobile with Firebase (Google I / O'19)</a> <br>  <a href="https://www.youtube.com/watch%3Fv%3DZ-dqGRSsaBs">Machine Learning SDK for mobile developers (Google I / O'18)</a> <br>  <a href="https://medium.com/coding-blocks/creating-a-credit-card-scanner-using-firebase-mlkit-5345140f6a5c">Creating a credit card scanner using Firebase ML Kit (Medium.com)</a> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/461867/">https://habr.com/ru/post/461867/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../461855/index.html">Salaries in IT in the first half of 2019: according to the salary calculator My Circle</a></li>
<li><a href="../461859/index.html">You don't know anything about food tech</a></li>
<li><a href="../461861/index.html">Office 365 Cloud Services: Check Point CloudGuard SaaS Testing</a></li>
<li><a href="../461863/index.html">How Chinese Mpow Headphones Bestseller on Amazon: Getting to Know the Brand</a></li>
<li><a href="../461865/index.html">Video course ‚ÄúIntroduction to reversing from scratch using IDA PRO. Chapter 1"</a></li>
<li><a href="../46187/index.html">"Classmates" have become a partner of "Runner"</a></li>
<li><a href="../461871/index.html">101 tips for becoming a good programmer (and human)</a></li>
<li><a href="../461873/index.html">ViewPager 2 - new functionality in the old wrapper</a></li>
<li><a href="../461875/index.html">5 nm vs 3 nm</a></li>
<li><a href="../461877/index.html">Java vs Kotlin for Android: developer opinions</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>