<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How is the rendering frame in GTA V</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The Grand Theft Auto series has come a long way since its first release in 1997. About 2 years ago, Rockstar released GTA V. It's an incredible succes...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How is the rendering frame in GTA V</h1><div class="post__text post__text-html js-mediator-article">  The Grand Theft Auto series has come a long way since its first release in 1997.  About 2 years ago, Rockstar released GTA V. It's an incredible success: in 24 hours, the game was bought by 11 million users, 7 world records were broken in a row.  After testing the new PS3, I was very impressed with both the overall picture and, in fact, the technical characteristics of the game. <br><br>  Nothing spoils the impression of the process like a loading screen, but in GTA V you can play for hours, overcoming endless hundreds of kilometers without interruption.  Considering the transfer of a solid stream of information and PS3 properties (256 Mb of RAM and a 256 Mb video card), I was completely surprised at how I was not thrown out of the game at the 20th minute.  This is where the wonders of technology. <br><br>  In this article I will talk about the <b>analysis of the frame</b> in the version for the PC in the environment of DirectX 11, which eats a couple of gigs of both the RAM and the graphics processor.  Despite the fact that my review comes with reference to the PC, I am sure that most of the points apply to the PS4 and to a certain extent to the PS3. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Frame analysis </h3><br>  So, consider the following frame: Michael on the background of his beloved Rapid GT, in the background is the beautiful Los Santos. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d5f/c39/aea/d5fc39aea5d806177cf246282af0c62e.jpg" alt="image"><br><br>  <b>Caution!</b>  <b>Traffic!</b> <br><a name="habracut"></a><br>  GTA V uses a deferred rendering system that works with multiple HDR buffers.  On the monitor, such buffers are not displayed correctly, and therefore I used the Reinhard method to bring everything back to the 8-bit per channel format. <br><br><h3>  Environment Cubemap </h3><br>  First of all, visualize the cubic texture of the environment.  This texture is generated in real time for each individual frame, which subsequently allows you to create realistic reflections.  It is brought to the fore. <br><br>  How is a cubic texture created?  For those who are not familiar with the technique, I will explain.  This is how to take panoramic pictures: putting the camera on a tripod, imagine that you are standing right in the center of a large cube and photograph all 6 of its faces one by one, turning 90 ¬∞ each time. <br><br>  The game uses the same principle: each face becomes a 128x128 HDR texture.  The first face is as follows: <br><br><img src="https://habrastorage.org/files/1b9/1b7/9ab/1b91b79ab23c43d28221bf71dbeed0e1.PNG"><br><br>  The same scheme is repeated for the 5 remaining faces, and as a result we get a cubic texture (cubemap): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/784/65c/82a/78465c82a6391fcf3acb4c2cdc187ae5.png" alt="image"><br>  <i>So the cube looks outside</i> <br><br><img src="https://habrastorage.org/files/67f/4ba/fc5/67f4bafc5ff44ac3b20ada68617190b6.png"><br>  <i>So it looks from the inside (unfortunately, it was not possible to insert the panoramic image presented in the original source in the form of a js-script on Habr)</i> <br><br>  For each facet, there are more than 30 calls to the drawing procedure, the cells of the graphic grid are very low polygonal, and therefore only the ‚Äúlandscape‚Äù (terrain, sky, some buildings) is drawn, the characters and cars remain intact. <br><br>  That is why in the game, if you look out of the car, the panorama looks great, but other cars and characters do not. <br><br><h3>  From cubic to double paraboloid texture </h3><br>  The resulting cubic environment texture is subsequently converted into a double paraboloid. <br><br>  Simply put, a cube is simply projected in a different space.  The projection is similar to spherical modeling, however, here we have 2 "hemispheres". <br><br><img src="https://habrastorage.org/files/087/24a/7b2/08724a7b288b4cd19dd80e2226613d46.png"><br>  <i>Cube in "turn" and the resulting "hemispheres"</i> <br><br>  What are such transformations for?  I think the whole thing (as always) is in optimization: in a cubic texture, fragment shaders, theoretically, have access to 6 faces of 128x128 in size, while in the case of the double paraboloid texture everything comes down to 2 ‚Äúhemispheres‚Äù 128x128.  In addition, since the camera is located on the roof of the car most of the time, most requests go to the upper hemisphere. <br><br>  The projection of the double paraboloid preserves the details of the reflection of the upper and lower parts of the object due to some errors of the sides.  For GTA, this is fine: the roofs and hoods of cars, as a rule, are shown from above, and therefore it is important that this reflection be of high quality. <br><br>  Plus, the edges of cubic maps often leave much to be desired: if the accuracy of the display of textures depends on the distance to the object, then individual seams can be found within the edges, especially along the contours, and no older face filters are provided in older graphic processors.  For double paraboloids, this problem is irrelevant, since  textures remain clear, regardless of distance, and no seams interfere with the perception of the picture as a whole. <br><br>  Addition: I noted in the comments that, in all likelihood, GTA IV also used a double paraboloid map, although not in the process of post-processing a cubic texture.  Graphic grid cells are directly transformed using a vertex shader. <br><br><h3>  Rejection and level of detail </h3><br>  Since the shader is responsible for this stage, I will not have an illustration for it. <br><br>  Depending on the distance to the camera, the object will be high or low poly or will not be drawn at all.  This happens, for example, with grass or flowers in the distance.  So, at this stage, the necessity of drawing the object is determined and, if it exists, with what level of detail. <br><br>  This is where the differences between running the game on PS3 (not enough computational support for shaders) and PC or PS4 are found. <br><br><h3>  G-Buffer Generation </h3><br>  The "main" share of rendering takes place right here.  All visible cells of the graphic grid are drawn one by one, but instead of immediately calculating the degree of shading, the draw calls only capture the necessary information in separate buffers, called G-Buffer.  GTA V uses MRT technology, which allows each draw call to capture up to 5 special commands. <br><br>  During the subsequent compilation of buffers, it is possible to calculate the total shading indicators for each pixel.  Hence the name ‚Äúdeferred‚Äù as opposed to ‚Äúdirect‚Äù shading, in which each draw call is responsible for self-calculation of the final shielding shadow indicator. <br><br>  At this stage, only opaque objects are drawn, since transparent parts, like glass, require additional pending processing, which is performed later. <br><br>  <b>G-Buffer Generation</b> <br><img src="https://habrastorage.org/files/5bd/bac/9b1/5bdbac9b16cb4874bd2ff34dd3b54541.PNG"><br>  <i>15% generation</i> <br><br><img src="https://habrastorage.org/files/183/86c/a61/18386ca61aa44bb1a6cb25e87306d6f2.PNG"><br>  <i>30% generation</i> <br><br><img src="https://habrastorage.org/files/b47/2da/396/b472da396c9f4fb1aeb9239154614f5a.PNG"><br>  <i>50% generation</i> <br><br><img src="https://habrastorage.org/files/798/f8b/7fe/798f8b7fea5a4d2a8b46db16260fc1c3.PNG"><br>  <i>75% generation</i> <br><br><img src="https://habrastorage.org/files/d00/80f/aff/d0080faffc5f4df583de2ae66e770dc5.PNG"><br>  <i>100% generation</i> <br><br>  For the reproduction of all these effects are responsible for the purpose of rendering - LDR-buffers (RGBA with 8 bits per channel), which store various information, which later will be needed to calculate the total shading indicators: <br><br>  ‚Ä¢ Diffuse map: retains the ‚Äúoriginal color‚Äù of the cell.  In fact, it is a property of the material and, in fact, does not change with different lighting.  But see the white highlights on the hood of the car?  It is noteworthy that in GTA V shading is calculated on the basis of incoming sunlight before the formation of a diffuse map itself.  The necessary information about ‚Äúmixing‚Äù is stored in the alpha channel (more on this later). <br><br>  ‚Ä¢ Normal Map: Saves normal vectors for each pixel (R, G, B).  The alpha channel is also used here, although I'm not sure how: it looks like it performs the function of a binary mask for individual plants located near the camera; <br><br>  ‚Ä¢ Glare map.  The information related to highlights and reflections is stored here: <br><br><ul><li>  Red: highlight intensity; </li><li>  Green: gloss (smoothness); </li><li>  Blue: saturation of intermediate highlights (usually a constant figure for all pixels with an image of the same material). </li></ul><br>  ‚Ä¢ Illumination map: apparently, the red channel stores the data on the illumination of each pixel due to sunlight (based on the normal pixel data, their position and direction of incoming sunlight).  I'm not quite sure about the green channel, but it seems that he is responsible for the illumination due to additional light sources.  Blue channel - data on the emission properties of pixels (non-zero value for neon lamps).  A significant part of the alpha channel is not involved, except for marking the pixels corresponding to the character's skin or image of vegetation. <br><br>  So, earlier I mentioned creating teams at the same time for 5 rendering targets, but I only talked about 4 of them. <br><br>  An overlooked visualization is a special buffer that combines depth and pattern indicators.  Here is what we get as a result: <br><br><img src="https://habrastorage.org/files/957/b22/cc5/957b22cc549c49c788fbb4c5bb8a3546.PNG"><br>  <i>Depth on the left and pattern on the right respectively</i> <br><br>  <b>Depth map</b> : it records information about the distance of each pixel to the camera. <br><br>  Intuitively, one would expect distant pixels to be white (depth 1), and those that are closer would be darker.  But this is not the case: apparently, in GTA V they used a logarithmic Z-buffer, changing Z. But why?  It seems to be a matter of floating-point numbers, which are much more accurate in the encoding process, if their value is close to 0. So, by changing Z, you can enter much more accurate data on the depth of distant objects, which, in turn, eliminate Z-errors.  Given the length of the gaming session, it was simply impossible not to use such a technique.  Although, GTA V and did not discover America, because a similar technique is found in the same Just Cause 2, for example. <br><br>  <b>Pattern</b> : Used to identify various rendered cells, assigning a common ID to all the pixels of a specific group of cells.  For example, here are some values ‚Äã‚Äãin the template: <br><br><ul><li>  0x89: character controlled by the player; </li><li>  0x82: player driven car; </li><li>  0x01: non-player characters; </li><li>  0x02: vehicles, including cars, bicycles, and so on; </li><li>  0x03: vegetation and foliage; </li><li>  0x07: sky </li></ul><br>  All these buffers were generated thanks to more than 1900 render calls. <br><br>  Please note that the rendering is performed as if ‚Äúbackwards‚Äù, which allows you to optimize all the necessary operations taking into account the fragment depth comparison with the depth buffer value at the rasterization stage: in the process of drawing a scene, many details fail the depth test, since they are overlapped by closely spaced pixels, drawn earlier.  If it is obvious that the pixel does not pass the depth test, the graphics processor can automatically skip it, even without starting the shader.  If we are dealing with heavy pixel shaders, only the standard direct rendering order is appropriate, while the ‚Äúreverse‚Äù ( <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC_%25D1%2585%25D1%2583%25D0%25B4%25D0%25BE%25D0%25B6%25D0%25BD%25D0%25B8%25D0%25BA%25D0%25B0">artist's algorithm</a> ) will be the most ineffective in this case. <br><br>  And now I would like to say a few words about the role of the alpha channel in the diffuse map.  Look at the screenshot: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5ed/c7a/ebc/5edc7aebc349875b6d7c93e45b4ca36b.jpg" alt="image"><br>  <i>Approximate fragment of the diffusion map</i> <br><br>  See some pixels missing?  This is especially noticeable in the example of trees.  As if there are no separate <b><a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BA%25D1%2581%25D0%25B5%25D0%25BB_(%25D0%25B3%25D1%2580%25D0%25B0%25D1%2584%25D0%25B8%25D0%25BA%25D0%25B0)">texels</a></b> in the sprites. <br><br>  I already paid attention to this feature earlier, launching games on PS3, and then I was very puzzled.  Maybe the whole thing in the excessive reduction of the sprite texture?  But now I know that this is not the case, because the compilation of elements has been carried out correctly. <br><br>  Such a model really looks strange, almost like a chessboard.  Is it possible that ... the game renders only 1 out of 2 pixels? <br><br>  To clarify, I looked into the D3D bytecode.  And here you are: <br><br><pre><code class="cpp hljs">dp2 r1.y, v0.xyxx, l(<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.0</span></span>, <span class="hljs-number"><span class="hljs-number">0.0</span></span>) <span class="hljs-comment"><span class="hljs-comment">// Dot product of the pixel's (x,y) with (0.5, 0.5) frc r1.y, r1.y // Keeps only the fractional part: always 0.0 or 0.5 lt r1.y, r1.y, l(0.5) // Test if the fractional part is smaller than 0.5</span></span></code> </pre> <br><br>  In fact, we see the condition (X + Y)% 2 == 0, which is observed for only one pixel out of 2 (where x and y are the coordinates of the pixel). <br><br>  This is only one of the reasons for the sampling of pikels (the second is the alpha value &lt;0.75), although it was quite enough to explain the specifics of the detected phenomenon. <br><br>  Information about which cells were rendered in the considered ‚Äúselective‚Äù mode is stored on the alpha channel of the diffuse map, as seen in the picture. <br><br><h3>  Diffuse map alpha channel </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/84b/126/426/84b12642640bea59fdc5c16773dbc835.jpg" align="right" alt="image">  So why are some models rendered this way?  Maybe it helped to save the fill or shading options?  I didn‚Äôt mean to say that the graphics processors do not provide for such detailing: the pixels are painted over not separately, but in groups of 2x2.  And it's not about performance, it's a matter of the level of detail: this model makes the filled cells more transparent due to different levels of detail. <br><br>  This method is called <a href="http://n00body.squarespace.com/journal/2009/9/14/stippled-alpha.html">alpha stippling</a> . <br><br><h3>  Shadows </h3><br>  The game uses CSM (cascading shadow maps): 4 shadow maps form 1024x4096 textures.  Each shadow map is designed for different visibility.  The more often the actions are repeated, the wider the pyramid of visibility of the camera, the greater the panorama of the frame.  Thanks to this approach, the shadows of objects near the game character have a higher resolution than the shadows of distant objects.  Here is a brief overview of the depth of 4 maps: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/178/1c7/22b/1781c722bdbda13cf2a146625a24e497.jpg" alt="image"><br><br><h3>  Shadow maps </h3><br>  This process may require a lot of energy, since you have to re-render the scene as much as 4 times, but cutting off the visibility pyramid allows you to lower the processing of unnecessary polygons.  In this case, the CSM was created as a result of 1000 draw calls. <br><br>  Having this depth data, you can calculate the shadow for each pixel.  Information about shadows is stored in the rendering target: the shadows cast as a result of direct sunlight are recorded in the red channel, the shadows from the clouds in - in red and green. <br><br>  In the shadow maps, a smoothing pattern is provided (if you look closely at the texture shown below, you will see the previously mentioned chessboard effect in the red channel).  This allows you to smooth the edges of the shadows. <br><br>  Later, these gaps are filled: the shadows from the sun and clouds are combined into one buffer, a certain depth blur is produced, and the result is stored in the alpha channel of the glare map. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6ed/275/e36/6ed275e36b999fa92057ab5fa8e93b2a.png" alt="image"><br>  <i>Shadows from the sun and clouds (green)</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/91e/aa5/b79/91eaa5b7900677c1a7ab81b3f63464d4.jpg" alt="image"><br>  <i>Blurred Shadows</i> <br><br>  A few words about the blur: the technique is not cheap, because  have to deal separately with different textures.  Thus, to facilitate the task, just before performing the blur, a ‚Äúlightweight‚Äù texture is created: the shadow buffer scale is reduced to 1: 8 and a light blur is made by the pixel shader by repeating the Grab () command four times.  This allows you to roughly estimate the total number of fully lit pixels.  Subsequently, when a full blur is needed, first of all, information about the ‚Äúlightweight‚Äù version of the procedure is read: as soon as the shader encounters a fully lit pixel, it automatically gives out units and it does not have to start the laborious process of full-scale blurring. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/317/dd0/247/317dd0247412e5c7d68d7afdab57a108.jpg" align="right" alt="image">  I will not go into details, since I highlighted the theme of reflection ( <i>image of the reflection map of the plane on the right</i> ) in the second part and in this scene, the effect is barely noticeable.  I can only say that this step generates a reflection map for the ocean surface.  The essence of the principle is reduced to redrawing the scene (650 rendering calls) within the framework of a tiny texture of 240x120, but already in the "upside down" mode to create a feeling of reflection in the water. <br><br><h3>  Obstruction of ambient light in screen space </h3><br>  Here we are dealing with the creation of a linear version of the depth buffer, on the basis of which the SSAO map is formed (blocking of ambient light in the screen space). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b5d/133/8ef/b5d1338ef36ea6a3ed68a1eeadddab86.jpg" alt="image"><br>  <i>Sharp image</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/43e/d69/204/43ed69204a9df34b425b232dc675c9f1.jpg" alt="image"><br>  <i>Blurred image</i> <br><br>  First, the first version appears with all the noises, after which the depth blur is successively carried out in both horizontal and vertical directions, which noticeably smoothes the image. <br><br>  All work is done at a resolution of only half of the original, which guarantees higher performance. <br><br><h3>  G-Buffer Combination </h3><br>  So, it is time to combine all the generated buffers! <br><br>  The pixel shader reads data from various buffers and determines the final pixel shading value in HDR. <br><br>  In case we work with night scenes, lights and other elements of illumination will gradually, one after another, be superimposed on top of the scene. <br><br><img src="https://habrastorage.org/files/4cf/4f4/a5d/4cf4f4a5dc3042dcb41e21b55f9abbe6.png"><br><br>  And we get the final image: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/80d/5f0/a13/80d5f0a13b649d6fadaffb2455ea27a3.jpg" alt="image"><br><br>  The picture is becoming more pleasant, although there is still not enough ocean, sky, transparent objects ... But first, the main thing: you need to refine the image of Michael. <br><br><h3>  Subsurface Scattering (subsurface scattering) </h3><br>  There are questions with Michael's skin shading: there are very dark areas on the face and it seems as if it is a dense plastic, and not a human body. <br><br>  That's what we need SSS for.  Dispersion imitates natural skin coverage.  Look at your ears or lips: thanks to SSS, they look much more real, a healthy pinkish tinge appears in them. <br><br><img src="https://habrastorage.org/files/734/0e6/089/7340e6089d564c12bec9a1b7c6636b43.png"><br><br>  But how did you manage to run SSS separately for Michael's image? <br><br>  First, only the silhouette is cut.  Here the previously generated template buffer comes to the rescue: all Michael pixels are assigned the value 0x89.  Thus, we can concentrate on them, but we need to apply SSS only to the skin, and not to clothes. <br><br>  In fact, when all the G-buffers were combined, in addition to the shading indicators stored in RGB, some data was recorded in the alpha channel.  More specifically, the alpha channels of the irradiance map and the glare map were used to create a binary mask: the pixels corresponding to Michael‚Äôs skin and the image of some plants were marked with a unit in the alpha channel.  Other pixels, in particular, clothing pixels, were assigned the alpha 0 value. <br><br>  Thus, it is possible to apply SSS, having as the initial data the combined information from the target of the G-buffer and a buffer comparing the depth and pattern indicators. <br><br>  Yes, it may seem that such minor local transformations are not worth such serious calculations.  And, perhaps, you would be right if it were not for one thing: playing the game, we instinctively look at the face, and therefore the treatment for this part of the body allows us to bring the game as close as possible to reality.  In GTA V, the SSS is applied both to the image of the main character and in the case of non-player characters. <br><br><h3>  Water </h3><br>  In the scene in question, not so much water got into the frame, but still: the ocean in the background, a couple of pools here and there. <br><br>  The visualization of water in GTA V is concentrated in two directions - reflection and refraction. <br><br>  The logarithmic Z-buffer created earlier allows to generate the second version, this time linear with the resolution half the size of the original one. <br><br>  Ocean and pools are rendered in turn in MRT mode.  So at the same time achieved several goals: <br><br><img src="https://habrastorage.org/files/b78/663/726/b78663726c5941449c58ecba0476e8b1.png"><br>  <i>Left diffusion of water, right opacity</i> <br><br><ul><li>  Diffuse water map: captures the original water color. </li><li>  Water opacity map: in the red channel, in fact, information is stored about the individual properties of water opacity (for example, for the ocean it is always 0.102, for pools it is 0.129).  In the green channel, it is marked how deep the pixel is located relative to the water surface (deep pixels correspond to less transparent water, which requires connecting a diffuse map, while water pixels from the surface are much more transparent). </li><li>  Please note: all pools are visualized without reference to the conditions, even if they are practically covered by data from other cells, they are still stored in the red channel.  As for the green channel, where only really visible pixels are counted, only ‚Äúwater‚Äù pixels fall on the finished image. </li></ul><br><br>  Now we can combine previously created buffers and generate a refraction map: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6bb/dc5/879/6bbdc5879c23d9bdff59d1b4ce92ca13.jpg" alt="image"><br>  <i>Water refraction map</i> <br><br>  On this map of refraction, the pools are filled with water (the deeper the water, the bluer it is), caustics are also added. <br><br>  It's time to start the final visualization of water: again, one by one, all the cells corresponding to the images of the ocean and pools are drawn, but this time the reflection and refraction are combined, and relief maps are formed, making corrections to the surface normals. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fdf/13b/e61/fdf13be61a8206c3ddcc8b115e7bfbee.jpg" alt="image"><br>  <i>Image before adding water</i> <br><br><img src="https://habrastorage.org/files/fe7/e9e/883/fe7e9e883f87476eb8817590ccd9fd92.png"><br>  <i>Refraction on the plane, reflection and relief maps</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/291/d34/89b/291d3489bdee8235adfb0fec56e48b6d.jpg" alt="image"><br>  <i>Image after adding water</i> <br><br><h3>  Atmosphere </h3><br>  Create a map of the so-called volumetric shadows: it helps to darken the atmosphere / fog, which are not directly illuminated by the sun. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f34/644/f25/f34644f25be1c5481d22d7de1566d35c.jpg" alt="image"><br><br>  The map is generated at half the resolution by scanning the pixels and comparing the received data with the shadow map from the sun.  After receiving the first option with all the noise buffer is blurred. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/291/d34/89b/291d3489bdee8235adfb0fec56e48b6d.jpg" alt="image"><br>  <i>Base image</i> <br><br>  Then the fog effect is added to the scene: it perfectly hides the missing details of the low poly buildings that are seen far away.  Here data is read from the volume shadow map (does not play a significant role at this stage) and the depth buffer, on the basis of which fog indicators are formed. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d25/51a/dbf/d2551adbfe1cf9a3eeb87a5d91c89816.jpg" alt="image"><br>  <i>Base image with fog</i> <br><br>  After the sky is visualized. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3c5/62e/de6/3c562ede60e757d403836b10463a6320.jpg" alt="image"><br><br>  And at the very end, after him, the clouds are drawn. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7d2/07c/144/7d207c14439fe0fa400e509a588e6582.jpg" alt="image"><br>  <i>Final image</i> <br><br><img src="https://habrastorage.org/files/16f/48d/d4a/16f48dd4a26b49d8a4710d1b9b334c93.PNG" align="right" alt="image">  In fact, the sky is rendered by a single draw call: the sky's graphic grid forms a huge dome covering the entire scene (see right). <br><br>  At this stage, separate textures are used, resembling the effect <a href="https://ru.wikipedia.org/wiki/%25D0%25A8%25D1%2583%25D0%25BC_%25D0%259F%25D0%25B5%25D1%2580%25D0%25BB%25D0%25B8%25D0%25BD%25D0%25B0">of Perlin noise</a> . <br><br>  Clouds are also visualized: an extensive grid appears on the horizon, this time in the form of a ring.  One normal map and one density map allow you to visualize the clouds: these are large seamless textures of 2048x512 (connected in an arc on the left and right sides). <br><br><img src="https://habrastorage.org/files/a2d/6b3/cd2/a2d6b3cd2abd430fae302b0119d07412.png"><br>  <i>The density of the clouds on the left</i> <br><br><h3>  Transparent objects </h3><br>  Now let's deal with all transparent objects: glasses, windshield, dust particles in the air ... <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7d2/07c/144/7d207c14439fe0fa400e509a588e6582.jpg" alt="image"><br><br>  All in all, 11 draw calls will be required, however, when processing the dust, you will have to repeatedly refer to the instancing. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1f3/d29/ebe/1f3d29ebe63b0bd9e773d4da50323197.jpg" alt="image"><br>  <i>Final image</i> <br><br><h3>  Bit smoothing </h3><br>  Remember the previous brief overview of individual trees smoothed on a diffuse map?  This is how the image looks without smoothing: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ac3/11d/5b0/ac311d5b03d861c8e1a58d072aed4e60.jpg" alt="image"><br><br>  It's time to fix it: we run a pixel shader for post-processing - and it reads the data of the buffer of the original color and the alpha channel of the diffuse map to find out which pixels are smoothed.  Up to 2 neighboring pixels may be required for each pixel to determine the final color of the ‚Äúsmoothed‚Äù element. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6b5/577/584/6b5577584b69672c27fefe9bf41bbc8a.jpg" alt="image"><br>  <i>After smoothing</i> <br><br>  This is a great trick, because with its help the whole image is ‚Äúaligned‚Äù in one approach: moreover, the variety of geometric shapes in a given scene does not matter. <br><br>  Note, however, that this filter is not ideal: in some cases, apparently, it failed, I still noticed the effect of a chessboard on the PS3, and on the PC. <br><br><h3>  Tonal compression and glow </h3><br>  Prior to that, the processed image was saved in the HDR format: each RGB channel is stored as a 16-bit floating point index, which made it possible to significantly expand the range of illumination intensity.  But monitors are not able to display such different values ‚Äã‚Äãand reduce everything to RGB colors with 8 bits per channel. <br><br>  <a href="https://en.wikipedia.org/wiki/Tone_mapping">Tonal compression</a> converts these color values ‚Äã‚Äãfrom HDR to LDR.  There are several functions by which one format is replaced by another.  The classic version, widely used - the Reinhard method (I used it when creating screenshots published earlier) - gives results that are close to the final form of the game. <br><br>  But did GTA V really use the Reinhard method?  We'll have to get into the shader bytecode again: <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">// Suppose r0 is the HDR color, r1.xyzw is (A, B, C, D) and r2.yz is (E, F) mul r3.xy, r1.wwww, r2.yzyy // (DE, DF) mul r0.w, r1.y, r1.z // BC [...] div r1.w, r2.y, r2.z // E/F [...] mad r2.xyz, r1.xxxx, r0.xyzx, r0.wwww // Ax+BC mad r2.xyz, r0.xyzx, r2.xyzx, r3.xxxx // x(Ax+BC)+DE mad r3.xzw, r1.xxxx, r0.xxyz, r1.yyyy // Ax+B mad r0.xyz, r0.xyzx, r3.xzwx, r3.yyyy // x(Ax+B)+ DF div r0.xyz, r2.xyzx, r0.xyzx // (x(Ax+BC)+DE) / (x(Ax+B)+DF) add r0.xyz, -r1.wwww, r0.xyzx // (x(Ax+BC)+DE) / (x(Ax+B)+DF) - (E/F)</span></span></code> </pre><br><br>  So, so ... what do we have here?  It is an equation of the type <code>(x(Ax+BC)+DE) / (x(Ax+B)+DF) - (E/F)</code> marked the breakthrough of John Heble in the film industry in 2009. <br><br>  It turns out that in GTA V, there is no talk of the Reinhard method, but the principle from Uncharted 2, according to which black areas are not discolored, is also not suitable. <br><br>  The conversion process to LDR is as follows: <br><br>  The resolution of the HDR buffer is reduced to ¬º from the original value. <br><br><ul><li>  The computational shader determines the average brightness of the buffer, outputting the result as a 1x1 texture. </li><li>  A new exposure is calculated, due to which the scene will be bright / dark. </li><li>  The brightness filter requests only pixels that have a brightness above a certain value (set by exposure). </li><li>  In this scene, only a few pixels were filtered: some illuminated areas on a car with a high reflectivity. </li><li>  Indicators of the brightness buffer are repeatedly reduced up to 1/16 of the original, and then increase to 1/2 of the original. </li><li>  A glow is added to the original HDR pixels, and then using the converter from Uncharted 2, the color is converted to LDR.  At the same time, gamma correction is performed, which allows replacing linear channels with sRGB. </li></ul><br><br>  The end result depends on the exposure.  Here are literally several examples of the effect of this parameter: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/019/f77/851/019f778513f5cfb25156a4ad0fd254c1.jpg" alt="image"><br><br>  The exposure changes gradually, frame by frame, you will not see drastic changes. <br><br>  So an attempt was made to imitate the nature of the human eye: did you notice that after a long journey through a dark tunnel and a sudden exit to the sun, the environment is perceived too bright for a while?  Then it stops ‚Äúcutting the eye‚Äù and becomes ‚Äúnormal‚Äù, and the exposure adapts to the new value.  It seems that in GTA V they went even further, bringing the adaptation of exposure along the ‚ÄúDark ‚Üí Bright‚Äù line to similar properties of the human eye as close as possible. <br><br><h3>  Smoothing and distorting the lens </h3><br>  If the FXAA method is used, then he is definitely paying attention to smoothing out the uneven contours of the cells. <br><br>  Then, using small pixel shaders to simulate a real camera, the lens distortion effect is triggered.  This not only distorts the image, it also initiates minor color changes along the frame contour, where the red channel dominates slightly above the green and blue. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/32b/d7b/99c/32bd7b99c1ba38b8cabe47874aacb480.jpg" alt="image"><br>  <i>Before distortion</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/de7/7cb/6f1/de77cb6f1af01d095104495d736b9c70.jpg" alt="image"><br>  <i>After distortion</i> <br><br><h3>  User interface </h3><br>  And the final touch: the user interface, represented by a mini-map in the lower left corner of the screen.  The map is actually divided into several square zones, the engine is responsible only for those that are displayed on the screen.  Each square is associated with a draw call.  I painted the squares to more clearly demonstrate the structure: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/105/883/ea2/105883ea2d3da51d1aa8d50159115268.png" alt="image"><br>  <i>Minimap</i> <br><br>  The clipping test allows not only to process the area in the lower left corner, but also to delete the contents of the area.  There are vectors on all roads (see the screenshot above), they are displayed as graphical grids and look great even with a significant increase. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/de7/7cb/6f1/de77cb6f1af01d095104495d736b9c70.jpg" alt="image"><br>  <i>Image before adding a minimap</i> <br><br>  At this stage, the minimap is sent to the main image buffer, a couple of small icons and widgets are added on top of it. <br><br>  Yes, they spent a lot of time, but it was worth it: here is the final shot in all its glory! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d5f/c39/aea/d5fc39aea5d806177cf246282af0c62e.jpg" alt="image"><br><br>  A total of <b>4155</b> render calls, <b>1113</b> textures and <b>88</b> rendering targets were required. <br><br><h4>  <b><a href="http://habrahabr.ru/company/ua-hosting/blog/272521/">PART No.2: Detailing, reflections and post effects in GTA V</a></b> </h4><br><hr><br><h5>  <i>Afterword from the translator.</i> <i><br><br></i>  <i>Dear readers, I hope you enjoyed this material.</i>  <i>If this is the case, then we, in the face of the ua-hosting.company team, will prepare a sequel for you, posted on the original author‚Äôs blog.</i> <i><br><br></i>  <i>We are trying to prepare interesting materials for you, in which there is no b-harsh SEO-optimization of the text and at least three references to yourself.</i>  <i>Therefore, you can feel free to subscribe to our blogs on <a href="http://habrahabr.ru/company/ua-hosting/profile/">Habr√©</a> and <a href="http://geektimes.ru/company/ua-hosting/profile/">Hiktimes</a> , and we will try so that you do not regret it.</i>  <i>I would also like to add that we are well aware that no one likes advertising, especially arrogant, and we in every possible way try to avoid it in our materials.</i>  <i>Therefore, we leave a link to the announcement of our regular <b>discount <a href="http://geektimes.ru/company/ua-hosting/blog/266658/">action in honor of</a> Cyber <a href="http://geektimes.ru/company/ua-hosting/blog/266658/">Monday</a></b> here in the basement, where it will not hurt anyone to enjoy the article above.</i> </h5></div><p>Source: <a href="https://habr.com/ru/post/271931/">https://habr.com/ru/post/271931/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../271915/index.html">.NET-hardcore in Moscow</a></li>
<li><a href="../271919/index.html">The digest of interesting materials for the mobile developer # 131 (November 23-29)</a></li>
<li><a href="../271921/index.html">Record a video call from the browser: we hoped to file a week</a></li>
<li><a href="../271927/index.html">Works and Copies</a></li>
<li><a href="../271929/index.html">The digest of interesting materials from the world of web development and IT for the last week ‚Ññ187 (November 23 - 29, 2015)</a></li>
<li><a href="../271935/index.html">What is under the hood of travel startups or why does a programmer need to go to Hack`n`Roll</a></li>
<li><a href="../271937/index.html">5 major risks in custom software development</a></li>
<li><a href="../271939/index.html">Asterisk + LUA: quick start</a></li>
<li><a href="../271941/index.html">Getting Started with Java 9 and the Jigsaw Project - Part One</a></li>
<li><a href="../271943/index.html">Change desktop background and lock screen from CW / XAML UWP application</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>