<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Another migration of PROXMOX to softRAID1, but now already 3.2 and on GPT partitions, installing FreeNAS 9.2 on a virtual machine and forwarding a physical disk to it</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 

 Once again, I needed a Proxmox server. The following is iron: AMD FX-4300, 4Gb, two 500Gb disks for proxmox itself and two more for storage....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Another migration of PROXMOX to softRAID1, but now already 3.2 and on GPT partitions, installing FreeNAS 9.2 on a virtual machine and forwarding a physical disk to it</h1><div class="post__text post__text-html js-mediator-article">  Hello! <br><br>  Once again, I needed a Proxmox server.  The following is iron: <b>AMD</b> FX-4300, 4Gb, two 500Gb disks for proxmox itself and two more for storage.  The tasks are as follows: one of the FreeNAS machines, I wanted to forward several disks (preferably physical) into it, in order to place a storage on them, and several other non-article VMs. <br><br>  I have a catch always trying to install the most recent versions, and not proven old ones.  It happened this time. <br>  Downloaded Proxmox VE 3.2 and FreeNAS 9.2.  But what came out of it under the cut. <br><a name="habracut"></a><br>  Having installed Proxmox once again (the latest version 3.2 at the moment) decided to transfer it to SoftRAID1.  But I found that, unlike 3.0, it (proxmox) converted the disk to GPT.  Accordingly, the recommendations in the <a href="http://habrahabr.ru/post/186818/">article</a> on which I was oriented were not entirely relevant.  In addition, in all articles on the transfer of Proxmox to SoftRAID, we are talking only about two sections (boot and LVM).  In my case, the partitions on the disk were 3. First GRUB, and then the standard boot and LVM. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      This should not stop us. <br><br><h4>  Translation of proxmox to softRAID on GPT sections </h4><br>  We go in the standard way and put all the necessary software.  And here we are waiting for another surprise due to the fact that from version 3.1 the repository for Proxmox has become paid.  Therefore, before installing the necessary packages, you need to disable it (perhaps it is more correct to specify a free test repository instead, but I managed to comment out just the paid one).  Open it in any editor. <br><br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nano /etc/apt/sources.list.d/pve-enterprise.list</span></span></code> </pre>  and comment out a single line. <br><br>  If you still want to add a free repository, then run the command: <br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://download.proxmox.com/debian wheezy pve pve-no-subscription"</span></span> &gt;&gt; /etc/apt/sources.list.d/proxmox.list</code> </pre>  Thank you <a href="http://habrahabr.ru/users/heathen/" class="user_link">heathen</a> for his comment. <br><br>  Now we put the necessary packages: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># aptitude update &amp;&amp; aptitude install mdadm initramfs-tools screen</span></span></code> </pre>  the latter is needed if you do it remotely.  Transferring LVM to RAID takes a long time and it is advisable to do this through the screen. <br><br>  We check that array creation is now available: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># modprobe raid1</span></span></code> </pre>  Next we copy the partitions from sda to sdb.  This is where the differences in the MBR and GPT begin.  For GPT, this is done like this: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># sgdisk -R /dev/sdb /dev/sda The operation has completed successfully.</span></span></code> </pre>  Assign a random UUID to a new hard disk. <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># sgdisk -G /dev/sdb The operation has completed successfully. # sgdisk --randomize-guids --move-second-header /dev/sdb The operation has completed successfully.</span></span></code> </pre><br><br>  Verify that the partitions are created as we wanted: <br><br><table><tbody><tr><th>  sda disk </th><th>  sdb disk </th></tr><tr><td><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># parted -s /dev/sda print Model: ATA WDC WD5000AAKS-0 (scsi) Disk /dev/sda: 500GB Sector size (logical/physical): 512B/512B Partition Table: gpt Number Start End Size File system Name Flags 1 1049kB 2097kB 1049kB primary bios_grub 2 2097kB 537MB 535MB ext3 primary boot 3 537MB 500GB 500GB primary lvm</span></span></code> </pre></td><td><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># parted -s /dev/sdb print Model: ATA ST3500320NS (scsi) Disk /dev/sdb: 500GB Sector size (logical/physical): 512B/512B Partition Table: gpt Number Start End Size File system Name Flags 1 1049kB 2097kB 1049kB primary bios_grub 2 2097kB 537MB 535MB primary boot 3 537MB 500GB 500GB primary lvm</span></span></code> </pre></td></tr></tbody></table><br><br>  Change the flags of the sdb2 and sdb3 sections to raid: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># parted -s /dev/sdb set 2 "raid" on # parted -s /dev/sdb set 3 "raid" on # parted -s /dev/sdb print Model: ATA ST3500320NS (scsi) Disk /dev/sdb: 500GB Sector size (logical/physical): 512B/512B Partition Table: gpt Number Start End Size File system Name Flags 1 1049kB 2097kB 1049kB primary bios_grub 2 2097kB 537MB 535MB primary raid 3 537MB 500GB 500GB primary raid</span></span></code> </pre>  Everything turned out right. <br><br>  Go ahead and clean up the superblocks just in case: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># mdadm --zero-superblock /dev/sdb2 mdadm: Unrecognised md component device - /dev/sdb2 # mdadm --zero-superblock /dev/sdb3 mdadm: Unrecognised md component device - /dev/sdb3</span></span></code> </pre>  The output of ‚Äúmdadm: Unrecognised md component device - / dev / sdb3‚Äù means that the disk did not participate in the RAID. <br>  Actually, it's time to create arrays: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># mdadm --create /dev/md1 --level=1 --raid-disks=2 missing /dev/sdb2 mdadm: Note: this array has metadata at the start and may not be suitable as a boot device. If you plan to store '/boot' on this device please ensure that your boot-loader understands md/v1.x metadata, or use --metadata=0.90 Continue creating array? y mdadm: Defaulting to version 1.2 metadata mdadm: array /dev/md1 started. # mdadm --create /dev/md2 --level=1 --raid-disks=2 missing /dev/sdb3 mdadm: Note: this array has metadata at the start and may not be suitable as a boot device. If you plan to store '/boot' on this device please ensure that your boot-loader understands md/v1.x metadata, or use --metadata=0.90 Continue creating array? y mdadm: Defaulting to version 1.2 metadata mdadm: array /dev/md2 started.</span></span></code> </pre><br><br>  To the question ‚ÄúContinue to create an array?‚Äù We answer in the affirmative. <br><br>  Let's see what we got: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /proc/mdstat Personalities : [raid1] md2 : active raid1 sdb3[1] 487731008 blocks super 1.2 [2/1] [_U] md1 : active raid1 sdb2[1] 521920 blocks super 1.2 [2/1] [_U]</span></span></code> </pre><br><br>  The output shows the state of the arrays - [_U].  This means that there is only one disk in the array.  It should be so, because the second (first) we have not yet included in the array.  (missing). <br><br>  Add information about arrays to the configuration file: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cp /etc/mdadm/mdadm.conf /etc/mdadm/mdadm.conf_orig # mdadm --examine --scan &gt;&gt; /etc/mdadm/mdadm.conf</span></span></code> </pre><br><br>  Copy the boot partition to the corresponding array.  (I added commands to unmount the partition here. Thanks for the information from the <a href="http://habrahabr.ru/users/skazkin/" class="user_link">skazkin</a> user. His experience has shown that in some cases, without these actions, the boot partition can be empty after a reboot): <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># mkfs.ext3 /dev/md1 mke2fs 1.42.5 (29-Jul-2012) Filesystem label= OS type: Linux Block size=1024 (log=0) Fragment size=1024 (log=0) Stride=0 blocks, Stripe width=0 blocks 130560 inodes, 521920 blocks 26096 blocks (5.00%) reserved for the super user First data block=1 Maximum filesystem blocks=67633152 64 block groups 8192 blocks per group, 8192 fragments per group 2040 inodes per group Superblock backups stored on blocks: 8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409 Allocating group tables: done Writing inode tables: done Creating journal (8192 blocks): done Writing superblocks and filesystem accounting information: done # mkdir /mnt/md1 # mount /dev/md1 /mnt/md1 # cp -ax /boot/* /mnt/md1 # umount /mnt/md1 # rmdir /mnt/md1</span></span></code> </pre>  Next, we need to comment out the line in / etc / fstab describing the mounting of the boot partition with the UUID and assign the mount of the corresponding array: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nano /etc/fstab</span></span></code> </pre><br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># &lt;file system&gt; &lt;mount point&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt; /dev/pve/root / ext3 errors=remount-ro 0 1 /dev/pve/data /var/lib/vz ext3 defaults 0 1 # UUID=d097457f-cac5-4c7f-9caa-5939785c6f36 /boot ext3 defaults 0 1 /dev/pve/swap none swap sw 0 0 proc /proc proc defaults 0 0 /dev/md1 /boot ext3 defaults 0 1</span></span></code> </pre><br><br>  It should be something like this. <br>  Reboot: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># reboot</span></span></code> </pre><br><br>  Configuring GRUB (we do it in the same way as in the original <a href="http://habrahabr.ru/post/186818/">article</a> ): <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># echo 'GRUB_DISABLE_LINUX_UUID=true' &gt;&gt; /etc/default/grub # echo 'GRUB_PRELOAD_MODULES="raid dmraid"' &gt;&gt; /etc/default/grub # echo 'GRUB_TERMINAL=console' &gt;&gt; /etc/default/grub # echo raid1 &gt;&gt; /etc/modules # echo raid1 &gt;&gt; /etc/initramfs-tools/modules</span></span></code> </pre><br><br>  Reinstall GRUB: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># grub-install /dev/sda --recheck Installation finished. No error reported. # grub-install /dev/sdb --recheck Installation finished. No error reported. # update-grub Generating grub.cfg ... Found linux image: /boot/vmlinuz-2.6.32-27-pve Found initrd image: /boot/initrd.img-2.6.32-27-pve Found memtest86+ image: /memtest86+.bin Found memtest86+ multiboot image: /memtest86+_multiboot.bin done # update-initramfs -u update-initramfs: Generating /boot/initrd.img-2.6.32-27-pve</span></span></code> </pre> <br><br>  Now add the boot partition from the first (sda) disk to the array.  First, mark it with the flag "raid": <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># parted -s /dev/sda set 2 "raid" on</span></span></code> </pre><br><br>  And then add: <pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># mdadm --add /dev/md1 /dev/sda2 mdadm: added /dev/sda2</span></span></code> </pre><br><br>  If you now look at the state of the arrays: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cat /proc/mdstat Personalities : [raid1] md2 : active (auto-read-only) raid1 sdb3[1] 487731008 blocks super 1.2 [2/1] [_U] md1 : active raid1 sda2[2] sdb2[1] 521920 blocks super 1.2 [2/2] [UU] unused devices: &lt;none&gt;</span></span></code> </pre><br><br>  then we will see that md1 became ‚Äúdual-drive‚Äù - [UU] <br><br>  Now you need to move the main section - LVM.  There are no differences from the ‚Äúoriginal‚Äù, with the exception of a different numbering of sections and: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># screen bash # pvcreate /dev/md2 Writing physical volume data to disk "/dev/md2" Physical volume "/dev/md2" successfully created # vgextend pve /dev/md2 Volume group "pve" successfully extended # pvmove /dev/sda3 /dev/md2 /dev/sda3: Moved: 2.0% ... /dev/sda3: Moved: 100.0% # vgreduce pve /dev/sda3 Removed "/dev/sda3" from volume group "pve" # pvremove /dev/sda3</span></span></code> </pre><br>  Here, on the recommendation of <a href="http://habrahabr.ru/users/skazkin/" class="user_link">skazkin,</a> added the pvremove command.  Without it (again, not always) another problem may appear: <blockquote>  the system will not understand what happened to the disks and the initramfs console will not boot further </blockquote><br><br>  Add the section sda3 to the array: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># parted -s /dev/sda set 3 "raid" on # mdadm --add /dev/md2 /dev/sda3 mdadm: added /dev/sda3 # cat /proc/mdstat Personalities : [raid1] md2 : active raid1 sda3[2] sdb3[1] 487731008 blocks super 1.2 [2/1] [_U] [&gt;....................] recovery = 0.3% (1923072/487731008) finish=155.4min speed=52070K/sec md1 : active raid1 sda2[2] sdb2[1] 521920 blocks super 1.2 [2/2] [UU] unused devices: &lt;none&gt;</span></span></code> </pre><br><br>  and see that it is added. <br><br>  Since I am acting on the original <a href="http://habrahabr.ru/post/186818/">article</a> , I went to pour coffee. <br><br>  After the array is rebuilt (and also not a quick one), this part can be considered complete. <br><br>  For those who, like me, I did not understand why this is <b>all</b> , I explain.  Since  We actually transferred the LVM volume from one block device to another, and it is not required to register it (as it was with the boot).  I stalled for a while at this place. <br><br><h4>  FreeNAS 9.2 on AMD processors </h4><br>  My next step was to install FreeNAS version 9.2 on proxmox.  Long tormented.  Until I tried to install from the same image (FreeNAS 9.2) on another proxmox-server.  It is slightly different from the one described in the article: firstly, it is on Core i7, secondly it is proxmox 3.1.  And there it naturally fell into account once.  Those.  the problem is either in AMD (there is definitely no such thing), or that proxmox 3.2 broke support for FreeBSD9 (brrr).  Long dug.  Then he began to experiment himself.  In the end, all the same AMD.  What do they have there for the problem, but as soon as I set the type of the Core 2 Duo FreeNAS 9.2 processor in the VM properties, it was installed without any problems. <br><br><h4>  Forwarding a physical disk in KVM (proxmox) </h4><br>  I have been looking for an answer to this question for a long time on the web, but I found only fragments.  Maybe someone and they can immediately understand what and how, but not me. <br>  In general, it is done this way (from the console): <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># nano /etc/pve/nodes/proxmox/qemu-server/100.conf</span></span></code> </pre><br><br>  and add the line at the end: <br><br><pre> <code class="bash hljs">virtio0: /dev/sdc</code> </pre><br><br>  where sdc is your device.  Further, you can specify other parameters separated by a comma (you can see them in the proxmox wiki). <br><br>  That's all.  True, I don‚Äôt know how much such a connection raises (or lowers) the speed of disk operations.  I still have tests ahead. </div><p>Source: <a href="https://habr.com/ru/post/218757/">https://habr.com/ru/post/218757/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../218745/index.html">[video] How to avoid unplanned rollback: analysis of 7 real negotiation situations</a></li>
<li><a href="../218747/index.html">Solar charging for lithium battery</a></li>
<li><a href="../218749/index.html">JoysMaker R2 Black - 3D printer for the inexperienced</a></li>
<li><a href="../218751/index.html">We work asynchronously in PHP or the story of another chat</a></li>
<li><a href="../218753/index.html">Variation in programming</a></li>
<li><a href="../218759/index.html">Windows command line subtleties</a></li>
<li><a href="../218761/index.html">Capture video in OpenGL applications using Intel INDE Media Pack</a></li>
<li><a href="../218763/index.html">The sound on the chip AY-3-8910 (or Yamaha YM2149F) comes from the ZX Spectrum on the PC via LPT-port</a></li>
<li><a href="../218765/index.html">Have you seen your printed circuit boards under the microscope?</a></li>
<li><a href="../218767/index.html">Using the PerfView utility in customer support</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>