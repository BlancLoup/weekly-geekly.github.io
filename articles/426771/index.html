<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Making a machine learning project in Python. Part 3</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A Complete Machine Learning Walk-Through Translation In Python: Part Three 

 Many people do not like the fact that machine learning models are black ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Making a machine learning project in Python. Part 3</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/f9c/42e/247/f9c42e24705e7fd918dbb79b851d90f8.png"><br><br>  <i><a href="https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-three-388834e8804b">A Complete Machine Learning Walk-Through</a> Translation <a href="https://towardsdatascience.com/a-complete-machine-learning-walk-through-in-python-part-three-388834e8804b">In Python: Part Three</a></i> <br><br>  Many people do not like the fact that machine learning models are <a href="https://datascience.stackexchange.com/questions/22335/why-are-machine-learning-models-called-black-boxes">black boxes</a> : we put data in them and without any explanation we get answers - often very accurate answers.  In this article we will try to understand how the model created by us makes predictions and what it can tell about the problem we are solving.  And we will conclude with a discussion of the most important part of the machine learning project: we will document what has been done and present the results. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In the <a href="https://habr.com/company/nixsolutions/blog/425253/">first part,</a> we covered data cleansing, exploration analysis, design, and feature selection.  In the <a href="https://habr.com/company/nixsolutions/blog/425907/">second part, we</a> studied the filling of missing data, the implementation and comparison of machine learning models, hyperparameter tuning using random search with cross-checking and, finally, the evaluation of the resulting model. <br><a name="habracut"></a><br>  All <a href="https://github.com/WillKoehrsen/machine-learning-project-walkthrough">project code</a> is on GitHub.  And the third Jupyter Notebook related to this article is <a href="https://github.com/WillKoehrsen/machine-learning-project-walkthrough/blob/master/Machine%2520Learning%2520Project%2520Part%25203.ipynb">here</a> .  You can use it for your projects! <br><br>  So, we are working on solving the problem using machine learning, more precisely, using supervised regression.  Based on <a href="http://www.nyc.gov/html/gbee/html/plan/ll84_scores.shtml">data on the energy consumption of buildings in New York,</a> we have created a model that predicts the number of Energy Star Score points.  We have a ‚Äú <a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/">regression based on gradient boosting</a> ‚Äù model, which is able to predict within 9.1 points (in the range from 1 to 100) based on test data. <br><br><h2>  Model Interpretation </h2><br>  The regression based on the gradient boosting is located approximately in the middle of <a href="">the model interpretability scale</a> : the model itself is complex, but consists of hundreds of rather simple <a href="https://en.wikipedia.org/wiki/Decision_tree_learning">decision trees</a> .  There are three ways to understand the work of our model: <br><br><ol><li>  Rate the <a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html">importance of the signs</a> . </li><li>  Visualize one of the decision trees. </li><li>  Apply the <a href="https://github.com/marcotcr/lime">LIME</a> method <a href="https://github.com/marcotcr/lime">- Local Interpretable Model-Agnostic Explainations</a> , local interpreted model <a href="https://github.com/marcotcr/lime">-dependent</a> explanations. </li></ol><br>  The first two methods are characteristic of ensembles of trees, and the third, as you can understand from its name, can be applied to any machine learning model.  LIME is a relatively new approach; this is a significant step forward in an attempt to <a href="https://pdfs.semanticscholar.org/ab4a/92795ee236632e6dbbe9338ae99778b57e1e.pdf">explain the work of machine learning</a> . <br><br><h4>  The importance of signs </h4><br>  The importance of signs allows you to see the relationship of each sign in order to predict.  The technical details of this method are complex (the mean decrease in foreignness <a href="https://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees.pdf">is measured</a> (the mean decrease impurity) or the <a href="https://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined">decrease in error due to the inclusion of the feature</a> ), but we can use relative values ‚Äã‚Äãto understand which features are more relevant.  In Scikit-Learn, it is possible <a href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html">to extract the importance of attributes</a> from any ensemble of "pupils" based on trees. <br><br>  In the code below, the <code>model</code> is our trained model, and using <code>model.feature_importances_</code> you can determine the importance of features.  Then we send them to the Pandas data frame and display the 10 most important features: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-comment"><span class="hljs-comment"># model is the trained model importances = model.feature_importances_ # train_features is the dataframe of training features feature_list = list(train_features.columns) # Extract the feature importances into a dataframe feature_results = pd.DataFrame({'feature': feature_list,'importance': importances}) # Show the top 10 most important feature_results = feature_results.sort_values('importance',ascending = False).reset_index(drop=True) feature_results.head(10)</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/d3c/bba/54c/d3cbba54c4cf1d1ce3da578fa51a07da.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/134/f20/cc5/134f20cc5819f113adeeacda14c5c073.png"><br><br>  The most important signs are <code>Site EUI</code> ( <a href="https://www.energystar.gov/buildings/facility-owners-and-managers/existing-buildings/use-portfolio-manager/understand-metrics/what-energy">energy consumption intensity</a> ) and <code>Weather Normalized Site Electricity Intensity</code> , which account for more than 66% of total importance.  Already in the third feature, the importance falls dramatically, this may mean that we do not need to use all 64 features to achieve high prediction accuracy (in <a href="https://github.com/WillKoehrsen/machine-learning-project-walkthrough/blob/master/Machine%2520Learning%2520Project%2520Part%25203.ipynb">Jupyter notebook</a> this theory is tested using only the 10 most important features, and the model was not too accurate). <br><br>  On the basis of these results, you can finally answer one of the initial questions: the most important indicators of the number of Energy Star Score points are the Site EUI and the Weather Normalized Site Electricity Intensity.  We will not go <a href="http://parrt.cs.usfca.edu/doc/rf-importance/index.html">too deep into the jungle of the importance of signs</a> , let us say only that with them you can begin to understand the mechanism of forecasting by the model. <br><br><h4>  Visualization of a single decision tree </h4><br>  Comprehending the entire regression model based on gradient boosting is hard, which cannot be said about individual decision trees.  You can visualize any tree using the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html"><code>Scikit-Learn- export_graphviz</code></a> .  First we extract the tree from the ensemble, and then save it as a dot-file: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tree <span class="hljs-comment"><span class="hljs-comment"># Extract a single tree (number 105) single_tree = model.estimators_[105][0] # Save the tree to a dot file tree.export_graphviz(single_tree, out_file = 'images/tree.dot', feature_names = feature_list)</span></span></code> </pre> <br>  Using the <a href="https://www.graphviz.org/">Graphviz visualizer, we</a> convert the dot-file to png by typing: <br><br> <code>dot -Tpng images/tree.dot -o images/tree.png</code> <br> <br>  Got a complete decision tree: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eaf/be2/c36/eafbe2c36016122245a6306c4cfb73c8.png"><br><br>  A little cumbersome!  Although this tree is only 6 layers deep, it is difficult to track all transitions.  Let's change the function call <code>export_graphviz</code> and limit the depth of the tree to two layers: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/40e/913/def/40e913defb78cbacdc515cbf18950cae.png"><br><br>  Each node (rectangle) of a tree contains four lines: <br><br><ol><li>  Asked a question about the meaning of one of the attributes of a particular dimension: it depends on which direction we will go out of this node. </li><li>  <code>Mse</code> is a measure of the error in a node. </li><li>  <code>Samples</code> - the number of data samples (measurements) in the node. </li><li>  <code>Value</code> - goal score for all data samples in a node. </li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/b75/b5b/8e2/b75b5b8e249139ef77fdf41878e4f395.png"><br>  <i>Separate node.</i> <br><br>  (The leaves contain only 2. ‚Äì 4. Because they represent the final assessment and have no children). <br><br>  The formation of the forecast for a given measurement in the decision tree begins with the top node - the root, and then goes down the tree.  At each node you need to answer the asked question "yes" or "no."  For example, the previous illustration asks: ‚ÄúSite EUI of the building is less than or equal to 68.95?‚Äù If yes, the algorithm goes to the right child node, if not, then to the left. <br><br>  This procedure is repeated on each layer of the tree until the algorithm reaches the leaf node on the last layer (these nodes are not shown in the illustration with a smaller tree).  The forecast for any measurement in the sheet is <code>value</code> .  If several dimensions come to the sheet ( <code>samples</code> ), then each of them will receive the same forecast.  As the tree depth increases, the error on the training data will decrease as the leaves will be larger and the samples will be divided more carefully.  However, a tree that is too deep will lead to <a href="https://towardsdatascience.com/overfitting-vs-underfitting-a-conceptual-explanation-d94ee20ca7f9">retraining on the training data</a> and will not be able to generalize the test data. <br><br>  In the <a href="https://habr.com/company/nixsolutions/blog/425907/">second article,</a> we configured the number of hyperparameters of the model that control each tree, for example, the maximum depth of the tree and the minimum number of samples required for each sheet.  These two parameters strongly influence the balance between over-training and under-training, and the decision tree visualization will allow us to understand how these settings work. <br><br>  Although we will not be able to study all the trees in the model, the analysis of one of them will help to understand how each ‚Äústudent‚Äù predicts.  This flowchart-based method is very similar to how a person makes a decision.  <a href="http://scikit-learn.org/stable/modules/ensemble.html">Ensembles from decision trees</a> combine the forecasts of numerous individual trees, which allows you to create more accurate models with less variability.  Such ensembles are <a href="https://blog.statsbot.co/ensemble-learning-d1dcd548e936">very precise</a> and easy to explain. <br><br><h4>  Local interpretable model-dependent explanations (LIME) </h4><br>  The last tool with which you can try to figure out how our model ‚Äúthinks‚Äù.  LIME allows you to explain <a href="https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime">how a single forecast of any machine learning model is formed</a> .  For this, locally, next to some measurement, a simplified model is created on the basis of a simple model like a linear regression (details are described in this work: <a href="https://arxiv.org/pdf/1602.04938.pdf">https://arxiv.org/pdf/1602.04938.pdf</a> ). <br><br>  We will use the LIME method to study the completely erroneous forecast of our model and understand why it is wrong. <br><br>  First we find this incorrect prediction.  To do this, we will train the model, generate a forecast and select the value with the largest error: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GradientBoostingRegressor <span class="hljs-comment"><span class="hljs-comment"># Create the model with the best hyperparamters model = GradientBoostingRegressor(loss='lad', max_depth=5, max_features=None, min_samples_leaf=6, min_samples_split=6, n_estimators=800, random_state=42) # Fit and test on the features model.fit(X, y) model_pred = model.predict(X_test) # Find the residuals residuals = abs(model_pred - y_test) # Extract the most wrong prediction wrong = X_test[np.argmax(residuals), :] print('Prediction: %0.4f' % np.argmax(residuals)) print('Actual Value: %0.4f' % y_test[np.argmax(residuals)])</span></span></code> </pre> <br>  <b>Prediction: 12.8615</b> <b><br></b>  <b>Actual Value: 100.0000</b> <br><br>  Then we will create an explainer and give it training data, mode information, tags for the training data and the names of the attributes.  Now you can transfer the observational data and forecasting function to explainer, and then ask to explain the reason for the forecast error. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> lime <span class="hljs-comment"><span class="hljs-comment"># Create a lime explainer object explainer = lime.lime_tabular.LimeTabularExplainer(training_data = X, mode = 'regression', training_labels = y, feature_names = feature_list) # Explanation for wrong prediction exp = explainer.explain_instance(data_row = wrong, predict_fn = model.predict) # Plot the prediction explaination exp.as_pyplot_figure();</span></span></code> </pre> <br>  Forecast explanation diagram: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d82/d86/b28/d82d86b28cc5766aa59d864c24889762.png"><br><br>  How to interpret a diagram: each entry along the Y axis represents one variable value, and the red and green bars reflect the influence of this value on the forecast.  For example, according to the top record, the impact of <code>Site EUI</code> over 95.90, as a result, about 40 points are subtracted from the forecast.  According to the second record, the influence of the <code>Weather Normalized Site Electricity Intensity</code> less than 3.80, and therefore about 10 points are added to the forecast.  The final forecast is the sum of the intercept and the effects of each of the listed values. <br><br>  Let's look at it from the other side and call the <code>.show_in_notebook()</code> method: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Show the explanation in the Jupyter Notebook exp.show_in_notebook()</span></span></code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/ea8/6ba/aa3/ea86baaa33390ee40ebcc1f24cb9f076.png"><br><br>  The left shows the process of making a decision by the model: the influence on the forecast of each variable is visually displayed.  The table on the right shows the actual values ‚Äã‚Äãof the variables for the specified measurement. <br><br>  In this case, the model predicted about 12 points, and in fact it was 100. At first, you may wonder why this happened, but if you analyze the explanation, it turns out that this is not a very bold assumption, but the result of the calculation based on specific values.  The value of <code>Site EUI</code> was relatively high and one could expect a low Energy Star Score (because it was strongly influenced by the EUI), which our model did.  But in this case, this logic turned out to be wrong, because in fact the building received the highest Energy Star Score - 100. <br><br>  Model errors can upset you, but such explanations can help you understand why the model was wrong.  Moreover, thanks to the explanations, you can begin to dig out why the building received the highest score despite the high value of Site EUI.  Perhaps we will learn something new about our task that would have escaped our attention if we didn‚Äôt start analyzing the model errors.  Such tools are not perfect, but they can greatly facilitate the understanding of the model and make <a href="https://www.youtube.com/watch%3Fv%3DhnSgIUA57hg">more correct decisions</a> . <br><br><h2>  Documenting and presenting results </h2><br>  In many projects little attention is paid to documentation and reports.  You can do the best analysis in the world, but if you do not <a href="http://blog.kaggle.com/2016/06/29/communicating-data-science-a-guide-to-presenting-your-work/">present the results properly</a> , they will not matter! <br><br>  When documenting a data analysis project, we package all versions of the data and code so that other people can reproduce or collect the project.  Remember that the code is read more often than they write, so our work should be clear to other people, and to us, if we return to it in a few months.  Therefore, insert useful comments into the code and explain your decisions.  <a href="http://jupyter.org/">Jupyter Notebooks</a> are a great tool for documenting, they allow you to first explain solutions, and then show the code. <br><br>  Also, Jupyter Notebook is a good platform for interacting with other specialists.  With the help of <a href="https://github.com/ipython-contrib/jupyter_contrib_nbextensions">extensions for notebooks,</a> you can <a href="https://github.com/kirbs-/hide_code">hide the code from the final report</a> , because, no matter how hard you believe it, not everyone wants to see a bunch of code in the document! <br><br>  You might not want to squeeze, but show all the details.  However, it is important <a href="http://sites.ieee.org/pcs/communication-resources-for-engineers/audience-purpose-and-context/understand-your-audience/">to understand your audience</a> when you submit your project, and <a href="https://hbr.org/2015/04/the-best-presentations-are-tailored-to-the-audience">compile a report accordingly</a> .  Here is an example of a brief summary of the essence of our project: <br><br><ol><li>  Using data on the energy consumption of buildings in New York, one can construct a model that predicts the number of Energy Star Score scores with an error of 9.1 points. </li><li>  Site EUI and Weather Normalized Electricity Intensity are the main factors affecting the forecast. </li></ol><br>  We wrote the detailed description and conclusions to Jupyter Notebook, but instead of PDF, we converted the <a href="https://www.latex-project.org/">Latex</a> .tex file, which we then edited to <a href="https://www.texstudio.org/">texStudio</a> , and the <a href="https://github.com/WillKoehrsen/machine-learning-project-walkthrough/blob/master/Building%2520Data%2520Report.pdf">resulting version was</a> converted to PDF.  The fact is that the default export result from Jupyter to PDF looks pretty decent, but it can be greatly improved in just a few minutes of editing.  In addition, Latex - a powerful document preparation system, which is useful to own. <br><br>  Ultimately, the value of our work is determined by the decisions that it helps to make, and it is very important to be able to ‚Äúpresent the goods by face‚Äù.  By correctly documenting, we help other people to reproduce our results and give us feedback, which will allow us to become more experienced and continue to rely on the results obtained. <br><br><h2>  findings </h2><br>  In our series of publications, we have disassembled an educational project on machine learning from the beginning to the end.  We started with data cleansing, then created a model, and finally learned to interpret it.  Recall the overall structure of the machine learning project: <br><br><ol><li>  Cleaning and formatting data. </li><li>  Exploratory data analysis. </li><li>  Design and selection of features. </li><li>  Comparison of metrics of several machine learning models. </li><li>  Hyperparametric adjustment of the best model. </li><li>  Evaluate the best model on the test dataset. </li><li>  Interpreting the results of the model. </li><li>  Conclusions and a well-documented report. </li></ol><br>  The set of steps may vary depending on the project, and machine learning is often an iterative process rather than a linear one, so this guide will help you in the future.  We hope you can now confidently implement your projects, but remember: no one acts alone!  If you need help, there are many very useful communities where you can get advice. <br><br>  These sources can help you: <br><br><ul><li>  <a href="http://shop.oreilly.com/product/0636920052289.do">Hands-on Machine Learning with Scikit-Learn and Tensorflow</a> ( <a href="https://github.com/ageron/handson-ml">Jupyter Notebook for this book</a> are available for download for free)! </li><li>  <a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a> </li><li>  <a href="https://www.kaggle.com/">Kaggle: The Home of Science and Machine Learning</a> </li><li>  <a href="https://www.datacamp.com/">Datacamp</a> : Good Practices for <a href="https://www.datacamp.com/">Practicing</a> Data Analysis Programming. </li><li>  <a href="https://www.coursera.org/">Coursera</a> : free and paid courses on many topics. </li><li>  <a href="https://www.udacity.com/">Udacity</a> : paid courses in programming and data analysis. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/426771/">https://habr.com/ru/post/426771/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../426753/index.html">Mail Design Cup 2018 - Competition for interface designers</a></li>
<li><a href="../426755/index.html">The developers of Windows 10 have implemented protection against ransomware viruses. It can be circumvented by injection DLL</a></li>
<li><a href="../426759/index.html">The image of a modern tester. What you need to know and be able to</a></li>
<li><a href="../426765/index.html">Basics of programming on the SAS Base. Lesson 4: Creating SAS datasets</a></li>
<li><a href="../426769/index.html">The book "Minecraft. Program your world in Python. 2nd international edition ¬ª</a></li>
<li><a href="../426773/index.html">Tips for professional use RecyclerView. Part 2</a></li>
<li><a href="../426777/index.html">Bad advice on communication with technical support</a></li>
<li><a href="../426779/index.html">How does the station</a></li>
<li><a href="../426781/index.html">Google will separately sell licenses for the Google Apps package and search with a browser</a></li>
<li><a href="../426783/index.html">Neurotic Bikes: Genesis</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>