<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Hardware acceleration of deep neural networks: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP and other letters</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="On May 14, when Trump prepared to unleash all the dogs at Huawei, I peacefully sat in Shenzhen at the Huawei STW 2019 - a large conference for 1000 pa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Hardware acceleration of deep neural networks: GPU, FPGA, ASIC, TPU, VPU, IPU, DPU, NPU, RPU, NNP and other letters</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/1d8/7a7/de6/1d87a7de6c72f1f712049ce550978d7c.png"><br><br>  On May 14, when Trump prepared to unleash all the dogs at Huawei, I peacefully sat in Shenzhen at the Huawei STW 2019 - a large conference for 1000 participants - which included reports by <a href="https://www.bloomberg.com/research/stocks/people/person.asp%3FpersonId%3D577547169%26capId%3D380075%26previousCapId%3D380075%26previousTitle%3DTaiwan%2520Semiconductor%2520Manufacturing%2520Company%2520Limited">Philip Wong</a> , vice president of TSMC research on the prospects for non-von Neumann computing architectures, and Heng Liao, Huawei Fellow, Chief Scientist Huawei 2012 Lab, on the development of a new architecture of tensor processors and neuroprocessors.  TSMC, if you know, makes neural accelerators for Apple and Huawei using 7 nm technology (which <a href="https://en.wikichip.org/wiki/7_nm_lithography_process">few people own</a> ), and Huawei is ready to compete with Google and NVIDIA for neuroprocessors. <br><br>  Google in China is banned, I did not bother to put a VPN on a tablet, so I <s>patriotically</s> used Yandex to see what the situation was with other manufacturers of similar hardware, and what was going on.  In general, I followed the situation, but only after these reports I realized how large the revolution was being prepared in the depths of companies and the silence of scientific cabinets. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Last year alone, more than $ 3 billion was invested in the topic.  Google has long declared neural networks to be a strategic area, actively building their hardware and software support.  NVIDIA, feeling that the throne was staggering, invests fantastic efforts in libraries of accelerating neural networks and new hardware.  Intel in 2016 spent 0.8 billion on the purchase of two companies engaged in hardware acceleration of neural networks.  And this is despite the fact that the main purchases have not yet begun, and the number of players has exceeded fifty and is growing rapidly. <br><br><div style="text-align:center;"><img width="66%" src="https://habrastorage.org/getpro/habr/post_images/74c/308/a37/74c308a372700574cc0e29f13347ede5.png"></div><br>  TPU, VPU, IPU, DPU, NPU, RPU, NNP - what does all this mean and who will win?  Let's try to figure it out.  Who cares - Wellcome under the cat! <br><a name="habracut"></a><br><hr>  <b><font color="#ff0000">Disclaimer: The</font></b> author had to completely rewrite the video processing algorithms for effective implementation on ASIC, and clients did prototyping on FPGA, so there is an idea of ‚Äã‚Äãthe depth of the difference of architectures.  However, the author has not worked directly with iron lately.  But I anticipate that I will have to delve into it. <br><br><h2>  Background problems </h2><br>  The number of required calculations is growing rapidly, people would gladly take more layers, more options for architecture, more actively play with hyper parameters, but ... run into performance.  At the same time, for example, with the growth of the performance of good old processors - big problems.  All good things come to an end: Moore's law, as you know, dries up and the growth rate of processor performance drops: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/704/8e8/bab/7048e8bab326e1f4f62058e5f3a925f4.png"></div><br>  <i>Calculations of the real performance of <a href="https://en.wikipedia.org/wiki/SPECint">SPECint</a> integer operations compared to <a href="https://en.wikipedia.org/wiki/VAX-11">VAX11-780</a> , hereafter often the logarithmic scale</i> <br><br>  If from the mid-80s to the mid-2000s ‚Äî in the blessed years of the heyday of computers ‚Äî growth was at an average speed of 52% per year, in recent years it has decreased to 3% per year.  And this is a problem (the translation of the recent article by the patriarch of the theme of John Hennessy about the problems and prospects of modern architectures <a href="https://habr.com/ru/post/440760/">was on Habr√©</a> ). <br><br>  There are many reasons, for example, the frequency of processors has ceased to grow: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/a49/5d6/be5/a495d6be5c00ce436bb4c2f1f7f356fc.png"></div><br>  It became more difficult to reduce the size of transistors.  The last attack that drastically reduces performance (including the performance of already released CPUs) is (drum roll) ... that's right, security.  <a href="https://en.wikipedia.org/wiki/Meltdown_(security_vulnerability)">Meltdown</a> , <a href="https://en.wikipedia.org/wiki/Spectre_(security_vulnerability)">Specter</a> and <a href="https://en.wikipedia.org/wiki/TLBleed">other</a> <a href="https://en.wikipedia.org/wiki/Foreshadow_(security_vulnerability)">vulnerabilities</a> cause enormous damage to the growth rate of the computational power of the CPU ( <a href="https://www.itwire.com/security/83301-openbsd-disables-hyperthreading-support-for-intel-cpus-due-to-likely-data-leaks.html">an example of disabling hyperthreading</a> (!)).  The topic has become popular, and new vulnerabilities of this kind are found <i>almost monthly</i> .  And this is a nightmare, because it hurts the performance. <br><br>  At the same time, the development of many algorithms is firmly tied to the growth of processor power that has become habitual.  For example, many researchers today do not worry about the speed of algorithms - something will be invented.  And it would be okay when training - the networks become large and ‚Äúheavy‚Äù to use.  This is especially clearly seen in the video, for which most approaches, in principle, are not applicable at high speed.  And they make sense often only in real time.  This is also a problem. <br><br>  Similarly, new compression standards are being developed, which imply an increase in decoder power.  And if the power of the processors will not grow?  The older generation remembers how in the 2000s there were problems to play high-resolution video in the fresh then <a href="https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC">H.264</a> on older computers.  Yes, the quality was better with a smaller size, but on fast scenes the picture hung up or the sound was torn.  I have to communicate with the developers of the new <a href="https://en.wikipedia.org/wiki/Versatile_Video_Coding">VVC / H.266</a> (release is planned for the next year).  They do not envy. <br><br>  So, what is the coming age preparing for us in the light of a decrease in the growth rate of processor performance in the application to neural networks? <br><br><h2>  CPU </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/f1e/b44/44b/f1eb4444b3e5e320447c6f65fa4ef14c.png"><br><br>  Regular <a href="https://en.wikipedia.org/wiki/Central_processing_unit">CPU</a> is a great number crusher that has been refined over the decades.  Alas, for other tasks. <br><br>  When we work with neural networks, especially deep ones, our network can take hundreds of megabytes.  For example, the memory requirements for <a href="https://github.com/albanie/convnet-burden">object detection</a> networks are: <br><div class="scrollable-table"><table><tbody><tr><td>  model <br></td><td>  input size <br></td><td>  param memory <br></td><td>  feature memory <br></td></tr><tr><td>  <a href="">rfcn-res50-pascal</a> <br></td><td>  600 x 850 <br></td><td>  122 MB <br></td><td>  1 GB <br></td></tr><tr><td>  <a href="">rfcn-res101-pascal</a> <br></td><td>  600 x 850 <br></td><td>  194 MB <br></td><td>  2 GB <br></td></tr><tr><td>  <a href="">ssd-pascal-vggvd-300</a> <br></td><td>  300 x 300 <br></td><td>  100 MB <br></td><td>  116 MB <br></td></tr><tr><td>  <a href="">ssd-pascal-vggvd-512</a> <br></td><td>  512 x 512 <br></td><td>  104 MB <br></td><td>  337 MB <br></td></tr><tr><td>  <a href="">ssd-pascal-mobilenet-ft</a> <br></td><td>  300 x 300 <br></td><td>  22 MB <br></td><td>  37 MB <br></td></tr><tr><td>  <a href="">faster-rcnn-vggvd-pascal</a> <br></td><td>  600 x 850 <br></td><td>  523 MB <br></td><td>  600 MB <br></td></tr></tbody></table></div><br><br>  In our experience, deep neural network coefficients for processing <a href="http://videomatting.com/">semi</a> - <a href="http://videomatting.com/">transparent boundaries</a> can take 150‚Äì200 MB.  Colleagues in the neural network for determining age and sex size coefficients of the order of 50 MB.  And when optimizing for the low-precision mobile version, it is about 25 MB (float32‚áífloat16). <br><br>  In this case, the delay schedule for accessing the memory, depending on the size of the data, is distributed <a href="https://www.anandtech.com/show/8426/the-intel-haswell-e-cpu-review-core-i7-5960x-i7-5930k-i7-5820k-tested/2">approximately as follows</a> (horizontal scale is logarithmic): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/1f4/2aa/134/1f42aa13427972cd1762383b33cfe974.png"></div><br><br>  Those.  with an increase in data volume of more than 16 MB, the delay increases 50 or more times, which is fatal to performance.  In fact, most of the CPU time when working with deep neural networks is <s>stupidly</s> waiting for data.  Interesting <a href="https://www.intel.ai/introducing-int8-quantization-for-fast-cpu-inference-using-openvino/">Intel data</a> on the acceleration of different networks, where, in fact, acceleration goes only when the network becomes small (for example, as a result of quantizing weights), to start at least partially entering the cache along with the processed data.  Note that the cache of a modern CPU consumes up to half the processor power.  In the case of heavy neural networks, it is ineffective and works unreasonably expensive heater. <br><br><div class="spoiler">  <b class="spoiler_title">For adherents of neural networks on the CPU</b> <div class="spoiler_text">  Even <a href="https://software.intel.com/en-us/openvino-toolkit">Intel OpenVINO,</a> according to our internal tests, loses the implementation of the framework on matrix multiplication + NNPACK on many network architectures (especially on simple architectures where bandwidth is important for realtime processing of data in single-threaded mode).  This scenario is relevant for various classifiers of objects in the image (where the neural network needs to be run a large number of times - 50‚Äì100 by the number of objects in the image) and the overhead of starting OpenVINO becomes unreasonably large. <br></div></div><br>  <b>Pros:</b> <br><br><ul><li>  ‚ÄúEveryone has it‚Äù, and usually idle, i.e.  Relatively low <i>input</i> cost of calculations and implementation. <br></li><li>  There are some non-CV networks that fit the CPU well, colleagues call, for example, Wide &amp; Deep and GNMT. <br></li></ul><br>  <b>Minus:</b> <br><ul><li>  CPU is inefficient when working with deep neural networks (when the number of layers of the network and the size of the input data are large), everything works painfully slowly. <br></li></ul><br><h2>  GPU </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/aee/06a/d5b/aee06ad5beab0896005e75769b3ba910.png"><br><br>  The topic is well known, so we will quickly denote the main thing.  In the case of neural networks, the <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a> has a significant performance advantage on massively parallel tasks: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/717/03d/d9b/71703dd9bce82918d1e0c7138a09adf0.png"></div><br>  Notice how the 72-core <a href="https://en.wikipedia.org/wiki/Xeon_Phi">Xeon Phi 7290 is</a> annealed, while the ‚Äúblue‚Äù is also the server-side Xeon, i.e.  Intel does not give up so easily, as will be lower.  But more importantly, the memory of video cards was originally designed for about 5 times higher performance.  In neural networks, data calculations are extremely simple.  Several elementary actions, and we need new data.  As a result, the speed of access to data is critical for the efficient operation of a neural network.  The high-speed on-board memory of the GPU and a more flexible cache management system than on the CPU solve this problem: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/30a/d1b/07e/30ad1b07e7b22d97dc3372e4351c2e3c.png"></div><br><br>  For several years, Tim Detmers has supported an interesting review <a href="https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/">entitled ‚ÄúWhich GPU (s) for Deep Learning:</a> ‚Äú What kind of GPU is better for deep learning ... ‚Äù).  It is clear that Tesla and Titans rule for learning, although the difference in architectures can cause interesting bursts, for example, in the case of recurrent neural networks (and the leader in general TPU, we note for the future): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6e6/bb0/43a/6e6bb043aab85679d1ddf01028e19728.png"></div><br>  However, there is a very useful performance chart for a dollar, where <a href="https://en.wikipedia.org/wiki/GeForce_20_series">RTX is</a> on a horse (most likely due to <a href="https://en.wikipedia.org/wiki/GeForce_20_series">their</a> <a href="https://developer.nvidia.com/tensor-cores">Tensor cores</a> ), if you have enough memory, of course: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f13/338/032/f13338032ca812397cbbe0228bda9ed5.png"></div><br>  Of course, the cost of computing is important.  The second place of the first rating and the last of the second - <a href="https://en.wikipedia.org/wiki/Nvidia_Tesla">Tesla V100</a> is sold for 700 thousand rubles, like 10 "ordinary" computers (+ an expensive Infiniband switch, if you want to train on several nodes).  True V100 and works for ten.  People are willing to overpay for a noticeable acceleration of learning. <br><br>  Total, summarized! <br><br>  <b>Pros:</b> <br><ul><li>  The cardinal - 10‚Äì100 times faster work compared to the CPU. <br></li><li>  Extremely effective for learning (and somewhat less effective for use). <br></li></ul><br>  <b>Minus:</b> <br><ul><li>  The cost of top-end video cards (with enough memory to train large networks) exceeds the cost of the rest of the computer ... <br></li></ul><br><h2>  Fpga </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/0d5/554/90b/0d555490b015730051341cc857d14606.png"><br><br>  <a href="https://en.wikipedia.org/wiki/Field-programmable_gate_array">FPGA</a> is more interesting.  This is a network of several millions of programmable blocks that we can also programmatically interconnect.  The network and the blocks <a href="https://hal.archives-ouvertes.fr/hal-01695375/document">look</a> something like this (the thin place is Bottleneck, pay attention, again before the chip's memory, but everything is easier, which will be discussed below): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0d8/8f9/789/0d88f9789f60720da892b7acae7ab8ec.png"><br>  Naturally, it makes sense to use FPGA already at the stage of applying a neural network (in most cases there is not enough memory for training).  Moreover, the topic of implementation on the FPGA is now beginning to actively develop.  For example, here is the <a href="http://cas.ee.ic.ac.uk/people/sv1310/fpgaConvNet.html">fpgaConvNet framework</a> , which helps to dramatically speed up the use of CNN on FPGAs and at the same time reduce power consumption. <br><br>  The key advantage of the FPGA is that we can store the network directly in cells, i.e.  a thin place in the form of overloaded 25 times per second (for video) in the same direction hundreds of megabytes of the same data magically disappears.  This allows for a lower clock frequency and no caches, instead of a decrease in performance, to obtain a noticeable increase.  Yes, and dramatically reduce <s>global warming</s> energy consumption per unit of calculation. <br><br>  Intel was actively involved in the process, releasing the <a href="https://docs.openvinotoolkit.org/">OpenVINO Toolkit</a> in open source last year, which includes the <a href="https://github.com/opencv/dldt">Deep Learning Deployment Toolkit</a> (part of <a href="https://en.wikipedia.org/wiki/OpenCV">OpenCV</a> ).  And the performance on FPGA on different grids looks quite interesting, and the advantage of the FPGA over the GPU (albeit the integrated GPU from Intel) is very significant: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/8ba/a66/37e/8baa6637e904732e4e883a196b8d803d.png"></div><br>  What particularly warms the soul of the author - FPS are compared, i.e.  frames per second is the most practical metric for video.  Given that Intel bought <a href="https://en.wikipedia.org/wiki/Altera">Altera</a> , the second-largest player on the FPGA market in 2015, the graph provides good food for thought. <br><br>  And, obviously, the entrance barrier to such architectures is higher, so some time must pass in order for convenient tools to appear that effectively take into account fundamentally different FPGA architecture.  But you should not underestimate the potential of technology.  It pains a lot of thin places she embroider. <br><br>  Finally, we emphasize that <a href="https://habr.com/ru/hub/fpga/top/alltime/">programming an FPGA</a> is a separate art.  As such, the program is not executed there, and all calculations are done in terms of data flows, flow delays (which affects performance) and used gates (which are always lacking).  Therefore, to start programming effectively, you need to thoroughly <a href="https://habr.com/ru/post/439638/">change your own firmware</a> (in the neural network that is between the ears).  With good efficiency, it does not work for everyone.  However, new frameworks will soon hide from the researchers an external difference. <br><br>  <b>Pros:</b> <br><br><ul><li>  Potentially, faster network performance. <br></li><li>  The power consumption is noticeably lower compared to the CPU and GPU (this is especially important for mobile solutions). <br></li></ul><br>  <b>Minuses:</b> <br><br><ul><li>  Mostly help with the acceleration of performance, to train on them, unlike the GPU, is noticeably less convenient. <br></li><li>  More complex programming compared to previous versions. <br></li><li>  Noticeably less specialists. <br></li></ul><br><h2>  ASIC </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/dee/5e4/059/dee5e40597454e07651718c325f2ac3f.png"><br><br>  Next comes <a href="https://en.wikipedia.org/wiki/Application-specific_integrated_circuit">ASIC</a> - short for Application-Specific Integrated Circuit, i.e.  integrated circuit under our task.  For example, implementing the neural network put into iron.  However, most compute nodes can work in parallel.  In fact, only data dependencies and uneven computations at different levels of the network can prevent us from constantly using all the ALUs running. <br><br>  Perhaps the greatest advertisement of ASIC among the general public in recent years has been mining cryptocurrency mining.  At the very beginning, the mining on the CPU was quite profitable, later I had to buy GPUs, then FPGAs, and then specialized ASICs, since the people (read the market) were ripe for orders in which their production became profitable. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/de1/fda/062/de1fda062c797262a51047f57eac2f11.png"><br>  In our area, too, <a href="https://coredeeplearning.ai/">services</a> have appeared (naturally!) <a href="https://coredeeplearning.ai/">That</a> help us to put a neural network on iron with the necessary characteristics for power consumption, FPS and price.  Magically, agree! <br><br>  BUT!  We lose network customizability.  And, of course, people think about it too.  For example, here‚Äôs an article with the saying title ‚Äú <a href="https://www.semanticscholar.org/paper/Can-a-reconfigurable-architecture-beat-ASIC-as-a-Jafri-Hemani/e0fcc49202ed437910d6bdf165bcf16e7a9f7c5a">Can a reconfigurable architecture beat ASIC as a CNN accelerator?</a> ‚Äù (‚ÄúCan a configurable architecture beat ASIC like a CNN accelerator?‚Äù).  There is enough work on this topic, because the question is not idle.  The main disadvantage of ASIC is that after we have driven the network into iron, it becomes difficult for us to change it.  They are most beneficial for cases where an already well-established network is needed by millions of chips with low power consumption and high performance.  And this situation is gradually emerging in the market of auto pilots, for example.  Or in surveillance cameras.  Or in the cameras of robot vacuum cleaners.  Or in the chambers of the home refrigerator.  Or in the camera coffee makers.  <s>Or in the iron chamber.</s>  Well, you get the idea, in <a href="https://www.eenewsanalog.com/news/always-face-recognition-promised-new-ai-chip/page/0/1">short</a> ! <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/51a/b5e/949/51ab5e9497fee71cf9f23606d3de4fb6.png"></div><br><br>  It is important that in mass production the chip is cheap, works fast and consumes a minimum of energy. <br><br>  <b>Pros:</b> <br><br><ul><li>  The lowest cost of the chip compared with all previous decisions. <br></li><li>  The lowest power consumption per unit of operation. <br></li><li>  Quite a high speed of work (including, if desired, a record). <br></li></ul><br>  <b>Minuses:</b> <br><br><ul><li>  The possibilities of updating the network and logic are very limited. <br></li><li>  The highest development cost compared to all previous solutions. <br></li><li>  Using ASIC is cost effective mainly for large runs. <br></li></ul><br><h2>  TPU </h2><br>  Recall that when working with networks there are two tasks - training (training) and execution (inference).  If FPGA / ASIC are primarily aimed at speeding up execution (including some kind of fixed network), then TPU (Tensor Processing Unit or tensor processors) is either a hardware learning acceleration or relatively universal acceleration of an arbitrary network.  The name is beautiful, you see, although in fact, rank 2 c <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BD%25D0%25B7%25D0%25BE%25D1%2580">tensors</a> with Mixed Multiply Unit (MXU) connected to high-speed memory (High-Bandwidth Memory - HBM) are used.  Below is the TPU Google 2nd and 3rd version architecture diagram: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/f5e/29c/696/f5e29c696e265435fadbc2e56f67e12e.png"></div><br><h2>  TPU Google </h2><br>  In general, TPU made the advertisement for Google, revealing the internal development in 2017: <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4af/ea1/4ca/4afea14cad5dc0a7d941ae6b7963c171.png"></div><br>  They began preliminary work on specialized processors for neural networks with their words back in 2006, in 2013 they created a project with good funding, and in 2015 they started working with the first chips that greatly helped neural networks for the Google Translate cloud service and not only.  And this was, we emphasize, the acceleration <i>of</i> network <i>performance</i> .  An important advantage for data centers is two orders of magnitude higher TPU energy efficiency compared to CPU (graph for TPU v1): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/877/a91/f61/877a91f61ce45f2d04c7c3fe9df55348.png"></div><br>  Also, as a rule, compared to a GPU, <i>performance of the</i> network <i>performance</i> is 10‚Äì30 times better: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/10a/302/83b/10a30283b9c0fe8f0c7c1993f0bc68aa.png"></div><br>  The difference is even 10 times significant.  It is clear that the difference with the GPU in 20-30 times determines the development of this direction. <br><br>  And, fortunately, Google is not alone. <br><br><h2>  TPU Huawei </h2><br>  Now, the long-suffering Huawei also began the development of TPU several years ago under the name Huawei Ascend, and in two versions at once - for data centers (like Google) and for mobile devices (which Google also began to do recently).  If you believe the materials of Huawei, then they overtook fresh Google TPU v3 by FP16 by 2.5 times and NVIDIA V100 by 2 times: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bb8/692/66a/bb869266a2cfa5e28c2f086026309b01.png"><br><br>  As usual, a good question: how this chip will behave on real tasks.  For on the graph, as you can see, peak performance.  In addition, Google TPU v3 is good in many respects because it can work effectively in clusters of 1024 processors each.  Huawei also announced server clusters for the Ascend 910, but no details.  In general, Huawei engineers have shown themselves to be extremely literate over the past 10 years, and there is every chance that a 2.8 times greater peak performance compared to Google TPU v3, coupled with the latest 7 nm process technology, will be used in the case. <br><br>  Memory and data bus are critical for performance, and you can see from the slide that considerable attention is paid to these components (including the speed of communication with the memory noticeably faster than that of the GPU): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/98e/7bd/ab1/98e7bdab1c9e1a81e891bb72db3976c2.png"><br><br>  Also in the chip there is a slightly different approach - not two-dimensional MXU 128x128 are scaled, but calculations in a three-dimensional cube of a smaller size 16x16xN, where N = {16,8,4,2,1}.  Therefore, the key question is how well this will fall on the actual acceleration of specific networks (for example, calculations in a cube are convenient for images).  Also, a closer look at the slide shows that, unlike Google, the chip immediately starts working with compressed FullHD video.  For the author, this sounds <b>very</b> encouraging! <br><br>  As mentioned above, processors for mobile devices are developed in the same line, for which energy efficiency is critical, and on which the network will be mainly executed (that is, separately - processors for cloud learning and separately - for execution): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ea0/a3c/7cb/ea0a3c7cb72def7195afa2011ba02907.png"><br><br>  And according to this parameter, everything looks pretty good compared to NVIDIA at least (note that they didn‚Äôt compare Google with Google‚Äôs, however, TPU doesn‚Äôt give Google hands on a cloud).  And their mobile chips will compete with processors from Apple, Google and other companies, but it‚Äôs still too early to sum up. <br><br>  It is clearly seen that the new Nano, Tiny and Lite chips should be even better.  It becomes clear <s>why Trump was scared</s> why many manufacturers are carefully examining the successes of Huawei (overtaking in the US all iron-producing companies in the US, including Intel in 2018). <br><br><h2>  Analog Deep Networks </h2><br>  As you know, technology often develops in a spiral, when, at a new stage, old and forgotten approaches become relevant. <br><br>  Something similar may well happen with neural networks.  You may have heard that once the multiplication and addition operations were performed with electron tubes and transistors (for example, color space conversion ‚Äî a typical matrix multiplication ‚Äî was in every color television until the mid-90s)?  There was a good question: if our neural network is relatively resistant to inaccurate calculations inside, what if we translate these calculations into an analog form?  We immediately get a noticeable acceleration of calculations and a potentially dramatic reduction in energy consumption for performing a single operation: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/6b2/e08/aff/6b2e08afff1639668e02a548ed663fba.png"></div><br><br>  With this approach, DNN (Deep Neural Network) is calculated quickly and energy efficiently.  But there is a problem - it is DAC / ADC (DAC / ADC) - converters from digital to analog and vice versa, which reduce both energy efficiency and process accuracy. <br><br>  However, back in 2017, IBM Research <a href="https://arxiv.org/ftp/arxiv/papers/1706/1706.06620.pdf">offered analog CMOS</a> for RPU ( <a href="http://web.mit.edu/writing/gradexam/2016/readings/Resistive_Computing_PC_Magazine.pdf">Resistive Processing Units</a> ), which allow storing the processed data also in analog form and significantly increase the overall efficiency of the approach: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/0d3/720/3a8/0d37203a85a89791e8701098583e01ea.png"></div><br>  Also, in addition to analog memory, a reduction in the accuracy of a neural network can be of great help - this is the key to miniaturizing RPUs, which means increasing the number of computational cells on a chip.  And here IBM is also among the leaders, and in particular, recently this year they quite successfully hardened the network to 2-bit accuracy and are going to bring the accuracy to one-bit (and two-bit during training), which will potentially allow 100 times (!) Increase in performance compared to modern GPUs: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/1c4/1eb/c09/1c41ebc09f5e53d206534eefbb105eac.png"></div><br>  It‚Äôs still too early to talk about analog neurochips, because so far all of this is being tested at the level of early prototypes: <br><br><div style="text-align:center;"><img width="35%" src="https://habrastorage.org/getpro/habr/post_images/498/f6c/bdb/498f6cbdb53f20806cf41a78b7110e8b.png"></div><br>  However, potentially the direction of analog computing looks <b>extremely</b> interesting. <br><br>  The only thing that confuses is that this is IBM, <a href="https://patents.google.com/%3Fq%3D%2522resistive%2Bprocessing%2Bunit%2522%26q%3DRPU%26oq%3D%2522resistive%2Bprocessing%2Bunit%2522%2BRPU">which has already filed dozens of patents on the topic</a> .<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">According to the experience, due to the peculiarities of the corporate culture, they are relatively weakly cooperating with other companies and, owning some kind of technology, are more likely to slow down its development in others than to effectively share. For example, IBM at the time refused to license arithmetic compression for JPEG to the </font></font><a href="https://en.wikipedia.org/wiki/International_Organization_for_Standardization"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ISO</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> committee, </font><font style="vertical-align: inherit;">while in the draft standard there was an option with arithmetic compression. As a result, JPEG went into life with compression according to Huffman and pressed 10‚Äì15% worse than it could. The same situation was with video compression standards. And the industry massively switched to arithmetic compression in codecs only when 5 IBM patents expired 12 years later ... Let's hope that IBM will be more inclined to cooperate this time, and, accordingly, we </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">wish maximum success in the field to everyone who is not associated with IBM</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, the benefit of </font></font><a href="https://www.google.com/search%3Fq%3Danalog%2Bdnn%2B-ibm"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">such people and companies a lot</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If it works, </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">it will be a revolution in the use of neural networks and a revolution in many areas of computer science.</font></font></b> <br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Different other letters </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In general, the topic of accelerating neural networks has become fashionable; all large companies and dozens of start-ups are engaged in it, and </font></font><a href="https://www.nytimes.com/2018/01/14/technology/artificial-intelligence-chip-start-ups.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">at least 5 of them attracted more than $ 100 million in</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> investment by the beginning of 2018. In total, in 2017, startups associated with the development of chips were invested 1.5 billion dollars. Given that investors did not notice chip makers for a good 15 years (for there was nothing to catch there against the background of giants). In general - now there is a real chance for a small iron revolution. Moreover, it is extremely difficult to predict which architecture will win, the need for a revolution has matured, and the possibilities for increasing productivity are great. The classic revolutionary situation has matured: </font></font><a href="https://en.wikipedia.org/wiki/Gordon_Moore"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Moore is</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no longer able, but </font></font><a href="https://en.wikipedia.org/wiki/Jeff_Dean_(computer_scientist)"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dean</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is not yet ready.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Well, since the most important market law is different, many new letters have appeared, for example: </font></font><br><br><ul><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Neural Processing Unit ( </font></font><a href="https://en.wikipedia.org/wiki/NPU"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">NPU</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - Neuroprocessor, sometimes beautifully - neuromorphic chip - in general, the common name for the neural network accelerator, which is called </font></font><a href="https://www.theverge.com/circuitbreaker/2018/11/14/18094632/samsung-galaxy-s10-processor-exynos-9820-npu-ai"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Samsung</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="https://www.anandtech.com/show/13503/the-mate-20-mate-20-pro-review/4"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Huawei</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> chips </font><font style="vertical-align: inherit;">and the list goes on ...</font></font><br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/d28/900/72e/d2890072e95d7e63f70543b09ba759bb.png"></div> <i>                <b></b> </i> <br><br> ,    ,    ,      Apple  Huawei,     TSMC. ,    ,       2-8     : <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/ee/ga/iu/eegaiu5u_rw5trv0-exwjngc7sw.png"></div><br></li><li> <b>Neural Network Processor (NNP)</b> ‚Äî  . <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/917/1bb/ad2/9171bbad26941f97399ce80a56373e51.png"></div><br>     , , <a href="https://www.intel.ai/nervana-nnp/">Intel</a> (    <a href="https://en.wikipedia.org/wiki/Nervana_Systems">Nervana Systems</a> ,  Intel   2016  $400+ ). ,   <a href="https://www.researchgate.net/figure/AAC-Neural-Network-Processor-NNPR-Architecture_fig2_252593043"></a> ,   <a href="https://books.google.ru/books%3Fid%3DTZ6VDQAAQBAJ%26pg%3DPT357%26lpg%3DPT357%26dq%3Dneural%2Bnetwork%2Bprocessor%2BNNP%2B-intel%26source%3Dbl%26ots%3DbnkFUZPUsK%26sig%3DACfU3U0tXBmGgmRemGp0t0vmbSXI9ll4BQ%26hl%3Den%26sa%3DX%26ved%3D2ahUKEwidgs3A87jiAhXstYsKHUyDCsU4ChDoATAEegQICBAB"></a>  NNP   . <br></li><li> <b>Intelligence Processing Unit (IPU)</b> ‚Äî   ‚Äî  ,   <a href="https://en.wikipedia.org/wiki/Graphcore">Graphcore</a> (,   $310  ). <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/e93/18f/8e7/e9318f8e7711ea5b3a669946484c72bf.png"></div><br>      ,     ,     RNN  180‚Äì240  ,   NVIDIA P100. <br></li><li> <b>Dataflow Processing Unit (DPU)</b> ‚Äî     ‚Äî    <a href="https://www.crunchbase.com/organization/wave-semiconductor">WAVE Computing</a> ,   $203  .     ,   Graphcore: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/f53/e5e/25a/f53e5e25abb2fc2fee636d1949242be8.png"></div><br>     100  ,      25+  ,   GPU ( ,    1000 ). ‚Ä¶ <br></li><li> <b>Vision Processing Unit ( <a href="https://en.wikipedia.org/wiki/Vision_processing_unit">VPU</a> )</b> ‚Äî   : <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/80e/756/110/80e7561100a794bb005635899b06ec40.png"></div><br>      , , <a href="https://www.movidius.com/myriadx">Myriad X VPU</a>  Movidius (  <a href="https://en.wikipedia.org/wiki/Movidius"> Intel</a>  2016). <br></li><li>    IBM (, ,   <a href="http://web.mit.edu/writing/gradexam/2016/readings/Resistive_Computing_PC_Magazine.pdf">RPU</a> ) ‚Äî <a href="https://www.mythic-ai.com/about-us/"> Mythic</a> ‚Äî  <b>Analog DNN</b> ,            .     , <a href="https://medium.com/mythic-ai/mythic-hot-chips-2018-637dfb9e38b7"> </a> : <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/01e/ae6/253/01eae6253a1d7ef3e2dec9401857a33a.png"></div><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And it lists only the largest areas, in the development of which hundreds of millions have been invested (when developing iron, this is important). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In general, as we see, all the flowers are flourishing. </font><font style="vertical-align: inherit;">Gradually, companies will digest billions of dollars in investment (usually it takes 1.5‚Äì3 years to produce chips), the dust will settle, the leader will become clear, the winners will write history as usual, and the name of the technology most successful on the market will become generally accepted. </font><font style="vertical-align: inherit;">This has already happened more than once (‚ÄúIBM PC‚Äù, ‚ÄúSmartphone‚Äù, ‚ÄúXerox‚Äù, etc.).</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> A couple of words about the correct comparison </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As noted above, it is not easy to compare the performance of neural networks correctly. </font><font style="vertical-align: inherit;">Exactly why Google publishes a graph on which the TPU v1 makes the NVIDIA V100. </font><font style="vertical-align: inherit;">NVIDIA, seeing such a disgrace, publishes a chart where Google TPU v1 loses V100. </font><font style="vertical-align: inherit;">(Duc!) Google publishes the following graph, where V100 loses Google TPU v2 &amp; v3 with a bang. </font><font style="vertical-align: inherit;">And finally, Huawei is a graph where everyone loses Huawei Ascend, but V100 is better than TPU v3. </font><font style="vertical-align: inherit;">Circus, in short. </font><font style="vertical-align: inherit;">What is characteristic - there is </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> truth in </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">every</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> chart! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The root causes of the situation are clear:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> You can measure the speed of learning or the speed of execution (whichever is more convenient). </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> You can measure different neural networks, since the speed of implementation / training of different neural networks on specific architectures may differ significantly due to the network architecture and the amount of data required. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> And you can measure the peak performance of the accelerator (perhaps the most abstract value of all of the above). </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As an attempt to establish order in this zoo, the </font></font><a href="https://mlperf.org/results/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MLPerf</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> test </font><a href="https://mlperf.org/results/"><font style="vertical-align: inherit;">appeared</font></a><font style="vertical-align: inherit;"> , for which version 0.5 is now available, i.e. </font><font style="vertical-align: inherit;">It is in the process of developing a comparison methodology, which is planned to be completed in the </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">third quarter of this year</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> before the first release </font><font style="vertical-align: inherit;">:</font></font><br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/d14/e35/575/d14e3557543c05b83b99f12dc9e572ce.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Since there is one of the main contributors of TensorFlow in the authors, there is every chance of finding out what the fastest way to train and possibly use (for the mobile version of TF will probably also be included in this test over time). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Recently, the international organization </font></font><a href="https://www.ieee.org/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">IEEE</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , which publishes the third part of the world technical literature on electronics, computers and electrical engineering, is not childishly </font></font><a href="https://habr.com/ru/news/t/454230/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">banned by Huawei</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , soon, however, </font></font><a href="https://www.ieee.org/about/news/2019/statement-update-ieee-lifts-restrictions-on-editorial-and-peer-review-activities.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">having canceled a</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ban. </font><font style="vertical-align: inherit;">In the </font></font><a href="https://mlperf.org/results/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">current</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> MLPerf rating, Huawei is not yet available, while Huawei TPU is a serious competitor to Google TPU and NVIDIA cards (that is, besides political ones, there are also economic reasons to ignore Huawei, let's face it). </font><font style="vertical-align: inherit;">With undisguised interest we will follow the development of events!</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All in the sky! </font><font style="vertical-align: inherit;">Closer to the clouds!</font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> And, since we are talking about training, it is worth saying a few words about its specificity: </font></font><br><br><ul><li> With the rampage of research into deep neural networks (with dozens and hundreds of layers that really break everyone), it was necessary to grind hundreds of megabytes of coefficients, which immediately made all the processors caches of previous generations ineffective.  At the same time, a classic ImageNet is <a href="https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html">reasoned</a> about a strict correlation between the size of the network and its accuracy (the higher, the better, the more to the right, the larger the network, the horizontal axis is logarithmic): <br><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/5fa/788/84f/5fa78884f561b6f93dfa126be53376f4.png"></div><br></li><li>  The course of computations inside the neural network goes in a fixed pattern, i.e.  where all ‚Äúbranching‚Äù and ‚Äútransitions‚Äù (in terms of the last century) will occur in the overwhelming majority of cases, it is precisely known in advance, which leaves without work the speculative execution of instructions that previously markedly increases productivity: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/ss/-6/9n/ss-69n-vr5c3rszuvmhkaomtct0.png"></div><br>  This makes inefficient the sophisticated <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2583%25D0%25BF%25D0%25B5%25D1%2580%25D1%2581%25D0%25BA%25D0%25B0%25D0%25BB%25D1%258F%25D1%2580%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C">superscalar</a> mechanisms of predicting branching and predictions of previous <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2583%25D0%25BF%25D0%25B5%25D1%2580%25D1%2581%25D0%25BA%25D0%25B0%25D0%25BB%25D1%258F%25D1%2580%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C">decades of</a> improving processors (this part of the chip also, unfortunately, on DNN rather contributes to global warming, as does the cache). <br></li><li>  At the same time, neural network training is relatively weakly <a href="https://habr.com/ru/company/oleg-bunin/blog/319526/">scaled horizontally</a> .  Those.  we can not take 1000 powerful computers and get the acceleration of learning 1000 times.  And even in 100 we cannot (at least the theoretical problem of deterioration in the quality of education on a large amount of batch is not solved yet).  It‚Äôs generally quite difficult for us to distribute something across several computers, because as soon as the access speed to a single memory in which the network is located decreases, the speed of its learning drops dramatically.  Therefore, if the researcher has access to 1000 powerful computers <s>for free</s> , he will certainly take them all soon, but most likely (if there is no infiniband + RDMA) there will be many neural networks with different hyperparameters there.  Those.  the total training time will be only several times less than with 1 computer.  There is possible a game with the size of a batch, and additional training, and other new modern technologies, but the main conclusion is yes, with an increase in the number of computers, the work efficiency and the probability of achieving results will increase, but not linearly.  And today the time of the Data Science researcher is expensive and often if you can spend a lot of cars (albeit unwisely), but to get acceleration is done (see the example with 1, 2 and 4 expensive V100 in the clouds just below). <br></li></ul><br>  Exactly these moments explain why so many people rushed towards the development of specialized iron for deep neural networks.  And why they got their billions.  The light at the end of the tunnel is really visible there and not only at Graphcore (which, we recall, RNN training was accelerated 240 times). <br><br>  For example, gentlemen from IBM Research <a href="https://blocksandfiles.com/2019/02/11/ibms-ai-chips-change-phase/">are optimistic</a> , to develop special chips that will increase the efficiency of computing by an order of magnitude already after 5 years (and 10 years later by 2 orders of magnitude, having reached an increase of 1000 times compared to the level of 2016, on this graph, though , in efficiency per watt, but the power of the cores will also increase): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/51c/81f/226/51c81f2267b99ba00231532e606cf7fd.png"></div><br>  All this means the emergence of glands, the training on which will be relatively fast, but which will cost much, which naturally leads to the idea of ‚Äã‚Äãsharing the time of using this expensive piece of iron between researchers.  And this idea today no less naturally leads us to cloud computing.  And the transition of learning to the clouds has long been active. <br><br>  Note that already now the training of the same models may differ in time by an order of magnitude with different cloud services.  Below is the lead Amazon, and in the last place free Colab from Google.  Notice how the result changes from the number of V100 among the leaders - an increase in the number of cards by 4 times (!) Increases productivity by less than a third (!!!) from blue to purple, and by Google even less: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/78d/892/d83/78d892d833c0858572548aee7fca2697.png"></div><br>  It seems that in the coming years the difference will grow to two orders of magnitude.  Lord  Cooking money!  Together we will return multi-billion investments to the most successful investors ... <br><br><h2>  In short </h2><br>  Let's try to summarize the key points in the table: <br><div class="scrollable-table"><table><tbody><tr><td>  Type of <br></td><td>  What speeds up <br></td><td>  Comment <br></td></tr><tr><td>  CPU <br></td><td>  Mostly performance <br></td><td>  Usually the worst in speed and energy efficiency, but quite suitable for small neural networks <br></td></tr><tr><td>  GPU <br></td><td>  Execution + <br>  training <br></td><td>  The most universal solution, but rather expensive, both in terms of computation cost and energy efficiency. <br></td></tr><tr><td>  Fpga <br></td><td>  Performance <br></td><td>  Relatively versatile solution for network performance, in some cases allows to drastically speed up the execution <br></td></tr><tr><td>  ASIC <br></td><td>  Performance <br></td><td>  The cheapest, fastest, and most energy efficient network implementation option, but large runs are needed. <br></td></tr><tr><td>  TPU <br></td><td>  Execution + <br>  training <br></td><td>  The first versions were used to speed up execution, now they are used to speed up execution and training very effectively. <br></td></tr><tr><td>  IPU, DPU ... NNP <br></td><td>  Mostly training <br></td><td>  Many marketing letters that will be safely forgotten in the coming years.  The main advantage of this zoo is checking the different directions of DNN acceleration. <br></td></tr><tr><td>  Analog DNN / RPU <br></td><td>  Execution + <br>  training <br></td><td>  Potentially analogue accelerators can revolutionize the speed and energy efficiency of performing and training neural networks. <br></td></tr></tbody></table></div><br><h2>  A few words about software acceleration </h2><br>  In fairness, we mention that today a big topic is the software acceleration of the implementation and training of deep neural networks.  Implementation can be significantly accelerated primarily due to the so-called quantization of the network.  Perhaps this is, first, because the range of weights used is not so large and it is often possible to roughen weights from a 4-byte floating-point value to 1 byte integer (and, recalling IBM's successes, even stronger).  Secondly, the trained network as a whole is quite resistant to noise in the calculations and the accuracy of work during the transition to <a href="https://doc.embedded-wizard.de/int8-int16-int32-type%3Fv%3D9.00">int8</a> drops slightly.  At the same time, despite the fact that the number of operations may even increase (due to scaling when counting), the fact that the network decreases in size by 4 times and can be considered as fast vector operations significantly increases the overall execution speed.  This is especially important for mobile applications, but it also works in the clouds (an example of speeding up execution in Amazon clouds): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/e52/bbd/2cc/e52bbd2cc36e24eff9a788c2d8371c83.png"></div><br>  There are other ways to <a href="https://www.google.com/search%3Fq%3Ddeep%2Blearning%2Binference%2Bspeedup">accelerate network execution</a> and even more ways to <a href="https://www.google.com/search%3Fq%3Ddeep%2Blearning%2Btraining%2Bspeed-up">accelerate learning</a> .  However, these are separate big topics about which not this time. <br><br><h2>  Instead of conclusion </h2><br>  In his lectures, the investor and author <a href="https://tonyseba.com/biography/">Tony Seb</a> gives a great example: in 2000, the supercomputer ‚Ññ1 with a capacity of 1 teraflops occupied 150 square meters, cost $ 46 million and consumed 850 kW: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/2v/6y/xe/2v6yxeo59aewcqngaybonbftsae.png"></div><br>  15 years later, NVIDIA GPU with a capacity of 2.3 teraflops (2 times more) was placed in the hand, cost $ 59 (improvement about a million times) and consumed 15 W (improvement 56 thousand times): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/webt/gx/wz/qq/gxwzqqj1si3e3ho33nnypq_au3w.png"></div><br>  In March of this year, <a href="https://cloud.google.com/blog/products/ai-machine-learning/googles-scalable-supercomputers-for-machine-learning-cloud-tpu-pods-are-now-publicly-available-in-beta">Google introduced TPU Pods</a> - in fact, liquid-cooled supercomputers based on TPU v3, a key feature of which is that they can work together on 1024 TPU systems.  They look pretty impressive: <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/37c/849/2ec/37c8492ec7e380c5a26f8a420fc591d9.png"></div><br>  Exact data is not given, but it is said that the system is comparable with the Top-5 supercomputers in the world.  TPU Pod allows you to dramatically increase the speed of learning neural networks.  To increase the speed of interaction, TPUs are connected by high-speed highways to a toroidal structure: <br><br><img width="25%" src="https://habrastorage.org/getpro/habr/post_images/0f0/e26/532/0f0e2653272b633b8af1b6103540e95c.gif"><br>  It seems that in 15 years this neuroprocessor twice as big in performance can also fit in your hand, like the <a href="https://terminator.fandom.com/wiki/Neural_Net_CPU">Skynet processor</a> (agree with something similar): <br><br><div style="text-align:center;"><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/cf2/5da/db6/cf25dadb60a350332cfef6bd147ec91b.png"></div>  <i>Shot from the director's version of the movie <a href="http://www.jamescamerononline.com/T2FAQ.htm">"Terminator-2"</a></i> <br><br>  Given the current rate of improvement of hardware accelerators of deep neural networks and the example above, this is completely realistic.  There is every chance in a few years to take in hand a chip with a performance like today's TPU Pod. <br><br>  By the way, it's funny that in the film the chip makers (apparently, imagining where the network can lead to self-study) by default turned off the additional training.  It is characteristic that the <a href="https://terminator.fandom.com/wiki/T-800">T-800</a> itself could not turn on the training mode and worked in the inference mode (see the longer <a href="https://terminator.fandom.com/wiki/Terminator_2:_Judgment_Day_(film)">directorial version</a> ).  Moreover, his <a href="https://terminator.fandom.com/wiki/Neural_Net_CPU">neural-net processor</a> was advanced and, with the inclusion of pre-training, could use previously accumulated data to update the model.  Not bad for 1991. <br><br>  This text was started in the hot 13-million Shenzhen.  I was sitting in one of the 27,000 electric vehicles in the city, and with great interest I looked at the 4 LCD screens of the car.  One small one is among the devices in front of the driver, two are centrally located in the dashboard and the last one is translucent in the rearview mirror combined with a DVR, interior surveillance camera and android on board (judging by the top line with the charge level and connection to the network).  There the driver's data was displayed (to whom to complain, if that), a fresh weather forecast and, it seems, there was a connection with the taxis.  The driver did not know English, and asked him about the impressions of the electric machine did not work.  Therefore, he idly pressed the pedal, slightly pushing the car in traffic.  And I watched with interest the futuristic view of the window - the Chinese in jackets were driving from work on electric scooters and monowheels ... and wondered how it would look like in 15 years ... <br><br>  Actually, already today the rear-view mirror, using the data of the DVR camera and <i>hardware acceleration of neural networks</i> , is quite able to control the car in a traffic jam and create a route.  Happy at least).  After 15 years, the system will obviously not only be able to drive a car, but will also gladly provide me with the characteristics of fresh Chinese electric cars.  In Russian, of course (as an option: English, Chinese ... Albanian, finally).  The driver is superfluous, poorly trained, link. <br><br>  Lord  Waiting for us <b>EXTREMELY INTERESTING</b> 15 years! <br><br>  Stay tuned! <br><br>  I'll be back!  ))) <br><br><img width="35%" src="https://habrastorage.org/getpro/habr/post_images/3e8/caf/2cd/3e8caf2cde12f7b9bce6cd64de106357.png"><br><br><div class="spoiler">  <b class="spoiler_title">Acknowledgments</b> <div class="spoiler_text">  I would like to sincerely thank: <br><br><ul><li>  Laboratory of Computer Graphics VMK MSU.  MV Lomonosov for his contribution to the development of computer graphics in Russia and not only, <br></li><li>  our colleagues, Mikhail Erofeev and Nikita Bagrov, whose examples are used above, <br></li><li>  personally Konstantin Kozhemyakov, who did a lot to make this article better and clearer, <br></li><li>  and, finally, many thanks to Alexander Bokov, Mikhail Erofeev, Vitaly Lyudvichenko, Roman Kazantsev, Nikita Bagrov, Ivan valiant, Egor Sklyarov, Alexei Solovyov, Eugene Lyapustin, Sergei Lavrushkina and Nicholas Oplachko for a large number of fittings of comments and edits to make this text a lot it is better! <br></li></ul><br></div></div></div><p>Source: <a href="https://habr.com/ru/post/455353/">https://habr.com/ru/post/455353/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455343/index.html">Training Cisco 200-125 CCNA v3.0. Day 9. The physical world of switches. Part 2</a></li>
<li><a href="../455345/index.html">Caution Doctor</a></li>
<li><a href="../455347/index.html">Functional interfaces ... in VBA</a></li>
<li><a href="../45535/index.html">TV program for your Mac</a></li>
<li><a href="../455351/index.html">VMware EMPOWER 2019 - the main announcements and conclusions of the last conference</a></li>
<li><a href="../455355/index.html">Cable TV networks for the smallest. Part 8: Optical Trunk Network</a></li>
<li><a href="../455359/index.html">Functional Swift is just</a></li>
<li><a href="../455361/index.html">We do the extension for the browser, checking the results of the exam</a></li>
<li><a href="../455367/index.html">VueJs + MVC minimum code maximum functionality</a></li>
<li><a href="../455369/index.html">Certification of database administrators and much more on the anniversary DevConfX (June 21-22 in Moscow)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>