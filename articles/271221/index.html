<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to make the test environment as close as possible to the combat</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="One of the possibilities for improving the quality of the product being produced is matching the environments on the combat servers and in the testing...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to make the test environment as close as possible to the combat</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/887/7f6/a75/8877f6a75ba94f69a481fca6d3b46b50.jpg" alt="image"><br><br>  One of the possibilities for improving the quality of the product being produced is matching the environments on the combat servers and in the testing environment.  We tried to minimize the number of errors associated with the difference in configurations by moving from our old test environment, where the settings of services were very different from the battle, to the new environment, where the configuration almost corresponds to the battle.  We did this with the help of docker and ansible, got a lot of profit, but did not avoid various problems.  I will try to tell about this transition and interesting details in this article. <br><a name="habracut"></a><br>  At HeadHunter, about 20 engineers are involved in the testing process: manual testing specialists, automatics testers, and a small QA team that develops its process and infrastructure.  The infrastructure is quite extensive - about 100 test benches with their own tools for their deployment, more than 5,000 autotests with the ability to run them by any technical department employee. <br><br>  On the other hand, production, where even more servers and where the price of the error is much higher. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      What do our testers want?  Release the product without bugs and downtime in a combat environment.  And they achieve this on the current test environment.  However, sometimes they skip problems that cannot be caught in this environment.  Such as configuration changes. <br><br><img src="https://habrastorage.org/files/290/56f/cb9/29056fcb92c6482dbaac914fcdc9b67a.png" alt="image"><br><br>  The old test environment had a number of features: <br><br><ul><li>  Its configuration files for applications that are different from production and require constant support. </li><li>  Lack of application deployment testing. </li><li>  The absence of some applications that exist and work in a combat environment. </li><li>  Completely your web server and webdav test configs. </li></ul><br><br>  All these points led to various bugs, the most unpleasant of which was almost an hour of simple site due to the incorrect nginx config. <br><br>  What I wanted from the new environment: <br><br><ul><li>  use of configuration as close as possible to the combat environment (the same nginx locations, balancers, etc.); </li><li>  the ability to do everything that we could do on the old environment, and even a little more - to test the application application and their interaction; </li><li>  the relevance of the configuration in the test environment relative to the combat environment; </li><li>  ease in supporting change. </li></ul><br><h2>  The beginning of the way </h2><br>  The idea of ‚Äã‚Äãusing a configuration with production in a test environment appeared quite a long time ago (about 3 years ago), but there were other more important tasks, and we did not take on the implementation of this idea. <br><br>  And we took it by chance, deciding to try a new technological trend in the form of a docker at the time when version 0.8 was released. <br><br>  To understand what is so special about our test environment, we need to know a little bit about what hh.ru production consists of. <br><br>  Production of hh.ru is a fleet of servers on which separate parts of the site are running - applications, each of which is responsible for its own function.  Applications communicate with each other via http. <br><br>  The idea was to recreate the same structure, replacing the server park with lxc containers.  And so that it would be convenient to manage this whole economy, use an add-on called docker. <br><br><h2>  Not quite docker-way </h2><br>  We did not follow the true docker-way, when one service was launched in one container.  The task was to make test benches similar to the production and service infrastructure. <br><br><img src="https://habrastorage.org/files/850/dc9/010/850dc9010911448cbaca82d8334aa391.png" alt="image"><br><br>  The operation service uses ansible for application deployment and server configuration.  The order of display and configs patterns for each service is described in the yaml file, in the terminology of ansible - playbook.  It was decided to use the same playbooks in the test environment.  The flexibility here is in the ability to replace values ‚Äã‚Äãand have a completely different, test set of variable values. <br><br>  As a result, we have formed several docker images - a base image in the form of ubuntu with init &amp; ssh, with a test database, with cassandra, with a deb-repository and a maven-repository, which are available in the internal docker registry.  From these images we create the number of containers we need.  Then ansible rolls out the necessary playbooks, to the containers-hosts specified in inventory (list of server groups with which ansible will work). <br><br>  All running containers are in the same (internal for each host machine) network, and each container has a fixed IP address and DNS name ‚Äî and this is also not a docker-way.  To completely recreate the production infrastructure, some containers even have the IP addresses of the combat servers. <br><br><img src="https://habrastorage.org/files/922/91b/f81/92291bf81154471ab5c12fb71d8a6a88.png" alt="image"><br><br>  You can access containers by ssh directly from the host machine.  Inside the container there is the same location of the files as on the production servers, the same versions of applications and dependencies. <br><br>  For easy search of errors on the host machine, directories with logs from each container are aggregated using the mechanism of symbolic links.  Each request for a production has a unique number, and now the same system is available in the test environment, which allows having the request id to find its mention in the logs of all services. <br><br>  We create and configure the host machine for containers from our test bench management system using vmbuilder &amp; ansible.  New machines took more RAM due to additional services that were not previously in the test environment. <br><br>  To facilitate the work, a lot of scripts (python, shell) were written - to start playbooks (so testers didn't memorize various tags and ansible launch parameters), to work with containers, to create the whole environment from scratch and update, update and restore test databases and etc. <br><br>  The assembly of services from the code for testing is also automated here, and the new environment uses exactly the same build program as for the release of releases for production.  The build is done in a special container, where all the necessary build dependencies are installed. <br><br><h2>  What problems were and how we solved them </h2><br>  <b>Container IPs and their permanent list.</b> <br><br>  The default docker gives the IP addresses from the subnet in random order and the container can get any address.  In order to somehow tie up in the DNS zone, we decided to send each container its unique, assigned to it address specified in the config file. <br><br>  <b>Parallel run ansible.</b> <br>  The operation service never encounters the task of creating a production from scratch, so they never run all the ansible-playbooks at once.  We tried, and the sequential launch of playbooks to deploy all services took about 3 hours.  Such a time did not suit us completely when deploying a test environment.  It was decided to launch playbooks in parallel.  And here we were again waited by an ambush: if we start everything in parallel, then some services do not start due to the fact that they are trying to establish a connection with other services that have not yet had time to start. <br><br>  Designating an approximate map of these dependencies, we selected groups of services and launch the playbooks in parallel within these groups. <br><br>  <b>Tasks that are missing from the application release process by the maintenance service.</b> <br>  The operation service is engaged in the deployment of already assembled applications in deb-packages, the tester, as well as the developer, has to build applications from the source code, while assembling there are other dependencies regarding those that are necessary during installation.  To solve the assembly problem, we added another container, in which we put all the necessary dependencies.  A separate script collects the package inside this container from the source of the currently needed application.  Then the package is poured into the local repository and rolled out on those containers where it is needed. <br><br>  <b>Problems with increased iron requirements.</b> <br>  In view of the larger number of services (balancers) and other application settings with respect to the old test environment, more RAM was needed for the new stands. <br><br>  <b>Problems with timeouts.</b> <br>  In production, each service is on its own server, quite powerful.  In a test environment, services in containers share common resources.  The established timeouts in production turned out to be inoperative in the test environment.  They were rendered into variables ansible and modified for the test environment. <br><br>  <b>Problems with retracts and aliases for containers.</b> <br>  Some errors occurring in the services are treated in production by repeated requests for another copy of the service (retracting).  In the old environment, retracts were not used at all.  In the new environment, although we raised one container with the service one, dns-aliases were made for some services for retracts to work.  Retrai make the system a little more stable. <br><br>  <b>pgbouncer and number of connections.</b> <br>  In the old test environment, all services went straight to postgres, in production before each postgres is pgbouncer, the database connection manager.  If there is no manager, then each connection to the database is a separate postgres process, eating off additional memory.  After installing pgbouncer, the number of postgres processes was reduced from 300 to 35 on a running test bench. <br><br>  <b>Service monitoring.</b> <br>  For ease of use and running autotests, easy monitoring was done, which with a delay of less than a minute shows the status of the services inside the containers.  Made on the basis of haproxy, which makes http check for services from the config.  The configuration file for haproxy is generated automatically based on ansible inventory and data on addresses and ports from ansible playbooks.  Monitoring has a user-friendly view for people and json-view for autotests, which check the state of the stand before launch. <br><br>  There were many other problems: with containers, with ansible, with our applications, with their settings, etc.  We have decided most of them and continue to decide what is left. <br><br><h2>  How to use all this for testing? </h2><br>  Each test bench has its own dns-name on the network.  Previously, to get access to the test bench, the tester entered hh.ru.standname and received a copy of the website hh.ru.  It was quite simple, but did not allow testing many things and made it difficult to test mobile applications. <br><br><img src="https://habrastorage.org/files/34c/de4/da9/34cde4da943d4675abe501422f3cc3a0.png" alt="image"><br><br>  Now, thanks to the use of production configs for nginx, which do not really want to accept URIs that end in standname as input, the tester writes a proxy in his browser (squid is raised on each host machine) and opens in the browser hh.ru or any other site from of our pool (career.ru, jobs.tut.by, etc) and any of the third level domains, for example, nn.hh.ru. <br><br>  Using a proxy allows you to easily test mobile applications by simply turning on the proxy in the wifi settings. <br><br>  If the tester or developer wants to access the service and its port directly from the outside, then port forwarding is done using haproxy, for example, for debugging or other purposes. <br><br><h2>  Profit </h2><br>  At the moment, we are still in the process of transition to a new environment and realize all sorts of wishes that have appeared in the process of work.  We have already transferred manual testing to new stands and the main release stand.  In the near future we plan to transfer the auto-testing infrastructure to a new environment. <br><br><img src="https://habrastorage.org/files/8fc/c58/371/8fcc583714384ecbb20db298015dbbb6.jpg" alt="image"><br><br>  At the moment, we already have a profit in the following cases: <br><br><ul><li>  for the release of a new service, the developer himself can write and test the ansible playbook and only then transfer it to the operation service.  This allows you to test configs and the layout process as closely as possible to production; </li><li>  You can easily raise several instances of the same service in different containers and check the version compatibility or balancer operation when one of them is disconnected, or how the site will work if two instances out of three single services suddenly stop responding; </li><li>  You can check for some complex changes in the nginx configs or change the way webdav works; </li><li>  for debugging, you can easily connect a test server to our monitoring and conveniently investigate problems; </li><li>  while working on the new environment, many bugs were found and fixed in the production configuration and in ansible playbooks. </li></ul><br>  As with all innovations, there are those who are satisfied, and those who accept the new one with hostility.  The main stumbling block is the use of a proxy - someone is not very comfortable, which is understandable.  Together we will try to solve this problem. <br><br>  The following plans complete the automation of the creation of stands for each release and the exclusion of a person from the build process and auto-testing releases. </div><p>Source: <a href="https://habr.com/ru/post/271221/">https://habr.com/ru/post/271221/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../271211/index.html">Hackers invented a new money theft scheme, stealing 250 million rubles</a></li>
<li><a href="../271213/index.html">We invite you to the December Moscow mitap RuHaskell</a></li>
<li><a href="../271215/index.html">Protected social networks - myth or reality?</a></li>
<li><a href="../271217/index.html">Winium: now for Windows Phone</a></li>
<li><a href="../271219/index.html">Dedicated servers in the Netherlands for Habrahabr readers without a pro-rate for November + additional 10% discount</a></li>
<li><a href="../271223/index.html">We connect GNS3 topology to Cisco dCloud</a></li>
<li><a href="../271225/index.html">Your Joomla site incorrectly gives page 404</a></li>
<li><a href="../271227/index.html">PaaS vs Licenses. Four noble truths of IT infrastructure</a></li>
<li><a href="../271229/index.html">HP P2000 MSA G3 Performance Testing</a></li>
<li><a href="../271231/index.html">Downloading tracks from Autotravel.ru</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>