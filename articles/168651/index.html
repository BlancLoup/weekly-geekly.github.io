<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Performance testing: pitfalls</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I am engaged in the creation of high-loaded applications for stock trading. Loaded both in data volume and in the number of users and requests. Natura...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Performance testing: pitfalls</h1><div class="post__text post__text-html js-mediator-article">  I am engaged in the creation of high-loaded applications for stock trading.  Loaded both in data volume and in the number of users and requests.  Naturally, for such applications, performance is of paramount importance, and, as a result, testing thereof. <br><br>  Observing this testing from the side, I have accumulated a certain amount of information, which, I think, will be interesting. <br><a name="habracut"></a><br><h2>  Stone 1st.  Conversion factor </h2><br>  Testing of such applications requires the deployment of a whole network of test machines.  As a result, this leads to the formation of a test cluster.  Attempting to deploy such a cluster based on machines that are physically located in the development center leads to the creation of its own data center with all the costs of such a solution.  A good alternative was to use solutions like Amazon Web Services. <br><br>  The natural desire to save on the rental of hosts or on the purchase of equipment leads to the choice of those with lowered relative to the production installation characteristics.  Understand at times.  And here the conversion factor between synthetic performance indices comes into effect.  Those.  the processor in production will be 2 times faster, the number of cores will be 4 times more, the amount of RAM will be 6 times more, the performance of the disk subsystem will be 3.5 times better, the network speed will be 100 times more.  We add, divide by the number of indicators, multiply by a certain correction factor and obtain the conversion factor, which we will multiply the results of performance testing.  You can come up with a more complex formula, for example, assign a certain weight to each indicator. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Upon closer examination, this approach is only suitable for preparing test suites for future testing on installations close to production, and to detect the most obvious performance problems.  (Which is already quite a lot and important.) Why?  Yes, at least because with this approach the effect of bottlenecks is completely ignored. <habracut><br><br>  An example from life.  Tests were run on one host, the application under test was run on another.  The test suite included requests that give a different amount of data in the response.  Queries that give relatively little data gave satisfactory results, while queries that give large volume responses yielded unsatisfactory results.  At the same time, the host of the application under test was far from overload.  The conclusion suggests itself: the application under test does not cope well with requests that give a large return, it does not use all the resources of the machine,  needs redesign.  And what really?  But in fact, it turned out that the low speed of the network led to a long transfer of answers, which especially affected the large volume responses and, thus, simply did not allow creating a large load on the application under test. <br><br>  Here is an example of bottleneck, which appeared on the test (slow) installation, and simply impossible on production.  Let's call it the ‚Äúdownstream‚Äù bottleneck.  Anyway, the ‚Äúdownstream‚Äù bottleneck is not dangerous and causes only an unproductive waste of time for the tester and developer. <br><br>  But can it be the opposite, is it possible to have an ‚Äúascending‚Äù bottleneck, which is really dangerous and can cause great trouble not only to the developer, tester, etc., but also to the client?  Those.  Let us imagine that the indicators achieved at the test installation fully satisfy us.  For example, the conversion factor is 5, we need to ensure 100,000 operations per second, the test installation gave 25,000. It seems that everything is OK, can you sleep well?  No matter how wrong!  Similarly, in such a situation, a bottleneck may appear, undetected (and fundamentally undetectable!) In a test installation.  Because of which the real, effective conversion factor will be not 5, but 3. Ie  not 125,000, but only 75,000 - 25% worse than necessary. <br><br>  And for the ‚Äúascending‚Äù bottlenecks, a huge scope of possibilities opens up: we form the coefficient of recalculation of productivity on the basis of synthetic indices, in which the weight of individual indicators is chosen almost arbitrarily.  It is not enough to estimate the weight of one of the indicators, and ... <br><br><h2>  Stone 2nd.  Testing application </h2><br>  Performance testing requires load creation.  Load from multiple hosts, from multiple user accounts, sending a large number of different requests.  There is a natural desire to automate all this.  Make your own utility or find a ready, which, fortunately, quite a lot. <br><br>  But, creating a large load on the application under test, the test application itself is under load.  This is not entirely obvious, but nonetheless it is.  Let us turn to the analogy.  We all know that any measuring device acts on the process being measured and, therefore, inevitably introduces an error.  Similarly, the features of test implementation affect the test results, which is not always taken into account.  For testers, it is natural to look at the application under test first (and any), and not at the tests themselves.  I am by no means mean trivial test errors.  I mean the unobvious impact of tests on the testing process. <br><br>  For example.  A testing application sends thousands of requests per second.  It turns out that each next request is slower than the previous one.  Well, the system does not withstand the load, the delays increase, the queue of processing requests grows?  And it turns out that a testing application (in Java) allocates a certain amount of memory for each request-response, and the more requests and responses received, the slower the memory is allocated in it, the fewer requests a testing application can send per unit of time. <br><br><h2>  Stone 3rd.  Black box </h2><br>  The most common approach to testing is the presentation of the application under test as a black box.  Testers are not included in the implementation details, do not take into account the features and interrelationships of individual components of the application. <br><br>  This approach is good in general.  Moreover, the necessary initial conditions of this general case include a large (almost unlimited) amount of time for testing.  That almost never occurs in reality.  On the contrary, testers in most cases lag behind developers.  Because at least, that the developers are endlessly reworking something. <br><br>  In such circumstances, it is important to minimize the time required for testing, while not reducing its quality, of course.  To solve this problem, close interaction between testers and developers is needed, during which developers, naturally, who know their application, can pre-specify testers' weak points, possible bottlenecks, etc.  At the same time, of course, the approach to the tested application is lost, as if to a black box.  But, firstly, it is justified, and secondly, it is not necessary to do all the tests in collaboration with the developers. <br><br>  The article describes 3 pitfalls.  Perhaps someone would like to talk about other unobvious performance testing issues.  Waiting for your feedback! <br><br>  UPD.  An article about non-obvious, non-surface methodological errors in performance testing.  If you have a burning desire to stigmatize and condemn these mistakes - you should not, I agree with you in advance.  It would be more interesting if you shared other types of such errors. </habracut></div><p>Source: <a href="https://habr.com/ru/post/168651/">https://habr.com/ru/post/168651/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../168639/index.html">So is it Total Commander or Explorer?</a></li>
<li><a href="../168641/index.html">The chain of effective online sales: concept</a></li>
<li><a href="../168643/index.html">Life without testers: myth or reality?</a></li>
<li><a href="../168645/index.html">Home robot should cost $ 2999</a></li>
<li><a href="../168647/index.html">The first post on our blog. Let's talk about customer service in a web-studio / interactive agency</a></li>
<li><a href="../168653/index.html">How to localize the application in many languages, so as not to be painfully embarrassing</a></li>
<li><a href="../168655/index.html">The release of RubyMine 5</a></li>
<li><a href="../168657/index.html">Pixel lights fast and easy</a></li>
<li><a href="../168661/index.html">Yahoo! will show ads google</a></li>
<li><a href="../168665/index.html">Open source software in operational printing</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>