<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Upgrade viola jones</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In my previous topic, I tried to show how the Viola Jones method works, using what technologies and internal algorithms. In this post, in order not to...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Upgrade viola jones</h1><div class="post__text post__text-html js-mediator-article">  In <a href="http://habrahabr.ru/blogs/algorithm/133826/">my previous topic,</a> I tried to show how the Viola Jones method works, using what technologies and internal algorithms.  In this post, in order not to interrupt the chain, there will also be a lot of theory, it will be shown <i>at the expense of what</i> can be improved and so beautiful method.  If we also describe the <i>software implementation here</i> , then there will be a huge canvas that will be very inconvenient to read, and it will not look at all - it was decided to divide the amount of information into two separate posts.  Below is a theory, few pictures, but many useful things. <br><a name="habracut"></a><br><br><h3>  Problems at the stages of recognition of emotions </h3><br>  The person does not even notice how he simply copes with the tasks of detecting faces and emotions with the help of his vision.  When the eye looks at the surrounding faces of people, objects, nature, one does not subconsciously feel how much work the brain does to process the entire flow of visual information.  It will not be difficult for a person to find a familiar person in a photograph, or to distinguish a snide grimassu from a smile. <br>  A person tries to <i>recreate and build a</i> computer system for detecting faces and emotions - he partly succeeds in this, but every time he has to face big problems in recognition.  Computers nowadays can easily store huge amounts of information, pictures, video and audio files.  But to find computing systems with the same ease, for example, the right photo with a certain emotion of the right person from your own personal photo gallery is a difficult task.  The solution of this problem is hampered by some <b>factors</b> : <br><ul><li>  Different size of the desired objects, as well as image scale; </li><li>  The designated object can be anywhere on the image; </li><li>  A completely different object may be similar to the one you are looking for; </li><li>  The subject, which we perceive as something separate, is not highlighted in the image, and is located against the background of other objects, merges with them; </li><li>  Old and unprocessed photos - there are always ‚Äúdistracting‚Äù system scratches, interferences, distortions, and various types of <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D1%2583%25D0%25B0%25D1%2580_%25D0%25BC%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE%25D0%25BA%25D1%2580%25D0%25B0%25D1%2581%25D0%25BE%25D1%2587%25D0%25BD%25D0%25BE%25D0%25B9_%25D0%25BF%25D0%25B5%25D1%2587%25D0%25B0%25D1%2582%25D0%25B8">moire</a> often appear on scanned photos; </li><li>  Do not forget that in many recognition algorithms (also in Viola-Jones), the work goes directly from 2D space.  Therefore, the rotation of the desired object and the change in viewing angle relative to the specified coordinate axes of the projection affect its projection in 2D.  The same object can give a completely different picture, depending on the rotation or distance to it.  The desired face can be rotated in the image plane.  Even a relatively small change in the orientation of the face relative to the camera entails a serious change in the image of the face and the recognition of the facial expression of that person is out of the question; </li><li>  The quality of the image or frame: light and wrong <a href="http://ru.wikipedia.org/wiki/%25D0%2591%25D0%25B0%25D0%25BB%25D0%25B0%25D0%25BD%25D1%2581_%25D0%25B1%25D0%25B5%25D0%25BB%25D0%25BE%25D0%25B3%25D0%25BE_%25D1%2586%25D0%25B2%25D0%25B5%25D1%2582%25D0%25B0">white balance</a> , color correction and other parameters certainly affect the recognition of the object; </li><li>  Human race: skin color, location and size of individual recognizable traits; </li><li>  A strong change in facial expression.  For example, too ostentatious act can greatly influence the correct recognition of a certain emotion; </li><li>  Individual features of a person's face, such as a mustache, beard, glasses, wrinkles, significantly complicate automatic recognition; </li><li>  Part of the face may be invisible or clipped altogether; </li><li>  Persons may not be completely in the photo, but the car, as it seems to her, correctly identifies other objects behind the face and facial features and detects them. </li></ul><br>  The list can be continued for a long time.  But attention is focused on the most important points, so it makes no sense to list all the interfering parameters. <br><br><h3>  Criteria for choosing a method in the task of recognizing emotions </h3><br>  Comparison of the quality of recognition of various methods is complicated by many reasons.  One of them, and the most significant one, is that in most cases you can only rely on test data provided by the authors themselves, since conducting large-scale research on the implementation of most known methods and comparing them to each other on a single set of images is not possible: <br><ul><li>  a universal collection of test data is required; </li><li>  must have the same data sets; </li><li>  computational power is needed - resources of the level of one laboratory for this are small; </li><li>  high complexity of research data algorithms; </li><li>  Based on the information provided by the authors of the methods, it is also difficult to make a correct comparison, since methods are often tested on different sets of images, with different wording of the conditions for successful and unsuccessful detection.  In addition, verification for many methods of the first category was carried out on much smaller sets of images. </li></ul><br>  <i>The main points</i> influencing the choice of the method of solving the problem and the <i>conditions of the criteria</i> revealed the following (in the development process, those that are italicized are most optimal): 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      ‚Ä¢ restrictions on individuals: <br>  - a limited, predetermined set of people; <br>  - restrictions on the possible race of people or on the characteristic differences on the face (for example, facial hair, glasses); <br>  - <i>no restrictions</i> . <br><br>  ‚Ä¢ face position on the image: <br>  - frontal; <br>  - tilt at a known angle; <br>  - <i>any slope</i> . <br><br>  ‚Ä¢ image type: <br>  - color; <br>  - black and white; <br>  - <i>any</i> . <br><br>  ‚Ä¢ face scale: <br>  - fully in the "main frame"; <br>  - localized in a certain place with a certain size; <br>  - it <i>doesn't matter</i> . <br><br>  ‚Ä¢ resolution and image quality: <br>  - bad / bad; <br>  - bad / good; <br>  - only good; <br>  - <i>any</i> . <br><br>  ‚Ä¢ noise, interference, moire: <br>  - weak; <br>  - strong; <br>  - it <i>doesn't matter</i> . <br><br>  ‚Ä¢ number of faces in the image: <br>  - known; <br>  - approximately known; <br>  - <i>unknown</i> . <br><br>  ‚Ä¢ lighting: <br>  - known; <br>  - approximately known; <br>  - <i>any</i> . <br><br>  ‚Ä¢ background: <br>  - fixed; <br>  - contrast monophonic; <br>  - low contrast noise; <br>  - unknown; <br>  - <i>no difference</i> . <br><br>  ‚Ä¢ importance of research: <br>  - it is <i>more important to identify all faces and features</i> ; <br>  - to minimize the errors of false detections by minimizing the detections on the image. <br><br><h3>  Suggested improvements </h3><br>  The Viola-Jones method sometimes produces startling errors, as some Habrawers mentioned.  Therefore, the most logical solution in this situation was to modify the Viola-Jones algorithm and improve its recognition characteristics, if at all possible. <br>  The method proposed below <i>suggests a possible disposal</i> of: <br><ul><li>  large restrictions in the form of insufficient illumination, existing noise in the image and an indistinguishable object in the background using image pre-processing; </li><li>  face tilt problems by training new cascades specially trained to find a bowed head; </li><li>  inaccuracies of detecting emotions on a person‚Äôs face through the above points, by introducing new Haar primitives that extend the standard set implemented in the Viola-Jones algorithm and by training a large number of cascades specifically tailored to a particular state of a particular emotion face. </li></ul><br>  Taking into account the above, this method of work should reduce the time for determining emotions. <br>  The scheme of the new method is presented in the figure below: <br><img src="https://habrastorage.org/storage1/ec8e2e68/cb94528a/05338a95/1e595d1e.jpg"><br><br><h4>  Additional Haar primitives </h4><br>  So, to improve the capabilities of the algorithm and the quality of finding inclined facial features, I propose such a variant of new types of features, as in the figure. <img src="https://habrastorage.org/storage1/f5042333/9f96d9d0/f23ebba3/e2bf3461.jpg">  Such signs were not chosen by chance, but in connection with the silhouettes of the desired features.  Using these features, the program will search much faster for the nose, eyebrows and mouth, but it will spend a little more time on other facial features than in the standard algorithm, but additional features should be implemented: in general, recognition milliseconds are won and slightly, but accuracy increases. <br><br><h4>  Preliminary image processing </h4><br>  <a href="http://habrahabr.ru/blogs/algorithm/133826/">The described algorithms</a> that the Viola-Jones method uses are quite widely used.  The method itself can successfully interact with other algorithms and can be adapted to specific needs and requirements.  Also, it works not only with static images, you can freely process data in real time.  However, even he does not provide the perfect recognition of emotions.  Let's try to raise the recognition percentage even higher with the help of Viola-Jones.  For this purpose, in order to increase productivity, it was decided to perform <i>image pre-processing</i> . <br>  To create a fast and reliable way to determine the likely areas of a person‚Äôs face in order to speed up processing at further stages of detection, an <i>algorithm is</i> proposed <i>for determining and distinguishing boundary contours</i> .  <i>The idea of</i> this approach is that it is possible to <i>emphasize the areas</i> in which the <i>person and his features</i> are <i>most likely</i> to be found.  Thus, not only is the acceleration of the algorithm achieved, but the probability of false detection of faces is also reduced.  These steps will be illustrated in the original image: <br><img src="https://habrastorage.org/storage1/07c30832/435c2682/e3588a2d/319709d9.jpg"><br><br><h5>  Grayscale image </h5><br>  First you need to translate the image in <a href="http://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B5%25D1%2580%25D0%25B0%25D1%258F_%25D1%2588%25D0%25BA%25D0%25B0%25D0%25BB%25D0%25B0">grayscale</a> .  For this purpose it is convenient to present the image in the color model <a href="http://ru.wikipedia.org/wiki/YUV">YUV</a> .  For this conversion is performed: <br><ul><li> <code>Y = 0.299 * R + 0.587 * G + 0.114 * B; U = -0.14713 * R - 0.28886 * G + 0.436 * B; (1.1) V = 0.615 * R - 0.51499 * G - 0.10001 * B;</code> </li> <li> <code>Y = 0.299 * R + 0.587 * G + 0.114 * B; U = -0.14713 * R - 0.28886 * G + 0.436 * B; (1.1) V = 0.615 * R - 0.51499 * G - 0.10001 * B;</code> </li> <li> <code>Y = 0.299 * R + 0.587 * G + 0.114 * B; U = -0.14713 * R - 0.28886 * G + 0.436 * B; (1.1) V = 0.615 * R - 0.51499 * G - 0.10001 * B;</code> </li> </ul>  , where R, G and B are <i>the intensities of the specified colors</i> , if absolutely accurate, then these are matrices describing the components of the model (R, G, B).  Y in the formula is the <i>brightness component</i> , and U and V are the <i>color difference components</i> , the so-called chroma signals (these three parameters are also represented by matrices describing the components of the model (Y, U, V)).  The translation coefficients present in the formula are constant and determined by the characteristics of human perception.  For a halftone image, only the value of the first component is important. <br><img src="https://habrastorage.org/storage1/44e4d14e/0c0be82d/c99f0add/73ee43f1.jpg"><br><br><h5>  Spatial Filtering </h5><br>  The translated halftone image is smoothed and <i>spatial differentiation</i> is performed - the gradient of the intensity function at each point of the image is calculated.  It is assumed that the regions correspond to real objects, or their parts, and the boundaries of regions correspond to the boundaries of real objects.  Detection of brightness gaps using a <i>sliding mask (sliding window) is used</i> .  In different textbooks and articles, it is variously called, usually referred to as a filter or a kernel, which is a square matrix corresponding to the mapped Y of the original image.  The spatial filtering scheme is shown below: <br><img src="https://habrastorage.org/storage1/9123bfb3/3a56a68e/c6d3d576/85cfae2a.jpg"><br><br><h5>  Sobel operator </h5><br>  In the applied method, using special kernels, known as " <a href="http://ru.wikipedia.org/wiki/%25D0%259E%25D0%25BF%25D0%25B5%25D1%2580%25D0%25B0%25D1%2582%25D0%25BE%25D1%2580_%25D0%25A1%25D0%25BE%25D0%25B1%25D0%25B5%25D0%25BB%25D1%258F">Sobel operators</a> ", operating in the image area of ‚Äã‚Äã3 * 3 size, weighting factor 2 is used for middle elements.  The coefficients of the kernel are chosen so that when it is applied, the smoothing in one direction is simultaneously performed and the spatial derivative is calculated in the other.  The masks used by the Sobel operator are shown below: <br><img src="https://habrastorage.org/storage1/ad5fcd13/66694c05/e9c27cc7/447917f9.png"><br>  From them we get the components of the gradient G <sub>x</sub> and G <sub>y</sub> : <br> <code>G <sub>x</sub> = (z <sub>7</sub> + 2z <sub>8</sub> + z <sub>9</sub> ) ‚Äì (z <sub>1</sub> + 2z <sub>2</sub> + z <sub>3</sub> ) (1.2) <br>  <br> G <sub>y</sub> = (z <sub>3</sub> + 2z <sub>6</sub> + z <sub>9</sub> ) ‚Äì (z <sub>1</sub> + 2z <sub>4</sub> + z <sub>7</sub> ) (1.3) <br></code> <br>  To calculate the magnitude of the gradient, these components must be used together: <br><img src="https://habrastorage.org/storage1/c8be459e/4ccc38b6/4ba4a7f7/1e9df554.jpg">  (1.4) <br>  or <img src="https://habrastorage.org/storage1/96a141f7/6242e738/bd81160f/84dc7529.jpg">  (1.5) <br>  The result shows how "sharply" or "smoothly" the brightness of the image at each point changes, which means that the probability of finding a point on the face, as well as the orientation of the border.  The result of the work of the Sobel operator at the point of the constant brightness region will be the zero vector, and at the point lying on the border of the regions of different brightness, the vector intersecting the boundary in the direction of increasing brightness.  Also, the result of the work is the translation of the photo into boundary contours (this can be seen in the transformed picture): <br><img src="https://habrastorage.org/storage1/72f2075a/dad43f03/a735c95f/e782a472.jpg"><br><h5>  Threshold classification </h5><br>  Further, the same halftone image is subjected to a <i>threshold classification (thresholding)</i> , or the choice of the threshold in brightness.  The meaning of this threshold is to separate the desired light object (foreground) and dark background (background), where the object is a collection of those pixels whose brightness exceeds the threshold (I&gt; T), and the background is a collection of other pixels whose brightness is lower threshold (I &lt;T).  An approximate algorithm for working with the <i>global threshold</i> in automatic mode looks like this: <br><ol><li>  The initial estimate of the threshold T is selected ‚Äî this may be the average image brightness level (half the sum of the minimum and maximum brightness, min + max / 2) or any other threshold criterion; </li><li>  Some segmentation of the image is performed using the threshold T - as a result, two groups of pixels are formed: G <sub>1</sub> consisting of pixels with brightness greater than T, and G <sub>2</sub> consisting of pixels with brightness less than or equal to T; </li><li>  Calculate the average values ‚Äã‚Äãof Œº <sub>1</sub> and Œº <sub>2 the</sub> brightness of the pixels in the areas of G <sub>1</sub> and G <sub>2</sub> ; </li><li>  A new threshold value T = 0.5 * (Œº <sub>1</sub> + Œº <sub>2</sub> ) is calculated; </li><li>  The steps from the second to the fourth are repeated until the difference in the values ‚Äã‚Äãof the threshold T of the previous one and the T calculated in the last iteration is less than the pre-specified parameter Œµ. </li></ol><br><img src="https://habrastorage.org/storage1/ba9c2270/0a232636/07689f7e/c7294bd3.jpg"><br><h5>  Otsu binarization </h5><br>  The Sobel operator cannot do this, since it gives a lot of noise and interference, therefore the halftone image must be <i>binarized</i> .  There are dozens of methods for selecting the threshold, but it is the method that was invented by the Japanese scientist <a href="http://en.wikipedia.org/wiki/Otsu%27s_method">Nobuyuki Otsu (Otsu's Method)</a> in 1979 [1], because  he is fast and efficient.  The brightness values ‚Äã‚Äãof the image pixels can be viewed as random variables, and their histogram as an estimate of the density of the probability distribution.  If the probability density distributions are known, then the optimal (in the sense of minimum error) threshold can be defined for segmentation of an image into two classes c <sub>0</sub> and c <sub>1</sub> (objects and background).  <b>A histogram</b> is a set of bins (or columns), each of which characterizes the number of sampling elements hit into it.  In the case of a halftone image, these are pixels of different brightness, which can take integer values ‚Äã‚Äãfrom 0 to 255. Thanks to the histogram, a person can see <i>two clearly separated classes</i> .  The essence of the Otsu method is to set the threshold between classes in such a way that each of them is <i>as ‚Äúdense‚Äù as possible</i> .  If expressed in mathematical language, it comes down to minimizing the intraclass variance, which is defined as the weighted sum of the variances of the two classes (that is, the sum of deviations from the mathematical expectations of these classes): <br>  <code>œÉ <sub>w</sub> <sup>2</sup> = w <sub>1</sub> * œÉ <sub>1</sub> <sup>2</sup> + w <sub>2</sub> * œÉ <sub>2</sub> <sup>2</sup> ,</code> (1.6) <br>  where œÉ <sub>w</sub> is the intraclass dispersion, œÉ <sub>1</sub> and œÉ <sub>2</sub> are the dispersions, and w <sub>1</sub> and w <sub>2</sub> are the probabilities of the first and second classes, respectively.  In the Otsu algorithm, minimizing intraclass dispersion is <b>equivalent to</b> maximizing interclass dispersion, which is: <br>  <code>œÉ <sub>b</sub> <sup>2</sup> = w <sub>1</sub> * w <sub>2</sub> (Œº <sub>1</sub> ‚Äì Œº <sub>2</sub> ) <sup>2</sup> ,</code> (1.7) <br>  where œÉ <sub>b</sub> is the interclass dispersion, w <sub>1</sub> and w <sub>2</sub> are the probabilities of the first and second classes, respectively, and Œº <sub>1</sub> and Œº <sub>2</sub> are the arithmetic average values ‚Äã‚Äãfor each of the classes.  <i>The cumulative variance</i> is expressed by the formula <br> <code>œÉ <sub>T</sub> <sup>2</sup> = œÉ <sub>w</sub> <sup>2</sup> + œÉ <sub>b</sub> <sup>2</sup> .</code>  (1.8) <br>  <i>The general scheme of the fast algorithm</i> is as follows: <br><ol><li>  Calculate the histogram (one pass through an array of pixels).  Further only the histogram is necessary;  passes through the entire image is no longer required. </li><li>  Starting from the threshold t = 1, we go through the entire histogram, recalculating the variance œÉ <sub>b</sub> (t) at each step.  If at some of the steps the variance is greater than the maximum, then the variance is updated and T = t. </li><li>  The required threshold is T. </li></ol><br>  In a more accurate implementation, there are parameters that allow to speed up the algorithm, for example, the passage through the histogram is not made from 1 to 254, but from min to (max -1) brightness.  The ‚Äúclassic‚Äù binarization is shown in the photo below: <br><img src="https://habrastorage.org/storage1/e7b5d5d1/49796b1c/fa18e433/4a5a4e38.jpg"><br><br>  <i>The advantages of</i> the Otsu method are: <br><ul><li>  Ease of implementation; </li><li>  We adapt well to different images, choosing the most optimal threshold; </li><li>  The time complexity of the algorithm is O (N) operations, where N is expressed by the number of pixels in the image. </li></ul>  <i>Disadvantages of the</i> method: <br><ul><li>  This threshold binarization is very dependent on the uniform distribution of brightness in the image.  The solution is to use several local thresholds instead of one global. </li></ul><br><h5>  Canny Boundary Detector </h5><br>  So, based on the previous image processing steps, the edges of the image and the splitting by the threshold are obtained in accordance with the brightness values.  Now, in order to fix the result, it is necessary to apply the <i>Canny <a href="http://habrahabr.ru/blogs/image_processing/114589/">border detector</a></i> to find and finally link the edges of the objects in the image to the contours.  In his research [2], John Kanni achieved the construction of a filter that is optimal in terms of the selection, localization, and minimization of several responses of one edge. <br>  With the help of the Kanni algorithm, the following problems are solved: <br><ol><li>  Suppression of "false" highs.  Only some of the highs are marked as boundaries; </li><li>  Subsequent double threshold filtering.  Potential boundaries are determined by thresholds.  Splitting into thin edges (edge ‚Äã‚Äãthinning); </li><li>  Suppression of all ambiguous edges that do not belong to any region boundaries.  Linking edges to contours (edge ‚Äã‚Äãlinking). </li></ol><br>  In the work of Kanni, such a concept is used as the <i>suppression of ‚Äúfalse‚Äù maxima (non-maximum suppression)</i> , which means that the pixels of the borders are assigned such points where a <i>local gradient maximum is reached in the direction of the gradient vector found</i> .  Using the Sobel operator, we obtain the angle of the direction of the boundary vector (follows from formula 1.4): <br>  <code>Œ≤(x,y) = arctg(G <sub>x</sub> /G <sub>y</sub> ),</code> (1.9) <br>  where Œ≤ (x, y) is the angle between the direction of the ‚àáf vector at the point (x, y) and the y axis, and Gy and Gx are the components of the gradient.  The direction of the vector must be a multiple of 45 ¬∞.  The angle of the direction of the border vector is rounded to one of the four corners representing the vertical, horizontal and two diagonals (for example, 0, 45, 90 and 135 degrees).  Then the conditions are checked to see if the magnitude of the gradient reaches a local maximum in the corresponding direction of the vector.  The following is an example <i>for a 3 * 3 mask</i> : <br><ul><li>  if the angle of the gradient direction is zero, the point will be considered a border if its intensity is greater than the point above and below the point in question; </li><li>  if the angle of the gradient direction is 90 degrees, the point will be considered a border if its intensity is greater than the point on the left and right of the point in question; </li><li>  if the angle of the gradient direction is 135 degrees, the point will be considered a border if its intensity is greater than the points located in the upper left and lower right corner of the point in question; </li><li>  if the angle of the gradient direction is 45 degrees, the point will be considered the border if its intensity is greater than the points located in the upper right and lower left corner of the point in question. </li></ul><br>  After the suppression of local uncertainties, the edges become more accurate and thin.  Thus, a binary image is obtained that contains boundaries (the so-called <i>‚Äúthin edges‚Äù</i> ).  They can be seen on the converted image: <br><img src="https://habrastorage.org/storage1/416d2892/4b05fcdb/e60586eb/5bfcad8f.jpg"><br>  Border selection Kanni uses <i>two filtering thresholds</i> : <br><ul><li>  if the pixel value is higher than the upper limit - it takes the maximum value and the border is considered reliable, the pixel is selected; </li><li>  if lower, the pixel is suppressed; </li><li>  points with a value falling in the range between the thresholds take a fixed average value; </li><li>  then found pixels with a fixed average are added to the group if they are in contact with the group in one of four directions. </li></ul><br><h5>  Combing image </h5><br>  After the Kanni detector, the image undergoes a <a href="http://gliffer.ru/articles/algoritmi--matematicheskaya-morfologiya/">morphological operation of</a> dilatation - expansion, thickening of the found boundaries due to the fact that the structure-forming set runs over them. <br>  And finally, the last step is subtracting the resulting binarized image from the original.  Approximately this image should come out at the exit from the training of a cunning cat and a tear-stained, stunned child to recognize their emotions: <br><img src="https://habrastorage.org/storage1/36eed439/36ccf901/9b93e17a/eb103ee9.jpg"><br><br><h3>  findings </h3><br>  This approach is universal, expandable and if a sufficient number of cascade classifiers have been trained, the method of finding works very quickly and almost without error. <br><br>  Summing up: <br>  The mechanism of operation of the Viola-Jones algorithm (Viola-Jones), described in detail, is available and described in this post, my suggested approaches to improving the efficiency of solving the problem of detecting human emotions.  Naturally, this is not the only way to modify the method.  You can try your own.  With the help of such upgrades, a theoretical improvement of the method is possible. <br>  Now the consistency of this algorithm and its applicability in practice should be shown.  I will tell about its implementation in my next post and show it with the help of illustrations.  Thanks for attention!  I hope that you were interested! <br><br><h3>  Bibliography </h3><br>  1. Otsu, N., ‚ÄúA Threshold Selection Method from Gray-Level Histograms,‚Äù IEEE Transactions on Systems, Man, and Cybernetics, Vol.  9, No. 1.1979., Pp.  62-66 <br>  2. John Canny, ‚ÄúA Computational Approach to Edge Detection,‚Äù IEEE Transactions, Vol.8, No. 6, 1986., pp.679 -698 <br>  3. <a href="http://habrahabr.ru/blogs/image_processing/114452/">Algorithms for image contour selection</a> </div><p>Source: <a href="https://habr.com/ru/post/133909/">https://habr.com/ru/post/133909/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../133897/index.html">Cross platform is cool</a></li>
<li><a href="../133900/index.html">Symbian Web Runtime: easy mobile application development</a></li>
<li><a href="../133903/index.html">Optimization of code execution speed</a></li>
<li><a href="../133906/index.html">Anti-rootkit hypervisor: how it works</a></li>
<li><a href="../133907/index.html">Binary compatibility in the examples and not only</a></li>
<li><a href="../133910/index.html">Combat rebuilding cache with RED</a></li>
<li><a href="../133911/index.html">Model-oriented design, or continue taming the Cortex M3 with Matlab / Simulink</a></li>
<li><a href="../133912/index.html">How to make a combined user script for the site?</a></li>
<li><a href="../133914/index.html">Anti-piracy</a></li>
<li><a href="../133915/index.html">Restoring a messed up SSD</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>