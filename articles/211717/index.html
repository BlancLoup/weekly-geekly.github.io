<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Passing messages between threads. Classic blocking algorithms</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Once I got out of the sandbox with a scoop in my hand and a post about non-blocking queues and data transfer between threads. That post was not so muc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Passing messages between threads. Classic blocking algorithms</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/930/f52/4cc/930f524cc0e94fa6a429d9337368b180.png" align="left">  Once I got out of the sandbox with a scoop in my hand and a <a href="http://habrahabr.ru/post/209824/">post</a> about non-blocking queues and data transfer between threads.  That post was not so much about algorithms and their implementation, but about measuring performance.  Then in the comments I was <a href="https://habrahabr.ru/post/209824/">asked a</a> perfectly <a href="https://habrahabr.ru/post/209824/">reasonable</a> question about the usual blocking transmission algorithms - how much slower they are and how to choose the optimal algorithm for a specific task. <br>  Of course, I promised, and enthusiastically set to work, I even got funny results, however ... there wasn‚Äôt enough zest, it was boring and flat.  As a result, my inner perfectionist merged with my undisguised procrastinator and together they overcame me, the post had long settled in drafts and even my conscience did not tremble at the sight of a forgotten headline. <br>  However, everything is changing, new technologies are appearing, old technologies are disappearing in the archives, and I suddenly decided that it was time to repay debts and keep promises.  As a punishment, I had to rewrite everything from scratch, if the miser pays twice, then the lazy one reworks twice, which is what I need. <br>  Yes, I apologize for the KDPV - it is of course completely from a different subject area, but it is nevertheless ideally suited to illustrate the interaction between the threads. <br><a name="habracut"></a><br><br><h5>  <b>So what woke Herzen?</b> </h5><br>  Pushed me into action getting to know <div class="spoiler">  <b class="spoiler_title">D language</b> <div class="spoiler_text">  no need to continue the analogy, I did not mean to say that he was terribly far from the people </div></div>  - an extremely conceptually beautiful language that inherited and powerfully moved C ++ idioms forward and at the same time retained effective low-level tools, down to pointers.  Perhaps because of this, in my opinion, in the standard library D there is some kind of duality - most of the functionality can be called either out of the box, in a simple and easy way, or through an interface close to the native, but fully using the resources and capabilities of the system.  If C ++ covers the entire range with a continuous spectrum, then this division is usually well visible in D.  See for yourself: you need to measure the time interval, there is a wonderful <a href="http://dlang.org/phobos/std_datetime.html">std.datetime</a> module, but the measurement quantum is 100 ns, which is absolutely not enough for me, please - there is a no less remarkable module - <a href="http://dlang.org/phobos/core_time.html">core.time</a> .  The std.concurrency.spawn, <a href="http://dlang.org/phobos/std_concurrency.html">which</a> is lightweight to the limit, does not suit <a href="http://dlang.org/phobos/std_concurrency.html">you</a> - you can use the whole bunch of <a href="http://dlang.org/phobos/core_thread.html">core.thread</a> .  And so almost everywhere, except for one, but an extremely important place - the separation of data between threads.  Yes, yes, all variables local to this thread are located in the thread local storage and you cannot force another thread to see their address by any means.  And for the exchange of data, there are built-in queues, it must be admitted that they are very convenient - polymorphic, with the possibility of an extraordinary sending of important messages and an extremely pleasant interface.  It is possible to send data through them naturally or by value, or immutable links.  When I read about it for the first time, I just jumped out of indignation - ‚ÄúBut how did your nasty hand rise ...‚Äù - and then I thought about it, remembered my projects over the past years and acknowledged - yes, the whole exchange between the streams follows this pattern , but what does not pass is a clear design error. <br>  Nevertheless, the question hung in the air - how effective are the queues in D?  If not, this negates all the other effectiveness of the language, such a built-in bottle neck.  So I woke up and took up the measurements again. <br><br><h5>  <b>What exactly are we going to measure?</b> </h5><br>  The question is actually not an easy one; I wrote about this last time, and I repeat.  The usual ‚Äúnaive‚Äù approach when sending N messages, measuring the total time and dividing by N <b>does not work</b> .  Let's see, we measure the <i>performance of the queue</i> , right?  Therefore, we can assume that in the process of measuring the speed of <i>the message generator</i> and <i>the message receiver</i> <i>tends to infinity</i> , with a reasonable assumption that <i>no</i> data <i>is copied</i> inside the queue, it is beneficial to <i>put as much data</i> into the queue <i>as possible</i> , then <i>perform a one-time transfer of</i> some internal pointer and everything is already there .  At the same time, the <i>average time per message</i> will fall as 1 / N (actually limited to the bottom by the insertion / deletion time, which can be a few nanoseconds) while <i>the delivery time of</i> each message in theory remains constant, and even grows like O (N) in practice . <br>  Instead, I use the opposite approach - each message is sent, the time is measured, and only then the next ( <i>latency</i> ) is sent.  As a result, the results are presented in the form of histograms, along the X axis - time, along the Y axis - the number of packets delivered during this time.  The most interesting are numerically two parameters - the median average distribution time and the percentage of messages not met in some (arbitrary) upper limit. <br>  Strictly speaking, this approach is also not quite adequate; nevertheless, it describes the requirements for speed much more accurately.  I will do some self-criticism in conclusion, until I say that a complete description would include the generation of all possible types of traffic and analysis of it using statistical methods, a full-fledged scientific work from the field of QA theory would have turned out, or rather, I would get another procrastination attack. <br>  One more thing, I mention this because last time there was a long debate, the message generator can insert them into the queue as quickly as possible, but on the condition that the recipient <b>on average has</b> time to extract and process them, otherwise the whole measurement is simply meaningless.  If your receiving stream does not have time to process the data stream, you need to make the code faster, parallelize the processing, change the message protocol, but in any case the queue itself has nothing to do with it.  It seems to be a simple idea, but the last time had to be repeated several times in the comments.  Fluctuations of speed, when suddenly there are many messages in the queue, are quite possible and even unavoidable, this is just one of the factors that a well-designed algorithm should smooth out, but this is possible only if the maximum reception speed is greater than the average sending speed. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  <b>Let's start with</b> </h5><br><img src="https://habrastorage.org/files/f27/d6c/790/f27d6c79046041bf8f0697246b32fb63.png"><br>  What's this?  And this is actually the result, all my works were in one picture, but now I will explain for a long time what and why is drawn here. <br><br><h5>  <b><a href="">Pink.</a></b>  <b><a href="">Standard mechanism D</a></b> </h5><br>  5 microseconds, is it a lot or a little?  In almost all cases, it is not enough (that is, it is very good).  For the overwhelming majority of real-world projects, this is more than enough speed, moreover, not so long ago this transfer time could be obtained only with the help of special hardware and / or very special software.  Here we have a tool from the standard library, with many other tasty buns and fast enough for all practical needs.  The rating is excellent.  But however, not great, because this implementation has some disadvantages not related to speed, I will tell about it in the abusive part. <br>  Once again, we are pleased to see that the main programming magic is the absence of any magic.  If you climb under the hood (of course, I could not help but have a look), we will see that the code is completely normal - simply linked lists protected by mutexes.  I will not even give him here because in the sense of the implementation of the queue he will not tell us anything new.  But those few who really need faster algorithms, including non-blocking ones, can easily write their own version removing all convenient but slowing down buns.  But I will give my code, just to show how D is still concise and expressive language. <br><div class="spoiler">  <b class="spoiler_title">code to illustrate</b> <div class="spoiler_text"><pre><code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> std.stdio, std.concurrency, std.datetime, core.thread; <span class="hljs-type"><span class="hljs-type">void</span></span> main() { <span class="hljs-keyword"><span class="hljs-keyword">immutable</span></span> <span class="hljs-type"><span class="hljs-type">int</span></span> N=<span class="hljs-number"><span class="hljs-number">1000000</span></span>; auto tid=spawn(&amp;f, N); <span class="hljs-keyword"><span class="hljs-keyword">foreach</span></span>(i; <span class="hljs-number"><span class="hljs-number">0.</span></span>.N) { tid.send(thisTid, MonoTime.currTime.ticks); // wait <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> receiver <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> handle message receiveOnly!<span class="hljs-type"><span class="hljs-type">int</span></span>(); } } <span class="hljs-type"><span class="hljs-type">void</span></span> f(<span class="hljs-type"><span class="hljs-type">int</span></span> n) { <span class="hljs-keyword"><span class="hljs-keyword">foreach</span></span>(i; <span class="hljs-number"><span class="hljs-number">0.</span></span>.n) { auto m=receiveOnly!(Tid,long)(); writeln(MonoTime.currTime.ticks-m[<span class="hljs-number"><span class="hljs-number">1</span></span>]); // ask <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> the next message m[<span class="hljs-number"><span class="hljs-number">0</span></span>].send(<span class="hljs-number"><span class="hljs-number">0</span></span>); } }</code> </pre> <br></div></div><br><br><h5>  <b><a href="">Blue.</a></b>  <b><a href="">Cruel and undisguised C ++.</a></b> </h5><br>  400 nanoseconds!  Bingo!  Side by side all non-blocking and other tricky algorithms!  Or is it still not? <br>  No, of course, this is a crude provocation, the fact is that in this version the reading thread never falls asleep, it continues to continuously check the queue for incoming messages in a loop.  This option works as long as your CPU simply has nothing more to do, as soon as competing processes appear, especially if they are just as careless about shared resources, everything starts to slip unpredictably.  Yes, there is an option with the forced assignment of one of the cores to serve this thread, but architecturally this is a <b>very bad</b> decision, I will return to this later.  There are places where it is justified or even necessary, but if you work in such a place, you probably already know everything yourself, for you this post is completely superfluous. <br>  However, we received important information - on modern systems, the speed of transactions is not determined by the speed of mutexes or data copying, the main factor is the wake up time for the stream after a forced or voluntary pause.  Hence the moral - if you don‚Äôt want or can‚Äôt afford a dedicated CPU to process messages from the queue, think twice before using quick and complex but inconvenient solutions, the loss of adjusting the application architecture to them will almost completely outweigh the insignificant gain that the algorithm itself during the transaction.  And yes, here I mean <div class="spoiler">  <b class="spoiler_title">boost :: lockfree</b> <div class="spoiler_text">  This is an exemplary implementation of a non-blocking queue, but the message type must have a <i>trivial destructor and an assignment operator</i> , the condition is so harsh for C ++ that I actually have never brought the code to the final product. <br></div></div><br>  So what can be done while remaining within the bounds of reason? <br><br><h5>  <b><a href="">Red.</a></b>  <b><a href="">Reasonable and weighted C ++.</a></b> </h5><br>  If <i>usleep ()</i> and others like it came to mind - forget, you are guaranteed to increase the response time to at least 40 microseconds, this is the best that the modern core can guarantee.  Slightly better <i>yield ()</i> , although it works well for small loads, it tends to share processor time with anyone. <div class="spoiler">  <b class="spoiler_title">This is all about seals of course.</b> <div class="spoiler_text">  Experience shows that on each server there is at least one process that currently draws seals, and it will not give the CPU to anyone until all the seals on the internet have been carefully drawn and clicked </div></div>  There is only one way out and it is obvious - use <i>std :: condition_variable</i> , since we have already used mutexes for synchronization and the changes in the code will be minimal.  In this embodiment, the recipient falls asleep on a variable if the queue is empty, and the message generator sends a signal if it suspects that the partner can sleep.  In this case, the kernel has all the possibilities for optimization and we get the result, 3 microseconds.  It is already possible to say that wow, we literally step on the heels of every tricky implementation, while the basic code is extremely simple and can be adapted to all occasions.  There is no polymorphism here, of course, and did not sleep in D, but it turned out almost twice as fast.  No kidding, this is a very real competitor to non-blocking algorithms. <br><br><h5>  <b><a href="">Green.</a></b>  <b><a href="">Scalability, scalability.</a></b> </h5><br>  And this is an architectural solution that I have been searching for and carrying for a long time, although the result looks extremely simple.  It is often asked how many maximum messages per second can be transmitted through a queue and the like, forgetting that the opposite situation happens at least as often - even if we have a number of threads that do their work and should send messages from time to time, not too often, but important.  We do not want to hang on each such stream an individual listener who will still sleep for the most part, so we will have to create one common processing center that will poll all the queues and serve the messages as they arrive.  But since today we don‚Äôt have a long code evening, but an evening of short conceptual fragments, I suggest using <i>boost :: asio</i> , as a huge bonus this stream can also serve sockets and timers.  Here, by the way, it would be easy to do without a queue at all, capturing data directly in the function being transferred, the queue rather serves as an aggregator and a buffer for data, and of course for the meaningful link with the previous examples. <br>  And what do we get?  4.3 microseconds on the process of only one generator, not bad at all.  It should be borne in mind that the result will inevitably deteriorate in a system where many threads simultaneously write messages, but scalability is almost unlimited and it is worth a lot. <br>  Once again I want to emphasize what the philosophical meaning of this fragment is - we are sending to another thread not just data, but data plus a functor, who himself knows how to work with them, something like an inter-stream virtuality.  This is such a general concept that it could probably claim to be a separate design pattern. <br><hr><br>  This completes the experimental part, if you need a code for all tests, then <a href="https://github.com/sdegtiarev/mque">here it is</a> .  Carefully, this is not a ready-made library, so I do not advise you to copy it thoughtlessly, but it can serve as a completely suitable tutorial for developing your code.  Additions and improvements are welcome. <br><br><h5>  <b>Different reasoning, in the case and not very.</b> </h5><br>  Why do we need message queues at all?  As example D teaches us, this is the most kosher pattern for designing multi-threaded systems, for which the future means the future and behind the queues too.  But are all the queues the same?  What are the options and what are the differences?  That's about it and talk. <br>  First you need to distinguish between <i>data</i> <i>streams</i> and <i>message flows</i> .  With data streams everything is relatively simple, each transmitted fragment does not carry a semantic load and the boundaries between fragments are rather arbitrary.  Copying costs are comparable to or exceed the resources consumed by the queue itself and the recipe in this case is extremely simple - increase the internal buffer as much as possible, get just incredible speed.  A data quantum, a large file, for example, can be considered as one message, so large that it cannot be technically transmitted at once.  Well, that's all, there is probably nothing more to say about it.  But in the message flow, each next fragment carries a complete piece of information and should cause an immediate reaction, we are talking about them today. <br>  It is also useful to analyze the architecture from the point of view of <i>connectivity</i> , what connects with what.  The simplest type is the ‚Äúpipe‚Äù that connects two streams, the writer and the reader, its main purpose is to provide a decoupling of the input and output streams, ideally neither of them should be aware of the problems of the other.  The second atomic type of queue is a ‚Äúfunnel‚Äù where an arbitrary number of threads can write, but only one reads.  This is probably the most requested case, the simplest example is the logger.  And in general, this is all, the opposite case, when one thread writes and reads a little, is realized with the help of a bundle of "pipes" and therefore is not atomic, and if you suddenly need a queue, anyone can write and read from anyone else I would strongly advise to revise my attitude to life in general and to the design of multithreaded systems in particular. <br>  Returning to the decoupling of the input and output streams, this inevitably leads to the conclusion that the ideal queue must be <b>dimensionless</b> , that is, if necessary, contain <i>infinitely many</i> messages.  A simple example: let an extremely important and responsible stream want to write a short message to the log and return to its most important matters.  However, our log is built on the basis of a queue with a fixed-size buffer, and here is just that someone dropped ‚ÄúWar and Peace‚Äù in full.  What to do?  Such a low priority task as a logger should not block a calling thread, return an error or an exception is extremely undesirable from an architectural point of view (we shift the responsibility to the calling function, we commit it to track all possible outcomes, we extremely complicate the calling code and the probability of error, and instead of we get nothing - what to do is still not clear).  And in general, what was all this talk about non-blocking queues for, if it‚Äôs right here before our eyes is blocked ?.  That is why I have already mentioned that standard queues D are not a universal solution for inter-thread communication, moreover, a non-blocking boost :: lockfree :: queue in one of the options also uses a fixed buffer and is not actually a non-blocking queue, although it uses a non-blocking algorithm . <br>  Fortunately, RAM is now one of the cheapest resources, so the <b>adaptive strategy</b> will probably be the most optimal among the universal ones - memory, if necessary, is allocated from the heap ( <s>not to run twice in</s> large pieces) and is never released, thus the queue size adjusts to bursts traffic and, with normal statistics, referring to the allocator happens less and less.  Experience shows that even on medium-sized servers such an approach easily gives several hours of handicap, in which you can manage to fix something, hang it up to a planned stop, <s>or at least find another job</s> . <br>  And finally - the statistical nature of traffic.  I have already spoken about the difference in data transmission and transmission of messages, but messages can also have different distribution in time.  Oddly enough, the easiest case if the data arrive as quickly as possible (but not faster than we manage to remove them from the queue), but at the same time evenly.  At the same time, various accelerators work most effectively, from spin locks to tools built into the system.  More difficult is the case when powerful bursts occur in the message flow, which are guaranteed to surpass the processing speed.  In this mode, the queue should effectively accumulate incoming messages, allocating memory if necessary and not allowing for a significant slowdown. <div class="spoiler">  <b class="spoiler_title">It was here that I cheated</b> <div class="spoiler_text">  in tests, messages are sent strictly one by one and I have not investigated in any way the behavior of queues D when locked on the record, nor the behavior of C queues if necessary, allocating memory.  I also did not explore the mutual influence and struggle for the resources of several threads, especially when there are more of them than physical CPUs.  In terms of volume, it easily pulls into a separate post. </div></div>  However, the most possible heavy mode - when messages come <i>very</i> rarely, but require an immediate response.  During this time, anything can happen, including dropping into a swap.  If during the normal distribution of intervals such events occur rarely and fall into those fractions of a percent that we have discarded in tests, then the efficiency may fall by orders of magnitude. <br><br><h6>  <b>Incomplete list of rakes in stock.</b> </h6><br><ul><li>  <b>Observe the balance of writing and reading</b> : if the reading thread does not cope, no algorithm will save you.  Do what you want to speed it up, just do not blame the queue. </li><li>  <b>Cumulative slowdown</b> : it happens that for some implementations the speed of the queue depends on the number of messages in it, then a random burst of activity can slow down the queue so much that it does not become free until the next burst, approximately like a traffic jam.  Such instability is quite difficult to simulate when testing. </li><li>  <b>Night Watchman Syndrome</b> : Sometimes messages come very rarely, once an hour or even once a day, from the point of view of the OS it is still an eternity.  If the watchman sits and waits for an alarm, and the signal has never been in his entire life, what will he do at a critical moment?  It is difficult to fight such a spontaneous sleep. </li><li>  <b>Consider the distribution tail</b> : in the above tests, 2-3 messages per 1000 were processed abnormally long, this is a common feature for all general-purpose operating systems.  If you suddenly need to lower this number, you have to work hard. </li><li>  <b>Do not rely on dedicated CPUs</b> : this is a powerful accelerating factor, but absolutely not scalable.  Unlike memory, CPU is an expensive resource.  Even if you have 100,500 cores in your system, there will definitely be 100,500 + 1 developers who want one thing for themselves. <div class="spoiler">  <b class="spoiler_title">for personal use.</b> <div class="spoiler_text">  The same PM who, a year ago, himself proposed to reserve the core for speeding up the queues, will now look into the soul with honest blue eyes and say, ‚ÄúTo me, the guys from the frontend came to me, on their home page the company logo was redrawn with ugly jerks.  They ask to single out one of the cores to them, you understand - this is serious and obvious to all, even the customer paid attention the other day.  And your server seems to be working, and no one sees it anyway. ‚Äù  If this happens, I recommend deep breathing and slowly count to ten, otherwise destruction and sacrifice are inevitable. </div></div></li><li>  <b>There is a lot more</b> : but I forgot what exactly </li></ul><br><hr><br>  It is customary to end on an optimistic note: behind multithreading it‚Äôs not that the future is more like the present.  And behind the powerful, flexible and universal messaging mechanisms - the future, but they are not really written, apparently waiting for us. <br>  All success. </div><p>Source: <a href="https://habr.com/ru/post/211717/">https://habr.com/ru/post/211717/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../211707/index.html">Artificial Intelligence for Programmers</a></li>
<li><a href="../211709/index.html">Purple squiggles</a></li>
<li><a href="../211711/index.html">The US Army is funding the development of unmanned trucks</a></li>
<li><a href="../211713/index.html">Monitoring of Runet technologies for 2013</a></li>
<li><a href="../211715/index.html">Campaign for the reform of copyright in the digital age "Time to change copyright!"</a></li>
<li><a href="../211721/index.html">One more step to perfect bookmarks</a></li>
<li><a href="../211723/index.html">The Elder Scrolls Online Free Beta Keys</a></li>
<li><a href="../211725/index.html">6 concepts that should finally make me work</a></li>
<li><a href="../211727/index.html">LibRaw, Coverity SCAN, PVS-Studio</a></li>
<li><a href="../211729/index.html">Exit school of programming: what can be done with students in three days in a dark forest</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>