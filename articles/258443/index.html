<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Kubernetes Basics</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this publication, I wanted to tell you about an interesting, but unfairly little described on Habr√©, container management system Kubernetes. 



 W...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Kubernetes Basics</h1><div class="post__text post__text-html js-mediator-article">  In this publication, I wanted to tell you about an interesting, but unfairly little described on Habr√©, container management system Kubernetes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6ea/1b1/d57/6ea1b1d575af9daaec0e0b095cc88420.jpg" alt="image"><br><br><h4>  <b>What is Kubernetes?</b> </h4><br>  Kubernetes is an open source project designed to manage a cluster of Linux containers as a single system.  Kubernetes manages and runs Docker containers on a large number of hosts, and also provides co-location and replication of a large number of containers.  The project was launched by Google and is now supported by many companies, including Microsoft, RedHat, IBM and Docker. <br><a name="habracut"></a><br>  Google has been using container technology for over a decade.  She started with the launch of more than 2 billion containers in one week.  With the help of the Kubernetes project, the company shares its experience in creating an open platform designed for scalable container launch. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The project has two objectives.  If you are using Docker containers, the next question is how to scale and run containers at once on a large number of Docker hosts, and also how to balance them.  The project proposes a high-level API that defines a logical grouping of containers, which allows defining pools of containers, balancing the load, and also specifying their location. <br><br><h4>  <b>Kubernetes concepts</b> </h4><br>  <b>Nodes</b> ( <a href="">node.md</a> ): A <a href="">node</a> is a machine in a Kubernetes cluster. <br>  <b>Pods</b> ( <a href="">pods.md</a> ): Pod is a group of containers with shared partitions running as a unit. <br>  <b>Replication Controllers</b> ( <a href="">replication-controller.md</a> ): replication controller ensures that a certain number of "replicas" pods will be started at any time. <br>  <b>Services</b> ( <a href="">services.md</a> ): A service in Kubernetes is an abstraction that defines a logical, integrated set of pods and access policies for them. <br>  <b>Volumes</b> ( <a href="">volumes.md</a> ): Volume (section) is a directory, possibly with data in it, which is available in a container. <br>  <b>Labels</b> ( <a href="">labels.md</a> ): Labels are key / value pairs that are attached to objects, such as pods.  Labels can be used to create and select sets of objects. <br>  <b>Kubectl Command Line Interface</b> ( <a href="">kubectl.md</a> ): kubectl command line interface for managing Kubernetes. <br><br><h4>  <b>Kubernetes architecture</b> </h4><br>  A working Kubernetes cluster includes an agent running on nodes (kubelet) and wizard components (APIs, scheduler, etc), on top of a solution with distributed storage.  The above diagram shows the desired, ultimately, state, although some things are still being worked on, for example: how to make the kubelet (all components, in fact) run independently in a container, which will make the scheduler 100% connectable. <br><img src="https://habrastorage.org/getpro/habr/post_images/8de/5be/a86/8de5bea86458f0a50a18631e16b867f6.png" alt="image"><br><br><h5>  <b>Noda Kubernetes</b> </h5><br>  When looking at the system architecture, we can break it down into services that run on each node and cluster management level services.  On each Kubernetes node, the services necessary for managing the node on the part of the wizard and for launching applications are launched.  Of course, Docker is launched on each node.  Docker provides loading of images and launch of containers. <br><br><h6>  <b>Kubelet</b> </h6><br>  Kubelet manages pods of their containers, images, sections, etc. <br><br><h6>  <b>Kube-Proxy</b> </h6><br>  Also, a simple proxy balancer is launched on each node.  This service runs on each node and is configured in the Kubernetes API.  Kube-Proxy can perform the simplest redirection of TCP and UDP (round robin) streams between a set of backends. <br><br><h5>  <b>Kubernetes Management Components</b> </h5><br>  The Kubernetes control system is divided into several components.  At the moment, they all run on the master node, but soon it will be changed to enable the creation of a fault-tolerant cluster.  These components work together to provide a unified view of the cluster. <br><br><h6>  <b>etcd</b> </h6><br>  The status of the wizard is stored in the etcd instance.  This ensures reliable storage of configuration data and timely notification of other components of state changes. <br><br><h6>  <b>Kubernetes API Server</b> </h6><br>  Kubernetes API provides the operation of the api-server.  It is designed to be a CRUD server with embedded business logic implemented in individual components or plug-ins.  It mainly handles REST operations, checking them and updating the corresponding objects in etcd (and eventually in other repositories). <br><br><h6>  <b>Scheduler</b> </h6><br>  Scheduler binds unallocated pods to the nodes via the / binding API call.  Scheduler connect;  Support for multiple schedulers and custom schedulers is planned. <br><br><h6>  <b>Kubernetes Controller Manager Server</b> </h6><br>  All other cluster-level functions are represented in the Controller Manager.  For example, nodes are detected, managed, and controlled by node controller.  This entity can eventually be divided into separate components to make them independently connectable. <br><br>  ReplicationController is a mechanism based on the pod API.  Ultimately, it is planned to transfer it to the general mechanism of the plug-in when it is implemented. <br><br><h4>  <b>Cluster Configuration Example</b> </h4><br>  Ubuntu-server 14.10 was chosen as the platform for the example of configuration as the simplest for the example and, at the same time, allowing to demonstrate the basic cluster settings. <br><br>  To create a test cluster, three machines will be used to create nodes and a separate machine for remote installation.  You can not allocate a separate machine and install with one of the nodes. <br><br>  <b>List of used machines:</b> <br><ul><li>  Conf </li><li>  Node1: 192.168.0.10 - master, minion </li><li>  Node2: 192.168.0.11 - minion </li><li>  Node3: 192.168.0.12 - minion </li></ul><br><h5>  <b>Node preparation</b> </h5><br><h6>  <b>Requirements to run:</b> </h6><br><ol><li>  Docker version 1.2+ and bridge-utils are installed on all nodes </li><li>  All the machines are connected with each other, there is no need to access the Internet (in this case it is necessary to use the local docker registry) </li><li>  All nodes can be entered without entering a login / password using ssh-keys </li></ol><br><h6>  <b>Installing software on nodes</b> </h6><br>  Docker installation can be done under the article <a href="https://docs.docker.com/installation/ubuntulinux/">in the official sources</a> : <br><br><pre><code class="bash hljs">node% sudo apt-get update $ sudo apt-get install wget node% wget -qO- https://get.docker.com/ | sh</code> </pre> <br>  Additional setup Docker after installation is not needed, because  will be generated by the Kubernetes installation script. <br>  Install bridge-utils: <br><br><pre> <code class="bash hljs">node% sudo apt-get install bridge-utils</code> </pre><br><h6>  <b>Adding ssh keys</b> </h6><br>  Perform on the machine with which the installation script will run. <br>  If keys are not yet created, create them: <br><br><pre> <code class="bash hljs">conf% ssh-keygen</code> </pre><br>  Copy the keys to remote machines, after making sure that they have the necessary user, in our case core. <br><br><pre> <code class="bash hljs">conf% ssh-copy-id core@192.168.0.10 conf% ssh-copy-id core@192.168.0.11 conf% ssh-copy-id core@192.168.0.12</code> </pre><br><h5>  <b>Kubernetes installation</b> </h5><br>  Next we will install Kubernetes directly.  To do this, first download and unpack the latest available release from GitHub: <br><br><pre> <code class="bash hljs">conf% wget https://github.com/GoogleCloudPlatform/kubernetes/releases/download/v0.17.0/kubernetes.tar.gz conf% tar xzf ./kubernetes.tar.gz conf% <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> ./kubernetes</code> </pre><br><h6>  <b>Customization</b> </h6><br>  Configuring Kubernetes through standard example scripts is completely done before installation is done through configuration files.  During installation, we will use the scripts in the folder ./cluster/ubuntu/. <br><br>  First we change the script ./cluster/ubuntu/build.sh which downloads and prepares the Kubernetes, etcd and flannel binaries needed for installation: <br><br><pre> <code class="bash hljs">conf% vim ./cluster/ubuntu/build.sh</code> </pre><br>  In order to use the latter, at the time of this writing, the release 0.17.0 must be replaced: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># k8s echo "Download kubernetes release ..." K8S_VERSION="v0.15.0"</span></span></code> </pre><br>  On: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># k8s echo "Download kubernetes release ..." K8S_VERSION="v0.17.0"</span></span></code> </pre><br>  And run: <br><br><pre> <code class="bash hljs">conf% <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> ./cluster/ubuntu/ conf% ./build.sh <span class="hljs-comment"><span class="hljs-comment">#       ,   .</span></span></code> </pre><br>  Next, we specify the parameters of the future cluster, for which we edit the ./config-default.sh file: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">## Contains configuration values for the Ubuntu cluster #         , MASTER-   #     &lt;user_1@ip_1&gt; &lt;user_2@ip_2&gt; &lt;user_3@ip_3&gt;  -  #            ssh- export nodes="core@192.168.0.10 core@192.168.0.10 core@192.168.0.10" #    : a(master)  i(minion)  ai(master  minion),     ,      . export roles=("ai" "i" "i") #    export NUM_MINIONS=${NUM_MINIONS:-3} #  IP-  ,       . #    ,      , ..         . #  IP-    iptables  . export PORTAL_NET=192.168.3.0/24 #           flannel. #flannel       24   ,        Docker-. #     PORTAL_NET export FLANNEL_NET=172.16.0.0/16 # Admission Controllers      . ADMISSION_CONTROL=NamespaceLifecycle,NamespaceAutoProvision,LimitRanger,ResourceQuota #    Docker.       #   --insecure-registry   . DOCKER_OPTS=""</span></span></code> </pre><br>  This completes the setting and you can proceed to the installation. <br><br><h6>  <b>Installation</b> </h6><br>  First of all, you need to inform the system about our ssh-agent and use the ssh-key to do this: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">eval</span></span> `ssh-agent -s` ssh-add ///</code> </pre><br>  Next, go directly to the installation.  To do this, use the script ./kubernetes/cluster/kube-up.sh which needs to indicate that we are using ubuntu. <br><br><pre> <code class="bash hljs">conf% <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> ../ conf% KUBERNETES_PROVIDER=ubuntu ./kube-up.sh</code> </pre><br>  During installation, the script will require a sudo password for each node.  At the end of the installation, check the cluster status and display a list of nodes and addresses of Kubernetes api. <br><br><div class="spoiler">  <b class="spoiler_title">Sample script output</b> <div class="spoiler_text"><pre> <code class="bash hljs">Starting cluster using provider: ubuntu ... calling verify-prereqs ... calling kube-up Deploying master and minion on machine 192.168.0.10 &lt;  &gt; [sudo] password to copy files and start node: etcd start/running, process 16384 Connection to 192.168.0.10 closed. Deploying minion on machine 192.168.0.11 &lt;  &gt; [sudo] password to copy files and start minion: etcd start/running, process 12325 Connection to 192.168.0.11 closed. Deploying minion on machine 192.168.0.12 &lt;  &gt; [sudo] password to copy files and start minion: etcd start/running, process 10217 Connection to 192.168.0.12 closed. Validating master Validating core@192.168.0.10 Validating core@192.168.0.11 Validating core@192.168.0.12 Kubernetes cluster is running. The master is running at: http://192.168.0.10 ... calling validate-cluster Found 3 nodes. 1 NAME LABELS STATUS 2 192.168.0.10 &lt;none&gt; Ready 3 192.168.0.11 &lt;none&gt; Ready 4 192.168.0.12 &lt;none&gt; Ready Validate output: NAME STATUS MESSAGE ERROR etcd-0 Healthy {<span class="hljs-string"><span class="hljs-string">"action"</span></span>:<span class="hljs-string"><span class="hljs-string">"get"</span></span>,<span class="hljs-string"><span class="hljs-string">"node"</span></span>:{<span class="hljs-string"><span class="hljs-string">"dir"</span></span>:<span class="hljs-literal"><span class="hljs-literal">true</span></span>,<span class="hljs-string"><span class="hljs-string">"nodes"</span></span>:[{<span class="hljs-string"><span class="hljs-string">"key"</span></span>:<span class="hljs-string"><span class="hljs-string">"/coreos.com"</span></span>,<span class="hljs-string"><span class="hljs-string">"dir"</span></span>:<span class="hljs-literal"><span class="hljs-literal">true</span></span>,<span class="hljs-string"><span class="hljs-string">"modifiedIndex"</span></span>:11,<span class="hljs-string"><span class="hljs-string">"createdIndex"</span></span>:11},{<span class="hljs-string"><span class="hljs-string">"key"</span></span>:<span class="hljs-string"><span class="hljs-string">"/registry"</span></span>,<span class="hljs-string"><span class="hljs-string">"dir"</span></span>:<span class="hljs-literal"><span class="hljs-literal">true</span></span>,<span class="hljs-string"><span class="hljs-string">"modifiedIndex"</span></span>:5,<span class="hljs-string"><span class="hljs-string">"createdIndex"</span></span>:5}],<span class="hljs-string"><span class="hljs-string">"modifiedIndex"</span></span>:5,<span class="hljs-string"><span class="hljs-string">"createdIndex"</span></span>:5}} nil controller-manager Healthy ok nil scheduler Healthy ok nil Cluster validation succeeded Done, listing cluster services: Kubernetes master is running at http://192.168.0.10:8080</code> </pre><br></div></div><br>  Let's see which nodes and services are present in the new cluster: <br><pre> <code class="bash hljs">conf% cp ../kubernetes/platforms/linux/amd64/kubectl /opt/bin/ conf% /opt/bin/kubectl get services,minions -s <span class="hljs-string"><span class="hljs-string">"http://192.168.0.10:8080"</span></span> NAME LABELS SELECTOR IP PORT(S) kubernetes component=apiserver,provider=kubernetes &lt;none&gt; 192.168.3.2 443/TCP kubernetes-ro component=apiserver,provider=kubernetes &lt;none&gt; 192.168.3.1 80/TCP NAME LABELS STATUS 192.168.0.10 &lt;none&gt; Ready 192.168.0.11 &lt;none&gt; Ready 192.168.0.12 &lt;none&gt; Ready</code> </pre><br>  We see a list of installed nodes in the Ready state and two pre-installed services kubernetes and kubernetes-ro - this is a proxy for direct access to the Kubernetes API.  As with any Kubernetes service, kubernetes and kubernetes-ro can be accessed directly at the IP address from any of the nodes. <br><br><h4>  <b>Running a test service</b> </h4><br>  To start the service, you need to prepare a docker container, on the basis of which the service will be created.  In order not to complicate things, the example will use the public nginx container.  The obligatory components of the service are Replication Controller, which ensures the neglect of the necessary set of containers (more precisely, pod) and service, which determines which IP address and ports will listen to the service and the rules for distributing requests between the pods. <br><br>  Any service can be launched in 2 ways: manually and using a config file.  Consider both. <br><br><h5>  <b>Start the service manually</b> </h5><br>  Let's start by creating the Replication Controller: <br><br><pre> <code class="bash hljs">conf% /opt/bin/kubectl run-container nginx --port=80 --port=443 --image=nginx --replicas=6 -s <span class="hljs-string"><span class="hljs-string">"http://192.168.0.10:8080"</span></span></code> </pre><br>  Where: <br><ul><li>  nginx is the name of the future rc </li><li>  --port - ports on which rc containers will listen </li><li>  --image - the image from which the containers will be launched </li><li>  --replicas = 6 - number of replicas </li></ul><br>  Let's see what we got: <br><br><pre> <code class="bash hljs">/opt/bin/kubectl get pods,rc -s <span class="hljs-string"><span class="hljs-string">"http://192.168.0.10:8080"</span></span></code> </pre><br><div class="spoiler">  <b class="spoiler_title">Conclusion</b> <div class="spoiler_text"><pre> <code class="bash hljs">POD IP CONTAINER(S) IMAGE(S) HOST LABELS STATUS CREATED MESSAGE nginx-3gii4 172.16.58.4 192.168.0.11/192.168.0.11 run-container=nginx Running 9 seconds nginx nginx Running 9 seconds nginx-3xudc 172.16.62.6 192.168.0.10/192.168.0.10 run-container=nginx Running 9 seconds nginx nginx Running 8 seconds nginx-igpon 172.16.58.6 192.168.0.11/192.168.0.11 run-container=nginx Running 9 seconds nginx nginx Running 8 seconds nginx-km78j 172.16.58.5 192.168.0.11/192.168.0.11 run-container=nginx Running 9 seconds nginx nginx Running 8 seconds nginx-sjb39 172.16.83.4 192.168.0.12/192.168.0.12 run-container=nginx Running 9 seconds nginx nginx Running 8 seconds nginx-zk1wv 172.16.62.7 192.168.0.10/192.168.0.10 run-container=nginx Running 9 seconds nginx nginx Running 8 seconds CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS nginx nginx nginx run-container=nginx 6</code> </pre><br></div></div><br>  A Replication Controller was created with the name nginx and the number of replicas equal to 6. Replicas were randomly launched on the nodes, the location of each pod is indicated in the HOST column. <br>  The output may differ from the one given in some cases, for example: <br><ul><li>  Part of the pod is in pending state: this means that they have not yet started, you need to wait a bit </li><li>  The pod does not have a HOST defined: this means that the scheduler has not yet assigned a node on which the pod will be running. </li></ul><br><br>  Next, create a service that will use our Replication Controller as a backend. <br>  For http: <br><br><pre> <code class="bash hljs">conf% /opt/bin/kubectl expose rc nginx --port=80 --target-port=80 --service-name=nginx-http -s <span class="hljs-string"><span class="hljs-string">"http://192.168.0.10:8080"</span></span></code> </pre><br>  And for https: <br><br><pre> <code class="bash hljs">conf% /opt/bin/kubectl expose rc nginx --port=443 --target-port=443 --service-name=nginx-https -s <span class="hljs-string"><span class="hljs-string">"http://192.168.0.10:8080"</span></span></code> </pre><br>  Where: <br><ul><li>  rc nginx - the type and name of the resource used (rc = Replication Controller) </li><li>  --port - the port on which the service will ‚Äúlisten‚Äù </li><li>  - target-port - the container port on which requests will be broadcast </li><li>  --service-name - future service name </li></ul><br>  Check the result: <br><br><pre> <code class="bash hljs">/opt/bin/kubectl get rc,services -s <span class="hljs-string"><span class="hljs-string">"http://192.168.0.10:8080"</span></span></code> </pre><br><div class="spoiler">  <b class="spoiler_title">Conclusion</b> <div class="spoiler_text"><pre> <code class="bash hljs">CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS nginx nginx nginx run-container=nginx 6 NAME LABELS SELECTOR IP PORT(S) kubernetes component=apiserver,provider=kubernetes &lt;none&gt; 192.168.3.2 443/TCP kubernetes-ro component=apiserver,provider=kubernetes &lt;none&gt; 192.168.3.1 80/TCP nginx-http &lt;none&gt; run-container=nginx 192.168.3.66 80/TCP nginx-https &lt;none&gt; run-container=nginx 192.168.3.172 443/TCP</code> </pre><br></div></div><br>  To check the neglect, you can go to any of the nodes and run in the console: <br><br><pre> <code class="bash hljs">node% curl http://192.168.3.66</code> </pre><br>  In the output of curl we will see the standard welcome page of nginx.  Done, the service is running and available. <br><br><h5>  <b>Starting the service using configs</b> </h5><br>  For this launch method, you need to create configs for Replication Controller and service.  Kubernetes accepts configs in yaml and json formats.  Yaml is closer to me, so we will use it. <br><br>  Pre-clean our cluster from the previous experiment: <br><br><pre> <code class="bash hljs">conf% /opt/bin/kubectl delete services nginx-http nginx-https -s <span class="hljs-string"><span class="hljs-string">"http://192.168.0.10:8080"</span></span> conf% /opt/bin/kubectl stop rc nginx -s <span class="hljs-string"><span class="hljs-string">"http://192.168.0.10:8080"</span></span>     .</code> </pre><br>  <b>nginx_rc.yaml</b> <br><div class="spoiler">  <b class="spoiler_title">content</b> <div class="spoiler_text"><pre> <code class="bash hljs">apiVersion: v1beta3 kind: ReplicationController <span class="hljs-comment"><span class="hljs-comment">#   ReplicationController metadata: name: nginx-controller spec: #    replicas: 6 selector: name: nginx template: metadata: labels: name: nginx spec: containers: #  - name: nginx image: nginx #  ports: - containerPort: 80 - containerPort: 443 livenessProbe: #    enabled: true type: http #     pod'     initialDelaySeconds: 30 TimeoutSeconds: 5 # http  httpGet: path: / port: 80 portals: - destination: nginx</span></span></code> </pre><br></div></div><br>  Apply config: <br><br><pre> <code class="bash hljs">conf% /opt/bin/kubectl create -f ./nginx_rc.yaml -s <span class="hljs-string"><span class="hljs-string">"http://192.168.0.10:8080"</span></span></code> </pre><br>  Check the result: <br><br><pre> <code class="bash hljs">conf% /opt/bin/kubectl get pods,rc -s <span class="hljs-string"><span class="hljs-string">"http://192.168.0.10:8080"</span></span></code> </pre><br><div class="spoiler">  <b class="spoiler_title">Conclusion</b> <div class="spoiler_text"><pre> <code class="bash hljs">POD IP CONTAINER(S) IMAGE(S) HOST LABELS STATUS CREATED MESSAGE nginx-controller-0wklg 172.16.58.7 192.168.0.11/192.168.0.11 name=nginx Running About a minute nginx nginx Running About a minute nginx-controller-2jynt 172.16.58.8 192.168.0.11/192.168.0.11 name=nginx Running About a minute nginx nginx Running About a minute nginx-controller-8ra6j 172.16.62.8 192.168.0.10/192.168.0.10 name=nginx Running About a minute nginx nginx Running About a minute nginx-controller-avmu8 172.16.58.9 192.168.0.11/192.168.0.11 name=nginx Running About a minute nginx nginx Running About a minute nginx-controller-ddr4y 172.16.83.7 192.168.0.12/192.168.0.12 name=nginx Running About a minute nginx nginx Running About a minute nginx-controller-qb2wb 172.16.83.5 192.168.0.12/192.168.0.12 name=nginx Running About a minute nginx nginx Running About a minute CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS nginx-controller nginx nginx name=nginx 6</code> </pre><br></div></div><br>  A Replication Controller was created with the name nginx and the number of replicas equal to 6. Replicas were randomly launched on the nodes, the locations of each pods are indicated in the HOST column. <br><br>  <b>nginx_service.yaml</b> <br><div class="spoiler">  <b class="spoiler_title">Content</b> <div class="spoiler_text"><pre> <code class="bash hljs">apiVersion: v1beta3 kind: Service metadata: name: nginx spec: publicIPs: - 12.0.0.5 <span class="hljs-comment"><span class="hljs-comment"># IP       . ports: - name: http port: 80 #      targetPort: 80         protocol: TCP - name: https port: 443 targetPort: 443 protocol: TCP selector: name: nginx #        ReplicationController</span></span></code> </pre><br></div></div><br>  You may notice that when using the config, several ports can be assigned to one service. <br>  Apply config: <br><br><pre> <code class="bash hljs">conf% /opt/bin/kubectl create -f ./nginx_service.yaml -s <span class="hljs-string"><span class="hljs-string">"http://192.168.0.10:8080"</span></span></code> </pre><br>  Check the result: <br><br><pre> <code class="bash hljs">/opt/bin/kubectl get rc,services -s <span class="hljs-string"><span class="hljs-string">"http://192.168.0.10:8080"</span></span></code> </pre><br><div class="spoiler">  <b class="spoiler_title">Conclusion</b> <div class="spoiler_text"><pre> <code class="bash hljs">CONTROLLER CONTAINER(S) IMAGE(S) SELECTOR REPLICAS nginx-controller nginx nginx name=nginx 6 NAME LABELS SELECTOR IP PORT(S) kubernetes component=apiserver,provider=kubernetes &lt;none&gt; 192.168.3.2 443/TCP kubernetes-ro component=apiserver,provider=kubernetes &lt;none&gt; 192.168.3.1 80/TCP nginx &lt;none&gt; name=nginx 192.168.3.214 80/TCP 12.0.0.5 443/TCP</code> </pre><br></div></div><br>  To check the neglect, you can go to any of the nodes and run in the console: <br><br><pre> <code class="bash hljs">node% curl http://192.168.3.214 node% curl http://12.0.0.5</code> </pre><br>  In the output of curl we will see the standard welcome page of nginx. <br><br><h4>  <b>Margin notes</b> </h4><br>  As a conclusion, I want to describe a couple of important points about which I had to falter when designing a system.  They were connected with the work of kube-proxy, the very module that allows you to turn a disparate set of elements into a service. <br>  <b>PORTAL_NET.</b>  The essence itself is interesting, I suggest to get acquainted with how it is implemented. <br>  A brief excavation led me to the realization of a simple but effective model, let's look at the iptables-save output: <br><br><pre> <code class="bash hljs">-A PREROUTING -j KUBE-PORTALS-CONTAINER -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER -A OUTPUT -j KUBE-PORTALS-HOST -A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER -A POSTROUTING -s 10.0.42.0/24 ! -o docker0 -j MASQUERADE -A KUBE-PORTALS-CONTAINER -d 10.0.0.2/32 -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/kubernetes:"</span></span> -m tcp --dport 443 -j REDIRECT --to-ports 46041 -A KUBE-PORTALS-CONTAINER -d 10.0.0.1/32 -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/kubernetes-ro:"</span></span> -m tcp --dport 80 -j REDIRECT --to-ports 58340 -A KUBE-PORTALS-HOST -d 10.0.0.2/32 -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/kubernetes:"</span></span> -m tcp --dport 443 -j DNAT --to-destination 172.16.67.69:46041 -A KUBE-PORTALS-HOST -d 10.0.0.1/32 -p tcp -m comment --comment <span class="hljs-string"><span class="hljs-string">"default/kubernetes-ro:"</span></span> -m tcp --dport 80 -j DNAT --to-destination 172.16.67.69:58340</code> </pre><br>  All requests to the IP address of the service are wrapped in iptables on the port on which kube-proxy listens.  One problem arises in this connection: Kubernetes, in itself, does not solve the problem of communication with the user.  Therefore it is necessary to solve this question by external means, for example: <br><ul><li>  gcloud - paid development from Google </li><li>  bgp - using subnet announcements </li><li>  IPVS </li><li>  and other options that many </li></ul><br>  <b>SOURCE IP</b> Same.  when setting up the nginx service I had to face an interesting problem.  It looked like a line in the manual: ‚ÄúUsing the kite-proxy obscures of the access-IP of a packet accessing service.‚Äù  Literally - when using kube-proxy, it hides the source address of the packet, which means that all processing based on source-IP will have to be carried out before using kube-proxy. <br><br>  <b>That's all, thank you for your attention</b> <br>  Unfortunately, all the information that I want to convey, can not fit in one article. <br><br><h5>  <b>Use materials:</b> </h5><br><ul><li>  <a href="">Kubernetes architecture</a> </li><li>  <a href="">Kubernetes user guide</a> </li><li>  <a href="">Kubernetes ubuntu cluster</a> </li><li>  <a href="http://www.infoq.com/articles/scaling-docker-with-kubernetes">Scaling Docker with Kubernetes</a> </li><li>  <a href="">Kubernetes services</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/258443/">https://habr.com/ru/post/258443/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../258431/index.html">More IT professions for kids, even more</a></li>
<li><a href="../258433/index.html">We test Chinese iron and find out how cheap and angry it is</a></li>
<li><a href="../258437/index.html">Dagaz: Kicks to common sense (part 7)</a></li>
<li><a href="../258439/index.html">The new issue of the magazine "Radio Annual" number 35 (2015)</a></li>
<li><a href="../258441/index.html">Dynamic Meta Objects (part 1, study)</a></li>
<li><a href="../258445/index.html">Keeping recordings of conversations in mp3 in FreePBX / Asterisk</a></li>
<li><a href="../258447/index.html">We manage AB400S Wireless Switch socket without remote control</a></li>
<li><a href="../258449/index.html">How I hacked Starbucks for unlimited coffee</a></li>
<li><a href="../258451/index.html">Two opposite directions VIDEO ANALYTICS: "hard" and "flexible", who is stronger?</a></li>
<li><a href="../258457/index.html">TLS Logjam - FREAK with DH vulnerability</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>