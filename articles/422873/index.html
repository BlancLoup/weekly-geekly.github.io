<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Pizza ala-semi-supervised</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article I would like to tell you about some of the techniques for working with data when teaching a model. In particular, how to stretch the s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Pizza ala-semi-supervised</h1><div class="post__text post__text-html js-mediator-article">  In this article I would like to tell you about some of the techniques for working with data when teaching a model.  In particular, how to stretch the segmentation of objects on the boxes, as well as how to train the model and get the markup of the dataset, having marked only a few samples. <br><img src="https://habrastorage.org/webt/wo/lz/ab/wolzab2hx3sxc7qnqrmrucfhynk.png"><br><a name="habracut"></a><br><h2>  Task </h2><br>  There is a certain process of cooking pizza and photos from its different stages (including not only pizza).  It is known that if the dough recipe is spoiled, then there will be white bumps on the cake.  There is also a binary markup of the quality of the dough of each pizza made by experts.  It is necessary to develop an algorithm that will determine the quality of the test on the photo. <br><br>  Dataset consists of photos taken from different phones, in different conditions, different angles.  Copies of pizza - 17k.  Total photos - 60k. <br><br>  In my opinion, the task is quite typical and well suited to show different approaches to handling data.  To solve it you need: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      1. Select photos where there is a pizza shortbread; <br>  2. On the selected photos to highlight the cake; <br>  3. Train the neural network in the selected areas. <br><br><h2>  Photo Filtering </h2><br>  At first glance, it seems that the easiest thing would be to give this task to the markers, and then to train on clean data.  However, I decided that it would be easier for me to mark out a small part myself, than to explain with a marker, which angle was the right one.  Moreover, I did not have a hard criterion for the correct angle. <br><br>  Therefore, this is how I acted: <br><br>  1. Marked 100 pictures of the edge; <br><br><img src="https://habrastorage.org/webt/r3/_e/po/r3_epoxo9v9p_e0aljlsnqcdgge.png"><br><br>  2. I counted features after a global pulling from the resnet-152 grid with weights from imagenet11k_places365; <br><br><img src="https://habrastorage.org/webt/im/my/0c/immy0cck12em7iyx24yrxyzubwy.png"><br><br>  3. I took the average of the features of each class, getting two anchors; <br><br>  4. I calculated the distance from each anchor to all features of the remaining 50k photos; <br><br>  5. Top 300 in proximity to one anchor are relevant to the positive class, the top 500 closest to the other anchor - negative class; <br><br><img src="https://habrastorage.org/webt/ga/wn/xn/gawnxnw_jio9rg8yboqzqb3rmac.png"><br><br>  6. In these samples I trained LightGBM on the same features (XGboost is shown in the picture, because it has a logo and is more recognizable, but LightGBM does not have a logo); <br><br>  7. With the help of this model I got the markup of the whole dataset <br><br><img src="https://habrastorage.org/webt/bd/vb/rq/bdvbrqhqcf5crt3scgwhw5zttms.png"><br><br>  I used about the same approach in kaggle competitions as a <a href="https://www.kaggle.com/drn01z3/mxnet-xgboost-baseline-lb-0-57">baseline</a> . <br><br><div class="spoiler">  <b class="spoiler_title">The explanation on the fingers why this approach works at all</b> <div class="spoiler_text">  A neural network can be perceived as a strongly non-linear transformation of a picture.  In the case of classification, the picture is converted into the probabilities of the classes that were in the training set.  And these probabilities can essentially be used as features for Light GBM.  However, this is a rather poor description, and in the case of pizza, we will thus say that the class of the shortcake is conditionally 0.3 cats and 0.7 dogs, and trash is everything else.  Instead, you can use less sparse features, after the Global Average Pooling.  They have such a property that they will generate features from the samples of the training sample, which must be separated by a linear transformation (a fully connected layer with Softmax).  However, due to the fact that there was no explicit pizza in the imagenet'a train, in order to divide the classes of the new training set, it is better to take the nonlinear transformation in the form of trees.  In principle, you can go even further and take features from some intermediate layers of the neural network.  They will be better in that they have not yet lost the locality of the objects.  But they are much worse due to the size of the feature vector.  And besides, they are less linear than before a fully connected layer. <br></div></div><br><h2>  Small lyrical digression </h2><br>  In ODS recently complained that no one wrote about their Fails.  Correcting the situation.  About a year ago, I participated in the <a href="https://www.kaggle.com/c/noaa-fisheries-steller-sea-lion-population-count">Kaggle Sea Lions</a> competition with <a href="https://www.kaggle.com/nizhibitsky">Yevgeny Nizhibitsky</a> .  The task was to calculate the fur seals in the pictures from the drone.  The markup was given simply in the form of the coordinates of the carcasses, but at some point <a href="https://www.kaggle.com/iglovikov">Vladimir Iglovikov</a> marked them with boxes and generously shared it with the community.  At that time, I considered myself the father of semantic segmentation (after <a href="https://habr.com/company/avito/blog/325632/">Kaggle Dstl</a> ) and decided that Unet would greatly facilitate the task of counting if I learned how to distinguish the seals coolly. <br><br><div class="spoiler">  <b class="spoiler_title">Explanation of semantic segmentation</b> <div class="spoiler_text">  Semantic segmentation is essentially a pixel-by-pixel classification of a picture.  That is, each source pixel of the image must be assigned a class.  In the case of binary segmentation (the case of an article), this will be either a positive or a negative class.  In the case of multi-class segmentation, each pixel is inserted into the corresponding class from the training sample (background, grass, cat, person, etc.).  In the case of binary segmentation at that time, the <a href="https://arxiv.org/abs/1505.04597">U-net</a> neural network architecture worked well.  This neural network is similar in structure to a normal encoder-decoder, but with forwarding features from the encoder-part to the decoder at stages corresponding in size. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cdf/52f/999/cdf52f99903a93bd403db1b073f19ba0.png"><br><br>  In the vanilla form, however, no one uses it anymore, but at least Batch Norm is added.  Well, as a rule, they take a <a href="https://arxiv.org/abs/1801.05746">fat encoder</a> and inflate a decoder.  The new-fashioned <a href="http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf">FPN</a> segmentation grids, which show a good performance on some tasks, have now replaced the U-net-like archectures.  However, Unet-like architecture has not lost its relevance to this day.  They work well as a baseline, they are easy to train, and it‚Äôs very easy to vary the depth / size of the neurosis by changing different encoders. <br></div></div><br>  Accordingly, I began to teach segmentation, having as a target in the first stage only box seals.  After the first stage of training, I predicted the train and looked at what the predictions look like.  With the help of heuristics, it was possible to choose from an abstract confidence mask (confidence of predictions) and conditionally divide the predictions into two groups: where everything is good and where everything is bad. <br><br><img src="https://habrastorage.org/webt/wx/84/jt/wx84jttvncmdiykyt25csspnyec.png"><br><br>  Predictions where everything is good could be used to train the next iteration of the model.  Predicts, where everything is bad, it was possible to choose with large areas without seals, to manually mask the masks and also to dock in the train.  And so iteratively, we with Eugene taught a model that has learned to even segment the seals of sea lions for large individuals. <br><br><img src="https://habrastorage.org/webt/_a/g6/xz/_ag6xzccipgeizeek1qnncwxz4k.png"><br><br>  But it was a fierce fayl: we spent a lot of time trying to learn how to segment the seals abruptly and ... It almost didn‚Äôt help in their calculation.  The assumption that the density of seals (the number of individuals per unit area of ‚Äã‚Äãthe mask) is constant did not work, because the drone flew at different heights, and the pictures had a different scale.  And at the same time, the segmentation still did not single out individual individuals, if they lay tight - which happened quite often.  And before the <a href="https://www.kaggle.com/c/data-science-bowl-2018/discussion/54741">innovative approach to the division of objects</a> of the Tocoder team on the DSB2018, there was still a year.  As a result, we stayed at the broken trough and finished in 40th place of 600 teams. <br><br>  However, I made two conclusions: semantic segmentation is a convenient approach for visualizing and analyzing the operation of the algorithm, and you can weld masks out of the boxes with some effort. <br><br>  But back to the pizza.  In order to select a cake in the selected and filtered photos, the most appropriate option would be to give the task to the scribe.  At that time, we had already implemented the boxes and consensus algorithm for them.  So I just threw a couple of examples and put it on the markup.  In the end, I got 500 samples with exactly selected area of ‚Äã‚Äãthe cake. <br><br><img src="https://habrastorage.org/webt/pd/pr/nh/pdprnh1katpk2nl7gzq-edoy8cm.png"><br><br>  Then I dug out my code from the seals and more formally approached the current procedure.  After the first iteration of the training, it was just as well seen where the model is mistaken.  And the confidence of predictions can be defined as: <br>  1 - (gray area) / (mask area) # there will be a formula, I promise <br><br><img src="https://habrastorage.org/webt/hz/eb/9d/hzeb9dqh2fazgyo0gpdclqbp1du.png"><br><br>  Now, in order to do the next iteration of pulling the box on the mask, a small ensemble will predict a train with a TTA.  This can be considered to some extent WAAAAGH knowledge distillation, but it is more correct to call it Pseudo Labeling. <br><br><img src="https://habrastorage.org/webt/qk/he/eo/qkheeobxrxzxbim3gk8s8gihoyu.png"><br><br>  Next you need to use your eyes to choose a certain threshold of confidence, starting from which we form a new train.  And optionally you can mark the most difficult samples with which the ensemble has failed.  I decided that it would be useful, and I painted about 20 pictures somewhere while digesting dinner. <br><br><img src="https://habrastorage.org/webt/13/4e/wz/134ewz9jgris1o-9jmss90qt7ew.png"><br><br>  And now the final part of the pipeline: learning the model.  To prepare the samples, I extracted the area of ‚Äã‚Äãthe cake on the mask.  I also fanned the mask a little and applied it to the image to remove the background, since there should not be any information about the quality of the test.  And then I just refilled some models from the Imagenet Zoo.  In total, I managed to collect about 12k confident samples.  Therefore, I did not teach the entire neural network, but only the last group of convolutions, so that the model would not retrain. <br><br><div class="spoiler">  <b class="spoiler_title">Why do you need to freeze layers</b> <div class="spoiler_text">  From this there are two profits: 1. The network learns faster, because you do not need to count the gradients for frozen layers.  2. The network is not retrained, since it now has fewer free parameters.  At the same time, it is argued that the first several groups of convolutions during training at Imagenet generate fairly general features such as sharp color transitions and textures that are suitable for a very wide class of objects in a photograph.  This means that they can not be trained during Transer Learning. <br></div></div><br><img src="https://habrastorage.org/webt/_1/vq/-5/_1vq-5oea_ikx7b68me9v_hrjq0.png"><br><br>  Inception-Resnet-v2 turned out to be the best single model, and for her ROC-AUC on one fold was 0.700.  If you don‚Äôt select anything and feed raw pictures as they are, then ROC-AUC will be 0.58.  While I was developing a solution, DODO pizza cooked the next batch of data, and you could test the entire pipeline on an honest holdout.  We checked the entire pipeline on it and got a ROC-AUC 0.83. <br><br>  Let's now look at the errors: <br><br>  Top False Negative <br><br><img src="https://habrastorage.org/webt/nc/as/lz/ncaslzpmdyudlv5joncem_h9gcy.png"><br><br>  Here it can be seen that they are associated with the error of marking the shortcake, since there are clearly signs of a spoiled dough. <br><br>  Top False Positive <br><br><img src="https://habrastorage.org/webt/bu/hn/cz/buhncz0hnwtl5mf2l5bhqbfuat0.png"><br><br>  Here, errors are related to the fact that the first model selected a not very good angle, which is difficult to find the key signs of the quality of the test. <br><br><h2>  Conclusion </h2><br>  Colleagues sometimes tease me that I solve many problems by segmentation using Unet.  However, in my opinion, this is quite a powerful and convenient approach.  It allows you to visualize model errors and the confidence of its predictions.  In addition, the entire pipeline looks very simple and now there are a lot of repositories for any framework. </div><p>Source: <a href="https://habr.com/ru/post/422873/">https://habr.com/ru/post/422873/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../422863/index.html">Tribes, guilds, build train and no TDD: how does mobile development work in Uber, Spotify, Odnoklassniki and Avito</a></li>
<li><a href="../422865/index.html">Java Enterprise open lesson "CDI in action"</a></li>
<li><a href="../422867/index.html">Determination of the number of floors of the house by its photos without machine learning</a></li>
<li><a href="../422869/index.html">We master new programming languages, based on the already studied</a></li>
<li><a href="../422871/index.html">Identifying content profiles in VK</a></li>
<li><a href="../422875/index.html">Why you should not rent a VPS / VDS for 200 rubles or how to choose a virtual server</a></li>
<li><a href="../422877/index.html">"Is This IoT?" - learn not to call the Internet of Things everything</a></li>
<li><a href="../422879/index.html">Security Week 34: why do routers break</a></li>
<li><a href="../422881/index.html">Introducing the SOCI - C ++ Database Access Library</a></li>
<li><a href="../422883/index.html">Your copywriting sucks. Here's how to fix it.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>