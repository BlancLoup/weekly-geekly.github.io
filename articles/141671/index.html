<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Useful metrics for project evaluation</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In October, I already talked about how to evaluate testing, all the suffering and sympathizers can watch the recording here . And today I wanted to to...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Useful metrics for project evaluation</h1><div class="post__text post__text-html js-mediator-article">  In October, I already talked about how to evaluate testing, all the suffering and sympathizers can watch the recording <a href="http://software-testing.ru/library/around-testing/management/1494-confetqa-rukol">here</a> .  And today I wanted to touch on the topic of the project's metrics in general, and the metrics are not ‚Äúfor the killers‚Äù, but the metrics of ‚Äúuser-friendly‚Äù and ‚Äúproject-improving‚Äù.  That is why, instead of dry formulas and a list of metrics, I will tell 3 stories from the experience of introducing and using strictly defined metrics under strictly defined conditions - and about the results that were achieved with their help. <br><br><h4>  Why measure something? </h4><br>  There is a project.  Your favorite, dear, whom you want to grow and flourish. <br>  But how would you rate his prosperity if there are no criteria for this very prosperity? <br>  How can you quickly respond to problems before they become unrecoverable, if you do not use the ‚Äúsensor of the future G‚Äù? <br>  How do you understand what should be improved if you do not know the source of the problems? <br><br>  In short, metrics are needed to effectively manage the project: diagnose problems, localize them, correct and check whether the solutions you choose to solve the problem really help. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I will share different types of metrics, each of which is tested and brought considerable benefits.  Each time, introducing them, any team is very lazy and uncomfortable: you have to save additional information, measure something, raise bureaucracy.  But when we first benefit from a metric, discipline and a deep understanding of the importance of a particular metric comes to replace laziness. <br><br>  And if they don‚Äôt come, then the metric can be safely thrown away;) <a name="habracut"></a><br><br><h4>  Story 1: Who let him in here ?? </h4><br>  In one great company, management complained about a ‚Äúpoor quality product,‚Äù which was to blame for testing.  My task was to analyze the reasons for this unfortunate misunderstanding and solve them in any way, moreover yesterday. <br><br>  Task # 1 became obvious to me: <b>estimating% of the errors that were missed</b> : is it true that testers are missing something?  To do this, we entered the field ‚Äúinformed client‚Äù in the bug tracker, marked the old bugs in this way and counted.  The percentage was slightly more than 5%, and not all of them were critical. <br><br>  Is it a lot or a little?  In my experience, this is a pretty good percentage.  Where, then, is the opinion that testers miss a lot? <br><br>  We have entered another field: "reproduced on the release version."  Each time, registering a new error from the test bench, testers checked if it was in the latest user version: maybe users simply do not report specific errors?  The result for the first month - about 40 <b>% of the errors registered in the bug tracker are reproduced in the release version</b> . <br><br>  It turns out, we really miss a lot, but users do not report specific errors, but the opinion ‚Äúyour software sucks!‚Äù Is clearly formed.  Thus, we formed the sensor metrics: what is wrong: <br><br><ul><li>  % of errors missed in the release version </li><li>  % of errors reported by user </li></ul><br>  We set a goal (otherwise why do we need to measure something at all?)!  We want no more than 10% of errors in the release version.  But how to ensure this?  Excessively expand resources?  Increase the timeline? <br><br>  To answer this question, we need to dig further, and look for new metrics that will answer this question. <br><br>  In this case, we added one more field for all the missed errors: ‚ÄúThe reason for the omission.‚Äù  And the choice indicates why not brought this bug before: <br><br><ul><li>  unknown requirement (did not know or did not understand that it was needed) </li><li>  did not consider the test (did not think to test it SO) </li><li>  did not test (the test was, it was checked, but then the functional broke, and again this area was not checked) </li></ul><br>  According to this algorithm, I have already investigated the reasons for omissions in many companies, and the results are always different.  In the case under consideration, more than 60% of the errors were missed because the testers did not consider any test, that is, they did not even think that it needed to be tested.  Of course, we need to work on all fronts, but we started with 60%, relying on Pareto's law. <br><br>  Brainstorming ‚Äúhow to solve this riddle‚Äù led to various solutions: a weekly discussion of the missing defects in the testing group, the coordination of tests with analysts and developers, direct communication with users to study their environments and conditions, etc.  Introducing these new procedures little by little, in just 2 months we reduced the percentage of missed errors to 20%.  Not expanding the team, not increasing the time. <br><br>  Up to 10%, we have not yet reached, but in July it was 14% - we are already very close to the goal, and judging by the assurances of the implementers, customers have already noticed changes in quality.  <b>Not bad, huh?</b> <br><br><h4>  Story 2: Where are the Dros? </h4><br>  This story concerns one of my own projects.  We are developing some terribly necessary and useful service, and the terms of development did not really warm my soul.  Naturally, on my project everything is very good with testing, but why is the development barely weaving? <br><br>  Naturally, I began by trying to measure my subjective feelings ‚Äúslowly‚Äù.  How to understand this?  What to compare?  KLOC per month?  Fitch in the iteration?  Average breakdowns of terms regarding the plan?  Naturally, the first 2 metrics will not bring anything useful, so I began to watch% of deadlines for features (iterations do not have a fixed set of features, so they cannot seriously be late - we managed to do it and test it in 2 weeks).  But features! <br><br>  It turned out that for them we break deadlines by an average of 1.5-2 times!  I will not tell you what I should get this information from redmine, but here it is.  And I want to dig further, using the principle of "five" why "."  Why is that?  We are planning badly?  Do I want a result too fast?  Or low qualifications?  What time does it take? <br><br>  I began to analyze: on average, 1 to a small feature accounts for 15 to 40 bugs, and the time it takes to fix them takes more than developing the feature itself.  Why?  Is it a lot or a little?  Developers complain that there are a lot of requests to change the already developed functionality - is this true or is it a subjective error estimate? <br><br>  We dig further.  I enter into a poor unfortunate bug-tracker field that is swollen from additional fields: "The cause of the error."  Not passes, as in History # 1, namely appearances.  This field is filled in by the developer at the moment of committing, when he already knows exactly what and how he corrected.  And the answer options are as follows: <br><br><ul><li>  Code (here they took and nakosyachili) </li><li>  Failure to understand the requirements (‚ÄúWell, I didn‚Äôt understand what exactly it was needed!‚Äù) </li><li>  Changing requirements (product owner looked at the result and said ‚Äúeh, not really need a different way, but not the way I originally asked‚Äù) </li></ul><br>  Errors in the code we found about 30%.  Changes in the requirements - less than 5% (the developers were surprised, but recognized - it‚Äôs they who indicate the reason!).  And almost 70% of the errors were caused by a lack of understanding of the requirements.  In our case, when the bugfix takes more development, it is HALF OF TIME SPENDING ON THE DEVELOPMENT OF FICHE. <br><br>  <b>What to do?</b> <br><br>  We found a lot of solutions to the problem, starting from hiring a technical writer who will find out the requirements of the product owner and document in detail everything that we describe in a couple of lines and ending with the product owner, transferred to the secretaries, day and night documenting new features .  We didn‚Äôt like any of these options, they are too bureaucratic for a team of 4 developers sitting in the same office.  Therefore, we did the following: <br><br><ul><li>  Product owner briefly, as always, describes a new feature. </li><li>  The developer, when it comes to it, carefully considers the method of implementation, how it will look, what to do with this feature. </li><li>  After that, the developer and the RO sit together, and the developer tells in detail his thoughts on the <s>bright future of the</s> feature being developed. </li><li>  The developer under no circumstances starts working on a new feature without going through the above described algorithm of actions and not agreeing his vision with the RO </li><li>  The tester most often participates in this process, prompting in advance the difficult moments that he will test. </li></ul><br>  Now we have about 3-7 such ~ watch "chatters" a week, which is 2-3 people.  The number of bug fixes decreased, of which code errors became more than 50% - therefore, our next task will be to introduce code review, since  Now we have a new "main problem". <br><br>  But from the metric analyzer, we returned to the metric sensor and realized that never since spring had we broken the deadlines for the feature by more than 50%, although before that the average value of the breakdown was from 50% to 100%, and sometimes even more. <br><br>  And this is just the beginning!  ;-) <br><br><h4>  Story # 3: Who is slowing down developers? </h4><br>  Another story concerns my very recent experience in a third-party company.  Real-Agile, weekly iterations ... And weekly deadlines! <br><br>  The reason stated by the management of the company: "Developers admit too many bugs." <br><br>  I began to analyze how this happens.  I just participated in the process and watched from the side, as it is very well described in Imai‚Äôs book Gemba Kaizen.  And that's what I saw: Releases on Thursdays, Friday is preparatory to the new iteration day.  On Tuesday-Wednesday there is an assembly for testing.  On Wednesday-Thursday defects are made.  On Friday, instead of preparing for the new iteration, the developers urgently fix bugs every week. <br><br>  I asked in the task tracker, where features from the board are duplicated, to put down the statuses on the feature: the feature is accepted for development, the feature is given for testing, the feature is tested and sent for revision, the feature is tested and accepted for release. <br><br>  And what do you think, what is the average time between the ‚Äúfeature given for testing‚Äù and ‚Äúthe feature tested and sent back for refinement‚Äù?  1.5 days! <br><br>  And sometimes - with the ONLY blocking defect. <br><br>  The developers at this company complained about brake testers, but testers and management were against developers: ‚Äúyou yourself must test and not give away a raw product.‚Äù  But Caesar Caesar! <br><br>  So, there is a metric, 1.5 days is unacceptable a lot, we want to reduce at least three times - this should speed up releases by day.  How to do it?  Again, a brainstorm, again a bunch of ideas, 90% of the participants in the process insist that "developers must test themselves." <br><br>  But in the end we decided to try it differently: as soon as a feature, according to the developer, is ready, the tester sits down with him for one computer, takes a notebook with a pen and starts checking, commenting, writing out the noticed jambs in a notebook, without wasting time on the bug tracking  More than half of the bugs developers fix in this mode on the fly!  After all, the feature is only written, it is still held in the head! <br><br>  We reduced the period from 1.5 to 0.5 days very quickly, but in practice we achieved another, more serious change:% of the features transferred to the ‚Äúsent for revision‚Äù status decreased from almost 80 to almost 20!  That is 4 times!  That is, 80% of features are now immediately accepted after being transferred to the ‚Äútesting‚Äù status, because shortly before being transferred to this status, testing was carried out on the fly, which greatly reduces the time taken to register errors and the cost of correcting them. <br><br>  By the way, history 3 is the only one where we immediately reached our goal.  There are still breakdowns of iterations, but now this is an exception, and almost every Thursday the development team leaves home on time, and on Friday the preparation for the next iteration really begins. <br><br>  Bingo! <br><br><h4>  findings </h4><br>  I really did not want to draw dry formulas, to philosophize and to theorize.  I told specific stories from fresh (2012!) Experience.  Stories in which we shortened terms and improved quality without changing the budget. <br><br>  Are you still not ready to use the metrics with benefit? <br><br>  Then we go to you!  :) </div><p>Source: <a href="https://habr.com/ru/post/141671/">https://habr.com/ru/post/141671/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../141664/index.html">Buying like a game</a></li>
<li><a href="../141665/index.html">On experience and "derivatives"</a></li>
<li><a href="../141667/index.html">Anonymous chats for Android</a></li>
<li><a href="../141669/index.html">Trikopter (DIY Quadcopter: Part II.2)</a></li>
<li><a href="../141670/index.html">Tale of how I simplified my workflow. Or writing your spy in C #</a></li>
<li><a href="../141672/index.html">About pioneers</a></li>
<li><a href="../141673/index.html">Random sorting and output of random elements in XSLT</a></li>
<li><a href="../141674/index.html">MongoDB Secrets - Scalability and Performance (Workshop from the author)</a></li>
<li><a href="../141675/index.html">Translation of official Backbone.JS documentation</a></li>
<li><a href="../141676/index.html">Art positioning</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>