<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Sometimes more is less. When reducing the load increases the delay</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As in most posts , there is a problem with a distributed service, let's call this service Alvin. This time I did not find the problem myself, I was to...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Sometimes more is less. When reducing the load increases the delay</h1><div class="post__text post__text-html js-mediator-article">  As in <a href="https://mahdytech.com/2019/01/13/curious-case-999-latency-hike/">most posts</a> , there is a problem with a distributed service, let's call this service Alvin.  This time I did not find the problem myself, I was told by the guys from the client side. <br><br>  One day, I woke up with a disgruntled letter due to large delays from Alvin, whom we planned to launch soon.  In particular, the client was faced with a delay of the 99th percentile in the region of 50 ms, much higher than our delay budget.  This was surprising, since I thoroughly tested the service, especially for delays, because this is the subject of frequent complaints. <br><br>  Before giving Alvin to testing, I conducted many experiments with 40 thousand requests per second (QPS), all showed a delay of less than 10 ms.  I was ready to state that I did not agree with their results.  But once again looking at the letter, I noticed something new: I definitely did not test the conditions they mentioned, their QPS was much lower than mine.  I tested on 40k QPS, and they only on 1k.  I started another experiment, this time with lower QPS, just to appease them. <br><a name="habracut"></a><br>  As I write about this in a blog, you probably already understood: their numbers turned out to be correct.  I checked my virtual client again and again, with the same result: a low number of requests not only increases the delay, but increases the number of requests with a delay of more than 10 ms.  In other words, if at 40k QPS about 50 requests per second exceeded 50 ms, then at 1k QPS every second there were 100 requests above 50 ms.  Paradox! 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/xd/86/x-/xd86x-er30ek6tg4hkv4qdevvgq.png"><br><br><h1>  Narrowing the search range </h1><br>  Faced with the problem of latency in a distributed system with many components, the first thing to do is to make a short list of suspects.  Let's dig a little deeper into the architecture of Alvin: <br><br><img src="https://habrastorage.org/webt/lh/is/s5/lhiss5mqv9dq2h9moyhlss_chlk.png"><br><br>  A good starting point is a list of I / O transitions performed (network calls / disk search, etc.).  Let's try to figure out where the delay is.  In addition to the obvious I / O with the client, Alvin takes an extra step: he accesses the data store.  However, this storage works in the same cluster with Alvin, so there should be less delay there than with the client.  So, the list of suspects: <br><br><ol><li>  Network call from client to alvin. <br></li><li>  Network call from Alvin to the data warehouse. <br></li><li>  Search disk in data storage. <br></li><li>  Network call from the data warehouse to Alvin. <br></li><li>  Network call from Alvin to the client. </li></ol><br>  Let's try to cross out some items. <br><br><h3>  Data storage with nothing </h3><br>  First of all, I converted Alvin to a ping-ping server that does not process requests.  Upon receiving the request, it returns an empty response.  If the delay is reduced, then the error in the implementation of Alvin or the data warehouse is nothing unheard of.  In the first experiment we get the following schedule: <br><br><img src="https://habrastorage.org/webt/i4/zs/9s/i4zs9saymr5ra4tmry3diqbgdyy.png"><br><br>  As you can see, there is no improvement when using the ping-ping server.  This means that the data warehouse does not increase the delay, and the list of suspects is halved: <br><br><ol><li>  Network call from client to alvin. <br></li><li>  Network call from Alvin to the client. </li></ol><br>  Great!  The list is rapidly shrinking.  I thought I almost figured out the reason. <br><br><h3>  gRPC </h3><br>  Now is the time to introduce you to a new player: <a href="https://github.com/grpc/grpc">gRPC</a> .  It is Google‚Äôs open source library for in-process <a href="https://en.wikipedia.org/wiki/Remote_procedure_call">RPC</a> communication.  Although <code>gRPC</code> well optimized and widely used, I first used it in a system of this magnitude, and I expected my implementation to be suboptimal - to say the least. <br><br>  The presence of <code>gRPC</code> in the stack gave rise to a new question: maybe this is my implementation or is the <code>gRPC</code> itself causing a delay problem?  Add to the list of new suspect: <br><br><ol><li>  The client calls the <code>gRPC</code> library <br></li><li>  The <code>gRPC</code> library on the client makes a network call to the <code>gRPC</code> library on the server <br></li><li>  The <code>gRPC</code> library refers to Alvin (there is no operation in the case of a ping-pong server) </li></ol><br>  So that you understand what the code looks like, my client / Alvin implementation is not much different from the client-server <a href="https://github.com/grpc/grpc/tree/v1.19.0/examples/cpp/helloworld">async examples</a> . <br><br><blockquote>  <i>Note: The above list is a bit simplified, since <code>gRPC</code> allows you to use your own (template?) Streaming model, in which the <code>gRPC</code> execution <code>gRPC</code> and the user implementation are intertwined.</i>  <i>For the sake of simplicity, we will stick to this model.</i> </blockquote><br><h3>  Profiling will fix everything </h3><br>  Having crossed out the data store, I thought I was almost done: ‚ÄúNow it‚Äôs easy!  Apply the profile and find out where the delay occurs. ‚Äù  I am a <a href="https://mahdytech.com/2019/01/13/curious-case-999-latency-hike/">big fan of accurate profiling</a> , because CPUs are very fast and most often are not a bottleneck.  Most delays occur when the processor has to stop processing in order to do something else.  Exact profiling of the CPU is done precisely for this: it accurately records all <a href="https://www.tutorialspoint.com/what-is-context-switching-in-operating-system">context switches</a> and makes it clear where delays occur. <br><br>  I took four profiles: under high QPS (small delay) and with a ping-pong server on a low QPS (large delay), both on the client side and on the server side.  And just in case, I also took a sample of the processor profile.  When comparing profiles, I usually look for an anomalous call stack.  For example, on the bad side with high latency, there are much more context switches (10 or more times).  But in my case, the number of context switches almost coincided.  To my horror, there was nothing substantial there. <br><br><h1>  Additional debugging </h1><br>  I was desperate.  I did not know what other tools could be used, and my next plan was essentially to repeat experiments with different variations, and not to clearly diagnose the problem. <br><br><h3>  What if </h3><br>  From the very beginning, I was worried about the specific delay time of 50 ms.  This is a very big time.  I decided that I would cut the pieces out of the code until I could figure out exactly which part causes this error.  Then followed an experiment that worked. <br><br>  As usual, with hindsight it seems that everything was obvious.  I put the client on the same machine with Alvin - and sent a request to <code>localhost</code> .  And the delay increase has disappeared! <br><br><img src="https://habrastorage.org/webt/kl/bk/fv/klbkfv7ajppr9uqp_tywvxwgjre.png"><br><br>  Something was wrong with the network. <br><br><h3>  Master the skills of a network engineer </h3><br>  I must admit that my knowledge of network technologies is terrible, especially given the fact that I work with them daily.  But the network was the main suspect, and I needed to learn how to debug it. <br><br>  Fortunately, the Internet loves those who want to learn.  The combination of ping and tracert seemed like a good enough start to debug network transport problems. <br><br>  First, I launched <a href="https://docs.microsoft.com/en-us/sysinternals/downloads/psping">PsPing</a> on <a href="https://docs.microsoft.com/en-us/sysinternals/downloads/psping">Alvin</a> 's TCP port.  I used the default settings - nothing special.  Of the more than a thousand pings, none exceeded 10 ms, except for the first to warm up.  This contradicts the observed increase in the delay of 50 ms in the 99th percentile: there for every 100 queries we should have seen about one query with a delay of 50 ms. <br><br>  Then I tried the <a href="https://support.microsoft.com/en-ca/help/314868/how-to-use-tracert-to-troubleshoot-tcp-ip-problems-in-windows">tracert</a> : maybe the problem is on one of the nodes along the route between Alvin and the client.  But the tracer returned empty-handed. <br><br>  Thus, the reason for the delay was not my code, not the implementation of gRPC and not the network.  I already began to worry that I would never understand. <br><br><h3>  Now which OS are we on </h3><br>  <code>gRPC</code> widely used in Linux, but for Windows it is exotic.  I decided to conduct an experiment that worked: I created a Linux virtual machine, compiled Alvin for Linux and deployed it. <br><br><img src="https://habrastorage.org/webt/z1/t8/tk/z1t8tkyhrobvurzcdqlmpdtetzc.png"><br><br>  And that's what happened: in the Linux ping-pong server there were no such delays as that of a similar Windows node, although the data source was not different.  It turns out the problem is in the implementation of gRPC for Windows. <br><br><h3>  Nagle's algorithm </h3><br>  All this time, I thought I was missing the <code>gRPC</code> flag.  Now I realized that in fact it is in <code>gRPC</code> that the Windows flag is missing.  I found the internal RPC library, in which I was sure that it worked well for all the installed <a href="https://docs.microsoft.com/en-us/windows/desktop/winsock/windows-sockets-start-page-2">Winsock</a> flags.  Then I added all these flags to gRPC and deployed Alvin on Windows, in the corrected ping-pong server under Windows! <br><br><img src="https://habrastorage.org/webt/7o/it/sf/7oitsfp2rzxotix0cf1xhktbrri.png"><br><br>  <i>Almost</i> done: I started to remove the added flags one by one, until the regression returned, so I was able to pinpoint its cause.  It was the infamous <a href="https://docs.microsoft.com/en-us/windows/desktop/api/winsock/nf-winsock-setsockopt">TCP_NODELAY</a> , the Nagle switch. <br><br>  <a href="https://en.wikipedia.org/wiki/Nagle%2527s_algorithm">Nagle‚Äôs algorithm</a> attempts to reduce the number of packets sent over the network by delaying the transmission of messages until the size of the packet exceeds a certain number of bytes.  While this may be nice for the average user, it is destructive for real-time servers, because the OS will delay some messages, causing delays at low QPS.  <code>gRPC</code> set this flag in the Linux implementation for TCP sockets, but not for Windows.  I <a href="https://github.com/grpc/grpc/commit/1dce1009e67ea4b5934a61b1bcf8a217bd12cc76">fixed it</a> . <br><br><h1>  Conclusion </h1><br>  The big delay on low QPS was caused by optimization of OS.  Looking back, the profiling did not detect a delay, because it was done in kernel mode and not in <a href="https://blog.codinghorror.com/understanding-user-and-kernel-mode/">user mode</a> .  I don‚Äôt know if Nagle‚Äôs algorithm can be observed through ETW captures, but that would be interesting. <br><br>  As for the localhost experiment, it probably didn‚Äôt touch the actual network code, and Nagleg‚Äôs algorithm didn‚Äôt start, so the delay problems disappeared when the client accessed Alvin through localhost. <br><br>  The next time you see an increase in latency with a decrease in requests per second, Nagle's algorithm should be on your suspect list! </div><p>Source: <a href="https://habr.com/ru/post/451904/">https://habr.com/ru/post/451904/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../451896/index.html">The ‚Äúunbreakable‚Äù eyeDisk is protected by an iris scan, but transmits the password in clear text</a></li>
<li><a href="../451898/index.html">Innovations in Russian</a></li>
<li><a href="../4519/index.html">"Fat" investment hinders a good startup</a></li>
<li><a href="../451900/index.html">The first contribution to the browser API from Facebook</a></li>
<li><a href="../451902/index.html">Microsoft Azure Developer Camp Russia</a></li>
<li><a href="../451906/index.html">Exchange Vulnerability: How to Detect Privilege Elevation by Domain Administrator</a></li>
<li><a href="../451908/index.html">The history of computers: night at the Yandex Museum</a></li>
<li><a href="../451912/index.html">MuseNet Deep Neural Network Writes Music</a></li>
<li><a href="../451916/index.html">Asynchronous PHP and the story of one bike</a></li>
<li><a href="../451918/index.html">On the issue of TI</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>