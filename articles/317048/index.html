<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Limit the speed of traffic. Policer or shaper, what to use on the net?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When it comes to limiting bandwidth on network equipment, two technologists first come to mind: policer and shaper. Policer limits the speed by discar...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Limit the speed of traffic. Policer or shaper, what to use on the net?</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/files/5af/b65/79d/5afb6579d5e848188b220a39dfd7034f.jpg"></div><br>  When it comes to limiting bandwidth on network equipment, two technologists first come to mind: policer and shaper.  Policer limits the speed by discarding "extra" packets that lead to the excess of a given speed.  Shaper tries to smooth the speed to the desired value by buffering the packets.  I decided to write this article after reading <a href="http://blog.ipspace.net/2016/09/policing-or-shaping-it-depends.html">notes</a> on the blog of Ivan Pepelnyak (Ivan Pepelnjak).  It once again raised the question: what is better - a policer or a shaper.  And as often happens with such questions, the answer is: it all depends on the situation, since each of the technologies has its pros and cons.  I decided to deal with this a little more, by carrying out some simple experiments.  The results are rolled up. <a name="habracut"></a><br><br>  And so, let's start with a general picture of the difference between a policer and a shaper. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/733/6ca/99a/7336ca99a03543b4bba72bdf70e1521f.jpg"></div><br>  As you can see, the policer cuts off all the peaks, shaper does smoothing our traffic.  A pretty good comparison between the policer and the shaper can be found <a href="http://www.cisco.com/c/en/us/support/docs/quality-of-service-qos/qos-policing/19645-policevsshape.html">here</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Both technologies basically use the token mechanism.  In this mechanism, there is a virtual bucket limited in size (token bucket), into which tokens flow with some regularity.  Token, as a travel card, is consumed for the transfer of packets.  If there are no tokens in the bucket, then the packet is discarded (other actions can be performed).  Thus, we get a constant traffic transfer rate, since the tokens go to the bucket in accordance with the given speed. <br><div style="text-align:center;"><img src="https://habrastorage.org/files/f6f/83c/8b5/f6f83c8b52bf45c9aeec7b3bee962c6a.jpg"></div><br><div class="spoiler">  <b class="spoiler_title">Maybe it was worth making it easier?</b> <div class="spoiler_text">  The session speed is usually measured in the allotted time interval, for example, in 5 seconds or 5 minutes.  Taking an instantaneous value is meaningless, since data is always transmitted at the speed of the channel.  Moreover, if we do the averaging over different time intervals, we will get different graphs of the data transfer rate, since the traffic in the network is not uniform.  I think anyone came across this by building graphs in the monitoring system. <br><br>  The mechanism of tokens allows for flexibility in setting the speed limit.  The size of the bucket affects how we average our speed.  If the bucket is large (i.e., there can be a lot of tokens there), we will allow traffic to ‚Äújump out‚Äù more than the allotted limits at certain points in time (equivalent to averaging over a longer period of time).  If the bucket size is small, the traffic will be more uniform, rarely exceeding a predetermined threshold (the equivalent of averaging over a small period of time). <br></div></div><br>  In the case of a policer, a bucket is filled every time a new package arrives.  The number of tokens that are loaded into the bucket depends on the given policer speed and the time since the last packet arrived.  If there are no tokens in the bucket, the policer can drop packets or, for example, re-mark them (assign new DSCP or IPP values).  In the case of a shaper, the bucket is filled at regular intervals regardless of the arrival of the packets.  If there are not enough tokens, the packets fall into a special queue where tokens are expected to appear.  Due to this, we have anti-aliasing.  But if there are too many packets, the shaper queue eventually becomes full and the packets begin to be discarded.  It should be noted that the description given is simplified, since both the policer and shaper have variations (a detailed analysis of these technologies will take up the volume of a separate article). <br><br><h3>  Experiment </h3><br>  And what does this look like in practice?  To do this, we will assemble a test stand and conduct the following <a href="http://packetlife.net/blog/2008/jul/30/policing-versus-shaping/">experiment</a> .  Our booth will include a device that supports policer and shaper technologies (in my case, this is Cisco ISR 4000; any vendor‚Äôs hardware or software device that supports these technologies), <a href="https://iperf.fr/">iPerf</a> traffic <a href="https://iperf.fr/">generator</a> and <a href="https://www.wireshark.org/">Wireshark</a> traffic analyzer. <br><br>  First look at the work of the policer.  Set the speed limit to 20 Mbps. <br><br><div class="spoiler">  <b class="spoiler_title">Device configuration</b> <div class="spoiler_text"><pre><code class="python hljs">policy-map Policer_20 <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">-</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">default</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">police</span></span></span><span class="hljs-class"> 20000000 </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">interface</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GigabitEthernet0</span></span></span><span class="hljs-class">/0/1 </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">service</span></span></span><span class="hljs-class">-</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">policy</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">output</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Policer_20</span></span></span></span></code> </pre>  We use the automatically set value of the token bucket size.  For our speed, this is 625,000 bytes. <br></div></div><br>  In iPerf, we start generating traffic within four threads using the TCP protocol. <br><br><pre> <code class="python hljs">C:\Users\user&gt;iperf3.exe -c <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.115</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> -t <span class="hljs-number"><span class="hljs-number">20</span></span> -i <span class="hljs-number"><span class="hljs-number">20</span></span> -P <span class="hljs-number"><span class="hljs-number">4</span></span> Connecting to host <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.115</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span>, port <span class="hljs-number"><span class="hljs-number">5201</span></span> [ <span class="hljs-number"><span class="hljs-number">4</span></span>] local <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.20</span></span><span class="hljs-number"><span class="hljs-number">.8</span></span> port <span class="hljs-number"><span class="hljs-number">55542</span></span> connected to <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.115</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> port <span class="hljs-number"><span class="hljs-number">5201</span></span> [ <span class="hljs-number"><span class="hljs-number">6</span></span>] local <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.20</span></span><span class="hljs-number"><span class="hljs-number">.8</span></span> port <span class="hljs-number"><span class="hljs-number">55543</span></span> connected to <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.115</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> port <span class="hljs-number"><span class="hljs-number">5201</span></span> [ <span class="hljs-number"><span class="hljs-number">8</span></span>] local <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.20</span></span><span class="hljs-number"><span class="hljs-number">.8</span></span> port <span class="hljs-number"><span class="hljs-number">55544</span></span> connected to <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.115</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> port <span class="hljs-number"><span class="hljs-number">5201</span></span> [ <span class="hljs-number"><span class="hljs-number">10</span></span>] local <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.20</span></span><span class="hljs-number"><span class="hljs-number">.8</span></span> port <span class="hljs-number"><span class="hljs-number">55545</span></span> connected to <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.115</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> port <span class="hljs-number"><span class="hljs-number">5201</span></span> [ ID] Interval Transfer Bandwidth [ <span class="hljs-number"><span class="hljs-number">4</span></span>] <span class="hljs-number"><span class="hljs-number">0.00</span></span><span class="hljs-number"><span class="hljs-number">-20.01</span></span> sec <span class="hljs-number"><span class="hljs-number">10.2</span></span> MBytes <span class="hljs-number"><span class="hljs-number">4.28</span></span> Mbits/sec [ <span class="hljs-number"><span class="hljs-number">6</span></span>] <span class="hljs-number"><span class="hljs-number">0.00</span></span><span class="hljs-number"><span class="hljs-number">-20.01</span></span> sec <span class="hljs-number"><span class="hljs-number">10.6</span></span> MBytes <span class="hljs-number"><span class="hljs-number">4.44</span></span> Mbits/sec [ <span class="hljs-number"><span class="hljs-number">8</span></span>] <span class="hljs-number"><span class="hljs-number">0.00</span></span><span class="hljs-number"><span class="hljs-number">-20.01</span></span> sec <span class="hljs-number"><span class="hljs-number">8.98</span></span> MBytes <span class="hljs-number"><span class="hljs-number">3.77</span></span> Mbits/sec [ <span class="hljs-number"><span class="hljs-number">10</span></span>] <span class="hljs-number"><span class="hljs-number">0.00</span></span><span class="hljs-number"><span class="hljs-number">-20.01</span></span> sec <span class="hljs-number"><span class="hljs-number">11.1</span></span> MBytes <span class="hljs-number"><span class="hljs-number">4.64</span></span> Mbits/sec [SUM] <span class="hljs-number"><span class="hljs-number">0.00</span></span><span class="hljs-number"><span class="hljs-number">-20.01</span></span> sec <span class="hljs-number"><span class="hljs-number">40.9</span></span> MBytes <span class="hljs-number"><span class="hljs-number">17.1</span></span> Mbits/sec</code> </pre><br>  The average speed was 17.1 Mbit / s.  Each session received a different bandwidth.  This is due to the fact that the policer configured in our case does not distinguish streams and discards any packets that exceed the specified speed value. <br><br>  With Wireshark, we collect a traffic dump and build a data transfer schedule received on the sender side. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/0a8/336/72a/0a833672a02842068116587b2a8facd5.jpg"></div><br>  The black line shows the total traffic.  The multicolored lines are the traffic of each TCP stream.  Before drawing any conclusions and going into the question, let's see what we can do if we replace the policer with a shaper. <br><br>  Set up a shaper at a speed limit of 20 Mbps. <br><br><div class="spoiler">  <b class="spoiler_title">Device configuration</b> <div class="spoiler_text"><pre> <code class="python hljs">policy-map Shaper_20 <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">class</span></span></span><span class="hljs-class">-</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">default</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">shape</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">average</span></span></span><span class="hljs-class"> 20000000 </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">queue</span></span></span><span class="hljs-class">-</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">limit</span></span></span><span class="hljs-class"> 200 </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">packets</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">interface</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GigabitEthernet0</span></span></span><span class="hljs-class">/0/1 </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">service</span></span></span><span class="hljs-class">-</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">policy</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">output</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Shaper_20</span></span></span></span></code> </pre> <br>  When setting up, we use the automatic billing size of the bucket size of tokens BC and BE equal to 8000. But we change the queue size from 83 (default in IOS XE 15.6 (1) S2) to 200. This was done consciously in order to get a clearer picture typical of shaper 'but.  We will dwell on this question in more detail in the subcategory ‚ÄúDoes the queue depth affect our session?‚Äù. <br><pre> <code class="python hljs">cbs-rtr<span class="hljs-number"><span class="hljs-number">-4000</span></span><span class="hljs-comment"><span class="hljs-comment">#sh policy-map interface gigabitEthernet 0/0/1 Service-policy output: Shaper_20 Class-map: class-default (match-all) 34525 packets, 50387212 bytes 5 minute offered rate 1103000 bps, drop rate 0000 bps Match: any Queueing queue limit 200 packets (queue depth/total drops/no-buffer drops) 0/0/0 (pkts output/bytes output) 34525/50387212 shape (average) cir 20000000, bc 80000, be 80000 target shape rate 20000000</span></span></code> </pre><br></div></div><br>  In iPerf, we start generating traffic within four threads using the TCP protocol. <br><br><pre> <code class="python hljs">C:\Users\user&gt;iperf3.exe -c <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.115</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> -t <span class="hljs-number"><span class="hljs-number">20</span></span> -i <span class="hljs-number"><span class="hljs-number">20</span></span> -P <span class="hljs-number"><span class="hljs-number">4</span></span> Connecting to host <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.115</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span>, port <span class="hljs-number"><span class="hljs-number">5201</span></span> [ <span class="hljs-number"><span class="hljs-number">4</span></span>] local <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.20</span></span><span class="hljs-number"><span class="hljs-number">.8</span></span> port <span class="hljs-number"><span class="hljs-number">62104</span></span> connected to <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.115</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> port <span class="hljs-number"><span class="hljs-number">5201</span></span> [ <span class="hljs-number"><span class="hljs-number">6</span></span>] local <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.20</span></span><span class="hljs-number"><span class="hljs-number">.8</span></span> port <span class="hljs-number"><span class="hljs-number">62105</span></span> connected to <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.115</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> port <span class="hljs-number"><span class="hljs-number">5201</span></span> [ <span class="hljs-number"><span class="hljs-number">8</span></span>] local <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.20</span></span><span class="hljs-number"><span class="hljs-number">.8</span></span> port <span class="hljs-number"><span class="hljs-number">62106</span></span> connected to <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.115</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> port <span class="hljs-number"><span class="hljs-number">5201</span></span> [ <span class="hljs-number"><span class="hljs-number">10</span></span>] local <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.20</span></span><span class="hljs-number"><span class="hljs-number">.8</span></span> port <span class="hljs-number"><span class="hljs-number">62107</span></span> connected to <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.115</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span> port <span class="hljs-number"><span class="hljs-number">5201</span></span> [ ID] Interval Transfer Bandwidth [ <span class="hljs-number"><span class="hljs-number">4</span></span>] <span class="hljs-number"><span class="hljs-number">0.00</span></span><span class="hljs-number"><span class="hljs-number">-20.00</span></span> sec <span class="hljs-number"><span class="hljs-number">11.6</span></span> MBytes <span class="hljs-number"><span class="hljs-number">4.85</span></span> Mbits/sec [ <span class="hljs-number"><span class="hljs-number">6</span></span>] <span class="hljs-number"><span class="hljs-number">0.00</span></span><span class="hljs-number"><span class="hljs-number">-20.00</span></span> sec <span class="hljs-number"><span class="hljs-number">11.5</span></span> MBytes <span class="hljs-number"><span class="hljs-number">4.83</span></span> Mbits/sec [ <span class="hljs-number"><span class="hljs-number">8</span></span>] <span class="hljs-number"><span class="hljs-number">0.00</span></span><span class="hljs-number"><span class="hljs-number">-20.00</span></span> sec <span class="hljs-number"><span class="hljs-number">11.5</span></span> MBytes <span class="hljs-number"><span class="hljs-number">4.83</span></span> Mbits/sec [ <span class="hljs-number"><span class="hljs-number">10</span></span>] <span class="hljs-number"><span class="hljs-number">0.00</span></span><span class="hljs-number"><span class="hljs-number">-20.00</span></span> sec <span class="hljs-number"><span class="hljs-number">11.5</span></span> MBytes <span class="hljs-number"><span class="hljs-number">4.83</span></span> Mbits/sec [SUM] <span class="hljs-number"><span class="hljs-number">0.00</span></span><span class="hljs-number"><span class="hljs-number">-20.00</span></span> sec <span class="hljs-number"><span class="hljs-number">46.1</span></span> MBytes <span class="hljs-number"><span class="hljs-number">19.3</span></span> Mbits/sec</code> </pre><br>  The average speed was 19.3 Mbit / s.  In addition, each TCP stream received approximately the same bandwidth. <br><br>  With Wireshark, we collect a traffic dump and build a data transfer schedule received on the sender side. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/741/913/824/741913824bf949eaa03cba8452d4d53c.jpg"></div><br>  The black line shows the total traffic.  The multicolored lines are the traffic of each TCP stream. <br><br>  We make the first intermediate conclusions: <br><br><ul><li>  In the case of policer, the usable bandwidth was 17.1 Mbps.  Each stream at different points in time had a different bandwidth. </li><li>  In the case of shaper, the usable bandwidth was 19.3 Mbit / s.  All streams had approximately the same bandwidth. </li></ul><br>  Let us look in more detail at the behavior of the TCP session in the case of policer and shaper.  Fortunately, there are enough tools in Wireshark to make such an analysis. <br><br>  Let's start with graphs that show packets with reference to the time of their transfer.  The first one is policer, the second one is shaper. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/aea/4f2/ff6/aea4f2ff66004c28bc2a3360d8953aa3.jpg"></div><br>  From the graphs it can be seen that the packets in the case of shaper are transmitted more evenly in time.  In the case of a policer, abrupt accelerations of the session and periods of pauses are visible. <br><br><h3>  TCP session analysis with policer operation </h3><br>  Let's take a closer look at the TCP session.  We will consider the case of policer. <br><br>  TCP in its work relies on a fairly large set of algorithms.  Among them, the most interesting for us are the algorithms responsible for congestion control.  They are responsible for the speed of data transmission within the session.  The PC running iPerf runs on Windows 10. In Windows 10, <a href="https://tools.ietf.org/html/draft-sridharan-tcpm-ctcp-00">Compound TCP</a> (CTCP) is used as such an algorithm.  CTCP in its work borrowed quite a lot from the <a href="https://en.wikipedia.org/wiki/TCP_congestion_control">TCP Reno</a> algorithm.  Therefore, when analyzing a TCP session, it is quite convenient to look at the <a href="">picture</a> with the session states when the TCP Reno algorithm is running. <br><br>  The following picture shows the initial data transfer segment. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/afe/251/fcd/afe251fcd8ee4e8db311a15bac2be632.jpg"></div><br><ol><li>  At the first stage, we establish a TCP session (a triple handshake takes place). <br><br></li><li>  Next starts TCP session overclocking.  TCP <a href="https://tools.ietf.org/html/rfc5681">slow-start</a> algorithm works.  By default, the congestion window (cwnd) value for a TCP session in Windows 10 is equal to the volume of ten maximum TCP session data segments (MSS).  This means that this PC can send 10 packets at once, without waiting for confirmation of them in the form of an ACK.  The value of the maximum window, which the recipient provided (advertised window - awnd), is taken as the initial value of the slow-start (ssthresh) algorithm termination threshold and the transition to congestion avoiding mode.  In our case, ssthresh = awnd = 64K.  Awnd is the maximum value of the data that the receiver is ready to receive into the buffer. <br><br><div class="spoiler">  <b class="spoiler_title">Where to see the initial session data?</b> <div class="spoiler_text">  To see the TCP options, you can use PowerShell. <br><br>  We look at which global TCP template is used by default in our system. <br><div style="text-align:center;"><img src="https://habrastorage.org/files/f66/eef/418/f66eef4183bd41a29619b938f89cad8d.jpg"></div><br>  Next, execute the ‚ÄúGet-NetTCPSetting Internet‚Äù query and look for the value of the InitialCongestionWindow (MSS) value. <br><div style="text-align:center;"><img src="https://habrastorage.org/files/4ae/a14/503/4aea145038cf48d389aa76883f11448c.jpg"></div><br>  The awnd value can be found in the ACK packets received from the receiver: <br><div style="text-align:center;"><img src="https://habrastorage.org/files/a12/b54/98b/a12b5498b05f4ad196766d00abc1f894.jpg"></div><br></div></div><br>  In TCP mode, the slow-start window size (cwnd) increases each time an ACK is received.  However, it cannot exceed the value of awnd.  Due to this behavior, we have an almost exponential increase in the number of transmitted packets.  Our TCP session accelerates quite aggressively. <br><br><div class="spoiler">  <b class="spoiler_title">TCP packet transmission slow-start</b> <div class="spoiler_text"><ol><li>  The PC establishes a TCP connection (# 1-3). </li><li>  Sends 10 packets (# 4-13), without waiting for confirmation (ACK), since cwnd = 10 * MSS. </li><li>  Receives ACK (‚Ññ14), which confirms immediately two packages (‚Ññ4-5). </li><li>  Increases window size Cwnd = (10 + 2) * MSS = 12 * MSS. </li><li>  Sends an additional three packets (# 15-17).  In theory, the PC should have sent four packets: two, since it received confirmation for two packets that had been transmitted earlier;  plus two packages due to the increase in the window.  But in reality, at the very first stage, the system sends (2N-1) packets.  I could not find the answer to this question.  If someone tells me, I will be grateful. </li><li>  Receives two ACKs (# 18-19).  The first ACK confirms that the remote side has received four packets (No. 6-9).  The second - three (‚Ññ 10-12). </li><li>  Increases window size Cwnd = (12 + 7) * MSS = 19 * MSS. </li><li>  Sends 14 packets (# 20-33): seven new packets, since they received ACK to seven previously transmitted packets, and another seven new packets, as the window increased. </li><li>  And so on. </li></ol><br><div style="text-align:center;"><img src="https://habrastorage.org/files/1f7/495/852/1f749585201d4c9a973e5a6a8a8d911b.jpg"></div><br></div></div><br></li><li>  Policer does not prevent the session from being overclocked.  There are many tokens in the bucket (when policer is initialized, the bucket is filled with tokens completely).  For a speed of 20 Mbps, the bucket size is set to 625,000 bytes by default.  Thus, the session accelerates at a point in time to almost 18 Mbit / s (and we remember that we have four such sessions).  The window size cwnd reaches its maximum value and becomes equal to awnd, which means cwnd = ssthersh. <br><br><div class="spoiler">  <b class="spoiler_title">cwnd = ssthersh</b> <div class="spoiler_text">  When cwnd = ssthersh had an exact answer, I could not find whether the algorithm would change from slow-start to congestion avoidance.  RFC does not give an exact answer.  From a practical point of view, this is not very important, since the window size cannot continue to grow. <br></div></div><br></li><li>  Since our session accelerated quite strongly, the tokens are very quickly spent and ultimately end.  The bucket does not have time to fill (filling with tokens goes for a speed of 20 Mbit / s, while the total utilization of all four sessions at a time point approaches 80 Mbit / s).  Policer starts to drop packets.  So, they do not reach the remote side.  The recipient sends Duplicate ACK (Dup ACK), which signal to the sender that there has been a loss of packets and need to transfer them again. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f7e/573/963/f7e57396341040a68210403ad318ccb9.jpg"></div><br>  After receiving three Dup ACKs, our TCP session enters the recovery after loss phase (loss recovery, including Fast Retransmit / Fast Recovery algorithms).  The sender sets the new value ssthresh = cwnd / 2 (32K) and makes the window cwnd = ssthresh + 3 * MSS. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/047/feb/ad1/047febad132f47cf9c978a9631d35675.png"></div><br></li><li>  The sender tries to immediately begin to re-transmit the lost packets (TCP Fast Retransmission algorithm works).  At the same time, Dup ACKs continue to arrive, the purpose of which is to artificially increase the cwnd window.  This is necessary to restore the session speed due to packet loss as quickly as possible.  Due to the Dup ACK, the cwnd window grows to the maximum value (awnd). <br><br>  As soon as the number of packets sent to the cwnd window has been sent, the system stops.  To continue data transfer, it needs new ACK (not Dup ACK).  But ACK do not come.  All repeated packets are discarded by policer, so the tokens in the bucket are over, and too little time has passed to fill them up. <br><br></li><li>  In this state, the system waits until the timeout for receiving a new ACK from the remote side (Retransmission timeout - <a href="https://tools.ietf.org/search/rfc6298">RTO</a> ) works.  Our big pauses, which are visible on the charts, are connected with this. <br><br></li><li>  After the RTO timer is triggered, the system goes into slow-start mode and sets ssthresh = FlightSize / 2 (where FlightSize is the amount of not confirmed data), and the window cwnd = 1 * MSS.  Then again, an attempt is made to transfer the lost packets.  However, now only one packet is being sent, since cwnd = 1 * MSS. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/209/dd1/133/209dd1133c65474b896dea3164d5fef3.jpg"></div><br></li><li>  Since for some time the system did not transmit anything, tokens managed to accumulate in our bucket.  Therefore, in the end, the package reaches the recipient.  So, we get a new ACK.  From this moment on, the system starts transmitting previously lost packets in slow-start mode.  The session is being overclocked.  As soon as the window size cwnd exceeds the value of ssthresh, the session goes into congestion avoidance mode. <br><br>  In the Compound TCP algorithm, the sending window (wnd) is used to control the transmission rate, which depends on two weighted values: the overload window (cwnd) and the delay window (dwnd).  Cwnd, as before, depends on the received ACK, dwnd depends on the amount of round trip time (RTT).  The wnd window only grows once per RTT time period.  As we remember, in the case of slow-start, the cwnd window grew when each ACK was received.  Therefore, in congestion avoidance mode, the session is not accelerated so quickly. <br><br></li><li>  As soon as the session accelerates strongly enough (when more packets are transferred than there are tokens in the bucket), the policer is triggered again.  Begin to drop packets.  Next comes the loss recovery phase.  Those.  the whole process is repeated again.  And so it goes until we have completed the transfer of all data. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/523/284/8c1/5232848c1c6748fa98b68d2a72d8e1bd.jpg"></div><br>  TCP session during policer operation looks like a ladder (the transmission phase goes, followed by a pause). <br></li></ol><br><h3>  TCP session analysis when running shaper </h3><br>  Now let's take a closer look at the data segment for the shaper case.  For clarity, let's take a similar scale, as for the policer graph in Fig.6. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/c13/db0/187/c13db018750e4e82a484c0f8ca59a291.jpg"></div><br>  From the graph we see the same ladder.  But the size of the steps was significantly smaller.  However, if you look closely at the graph in Fig.  10, we will not see small ‚Äúwaves‚Äù at the end of each step, as was the case in Fig.  9. Such ‚Äúwaves‚Äù were the result of packet loss and retransmission attempts. <br><br>  Consider the initial data transfer segment for the shaper case. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f7c/8bd/74f/f7c8bd74f4d84ea796c39dc94fe6bf32.jpg"></div><br>  The session is established.  Next begins slow-start TCP overclocking.  But this acceleration is flatter and has pronounced pauses that increase in size.  Flatter overclocking is due to the fact that the default bucket size for shaper is all (BC + BE) = 20,000 bytes.  While for a policer, the bucket size is 625,000 bytes.  Therefore, the shaper works significantly earlier.  Packages begin to fall into the queue.  The delay from the sender to the receiver grows, and ACKs arrive later than it did in the case of the policer.  The window grows much slower.  It turns out that the more the system transmits packets, the more they accumulate in the queue, and therefore, the greater the delay in receiving the ACK.  We have a process of self-regulation. <br><br>  After a while, the cwnd window reaches the awnd value.  But at this point, we have accumulated quite a noticeable delay due to the presence of the queue.  Ultimately, when a certain RTT value is reached, an equilibrium state occurs, when the session rate does not change any more and reaches a maximum value for this RTT.  In my example, the average RTT is 107 ms, awnd = 64512 bytes, therefore, the maximum session speed will correspond to awnd / RTT = 4.82 Mbit / s.  Approximately this value gave us iPerf during measurements. <br><br>  But where are the pronounced pauses in the transmission?  Let's look at the schedule of packet transmission through the device with a shaper in case we have only one TCP session (Figure 12).  Let me remind you that in our experiment, data transfer occurs within four TCP sessions. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/62c/f41/426/62cf4142680a4671a6b21b1beb45d574.jpg"></div><br>  This graph shows very clearly that there are no pauses.  From this we can conclude that the pauses in Fig.10 and 11 are due to the fact that we simultaneously have four streams, and the queue in the shaper is one (the type of the FIFO queue). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/1ba/20a/55c/1ba20a55c34f42bfa7c8c2f94e8821e3.jpg"></div><br>  Figure 13 shows the location of the packets of different sessions in the FIFO queue.  Since packets are transmitted in batches, they will be queued in the same way.  In this regard, the delay between the arrival of packets on the receiving side will be of two types: T1 and T2 (where T2 is significantly superior to T1).  The total RTT value for all packets will be the same, but packets will arrive in batches spaced apart in time by the value of T2.  So pauses are obtained, since at the time T2 no ACKs come to the sender, while the session window remains unchanged (has a maximum value equal to awnd). <br><br><div class="spoiler">  <b class="spoiler_title">WFQ Queue</b> <div class="spoiler_text">  It is logical to assume that if we replace one common FIFO queue with several for each session, there will not be any pronounced pauses.  For such a problem, for example, a queue of type Weighted Fair Queuing ( <a href="https://ru.wikipedia.org/wiki/%25D0%2592%25D0%25B7%25D0%25B2%25D0%25B5%25D1%2588%25D0%25B5%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25BF%25D1%2580%25D0%25B0%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25BB%25D0%25B8%25D0%25B2%25D0%25B0%25D1%258F_%25D0%25BE%25D1%2587%25D0%25B5%25D1%2580%25D0%25B5%25D0%25B4%25D1%258C">WFQ</a> ) is <a href="https://ru.wikipedia.org/wiki/%25D0%2592%25D0%25B7%25D0%25B2%25D0%25B5%25D1%2588%25D0%25B5%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25BF%25D1%2580%25D0%25B0%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25BB%25D0%25B8%25D0%25B2%25D0%25B0%25D1%258F_%25D0%25BE%25D1%2587%25D0%25B5%25D1%2580%25D0%25B5%25D0%25B4%25D1%258C">suitable</a> .  It creates its own queue of packets for each session. <br><br><pre> <code class="python hljs">policy-map Shaper <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">shaper_class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">shape</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">average</span></span></span><span class="hljs-class"> 20000000 </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">queue</span></span></span><span class="hljs-class">-</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">limit</span></span></span><span class="hljs-class"> 200 </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">packets</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">fair</span></span></span><span class="hljs-class">-</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">queue</span></span></span></span></code> </pre> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/60a/e8e/6c8/60ae8e6c8d924378a61b6de18473751f.jpg"></div><br><br>  From the general graph, we immediately see that the graphs of all four TCP sessions are identical.  Those.  they all got the same bandwidth. <br><br>  And here is our schedule of packet distribution over transmission time at exactly the same scale as in Fig.  11. There are no pauses. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/7fb/2b7/c2e/7fb2b7c2e5f94b748d666b879e90dc57.jpg"></div><br><br>  It is worth noting that the WFQ queue will allow us to get not only a more even distribution of bandwidth, but also to prevent the "clogging" of one type of traffic by another.  We talked all the time about TCP, but UDP traffic is also present on the network.  UDP has no mechanisms for adjusting the rate of transmission (flow control, congestion control).  Because of this, UDP traffic can easily clog our shared FIFO queue in a shaper, which will dramatically affect TCP transmission.  Let me remind you that when the FIFO queue is completely full of packets, the tail-drop mechanism starts working by default, in which all newly arrived packets are discarded.  If we have a WFQ queue configured, each session waits for buffering in its turn, which means that TCP sessions will be separated from UDP sessions. <br></div></div><br>  The most important conclusion that can be made after analyzing the packet transfer schedules when working with a shaper is that we do not have any lost packets.  Due to the increase in RTT, the session speed adapts to the speed of the shaper. <br><br><div class="spoiler">  <b class="spoiler_title">Does the queue depth affect our session?</b> <div class="spoiler_text">  Of course!  Initially (if someone still remembers this) we changed the queue depth from 83 (the default value) to 200 packets.  We did this in order to have enough queues to get a sufficient RTT value, at which the total speed of the sessions becomes approximately equal to 20 Mbps.  So, the packages "do not fall out" from the shaper'a queue. <br><br>  At a depth of 83 packets, the queue overflows faster than the desired RTT value is reached.  Packets are discarded.  This is especially vivid at the initial stage, when the TCP slow-start mechanism works for us (the session accelerates as aggressively as possible).  It is worth noting that the number of discarded packets is incomparably smaller than in the case of policer, since an increase in RTT causes the session speed to grow more smoothly.  As we remember, in the CTCP algorithm, the window size also depends on the RTT value. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f85/0be/81f/f850be81f8d640b88e98c8bcd63d1da0.jpg"></div><br></div></div><br>  <b>Bandwidth utilization and latency charts for policer and shaper</b> <br><br>  In conclusion of our small research we will construct several more general graphs, after which we proceed to the analysis of the data obtained. <br><br>  Bandwidth utilization schedule: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/0a1/218/f8e/0a1218f8e64a48e1acf5372fd5090dcc.jpg"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/bee/31a/033/bee31a0332184a6d8330fa94762e76b2.jpg"></div><br><br>  In the case of a policer, we see a hopping schedule: the session accelerates, then losses occur, and its speed drops.  Then everything repeats again.  In the case of shaper, our session receives approximately the same bandwidth throughout the transmission.  Session speed is adjusted by increasing the RTT value.  In both graphs, explosive growth can be observed initially.  It is due to the fact that our buckets were initially completely filled with tokens and the TCP session, unrestrained, is accelerated to relatively large values ‚Äã‚Äã(in the case of shaper, this value is 2 times less). <br><br>  Schedule RTT delay for policer and shaper (in a good way, this is the first thing we remember when talking about shaper): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/cd0/f5d/d1c/cd0f5dd1c1d14ee7bf123f6198b76276.jpg"></div><br>  In the case of a policer (first graph), the RTT delay for most packets is minimal, about 5 ms.  The graph also contains significant jumps (up to 340 ms).  These are the moments when the packets were discarded and transmitted again.  Here it is worth noting how Wireshark considers RTT for TCP traffic.  RTT is the time between sending an original packet and receiving an ACK on it.  In this regard, if the original packet was lost and the system re-transmitted the packet, the RTT value grows, since the starting point is in any case the moment of sending the original packet. <br><br>  In the case of shaper, the RTT delay for most packets was 107 ms, since they all linger in the queue.  There are peaks up to 190 ms. <br><br><h3>  findings </h3><br>  So, what final conclusions can be made.  Someone may notice that this is so clear.  But our goal was to dig a little deeper.  Let me remind you that the experiment analyzed the behavior of TCP sessions. <br><br><ol><li> Shaper    13%    ,  policer (19.3  17.1 /)     20 /. <br><br></li><li>   shaper'       .         WFQ.   policer'         . <br><br></li><li>   shaper'     (,       ).   policer'      ‚Äì 12.7%. <br><br>  policer    ,       ,      policer'. ,         ,        . <br><br></li><li>   shaper'     (   ‚Äì   102 ).    ,   ,  shaper'           (jitter)  .      ,      jitter. <br><br>           ‚Äì       ( <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D0%25B7%25D0%25BB%25D0%25B8%25D1%2588%25D0%25BD%25D1%258F%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D0%25B5%25D0%25B2%25D0%25B0%25D1%258F_%25D0%25B1%25D1%2583%25D1%2584%25D0%25B5%25D1%2580%25D0%25B8%25D0%25B7%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F">Bufferbloat</a> ).       . <br><br></li><li>     shaper        .      ,        .  policer'   ,     . <br><br></li><li> Policer  shaper  ,  UDP   ¬´¬ª TCP.        . <br><br></li><li>  shaper'      ,   policer'.      . </li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">One more fact can be noted, though not directly related to the speed limitation task. </font><font style="vertical-align: inherit;">Shaper allows us to configure various types of queues (FIFO, WFQ, etc.), thus providing various levels of traffic prioritization when sending it through the device. </font><font style="vertical-align: inherit;">This is very convenient in cases where the actual speed of traffic transmission differs from the channel one (for example, this often happens with Internet access or WAN channels).</font></font><br><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The study of the impact of policer on the Internet</font></font></b> <div class="spoiler_text">      Google   <a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45411.pdf"></a> ,       policer'   .  ,   2%  7%         policer'.      policer'   21%,   6  ,   ,      .   ,    policer', ,   ,  policer  . <br><br>       policer'         . <br><br>  -: <br><br><ol><li>  policer'    (burst size).    ,  TCP      ,    ,        . </li><li>  policer'  shaper (   ). </li><li>   shaper,  policer .    shaper   ,  policer. Shaper  ,  . -   policer     ,   .     . </li></ol><br>      shaper    . Shaper       ,        (  BC = 8 000 ).        .      ,        .        . <br><br>  -       ,    policer'.          .   ‚Äî         TCP:  <a href="http://homes.cs.washington.edu/~tom/pubs/pacing.pdf">TCP Pacing</a> (     RTT,      ACK)      loss recovery (   ACK     ). <br></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Thus, there is no definitive answer to the question which is better to use: shaper or policer. </font><font style="vertical-align: inherit;">Each technology has its pros and cons. </font><font style="vertical-align: inherit;">For some, the additional delay and the load on the equipment is not as critical as for the other. </font><font style="vertical-align: inherit;">So the choice is made in the direction of shaper. </font><font style="vertical-align: inherit;">For some, it is important to minimize network buffering to fight jitter - this means our policer technology. </font><font style="vertical-align: inherit;">In some cases, both technologies can be used simultaneously. </font><font style="vertical-align: inherit;">Therefore, the choice of technology depends on each specific situation in the network.</font></font></div><p>Source: <a href="https://habr.com/ru/post/317048/">https://habr.com/ru/post/317048/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../317030/index.html">Management tool: game project outline</a></li>
<li><a href="../317034/index.html">How to patch a kernel without rebooting: livepatch, kpatch and Canonical Livepatch Service</a></li>
<li><a href="../317036/index.html">How blockchain transforms digital marketing</a></li>
<li><a href="../317038/index.html">Dynamic email</a></li>
<li><a href="../317044/index.html">Why corporate funds can earn more on investment than consumer?</a></li>
<li><a href="../317050/index.html">Neural networks on JS. Creating a network from scratch</a></li>
<li><a href="../317052/index.html">Android developers have reduced the size of updates by an average of 65%</a></li>
<li><a href="../317054/index.html">6 free CDNs to speed up and improve the security of your site</a></li>
<li><a href="../317056/index.html">Internet of Things protocol stack</a></li>
<li><a href="../317058/index.html">Colocation as an option to save on the deployment of its own infrastructure and solving technical problems</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>