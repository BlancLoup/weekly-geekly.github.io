<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Training with reinforcements for the smallest</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article explores the principle of the ‚ÄúLearning with reinforcement‚Äù machine learning method using the example of a physical system. The algorithm...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Training with reinforcements for the smallest</h1><div class="post__text post__text-html js-mediator-article">  <i>This article explores the principle of the <a href="https://ru.wikipedia.org/wiki/%25D0%259E%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D1%2581_%25D0%25BF%25D0%25BE%25D0%25B4%25D0%25BA%25D1%2580%25D0%25B5%25D0%25BF%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5%25D0%25BC" rel="tag">‚ÄúLearning with reinforcement‚Äù</a> machine learning method using the example of a physical system.</i>  <i>The algorithm for finding the optimal strategy is implemented in Python code using the <a href="https://ru.wikipedia.org/wiki/Q-%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5" rel="tag">Q-Learning</a> method.</i> <br><br>  Reinforcement training is a machine learning method in which a model is trained that has no knowledge of the system, but has the ability to perform some actions in it.  Actions take the system to a new state and the model receives some reward from the system.  Consider the work of the method on the <a href="https://www.youtube.com/watch%3Fv%3Df2nIKFMyfSg" rel="tag">example</a> shown in the video.  In the description of the video is the code for <a href="https://drive.google.com/file/d/0Bwyt-Hm2ItvRc0laTFhFNWFQdTg/view%3Fpli%3D1" rel="tag">Arduino</a> , which is implemented in <a href="https://github.com/MichaelSwanRu/Reinforcement-Learning/blob/master/Q-Learning/Example-and-Tutorial/Q-LEARNING.py" rel="tag">Python</a> . <br><br><h3>  Task </h3><br>  Using the ‚Äúreinforcement training‚Äù method, it is necessary to teach the trolley to move away from the wall to the maximum distance.  The award is presented in the form of the value of changing the distance from the wall to the trolley during movement.  Measurement of the distance D from the wall is made by a range finder.  The movement in this example is possible only at a certain offset of the ‚Äúdrive‚Äù consisting of two arrows S1 and S2.  Arrows are two servo with guides connected in the form of "knee".  Each servo in this example can be rotated by 6 identical angles.  The model has the ability to perform 4 actions that represent the control of two servo drives, action 0 and 1 rotate the first servo at a certain angle clockwise and counterclockwise, action 2 and 3 rotate the second servo drive at a certain angle clockwise and counterclockwise.  Figure 1 shows the working prototype of the trolley. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/575/e62/46e/575e6246e8774c1da1ad4227b28bc13c.jpg"><br>  <i>Fig.</i>  <i>1. Prototype carts for machine learning experiments</i> <br><a name="habracut"></a><br>  In Figure 2, the S2 arrow is highlighted in red, the S1 arrow is highlighted in blue, and 2 servo actuators in black. <br><br><img src="https://habrastorage.org/files/197/1b0/1dd/1971b01dd5394920962ccf559fae49b6.jpg"><br>  <i>Fig.</i>  <i>2. Engine system</i> <br><br>  The system diagram is shown in Fig. 3. The distance to the wall is marked D, the yellow shows the range finder, the system drive is highlighted in red and black. <br><br><img src="https://habrastorage.org/files/9c3/3c5/853/9c33c58534ef416c9a959fc3af41acde.jpg"><br>  <i>Fig.</i>  <i>3. System diagram</i> <br><br>  The range of possible positions for S1 and S2 is shown in Figure 4: <br><br><img src="https://habrastorage.org/files/15e/5c7/bef/15e5c7befc23445d95abb9f35a701246.jpg"><br>  <i>Fig.</i>  <i>4.a.</i>  <i>S1 boom range</i> <br><br><img src="https://habrastorage.org/files/440/b22/197/440b22197e2041299649b3415d8365ef.jpg"><br>  <i>Fig.</i>  <i>4.b.</i>  <i>S2 boom range</i> <br><br>  The limiting positions of the drive are shown in Figure 5: <br><br>  When S1 = S2 = 5, the maximum distance from the ground. <br>  When S1 = S2 = 0, the minimum distance to the ground. <br><br><img src="https://habrastorage.org/files/673/f72/b8d/673f72b8d8794aa5b39ab7f886437675.jpg"><br>  <i>Fig.</i>  <i>5. Border positions of arrows S1 and S2</i> <br><br>  The "drive" 4 degrees of freedom.  The action (action) changes the position of the arrows S1 and S2 in space according to a certain principle.  Types of actions are shown in Figure 6. <br><br><img src="https://habrastorage.org/files/e8f/25a/31b/e8f25a31b7d84118b50bd9be3291b9e2.jpg"><br>  <i>Fig.</i>  <i>6. Types of actions (Action) in the system</i> <br><br>  Action 0 increases the value of S1.  Step 1 reduces the value of S1. <br>  Step 2 increases the value of S2.  Step 3 reduces the value of S2. <br><br><h3>  Motion </h3><br>  In our task, the cart is driven only in 2 cases: <br>  In position S1 = 0, S2 = 1, action 3 drives the trolley away from the wall, the system receives a positive reward equal to the change in distance from the wall.  In our example, the reward is 1. <br><br><img src="https://habrastorage.org/files/026/470/6f4/0264706f41ab48f29c67e9a12c8891b9.jpg"><br>  <i>Fig.</i>  <i>7. Movement system with a positive reward</i> <br><br>  In the position S1 = 0, S2 = 0, action 2 drives the trolley to the wall, the system receives a negative reward equal to the change in distance from the wall.  In our example, the reward is -1. <br><br><img src="https://habrastorage.org/files/0f8/e8b/a3b/0f8e8ba3b3844668aa0316cc0deb3cfc.jpg"><br>  <i>Fig.</i>  <i>8. Movement of a system with a negative reward.</i> <br><br>  Under other conditions and any actions of the ‚Äúdrive‚Äù, the system will stand still and the reward will be equal to 0. <br>  It should be noted that the stable dynamic state of the system will be the sequence of actions 0-2-1-3 from the state S1 = S2 = 0, in which the cart will move in the positive direction with the minimum number of actions expended.  Raised the knee - unbent the knee - lowered the knee - bent the knee = trolley moved forward, repeat.  Thus, using the machine learning method, it is necessary to find the state of the system, such a specific sequence of actions, the reward for which will not be received immediately (actions 0-2-1 - reward for which is 0, but which are necessary to obtain 1 for the subsequent action ). <br><br><h3>  Q-Learning Method </h3><br>  The basis of the Q-Learning method is the system state weights matrix.  The Q matrix is ‚Äã‚Äãa combination of all possible system states and weights of the system response to various actions. <br>  In this problem, possible combinations of system parameters are 36 = 6 ^ 2.  In each of the 36 states of the system, it is possible to perform 4 different actions (Action = 0,1,2,3). <br>  Figure 9 shows the initial state of the Q matrix. The zero column contains the row index, the first row is the value of S1, the second is the value of S2, the last 4 columns are equal to the weights for actions equal to 0, 1, 2 and 3. Each row represents a unique system state. <br>  During the initialization of the table, all values ‚Äã‚Äãof the scales will be equal to 10. <br><br><img src="https://habrastorage.org/files/3f8/f09/06c/3f8f0906ccf9457ca614892012a6c735.jpg"><br>  <i>Fig.</i>  <i>9. Initialization of the Q matrix</i> <br><br>  After learning the model (~ 15000 iterations), the Q matrix has the form shown in Figure 10. <br><br><img src="https://habrastorage.org/files/e64/0ad/1be/e640ad1be08e444aa552597efc5ede62.jpg"><br>  <i>Fig.</i>  <i>10. Matrix Q after 15,000 iterations of learning</i> <br><br>  Please note that actions with weights equal to 10 are impossible in the system, therefore the value of the weights has not changed.  For example, in the extreme position when S1 = S2 = 0, action 1 and 3 cannot be performed, since this is a limitation of the physical medium.  These border actions are prohibited in our model, so the algorithm does not use 10-kts. <br><br>  Consider the result of the algorithm: <br>  ... <br>  Iteration: 14991, was: S1 = 0 S2 = 0, action = 0, now: S1 = 1 S2 = 0, prize: 0 <br>  Iteration: 14992, was: S1 = 1 S2 = 0, action = 2, now: S1 = 1 S2 = 1, prize: 0 <br>  Iteration: 14993, was: S1 = 1 S2 = 1, action = 1, now: S1 = 0 S2 = 1, prize: 0 <br>  Iteration: 14994, was: S1 = 0 S2 = 1, action = 3, now: S1 = 0 S2 = 0, prize: 1 <br>  Iteration: 14995, was: S1 = 0 S2 = 0, action = 0, now: S1 = 1 S2 = 0, prize: 0 <br>  Iteration: 14996, was: S1 = 1 S2 = 0, action = 2, now: S1 = 1 S2 = 1, prize: 0 <br>  Iteration: 14997, was: S1 = 1 S2 = 1, action = 1, now: S1 = 0 S2 = 1, prize: 0 <br>  Iteration: 14998, was: S1 = 0 S2 = 1, action = 3, now: S1 = 0 S2 = 0, prize: 1 <br>  Iteration: 14999, was: S1 = 0 S2 = 0, action = 0, now: S1 = 1 S2 = 0, prize: 0 <br><br>  Consider more: <br>  Take iteration 14991 as the current state. <br>  1. The current state of the system S1 = S2 = 0, this state corresponds to the line with the index 0. The highest value is 0.617 (we ignore the values ‚Äã‚Äãequal to 10, described above), it corresponds to Action = 0. This means, according to the Q matrix at the system state S1 = S2 = 0 we perform the action 0. The action 0 increases the value of the angle of rotation of the servo S1 (S1 = 1). <br>  2. The next state S1 = 1, S2 = 0 corresponds to the line with the index 6. The maximum weight value corresponds to Action = 2. We perform action 2 - increase S2 (S2 = 1). <br>  3. The next state S1 = 1, S2 = 1 corresponds to the line with the index 7. The maximum weight value corresponds to Action = 1. We perform the action 1 - decrease S1 (S1 = 0). <br>  4. The next state S1 = 0, S2 = 1 corresponds to the line with index 1. The maximum weight value corresponds to Action = 3. We perform action 3 - decrease S2 (S2 = 0). <br>  5. As a result, they returned to the state S1 = S2 = 0 and earned 1 reward point. <br><br>  Figure 11 shows the principle of choosing the optimal action. <br><br><img src="https://habrastorage.org/files/e98/70f/f3c/e9870ff3c1394758af4aa9d46ce1e4b1.jpg"><br>  <i>Fig.</i>  <i>11.a.</i>  <i>Q matrix</i> <br><br><img src="https://habrastorage.org/files/59a/a78/7b1/59aa787b16c3482fa0ed6a575077e0e6.jpg"><br>  <i>Fig.</i>  <i>11.b.</i>  <i>Q matrix</i> <br><br>  <b>Let us consider in more detail the learning process.</b> <br><br><div class="spoiler">  <b class="spoiler_title">Q-learning algorithm</b> <div class="spoiler_text"><pre><code class="python hljs">minus = <span class="hljs-number"><span class="hljs-number">0</span></span>; plus = <span class="hljs-number"><span class="hljs-number">0</span></span>; initializeQ(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">15000</span></span>): epsilon = math.exp(-float(t)/explorationConst); s01 = s1; s02 = s2 current_action = getAction(); setSPrime(current_action); setPhysicalState(current_action); r = getDeltaDistanceRolled(); lookAheadValue = getLookAhead(); sample = r + gamma*lookAheadValue; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> t &gt; <span class="hljs-number"><span class="hljs-number">14900</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'Time: %(0)d, was: %(1)d %(2)d, action: %(3)d, now: %(4)d %(5)d, prize: %(6)d '</span></span> % \ {<span class="hljs-string"><span class="hljs-string">"0"</span></span>: t, <span class="hljs-string"><span class="hljs-string">"1"</span></span>: s01, <span class="hljs-string"><span class="hljs-string">"2"</span></span>: s02, <span class="hljs-string"><span class="hljs-string">"3"</span></span>: current_action, <span class="hljs-string"><span class="hljs-string">"4"</span></span>: s1, <span class="hljs-string"><span class="hljs-string">"5"</span></span>: s2, <span class="hljs-string"><span class="hljs-string">"6"</span></span>: r} Q.iloc[s, current_action] = Q.iloc[s, current_action] + alpha*(sample - Q.iloc[s, current_action] ) ; s = sPrime; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> deltaDistance == <span class="hljs-number"><span class="hljs-number">1</span></span>: plus += <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> deltaDistance == <span class="hljs-number"><span class="hljs-number">-1</span></span>: minus += <span class="hljs-number"><span class="hljs-number">1</span></span>; print( minus, plus )</code> </pre> <br></div></div><br><br>  Full code on <a href="https://github.com/MichaelSwanRu/Reinforcement-Learning/blob/master/Q-Learning/Example-and-Tutorial/Q-LEARNING.py" rel="tag">github</a> . <br><br>  Set the starting position of the knee to its highest position: <br><br><pre> <code class="python hljs">s1=s2=<span class="hljs-number"><span class="hljs-number">5.</span></span></code> </pre> <br>  We initialize the Q matrix by filling in the initial value: <br><br><pre> <code class="python hljs">initializeQ();</code> </pre> <br>  Calculate the parameter <i>epsilon</i> .  This is the weight of the "randomness" of the algorithm in our calculation.  The more iterations of learning passed, the less random values ‚Äã‚Äãof actions will be selected: <br><br><pre> <code class="python hljs">epsilon = math.exp(-float(t)/explorationConst)</code> </pre> <br>  For the first iteration: <br><br><pre> <code class="python hljs">epsilon = <span class="hljs-number"><span class="hljs-number">0.996672</span></span></code> </pre> <br>  Save the current state: <br><br><pre> <code class="python hljs">s01 = s1; s02 = s2</code> </pre> <br>  We get the "best" value of the action: <br><br><pre> <code class="python hljs">current_action = getAction();</code> </pre> <br>  Consider the function in more detail. <br><br>  The getAction () function returns the value of the action that corresponds to the maximum weight at the current state of the system.  Take the current state of the system in the matrix Q and select the action that corresponds to the maximum weight.  Note that this function implements a random action selection mechanism.  With an increase in the number of iterations, the random choice of action decreases.  This is done so that the algorithm does not get hung up on the first options found and could take another path that may turn out better. <br><br>  In the initial initial position of the arrows, only two actions 1 and 3 are possible. The algorithm chose action 1. <br>  Next, we define the row number in the Q matrix for the next state of the system, in which the system will go after performing the action that we received in the previous step. <br><br><pre> <code class="python hljs">setSPrime(current_action);</code> </pre> <br>  In a real physical environment, after performing the action, we would be rewarded if movement followed, but since the trolley movement is modeled, it is necessary to introduce auxiliary functions of emulating the response of the physical environment to actions.  (setPhysicalState and getDeltaDistanceRolled ()) <br>  Perform functions: <br><br><pre> <code class="python hljs">setPhysicalState(current_action);</code> </pre>  - we simulate the reaction of the medium to the action chosen by us.  Change the position of the servos, shift the cart. <br><br><pre> <code class="python hljs">r = getDeltaDistanceRolled();</code> </pre>  - We calculate the reward - the distance traveled by the cart. <br><br>  After performing the action, we need to update the coefficient of this action in the Q matrix for the corresponding system state.  It is logical that if the action led to a positive reward, then the coefficient, in our algorithm, should decrease by a smaller value than with a negative reward. <br>  Now the most interesting part is to look at the future to calculate the weight of the current step. <br>  When determining the optimal action that needs to be performed in the current state, we choose the greatest weight in the Q matrix. Since we know the new state of the system into which we have passed, we can find the maximum weight value from the Q table for this state: <br><br><pre> <code class="python hljs">lookAheadValue = getLookAhead();</code> </pre> <br>  At the very beginning it is equal to 10. And we use the value of the weight of the action that has not yet been performed to calculate the current weight. <br><br><pre> <code class="python hljs">sample = r + gamma*lookAheadValue; sample = <span class="hljs-number"><span class="hljs-number">7.5</span></span> Q.iloc[s, current_action] = Q.iloc[s, current_action] + alpha*(sample - Q.iloc[s, current_action] ) ; Q.iloc[s, current_action] = <span class="hljs-number"><span class="hljs-number">9.75</span></span></code> </pre> <br>  Those.  we used the weight of the next step to calculate the weight of the current step.  The greater the weight of the next step, the less we will reduce the weight of the current one (according to the formula), and the current step will be preferable next time. <br>  This simple trick gives good results for the convergence algorithm. <br><br><h3>  Algorithm scaling </h3><br>  This algorithm can be extended to a larger number of degrees of freedom of the system (s_features), and a larger number of values ‚Äã‚Äãthat the degree of freedom (s_states) takes, but within small limits.  Fast enough, the Q matrix will take up all the RAM.  Below is an example of a code for constructing a pivot matrix of states and weights of a model.  When the number of "arrows" s_features = 5 and the number of different positions of the arrow s_states = 10, the Q matrix has dimensions (100000, 9). <br><br><div class="spoiler">  <b class="spoiler_title">Increasing the degrees of freedom of the system</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np s_features = <span class="hljs-number"><span class="hljs-number">5</span></span> s_states = <span class="hljs-number"><span class="hljs-number">10</span></span> numActions = <span class="hljs-number"><span class="hljs-number">4</span></span> data = np.empty((s_states**s_features, s_features + numActions), dtype=<span class="hljs-string"><span class="hljs-string">'int'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> h <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>, s_features): k = <span class="hljs-number"><span class="hljs-number">0</span></span> N = s_states**(s_features<span class="hljs-number"><span class="hljs-number">-1</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>*h) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> q <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>, s_states**h): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>, s_states): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>, N): data[k, h] = ik += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(s_states**s_features): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(numActions): data[i,j+s_features] = <span class="hljs-number"><span class="hljs-number">10.0</span></span>; data.shape <span class="hljs-comment"><span class="hljs-comment"># (100000L, 9L)</span></span></code> </pre><br></div></div><br><h3>  Conclusion </h3><br>  This simple method shows the "wonders" of machine learning, when the model is learning nothing about the environment and learns the optimal state in which the reward for action is maximum, and the reward is awarded not immediately, for some action, but for the sequence of actions. <br><br>  Thanks for attention! </div><p>Source: <a href="https://habr.com/ru/post/308094/">https://habr.com/ru/post/308094/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../308084/index.html">SObjectizer: from simple to complex. Part III</a></li>
<li><a href="../308086/index.html">Individual daily limit for outgoing calls (limitation of paid directions)</a></li>
<li><a href="../308088/index.html">Safely use Go language in web programming</a></li>
<li><a href="../308090/index.html">About the division of labor and its consequences</a></li>
<li><a href="../308092/index.html">It's time to tell you how I was Beeline for 4 years on Habr√© - and what I learned about Habr during that time</a></li>
<li><a href="../308098/index.html">CsvLogWriter plugin for JMeter</a></li>
<li><a href="../308100/index.html">Trello Clone on Phoenix and React. Parts 4-5</a></li>
<li><a href="../308102/index.html">Development for SailfishOS: menu</a></li>
<li><a href="../308104/index.html">From juniors to developers: we get the first job</a></li>
<li><a href="../308106/index.html">As I avoided burnout, having worked as a programmer for more than three decades</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>