<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Introduction to Neural Networks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Artificial neural networks are now at the peak of popularity. One may wonder whether the big name has played its role in marketing and the application...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Introduction to Neural Networks</h1><div class="post__text post__text-html js-mediator-article"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_sketch3-4.png" alt="image"><br><br>  Artificial neural networks are now at the peak of popularity.  One may wonder whether the big name has played its role in marketing and the application of this model.  I know some business managers who happily mention the use of ‚Äúartificial neural networks‚Äù and ‚Äúdeep learning‚Äù in their products.  Would they be so glad if their products used ‚Äúmodels with connected circles‚Äù or ‚Äúcars‚Äú make a mistake - will you be punished ‚Äù?‚Äù  But, without a doubt, artificial neural networks are a worthwhile thing, and this is evident due to their success in many applications: image recognition, processing of natural languages, automated trading and autonomous cars.  I am an expert in data processing and analysis, but I did not understand them before, so I felt like a master who had not mastered his tool.  But finally, I completed my ‚Äúhomework‚Äù and wrote this article to help others overcome the same obstacles that I encountered during my (still ongoing) training. <br><br>  <em>The R code for the examples presented in this article can be found <a href="">here</a> in the <a href="https://github.com/ben519/MLPB">Bible of machine learning problems</a> .</em>  <em>In addition, after reading this article, it is worth exploring part 2, <a href="https://gormanalysis.com/neural-networks-a-worked-example">Neural Networks - A Worked Example</a> , which provides details of creating and programming a neural network from scratch.</em> <br><a name="habracut"></a><br>  We will start with a motivating challenge.  We have a set of images in grayscale, each of which is a grid of 2 √ó 2 pixels, in which each pixel has a brightness value from 0 (white) to 255 (black).  Our goal is to create a model that will find images with the ‚Äúladder‚Äù pattern. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig1.png"></div><br><br>  At this stage, we are only interested in finding a model that <em>can</em> logically select data.  The selection methodology will be interesting to us later. <br><br><h2>  Preliminary processing </h2><br>  In each image we mark the pixels. <math> </math> $ inline $ x_ {1} $ inline $   , <math> </math> $ inline $ x_ {2} $ inline $   , <math> </math> $ inline $ x_ {3} $ inline $   , <math> </math> $ inline $ x_ {4} $ inline $   and generate the input vector <math> </math> $ inline $ x = \ begin {bmatrix} x_ {1} &amp; x_ {2} &amp; x_ {3} &amp; x_ {4} \ end {bmatrix} $ inline $   Which will be the input data of our model.  We expect our model to predict True (the image contains a staircase pattern) or False (the image does not contain a staircase pattern). <br><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig2-300x263.png"></div><br><br><table><thead><tr><th align="center">  ImageId </th><th align="center">  x1 </th><th align="center">  x2 </th><th align="center">  x3 </th><th align="center">  x4 </th><th align="center">  Isstatairs </th></tr></thead><tbody><tr><td align="center">  one </td><td align="center">  252 </td><td align="center">  four </td><td align="center">  155 </td><td align="center">  175 </td><td align="center">  TRUE </td></tr><tr><td align="center">  2 </td><td align="center">  175 </td><td align="center">  ten </td><td align="center">  186 </td><td align="center">  200 </td><td align="center">  TRUE </td></tr><tr><td align="center">  3 </td><td align="center">  82 </td><td align="center">  131 </td><td align="center">  230 </td><td align="center">  100 </td><td align="center">  FALSE </td></tr><tr><td align="center">  ... </td><td align="center">  ... </td><td align="center">  ... </td><td align="center">  ... </td><td align="center">  ... </td><td align="center">  ... </td></tr><tr><td align="center">  498 </td><td align="center">  36 </td><td align="center">  187 </td><td align="center">  43 </td><td align="center">  249 </td><td align="center">  FALSE </td></tr><tr><td align="center">  499 </td><td align="center">  one </td><td align="center">  160 </td><td align="center">  169 </td><td align="center">  242 </td><td align="center">  TRUE </td></tr><tr><td align="center">  500 </td><td align="center">  198 </td><td align="center">  134 </td><td align="center">  22 </td><td align="center">  188 </td><td align="center">  FALSE </td></tr></tbody></table><br><h2>  Single-layer perceptron (iteration of model 0) </h2><br>  We can build a simple model consisting of a single <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D1%2586%25D0%25B5%25D0%25BF%25D1%2582%25D1%2580%25D0%25BE%25D0%25BD">-</a> layer <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D1%2586%25D0%25B5%25D0%25BF%25D1%2582%25D1%2580%25D0%25BE%25D0%25BD">perceptron</a> .  A perceptron uses a weighted linear combination of input data to return a forecast estimate.  If the forecast estimate exceeds the selected threshold, then the perceptron predicts True.  Otherwise, it predicts False.  If more formally, then <br><br><p><math> </math> $$ display $$ f (x) = {\ begin {cases} 1 &amp; {\ text {if}} \ w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4&gt; threshold \\ 0 &amp; {\ text {otherwise}} \ end {cases }} $$ display $$ </p><br><br>  Let's put it another way <br><br><p><math> </math> $$ display $$ \ widehat y = \ mathbf w \ cdot \ mathbf x + b $$ display $$ </p><br><br><p><math> </math> $$ display $$ f (x) = {\ begin {cases} 1 &amp; {\ text {if}} \ \ widehat {y}&gt; 0 \\ 0 &amp; {\ text {otherwise}} \ end {cases}} $$ display $$ </p><br>  Here <math> </math> $ inline $ \ hat {y} $ inline $   - our <em>estimate of the forecast</em> . <br><br>  Graphically, we can represent the perceptron as input nodes that transmit data to the output node. <br><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_sketch1-3.png" alt="image"></div><br><br>  For our example, we build the following perceptron: <br><br><p><math> </math> $$ display $$ \ hat {y} = - 0.0019x_ {1} + 0.0016x_ {2} + 0.0020x_ {3} + 0.0023x_ {4} + 0.0003 $$ display $$ </p><br>  This is how the perceptron will work on some of the training images. <br><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig3.png" alt="image"></div><br><br>  This is definitely better than random guesswork and has common sense.  All staircase patterns have dark pixels in the bottom row, which creates large positive coefficients. <math> </math> $ inline $ x_ {3} $ inline $   and <math> </math> $ inline $ x_ {4} $ inline $   .  However, there are obvious problems with this model. <br><br><ol><li>  The model yields a real number, the value of which correlates with the concept of similarity (the larger the value, the higher the probability that there is a ladder in the image), but there is no basis for interpreting these values ‚Äã‚Äãas probabilities, because they can be outside the interval [0 , one]. </li><li>  The model cannot capture the non-linear relationships between variables and the target value.  To verify this, consider the following hypothetical scenarios: </li></ol><br><h5>  Case A </h5><br>  Let's start with the image x = [100, 0, 0, 125].  Will increase <math> </math> $ inline $ x_ {3} $ inline $   from 0 to 60. <br><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig4.png" alt="image"></div><br><br><h5>  Case B </h5><br>  Let's start with the previous image, x = [100, 0, 60, 125].  Will increase <math> </math> $ inline $ x_ {3} $ inline $   from 60 to 120. <br><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig5.png" alt="image"></div><br><br>  Intuitively, <strong>case A</strong> should increase much more. <math> </math> $ inline $ \ hat {y} $ inline $   than the <strong>case of B.</strong>  However, since our perceptron model is a linear equation, the increment is +60 <math> </math> $ inline $ x_ {3} $ inline $   in both cases will result in a gain of +0.12 <math> </math> $ inline $ \ hat {y} $ inline $   . <br><br>  Our linear perceptron has other problems, but let's solve these two first. <br><br><h2>  Single-layer perceptron with sigmoid activation function (iteration of model 1) </h2><br>  We can solve problems 1 and 2 by wrapping our perceptron in a sigmoid (with the subsequent selection of other weights).  It is worth recalling that <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B8%25D0%25B3%25D0%25BC%25D0%25BE%25D0%25B8%25D0%25B4%25D0%25B0">the "sigmoid" function</a> is an S-shaped curve bounded along the vertical axis between 0 and 1, so that it is often used to simulate the probability of a binary event. <br><br><p><math> </math> $$ display $$ sigmoid (z) = \ frac {1} {1 + e ^ {- z}} $$ display $$ </p><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig13-2.png" alt="image"></div><br><br>  In accordance with this thought, we can supplement our model with the following image and equation. <br><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_sketch2-3.png" alt="image"></div><br><br><p><math> </math> $$ display $$ z = w \ cdot x = w_1x_1 + w_2x_2 + w_3x_3 + w_4x_4 $$ display $$ </p><br><p><math> </math> $$ display $$ \ widehat y = sigmoid (z) = \ frac {1} {1 + e ^ {- z}} $$ display $$ </p><br>  Looks familiar?  Yes, this is our old friend, <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25BE%25D0%25B3%25D0%25B8%25D1%2581%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D1%2580%25D0%25B5%25D0%25B3%25D1%2580%25D0%25B5%25D1%2581%25D1%2581%25D0%25B8%25D1%258F">logistic regression</a> .  However, it will serve us well for interpreting the model as a linear perceptron with a sigmoid activation function, because it gives us more opportunities for a more general approach.  Also, since we can now interpret <math> </math> $ inline $ \ hat {y} $ inline $   as a <em>probability</em> , we need to change the decision rule accordingly. <br><br><p><math> </math> $$ display $$ f (x) = {\ begin {cases} 1 &amp; {\ text {if}} \ \ widehat {y}&gt; 0.5 \\ 0 &amp; {\ text {otherwise}} \ end {cases}} $$ display $$ </p><br><br>  Let's continue with our example of the task and we will assume that we have the following selected model: <br><br><p><math> </math> $$ display $$ \ begin {bmatrix} w_1 &amp; w_2 &amp; w_3 &amp; w_4 \ end {bmatrix} = \ begin {bmatrix} -0.140 &amp; -0.145 &amp; 0.121 &amp; 0.092 \ end {bmatrix} $$ display $$ </p><br><p><math> </math> $$ display $$ b = -0.008 $$ display $$ </p><br><p><math> </math> $$ display $$ \ widehat y = \ frac {1} {1 + e ^ {- (- 0.140x_1 -0.145x_2 + 0.121x_3 + 0.092x_4 -0.008)}} $$ display $$ </p><br>  Let's observe how this model behaves on the same examples of images from the previous section. <br><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig6.png" alt="image"></div><br><br>  We definitely managed to solve problem 1. See how it solves problem 2. <br><br><h5>  Case A </h5><br>  Let's start with the image [100, 0, 0, 100].  Will increase <math> </math> $ inline $ x_3 $ inline $   "from 0 to 50. <br><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig7.png" alt="image"></div><br><br><h5>  Case B </h5><br>  Let's start with the image [100, 0, 50, 100].  Will increase <math> </math> $ inline $ x_3 $ inline $   "from 50 to 100. <br><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig8.png" alt="image"></div><br><br>  Notice how the curvature of the sigmoid causes <strong>case A to</strong> "work" (quickly increase) with increasing <math> </math> $ inline $ z = w \ cdot x $ inline $   but the pace slows down with continued increase <math> </math> $ inline $ z $ inline $   .  This is consistent with our intuitive understanding that <strong>case A</strong> should reflect a greater increase in the probability of a staircase pattern than <strong>case B.</strong> <br><br><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig9.png" alt="image"><br><br>  Unfortunately, this model still has problems. <br><br><ol><li><math> </math> $ inline $ \ widehat y $ inline $   has a monotonous relationship with each variable.  But what if we need to recognize stairs of a lighter shade? </li><li>  The model does not take into account the interaction of variables.  Suppose that the bottom row of the image is black.  If the upper left pixel is white, then darkening the upper right pixel should increase the likelihood of the staircase pattern.  If the upper left pixel is black, then shading the upper right pixel should reduce the likelihood of the staircase.  In other words, an increase <math> </math> $ inline $ x_3 $ inline $   should potentially lead to an increase <em>or</em> decrease <math> </math> $ inline $ \ widehat y $ inline $   , depending on the values ‚Äã‚Äãof other variables.  In our current model, this can not be achieved. </li></ol><br><h2>  Multilayer perceptron with a sigmoid activation function (iteration of model 2) </h2><br>  We can solve both of the above problems by adding another <em>layer to the</em> perceptron model.  We will create several basic models similar to those presented above, but we will transmit the output of each basic model to the input of <em>another</em> perceptron.  This model is actually a ‚Äúvanilla‚Äù neural network.  Let's see how it can work in different examples. <br><br><h5>  Example 1: ladder pattern recognition </h5><br><ol><li>  We will build a model that is triggered by the recognition of "left ladders", <math> </math> $ inline $ \ widehat y_ {left} $ inline $ </li><li>  Let's build a model that works when recognizing the ‚Äúright stairs‚Äù, <math> </math> $ inline $ \ widehat y_ {right} $ inline $ </li><li> Add an assessment to the base models so that the final sigmoid will work only if <strong>both</strong> values ‚Äã‚Äã( <math> </math> $ inline $ \ widehat y_ {left} $ inline $   , <math> </math> $ inline $ \ widehat y_ {right} $ inline $   ) are great </li></ol><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_sketch3-4.png" alt="image"></div><br><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig10.png" alt="image"></div><br><br>  <strong><em>Another variant</em></strong> <br><br><ol><li>  Let's build a model that works when the bottom row is dark, <math> </math> $ inline $ \ widehat y_1 $ inline $ </li><li>  Construct a model that fires when the top left pixel is dark <strong>and the</strong> top right pixel is light, <math> </math> $ inline $ \ widehat y_2 $ inline $ </li><li>  Construct a model that fires when the top left pixel is bright <strong>and the</strong> top right pixel is dark, <math> </math> $ inline $ \ widehat y_3 $ inline $ </li><li>  Add the base models so that the final sigmoid function only works when <math> </math> $ inline $ \ widehat y_1 $ inline $   <strong>and</strong> <math> </math> $ inline $ \ widehat y_2 $ inline $   great or when <math> </math> $ inline $ \ widehat y_1 $ inline $   <strong>and</strong> <math> </math> $ inline $ \ widehat y_3 $ inline $   are great.  (Notice that <math> </math> $ inline $ \ widehat y_2 $ inline $   and <math> </math> $ inline $ \ widehat y_3 $ inline $   can't be big at the same time.) </li></ol><br><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_sketch4-3.png" alt="image"></div><br><br><div style="text-align:center;"><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig11.png" alt="image"></div><br><br><h5>  <strong>Example 2: Recognize stairs in a light shade.</strong> </h5><br><ol><li>  Build models that trigger with ‚Äúshaded bottom row‚Äù, ‚Äúshaded x1 and white x2‚Äù, ‚Äúshaded x2 and white x1‚Äù <math> </math> $ inline $ \ widehat y_1 $ inline $   , <math> </math> $ inline $ \ widehat y_2 $ inline $   and <math> </math> $ inline $ \ widehat y_3 $ inline $ </li><li>  Build models that are triggered by the ‚Äúdark bottom row‚Äù, ‚Äúdark x1 and white x2‚Äù, ‚Äúdark x2 and white x‚Äù, <math> </math> $ inline $ \ widehat y_4 $ inline $   , <math> </math> $ inline $ \ widehat y_5 $ inline $   and <math> </math> $ inline $ \ widehat y_6 $ inline $ </li><li>  Connect the models so that the "dark" identifiers are subtracted from the "shaded" identifiers before compressing the result with a sigmoid </li></ol><br><br><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_sketch5-4.png" alt="image"><br><br><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_fig12.png" alt="image"><br><br><h4>  Terminology Note </h4><br>  <em>One</em> layer perceptron has <em>one output layer</em> .  That is, the models we constructed will be called <em>two-</em> layer perceptrons, because they have an output layer, which is the input of another output layer.  However, we can call the same models neural networks, in which case the networks have <em>three</em> layers ‚Äî the input layer, the hidden layer, and the output layer. <br><br><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_sketch6-1.png"><br><br><h2>  Alternative activation functions </h2><br>  In our examples, we used the sigmoid activation function.  However, other activation functions can be used.  Often used <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B8%25D0%25BF%25D0%25B5%25D1%2580%25D0%25B1%25D0%25BE%25D0%25BB%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B5_%25D1%2584%25D1%2583%25D0%25BD%25D0%25BA%25D1%2586%25D0%25B8%25D0%25B8">tanh</a> and <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">relu</a> .  The activation function must be non-linear, otherwise the neural network will be simplified to a similar single-layer perceptron. <br><br><h2>  Multi-class classification </h2><br>  We can easily expand our model so that it works in a multi-class classification by using multiple nodes in the final output layer.  The idea here is that each output node corresponds to one of the classes. <math> </math> $ inline $ C $ inline $   which we strive to predict.  Instead of narrowing the output with a sigmoid, which reflects an element from <math> </math> $ inline $ \ mathbb {R} $ inline $   in the element from the interval [0, 1] we can use <a href="https://ru.wikipedia.org/wiki/Softmax">the softmax function</a> , which reflects the vector in <math> </math> $ inline $ \ mathbb {R} ^ n $ inline $   in vector in <math> </math> $ inline $ \ mathbb {R} ^ n $ inline $   in such a way that the sum of the elements of the resulting vector is equal to 1. In other words, we can create a network that gives the output vector [ <math> </math> $ inline $ prob (class_1) $ inline $   , <math> </math> $ inline $ prob (class_2) $ inline $   , ..., <math> </math> $ inline $ prob (class_C) $ inline $   ]. <br><br><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_sketch7-1.png"><br><br><h2>  Using three or more layers (deep learning) </h2><br>  You may ask yourself: is it possible to expand our ‚Äúvanilla‚Äù neural network so that its output layer is transmitted to the fourth layer (and then to the fifth, sixth, etc.)?  Yes.  This is usually called ‚Äúdeep learning.‚Äù  In practice, it can be very effective.  However, it is worth noting that any network consisting of more than one hidden layer can be simulated with a network with one hidden layer.  Indeed, according to the <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25A6%25D1%258B%25D0%25B1%25D0%25B5%25D0%25BD%25D0%25BA%25D0%25BE">universal approximation theorem,</a> any continuous function can be approximated using a neural network with one hidden layer.  The reason for the frequent selection of deep neural network architectures instead of networks with one hidden layer is that during the selection procedure they usually converge to a solution faster. <br><br><img src="https://gormanalysis.com/wp-content/uploads/2017/11/intro-to-nnets_sketch8-1.png"><br><br><h2>  Selection of a model for marked training samples (back propagation of a training error) </h2><br>  Alas, but we got to the selection procedure.  Before that, we talked about how neural networks <em>can</em> work efficiently, but did not discuss how the neural network is adapted to the marked training samples.  An analogue of this question might be: ‚ÄúHow can one select the best weights for the network based on several marked training samples?‚Äù.  The usual answer is gradient descent (although <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25BC%25D0%25B0%25D0%25BA%25D1%2581%25D0%25B8%25D0%25BC%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE_%25D0%25BF%25D1%2580%25D0%25B0%25D0%25B2%25D0%25B4%25D0%25BE%25D0%25BF%25D0%25BE%25D0%25B4%25D0%25BE%25D0%25B1%25D0%25B8%25D1%258F">MMP</a> may be appropriate).  If we continue to work on our example of the problem, then the procedure of the gradient descent may look like this: <br><br><ol><li>  We start with some marked tutorial data. </li><li>  We choose to minimize the differentiable loss function, <math> </math> $ inline $ L (\ mathbf {\ widehat Y}, \ mathbf {Y}) $ inline $ </li><li>  Choose a network structure.  Especially clearly need to determine the number of layers and nodes on each layer. </li><li>  We initialize the network with random weights. </li><li>  Pass the training data through the network to generate a forecast for each sample.  Let's measure the total error according to the loss function, <math> </math> $ inline $ L (\ mathbf {\ widehat Y}, \ mathbf {Y}) $ inline $   .  (This is called direct distribution.) </li><li>  We determine how much the current losses are changing with respect to the small changes in each of the weights.  In other words, we calculate the gradient <math> </math> $ inline $ L $ inline $   taking into account each weight in the network.  (This is called back distribution.) </li><li>  Make a small ‚Äústep‚Äù in the direction of the negative gradient.  For example, if <math> </math> $ inline $ w_ {23} = 1.5 $ inline $   , but <math> </math> $ inline $ \ frac {\ partial L} {\ partial w_ {23}} = 2.2 $ inline $   , then decrease <math> </math> $ inline $ w_ {23} $ inline $   by a small amount <em>should</em> lead to a slight decrease in current losses.  Therefore we change <math> </math> $ inline $ w_3: = w_3 - 2.2 \ times 0.001 $ inline $   (where 0.001 is the specified ‚Äústep size‚Äù). </li><li>  Repeat this process (from step 5) a fixed number of times or until the losses converge. </li></ol><br>  At least that is the basic idea.  When implemented in practice, there are many difficulties. <br><br><h3>  Difficulty 1 - computational complexity </h3><br>  In the selection process, among other things, we need to calculate the gradient <math> </math> $ inline $ L $ inline $   taking into account each weight.  It is difficult because <math> </math> $ inline $ L $ inline $   depends on each node in the output layer, and each of these nodes depends on <em>each</em> node in the layer in front of it, and so on.  This means that the calculation <math> </math> $ inline $ \ frac {\ partial L} {\ partial w_ {ab}} $ inline $   turns into a real nightmare with complex derivative formulas.  (Do not forget that many neural networks in the real world contain thousands of nodes in dozens of layers.) This problem can be solved by noting that when using a complex derivative formula, most <math> </math> $ inline $ \ frac {\ partial L} {\ partial w_ {ab}} $ inline $   reuses identical intermediate derivatives.  If you keep a close eye on this, you will be able to avoid the same repeated calculations thousands of times. <br><br>  Another trick is to use special activation functions, the derivatives of which can be written as a function of their value.  For example, the derivative <math> </math> $ inline $ sigmoid (x) $ inline $   = <math> </math> $ inline $ sigmoid (x) (1 - sigmoid (x)) $ inline $   .  This is convenient because during the straight pass when calculating <math> </math> $ inline $ \ widehat y $ inline $   for each training sample we need to calculate <math> </math> $ inline $ sigmoid (\ mathbf {x}) $ inline $   elementwise for some vector <math> </math> $ inline $ \ mathbf {x} $ inline $   .  During back propagation, we can reuse these values ‚Äã‚Äãto calculate the gradient <math> </math> $ inline $ L $ inline $   taking into account weights, which will save time and memory. <br><br>  The third trick is to divide the training samples into ‚Äúmini-groups‚Äù and to change the weights for each group, one after another.  For example, if we divide the training data into {batch1, batch2, batch3}, then the first pass through the training data will be <br><br><ol><li>  Change weights based on batch1 </li><li>  Change weights based on batch2 </li><li>  Change weights based on batch3 </li></ol><br>  where is the gradient <math> </math> $ inline $ L $ inline $   recalculated after each change. <br><br>  Finally, it is worth mentioning another technique - the use of a video processor instead of the central processor, because it is better suited to perform a large number of parallel computing. <br><br><h3>  Difficulty 2 - Gradient descent may have problems finding the absolute minimum </h3><br>  This is not a problem of neural networks, but of a gradient descent.  There is a possibility that during the gradient descent, the weights may get stuck at a local minimum.  It is also possible that the weight "jumped" at least.  One way to handle this is to use different step sizes.  Another way is to increase the number of nodes and / or layers in the network.  (But it is worth fearing an overly close fit).  In addition, some heuristic techniques can be effective, for example, using the <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">moment</a> . <br><br><h3>  Difficulty 3 - how to develop a common approach? </h3><br>  How to write a genetic program that can pick up values ‚Äã‚Äãfor any neural network with any number of nodes and layers?  The correct answer - no, you need to use <a href="https://www.tensorflow.org/">Tensorflow</a> .  But if you want to try, the most difficult part is the calculation of the loss function gradient.  The trick here is to determine that a gradient can be represented as a recursive function.  A five-layer neural network is simply a four-layer neural network that transmits data to some kind of perceptrons.  But a neural network with four layers is just a neural network with three layers, transferring data to some perceptrons, and so on.  More formally, this is called <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a> . </div><p>Source: <a href="https://habr.com/ru/post/342334/">https://habr.com/ru/post/342334/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../342318/index.html">PouchDB or what to do when the ‚Äúinternet is stable‚Äù</a></li>
<li><a href="../342324/index.html">Errors in smart contracts or Parity's new Security Alert</a></li>
<li><a href="../342326/index.html">PVS-Studio report is now in Html format</a></li>
<li><a href="../342330/index.html">Insight on metrics: how do I understand what metrics are and what is their main charm</a></li>
<li><a href="../342332/index.html">Storage of personal data on a foreign hosting: if it is possible, how?</a></li>
<li><a href="../342336/index.html">Migrating a database from InnoDB to MyRocks</a></li>
<li><a href="../342338/index.html">Where large companies are looking for innovation. Interview with Dmitry Izmestyev about the development of startups</a></li>
<li><a href="../342340/index.html">Buy ready MDM or develop your own?</a></li>
<li><a href="../342344/index.html">You are working in the wrong place (if you have an open office)</a></li>
<li><a href="../342346/index.html">How JS: WebSocket and HTTP / 2 + SSE work. What to choose?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>