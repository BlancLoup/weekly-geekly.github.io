<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to clean up the mailbox using a neural network. Part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In our blog, we write a lot about creating emails and working with e-mail. We have already discussed the complexity of the fight against spam , the fu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to clean up the mailbox using a neural network. Part 1</h1><div class="post__text post__text-html js-mediator-article"> <a href="https://habrahabr.ru/company/pechkin/blog/280198/"><img src="https://habrastorage.org/getpro/habr/post_images/dd5/dd0/b26/dd5dd0b268fa4e8707af6c0cb80830e4.png" alt="image"></a> <br><br>  In our blog, we write a lot about creating emails and working with e-mail.  We have already discussed the complexity of the <a href="https://habrahabr.ru/company/pechkin/blog/277259/">fight against spam</a> , the future of email, the <a href="https://habrahabr.ru/company/pechkin/blog/276761/">protection of postal correspondence</a> , as well as the techniques of <a href="https://habrahabr.ru/company/pechkin/blog/275809/">working with email</a> , used by the leaders of large IT companies. <br><br>  In the modern world, people get a lot of letters, and in full growth there is a problem with their classification and ordering of the mailbox.  An engineer from the USA, Andrei Kurenkov, in his blog <a href="http://www.andreykurenkov.com/writing/organizing-my-emails-with-a-neural-net/">told</a> about how he solved this task using a neural network.  We decided to highlight the course of this project and present you the first part of the story. <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The project code is available <a href="https://github.com/andreykurenkov/emailinsight/tree/master/pyScripts">here</a> . <br><br><h4>  Start </h4><br>  Kurenkov writes that one of his favorite mini-projects, <a href="http://www.andreykurenkov.com/projects/hacks/email-filer/">EmailFiler</a> , appeared due to the assignment on the course ‚ÄúIntroduction to machine learning‚Äù at the Georgia Institute of Technology.  The task was to take any data set, apply to it a number of controlled learning algorithms and analyze the results.  The bottom line is that you could use your own data if desired.  The developer did just that - exported his gmail data to investigate the capabilities of machine learning in solving the problem of email sorting. <br><br>  Kurenkov writes that he long ago understood that it is better to always have e-mails on hand in case you ever need to contact them.  On the other hand, he always wanted to reduce the number of unread incoming messages to zero, no matter how hopeless this idea was.  Therefore, many years ago, an engineer began sorting emails by about a dozen folders, and by the time they entered machine learning courses, thousands of letters had accumulated in these folders. <br><br>  Thus, the idea of ‚Äã‚Äãa project to create a classifier, which could offer a single category for each incoming letter, appeared - so as to send a letter to the appropriate folder at the touch of a button. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f9c/bb8/c8a/f9cbb8c8a512b8134a909ba73a69d9e0.png" alt="image"><br><br>  <i>Categories (folders) and the number of letters in each of them at the time of the start of the project</i> <br><br>  Letters were the input data, the categories of letters were included in the output data.  The difficulty was that the developer wanted to use letter texts and metadata, but it was not easy to understand how to turn all this into a machine-readable data set. <br><br>  Anyone who has studied the processing of natural languages ‚Äã‚Äãwill immediately suggest one simple approach ‚Äî the use of the Bag of Words model.  This is one of the simplest text classification algorithms ‚Äî find N common words for all texts and create a table of binary signs for each word (sign is 1 if the given text contains a word, and 0 if it does not). <br><br>  Kurenkov did this for a group of words found in all his letters, as well as for the top 20 senders of letters (as in some cases the sender strictly correlates with the category of the letter; for example, if the sender is the supervisor at the university, the category will be ‚ÄúResearch‚Äù ) and for the top 5 domains from which letters were sent to him (since domains like <a href="https://habrahabr.ru/users/gatech/" class="user_link">gatech</a> .edu strictly indicate categories such as, for example, ‚ÄúTraining‚Äù).  So, somewhere after an hour spent writing an email <a href="https://github.com/andreykurenkov/emailinsight/blob/master/pyScripts/mboxConvert.py">parser</a> , he was able to receive data about his mailbox in csv format (values ‚Äã‚Äãseparated by commas). <br><br>  How well did it all work?  Not bad, but I wanted more.  Kurenkov says that he was fond of the Orange ML machine learning framework for Python, and, as was the case for his assignment, <a href="https://github.com/andreykurenkov/emailinsight/blob/master/pyScripts/orangeClassify.py">tested</a> several algorithms on his data set.  Two algorithms stood out - decision trees showed themselves best of all, and neural networks worst of all: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bd9/c29/fd9/bd9c29fd958b8dffe39ed97308270554.png" alt="image"><br><br>  <i>So a small set of data coped decision trees</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d5f/3a2/08f/d5f3a208fcc680ca9cc66f3f19f70455.png" alt="image"><br><br>  <i>And so - neural networks</i> <br><br>  If you take a close look at these graphs from OpenOffice Calc, you can see that the best result of decision trees on the test is about 72%, and the neural networks are a measly 65%.  Horror!  This, of course, is better than expanding everything at random, given that there are 11 categories, but far from an excellent result. <br><br>  Why were the results so depressing?  The fact is that the signs obtained for the data set turned out to be very primitive - a simple search for the 500 most frequent words in letters will give us not too many useful indicators, but many commonly used constructions that are found in English, for example, ‚Äúthat‚Äù or ‚Äú is. "  The engineer understood this and did the following: completely removed the words from three or less letters from the list and wrote some code to select the most common words for each category separately;  but he still had no idea how to improve the result. <br><br><h4>  Attempt number two </h4><br>  After a couple of years, Kurenkov decided once again to try to solve the problem - even though the project at the university had already been delivered for a long time, I wanted to improve the results. <br><br>  This time, the engineer decided to use <a href="http://keras.io/">Keras</a> as one of the main tools, because it is written in Python and also gets along well with the <a href="http://www.numpy.org/">NumPy</a> , <a href="http://pandas.pydata.org/">pandas</a> and <a href="http://scikit-learn.org/stable/">scikit-learn</a> packages and is supported by Theano library. <br><br>  In addition, it turned out that Keras has several easy examples in which to begin work, including the <a href="https://github.com/fchollet/keras/blob/master/examples/reuters_mlp.py">task of</a> classifying various texts.  And what is interesting is that this example uses the same functions that the engineer used earlier.  It identifies the 1000 most common words in documents, binary signs, and trains a neural network with one hidden layer and dropout-regularization to predict the category of a given text based solely on these signs. <br><br>  So, the first thing that comes to mind is to try this example on your own data - to see if Keras will improve the work.  Fortunately, the old code for parsing the mailbox has been preserved, and Keras has a convenient Tokenizer class for receiving text attributes.  Therefore, you can easily create a data set in the same format as in the example and get new information about the current number of emails: <br><br><pre> Using theano backend.
 Label email count breakdown:
	 Personal: 440
	 Group work: 150
	 Financial: 118
	 Academic: 1088
	 Professional: 388
	 Group work / SolarJackets: 1535
	 Personal / Programming: 229
	 Professional / Research: 1066
	 Professional / TA: 1801
	 Sent: 513
	 Unread: 146
	 Professional / EPFL: 234
	 Important: 142
	 Professional / RISS: 173
 Total emails: 8023
</pre><br>  Eight thousand letters can not be called a large set of data, but still this is enough to work a little serious machine learning.  After translating the data into the desired format, it remains only to see if the neural network training works on this data.  An example using Keras makes it easy to do the following: <br><br><pre>  7221 train sequences
 802 test sequences
 Building model ...
 Train on 6498 samples, validate on 723 samples
 Epoch 1/5
 6498/6498 [==============================] - 2s - loss: 1.3182 - acc: 0.6320 - val_loss: 0.8166 - val_acc: 0.7718
 Epoch 2/5
 6498/6498 [==============================] - 2s - loss: 0.6201 - acc: 0.8316 - val_loss: 0.6598 - val_acc: 0.8285
 Epoch 3/5
 6498/6498 [==============================] - 2s - loss: 0.4102 - acc: 0.8883 - val_loss: 0.6214 - val_acc: 0.8216
 Epoch 4/5
 6498/6498 [==============================] - 2s - loss: 0.2960 - acc: 0.9214 - val_loss: 0.6178 - val_acc: 0.8202
 Epoch 5/5
 6498/6498 [==============================] - 2s - loss: 0.2294 - acc: 0.9372 - val_loss: 0.6031 - val_acc: 0.8326
 802/802 [==============================] - 0s     
 Test score: 0.585222780162 </pre><br>  <b>Accuracy: 0.847880299252</b> <br><br>  85% accuracy!  This is much higher than the measly 65% ‚Äã‚Äãof the old neural network.  Fine. <br><br>  But why? <br><br>  The old code did, in general, the following - defined the most common words, created a binary matrix of attributes and trained a neural network with one hidden layer.  Maybe all this is due to the new ‚Äúrelu‚Äù neuron, the dropout regularization, and the use of optimization methods other than the stochastic gradient descent?  Since the signs found on the old data are binary and presented in the form of a matrix, they are very easy to turn into a data set for training this neural network.  So, here are the results: <br><br><pre>  Epoch 1/5
 6546/6546 [==============================] - 1s - loss: 1.8417 - acc: 0.4551 - val_loss: 1.4071 - val_acc: 0.5659
 Epoch 2/5
 6546/6546 [==============================] - 1s - loss: 1.2317 - acc: 0.6150 - val_loss: 1.1837 - val_acc: 0.6291
 Epoch 3/5
 6546/6546 [==============================] - 1s - loss: 1.0417 - acc: 0.6661 - val_loss: 1.1216 - val_acc: 0.6360
 Epoch 4/5
 6546/6546 [==============================] - 1s - loss: 0.9372 - acc: 0.6968 - val_loss: 1.0689 - val_acc: 0.6635
 Epoch 5/5
 6546/6546 [==============================] - 2s - loss: 0.8547 - acc: 0.7215 - val_loss: 1.0564 - val_acc: 0.6690
 808/808 [==============================] - 0s     
 Test score: 1.03195088158 </pre><br>  <b>Accuracy: 0.64603960396</b> <br><br>  So, the old decision to define the categories of emails was unsuccessful.  Perhaps the reason was the combination of strong trait limitations (manually defined top senders, domains and words from each category) and too few words. <br><br>  An example using Keras simply selects the 1000 most common words in a large matrix without additional filtering and transmits it to the neural network.  If you do not severely limit the selection of features, more suitable ones can be chosen from them, which increases the overall accuracy of the algorithm. <br><br>  The reason is either in this or in the presence of errors in the code: changing it in order to make the selection process of signs less restrictive leads to an increase in accuracy of only 70%.  In any case, the engineer found out that it was possible to improve your old result using a modern machine learning library. <br><br>  Now there was a new question - is it possible to achieve even higher accuracy? <br><br>  <i>To be continued‚Ä¶</i> </div><p>Source: <a href="https://habr.com/ru/post/280198/">https://habr.com/ru/post/280198/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../280186/index.html">Scientists have created a neural network that recognizes "drunk" messages on Twitter</a></li>
<li><a href="../280188/index.html">Var and val in java?</a></li>
<li><a href="../280190/index.html">Android Cuvettes, Part 2: SDK and Libraries</a></li>
<li><a href="../280192/index.html">Problem about pair numbers</a></li>
<li><a href="../280196/index.html">Pros and cons: When it is worth and not worth using MongoDB</a></li>
<li><a href="../280200/index.html">Directed Phishing - a Modern Security Threat</a></li>
<li><a href="../280204/index.html">A bit of Storage Class Memory</a></li>
<li><a href="../280206/index.html">How ESLint reacts to the situation with the removal of packages from NPM</a></li>
<li><a href="../280208/index.html">Distributed nature of Tox Messenger</a></li>
<li><a href="../280210/index.html">About Go functionality</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>