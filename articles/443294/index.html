<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>What is allowed by Jupyter?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Our story began with a seemingly simple task. It was necessary to configure analytical tools for data science specialists and just data analysts. With...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>What is allowed by Jupyter?</h1><div class="post__text post__text-html js-mediator-article">  Our story began with a seemingly simple task.  It was necessary to configure analytical tools for data science specialists and just data analysts.  With such a task, we were approached by colleagues from the retail risk and CRM divisions, where the concentration of data science specialists is historically high.  Customers had a simple desire - to write code in Python, import advanced libraries (xgboost, pytorch, tensorflow, etc.) and run algorithms on data picked up from a hdfs cluster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/793/c16/22e/793c1622e8423e8cae171263790ab234.png"><br><br>  It seems that everything is simple and clear.  But there were so many pitfalls that we decided to write a post about it and post a ready-made solution on GitHub. <br><a name="habracut"></a><br>  First, some details about the source infrastructure: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  HDFS data warehouse (12 Oracle Big Data Appliance nodes, Cloudera distribution).  There are a total of 130 TB of data from various internal systems of the bank, there is also heterogeneous information from external sources. <br></li><li>  Two application servers that were supposed to deploy analytical tools.  It is worth mentioning that not only advanced analytics tasks are ‚Äúspinning‚Äù on these servers, so one of the requirements was the use of containerization tools (Docker) to manage server resources, use various environments and set them up. <br></li></ul><br>  As the main environment for the work of analysts decided to choose JupyterHub, which de facto has already become one of the standards for working with data and developing machine learning models.  Read more about it <a href="https://habr.com/ru/company/yandex/blog/353546/">here</a> .  In the future, we have already imagined JupyterLab. <br><br>  It would seem that everything is simple: you need to take and configure a bunch of Python + Anaconda + Spark.  Install Jupyter Hub on the application server, integrate with LDAP, connect Spark or connect to the data in hdfs in some other way and go ahead - build models! <br>  If you delve into all the source data and requirements, here is a more detailed list: <br><br><ul><li>  Run JupyterHub in Docker (base OS - Oracle Linux 7) <br></li><li>  Cloudera CDH 5.15.1 + Spark 2.3.0 cluster with Kerberos authentication in Active Directory configuration + dedicated MIT Kerberos in a cluster (see <a href="https://www.cloudera.com/documentation/enterprise/5-15-x/topics/cm_sg_kdc_def_domain_s2.html">Cluster-Dedicated MIT KDC with Active Directory</a> ), Oracle Linux OS 6 <br></li><li>  Active Directory integration <br></li><li>  Transparent authentication in Hadoop and Spark <br></li><li>  Python 2 and 3 support <br></li><li>  Spark 1 and 2 (with the possibility of using cluster resources to train models and parallelize data processing using pyspark) <br></li><li>  Ability to limit host resources <br></li><li>  Library set <br></li></ul><br>  This post is designed for IT professionals who are faced with the need to solve such problems in their work. <br><br><h2>  Solution Description </h2><br><h3>  Run in Docker + Cloudera Cluster Integration </h3><br>  There is nothing unusual here.  JupyterHub and Cloudera product clients are installed in the container (as ‚Äî see below), and the configuration files are mounted from the host machine: <br><br>  <b>start-hub.sh</b> <br><br><pre><code class="plaintext hljs">VOLUMES="-v/var/run/docker.sock:/var/run/docker.sock:Z -v/var/lib/pbis/.lsassd:/var/lib/pbis/.lsassd:Z -v/var/lib/pbis/.netlogond:/var/lib/pbis/.netlogond:Z -v/var/jupyterhub/home:/home/BANK/:Z -v/u00/:/u00/:Z -v/tmp:/host/tmp:Z -v${CONFIG_DIR}/krb5.conf:/etc/krb5.conf:ro -v${CONFIG_DIR}/hadoop/:/etc/hadoop/conf.cloudera.yarn/:ro -v${CONFIG_DIR}/spark/:/etc/spark/conf.cloudera.spark_on_yarn/:ro -v${CONFIG_DIR}/spark2/:/etc/spark2/conf.cloudera.spark2_on_yarn/:ro -v${CONFIG_DIR}/jupyterhub/:/etc/jupyterhub/:ro" docker run -p0.0.0.0:8000:8000/tcp ${VOLUMES} -e VOLUMES="${VOLUMES}" -e HOST_HOSTNAME=`hostname -f` dsai1.2</code> </pre> <br><br><h3>  Active Directory integration </h3><br>  For integration with Active Directory / Kerberos of iron and not so hosts, the standard in our company is the product <a href="https://github.com/BeyondTrust/pbis-open/wiki">PBIS Open</a> .  Technically, this product is a set of services that communicate with Active Directory, with which, in turn, clients work through unix domain sockets.  This product integrates with Linux PAM and NSS. <br><br>  We used the standard Docker method - the unix domain sockets of the host services were mounted in a container (the sockets were found empirically by simple manipulations with the lsof command): <br><br>  <b>start-hub.sh</b> <br><br><pre> <code class="plaintext hljs">VOLUMES="-v/var/run/docker.sock:/var/run/docker.sock:Z -v/var/lib/pbis/.lsassd:/var/lib/pbis/.lsassd:Z &lt;b&gt;-v/var/lib/pbis/.netlogond:/var/lib/pbis/.netlogond:Z -v/var/jupyterhub/home:/home/BANK/:Z -v/u00/:/u00/:Z -v/tmp:/host/tmp:Z -v${CONFIG_DIR}/krb5.conf:/etc/krb5.conf:ro &lt;/b&gt; -v${CONFIG_DIR}/hadoop/:/etc/hadoop/conf.cloudera.yarn/:ro -v${CONFIG_DIR}/spark/:/etc/spark/conf.cloudera.spark_on_yarn/:ro -v${CONFIG_DIR}/spark2/:/etc/spark2/conf.cloudera.spark2_on_yarn/:ro -v${CONFIG_DIR}/jupyterhub/:/etc/jupyterhub/:ro" docker run -p0.0.0.0:8000:8000/tcp ${VOLUMES} -e VOLUMES="${VOLUMES}" -e HOST_HOSTNAME=`hostname -f` dsai1.2</code> </pre><br>  In turn, PBIS packages are installed inside the container, but without executing the postinstall section.  So we put only executable files and libraries, but do not run services inside the container - for us this is unnecessary.  PAM and NSS Linux integration commands are run manually. <br><br>  <b>Dockerfile:</b> <br><br><pre> <code class="plaintext hljs"># Install PAM itself and standard PAM configuration packages. RUN yum install -y pam util-linux \ # Here we just download PBIS RPM packages then install them omitting scripts. # We don't need scripts since they start PBIS services, which are not used - we connect to the host services instead. &amp;&amp; find /var/yum/localrepo/ -type f -name 'pbis-open*.rpm' | xargs rpm -ivh --noscripts \ # Enable PBIS PAM integration. &amp;&amp; domainjoin-cli configure --enable pam \ # Make pam_loginuid.so module optional (Docker requirement) and add pam_mkhomedir.so to have home directories created automatically. &amp;&amp; mv /etc/pam.d/login /tmp \ &amp;&amp; awk '{ if ($1 == "session" &amp;&amp; $2 == "required" &amp;&amp; $3 == "pam_loginuid.so") { print "session optional pam_loginuid.so"; print "session required pam_mkhomedir.so skel=/etc/skel/ umask=0022";} else { print $0; } }' /tmp/login &gt; /etc/pam.d/login \ &amp;&amp; rm /tmp/login \ # Enable PBIS nss integration. &amp;&amp; domainjoin-cli configure --enable nsswitch</code> </pre><br>  It turns out that the PBIS container clients communicate with the PBIS host services.  JupyterHub uses a PAM authenticator, and with a properly configured PBIS on the host, everything works out of the box. <br><br>  In order not to let all users from AD in JupyterHub, you can use the setting that restricts users to specific AD groups. <br><br>  <b>config-example / jupyterhub / jupyterhub_config.py</b> <br><br><pre> <code class="plaintext hljs">c.DSAIAuthenticator.group_whitelist = ['COMPANY\\domain^users']</code> </pre><br><h3>  Transparent authentication in Hadoop and Spark </h3><br>  When logged in to JupyterHub, PBIS caches the user's Kerberos ticket in a specific file in the / tmp directory.  For transparent authentication in this way, it is sufficient to mount the host's / tmp directory into the container and set the KRB5CCNAME variable to the desired value (this is done in our authenticator class). <br><br>  <b>start-hub.sh</b> <br><br><pre> <code class="plaintext hljs">VOLUMES="-v/var/run/docker.sock:/var/run/docker.sock:Z -v/var/lib/pbis/.lsassd:/var/lib/pbis/.lsassd:Z -v/var/lib/pbis/.netlogond:/var/lib/pbis/.netlogond:Z -v/var/jupyterhub/home:/home/BANK/:Z -v/u00/:/u00/:Z -v/tmp:/host/tmp:Z -v${CONFIG_DIR}/krb5.conf:/etc/krb5.conf:ro -v${CONFIG_DIR}/hadoop/:/etc/hadoop/conf.cloudera.yarn/:ro -v${CONFIG_DIR}/spark/:/etc/spark/conf.cloudera.spark_on_yarn/:ro -v${CONFIG_DIR}/spark2/:/etc/spark2/conf.cloudera.spark2_on_yarn/:ro -v${CONFIG_DIR}/jupyterhub/:/etc/jupyterhub/:ro" docker run -p0.0.0.0:8000:8000/tcp ${VOLUMES} -e VOLUMES="${VOLUMES}" -e HOST_HOSTNAME=`hostname -f` dsai1.2</code> </pre> <br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">env['KRB5CCNAME'] = '/host/tmp/krb5cc_%d' % pwd.getpwnam(self.user.name).pw_uid</code> </pre> <br>  Thanks to the code above, a JupyterHub user can execute hdfs commands from a Jupyter terminal and run Spark jobs without additional actions for authentication.  Mounting the entire directory / tmp host in the container is not safe - we are aware of this problem, but its solution is still in development. <br><br><h3>  Python versions 2 and 3 </h3><br>  Here, it would seem, everything is simple: you need to install the necessary versions of Python and synonym them with Jupyter, creating the necessary Kernel.  This question is already covered in many places.  Conda is used to manage Python environments.  Why all the simplicity is only apparent, it will be clear from the next section.  Kernel example for Python 3.6 (this file is not in git - all kernel files are generated by code): <br><br>  <b>/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/share/jupyter/kernels/python3.6.6/kernel.json</b> <br><br><pre> <code class="plaintext hljs">{   "argv": [      "/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/bin/python",       "-m",       "ipykernel_launcher",       "-f",      "{connection_file}"   ],   "display_name": "Python 3",   "language": "python" }</code> </pre><br><h3>  Spark 1 and 2 </h3><br>  To integrate with SPARK clients, you also need to create Kernels.  Kernel example for Python 3.6 and SPARK 2. <br><br>  <b>/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/share/jupyter/kernels/python3.6.6-pyspark2/kernel.json</b> <br><br><pre> <code class="plaintext hljs">{   "argv": [       "/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/bin/python",       "-m",       "ipykernel_launcher",       "-f",      "{connection_file}"   ],   "display_name": "Python 3 + PySpark 2",   "language": "python",   "env": {       "JAVA_HOME": "/usr/java/default/",       "SPARK_HOME": "/opt/cloudera/parcels/SPARK2/lib/spark2/",       "PYTHONSTARTUP": "/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/shell.py",       "PYTHONPATH": "/opt/cloudera/parcels/SPARK2/lib/spark2/python/:/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.7-src.zip",       "PYSPARK_PYTHON": "/opt/cloudera/parcels/Anaconda-5.3.1-dsai1.0/envs/python3.6.6/bin/python"   } }</code> </pre><br>  Immediately, we note that the requirement to have the support of Spark 1 has developed historically.  However, it is possible that someone will face similar restrictions ‚Äî for example, you cannot install Spark 2 in a cluster.  Therefore, we describe here the pitfalls that we encountered on the path of implementation. <br>  First, Spark 1.6.1 <a href="https://issues.apache.org/jira/browse/SPARK-19019">does not work</a> with Python 3.6.  Interestingly, in CDH 5.12.1 this was fixed, but in 5.15.1 - for some reason not).  First, we wanted to solve this problem by simply applying the appropriate patch.  However, later this idea had to be abandoned, since this approach requires the installation of a modified Spark in a cluster, which turned out to be unacceptable for us.  The solution was found in creating a separate Conda environment with Python 3.5. <br><br>  The second problem does not allow Spark 1 to work inside the Docker.  The Spark driver opens a specific port over which the Worker establishes a connection with the driver ‚Äî for this, the driver sends it its IP address.  In the case of the Docker Worker, it tries to connect with the driver by the IP of the container and when using the network = bridge it doesn‚Äôt work out quite naturally. <br><br>  The obvious solution is to send not the IP of the container, but the IP of the host, which was <a href="https://issues.apache.org/jira/browse/SPARK-4563">implemented</a> in Spark 2 by adding the appropriate configuration settings.  This patch was creatively reworked and applied to Spark 1. Spark modified in this way does not need to be installed on the cluster hosts, so problems like incompatibility with Python 3.6 do not arise. <br><br>  Regardless of the version of Spark, for its performance it is necessary to have in the cluster the same versions of Python as in the container.  To install Anaconda directly to bypass the Cloudera Manager, we had to learn how to do two things: <br><br><ul><li>  collect your parcel with Anaconda and all the necessary environments <br></li><li>  install it in the docker (for consistency) <br></li></ul><br><h3>  Build parcel Anaconda </h3><br>  This turned out to be quite a simple task.  All you need is: <br><br><ol><li>  Prepare the contents of parcel by installing the appropriate versions of Anaconda and environment Python <br></li><li>  Create a metadata file (s) and put it in the meta directory <br></li><li>  Create parcel with a simple tar <br></li><li>  Validate parcel utility from Cloudera <br></li></ol><br>  The process is described in more detail on <a href="https://github.com/cloudera/cm_ext/wiki/Building-a-parcel">GitHub</a> , and the validator code is also there.  We borrowed the metadata in the official <a href="https://www.cloudera.com/downloads/partner/anaconda.html">parcel of</a> Anaconda for Cloudera, creatively reworking them. <br><br><h3>  Install parcel in Docker </h3><br>  This practice has been useful for two reasons: <br><br><ul><li>  Spark operability - it is impossible to put Anaconda in a cluster without parcel <br></li><li>  Spark 2 is distributed only in the form of parcel - one could, of course, install it in a container just as jar files, but this approach was rejected <br></li></ul><br>  As a bonus as a result of solving the problems above, we received: <br><br><ul><li>  ease of setting up Hadoop and Spark clients - when installing the same parcel in Docker and in a cluster, the paths on the cluster and in the container are the same <br></li><li>  ease of maintaining a uniform environment in the container and in the cluster ‚Äî when you update the cluster, the Docker image is simply rebuilt with the same parcels that were installed in the cluster. <br></li></ul><br>  To install parcel to Docker, first install the Cloudera Manager from RPM packages.  For the actual installation of parcel, Java code is used.  The Java client knows what the Python client doesn‚Äôt, so I had to use Java and lose some uniformity), which calls the API. <br><br>  <b>assets / install-parcels / src / InstallParcels.java</b> <br><br><pre> <code class="plaintext hljs">ParcelsResourceV5 parcels = clusters.getParcelsResource(clusterName); for (int i = 1; i &lt; args.length; i += 2) {   result = installParcel(api, parcels, args[i], args[i + 1], pause);   if (!result) {       System.exit(1);   } }</code> </pre><br><h3>  Host resource limits </h3><br>  <a href="https://github.com/jupyterhub/dockerspawner">DockerSpawner</a> , a component that runs end-user Jupyter in a separate Docker container ‚Äî and <a href="https://en.wikipedia.org/wiki/Cgroups">cgroups</a> ‚Äî is a resource management mechanism in Linux, used to manage host resources.  DockerSpawner uses the Docker API, which allows you to set the parent cgroup for the container.  In a regular DockerSpawner there is no such possibility, so we wrote simple code that allows you to set the correspondence between the AD entities and the parent cgroup in the configuration. <br><br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">def set_extra_host_config(self):       extra_host_config = {}       if self.user.name in self.user_cgroup_parent:           cgroup_parent = self.user_cgroup_parent[self.user.name]       else:           pw_name = pwd.getpwnam(self.user.name).pw_name           group_found = False           for g in grp.getgrall():               if pw_name in g.gr_mem and g.gr_name in self.group_cgroup_parent:                   cgroup_parent = self.group_cgroup_parent[g.gr_name]                   group_found = True                   break           if not group_found:               cgroup_parent = self.cgroup_parent extra_host_config['cgroup_parent'] = cgroup_parent</code> </pre><br>  A small modification was also made that launches Jupyter from the same image from which JupyterHub was launched.  Thus, there is no need to use more than one image. <br><br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">current_container = None host_name = socket.gethostname() for container in self.client.containers():   if container['Id'][0:12] == host_name:       current_container = container       break self.image = current_container['Image']</code> </pre><br>  What exactly to run in a container, Jupyter or JupyterHub, is determined in the startup script for environment variables: <br><br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">#!/bin/bash ANACONDA_PATH="/opt/cloudera/parcels/Anaconda/" DEFAULT_ENV=`cat ${ANACONDA_PATH}/envs/default` source activate ${DEFAULT_ENV} if [ -z "${JUPYTERHUB_CLIENT_ID}" ]; then   while true; do       jupyterhub -f /etc/jupyterhub/jupyterhub_config.py   done else   HOME=`su ${JUPYTERHUB_USER} -c 'echo ~'`   cd ~   su ${JUPYTERHUB_USER} -p -c "jupyterhub-singleuser --KernelSpecManager.ensure_native_kernel=False --ip=0.0.0.0" fi</code> </pre><br>  The ability to start Docker containers Jupyter from the Docker container JupyterHub is achieved by mounting the Docker daemon's socket into the container JupyterHub. <br><br>  <b>start-hub.sh</b> <br><br><pre> <code class="plaintext hljs">VOLUMES="-&lt;b&gt;v/var/run/docker.sock:/var/run/docker.sock:Z -v/var/lib/pbis/.lsassd:/var/lib/pbis/.lsassd:Z&lt;/b&gt; -v/var/lib/pbis/.netlogond:/var/lib/pbis/.netlogond:Z -v/var/jupyterhub/home:/home/BANK/:Z -v/u00/:/u00/:Z -v/tmp:/host/tmp:Z -v${CONFIG_DIR}/krb5.conf:/etc/krb5.conf:ro -v${CONFIG_DIR}/hadoop/:/etc/hadoop/conf.cloudera.yarn/:ro -v${CONFIG_DIR}/spark/:/etc/spark/conf.cloudera.spark_on_yarn/:ro -v${CONFIG_DIR}/spark2/:/etc/spark2/conf.cloudera.spark2_on_yarn/:ro -v${CONFIG_DIR}/jupyterhub/:/etc/jupyterhub/:ro" docker run -p0.0.0.0:8000:8000/tcp ${VOLUMES} -e VOLUMES="${VOLUMES}" -e HOST_HOSTNAME=`hostname -f` dsai1.2</code> </pre><br>  In the future it is planned to abandon this decision in favor of, for example, ssh. <br><br>  When using DockerSpawner in conjunction with Spark, another problem arises: the Spark driver opens random ports on which the connection is then established from outside by Workers.  We can control the range of port numbers from which random ones are selected by setting these ranges in the Spark configuration.  However, these ranges must be different for different users, since we cannot run Jupyter containers with the same published ports.  To solve this problem, a code was written that simply generates port ranges by user id from the JupyterHub database and launches the Docker container and Spark with the appropriate configuration: <br><br>  <b>assets / jupyterhub / dsai.py</b> <br><br><pre> <code class="plaintext hljs">def set_extra_create_kwargs(self):       user_spark_driver_port, user_spark_blockmanager_port, user_spark_ui_port, user_spark_max_retries = self.get_spark_ports()       if user_spark_driver_port == 0 or user_spark_blockmanager_port == 0 or user_spark_ui_port == 0 or user_spark_max_retries == 0:           return       ports = {}       for p in range(user_spark_driver_port, user_spark_driver_port + user_spark_max_retries):           ports['%d/tcp' % p] = None       for p in range(user_spark_blockmanager_port, user_spark_blockmanager_port + user_spark_max_retries):           ports['%d/tcp' % p] = None       for p in range(user_spark_ui_port, user_spark_ui_port + user_spark_max_retries):           ports['%d/tcp' % p] = None self.extra_create_kwargs = { 'ports' : ports }</code> </pre><br>  The disadvantage of this solution is that when the container with JupyterHub is restarted, everything stops working due to the loss of the database.  Therefore, when JupyterHub is restarted for, for example, a configuration change, we do not touch the container itself, but restart only the JupyterHub process inside it. <br><br>  <b>restart-hub.sh</b> <br><br><pre> <code class="plaintext hljs">#!/bin/bash docker ps | fgrep 'dsai1.2' | fgrep -v 'jupyter-' | awk '{ print $1; }' | while read ID; do docker exec $ID /bin/bash -c "kill \$( cat /root/jupyterhub.pid )"; done</code> </pre><br>  The cgroups themselves are created by standard Linux tools, the correspondence between the AD entities and cgroups in the configuration looks like. <br><br><pre> <code class="plaintext hljs">&lt;b&gt;config-example/jupyterhub/jupyterhub_config.py&lt;/b&gt; c.DSAISpawner.user_cgroup_parent = {   'bank\\user1'    : '/jupyter-cgroup-1', # user 1   'bank\\user2'    : '/jupyter-cgroup-1', # user 2   'bank\\user3'    : '/jupyter-cgroup-2', # user 3 } c.DSAISpawner.cgroup_parent = '/jupyter-cgroup-3'</code> </pre><br><h3>  Git code </h3><br>  Our solution is publicly available on GitHub: <a href="https://github.com/DS-AI/dsai/">https://github.com/DS-AI/dsai/</a> (DSAI - Data Science and Artificial Intelligence).  All code is decomposed into directories with sequence numbers - the code from each following directory can use artifacts from the previous one.  The result of the code from the last directory will be a Docker image. <br><br>  Each directory contains files: <br><br><ul><li>  assets.sh - creating the necessary artifacts for the assembly (download from the Internet or copy from the directories of the previous steps) <br></li><li>  build.sh - build <br></li><li>  clean.sh - cleaning up the artifacts you need to build <br></li></ul><br>  To completely rebuild the Docker image, you need to consistently run clean.sh, assets.sh, build.sh from directories by their sequence number. <br><br>  To build, we use a Linux machine with RedHat 7.4, Docker 17.05.0-ce.  By car, there are 8 cores, 32GB of RAM and 250GB of disk space.  It is strongly not recommended to use for building a host with the worst parameters for RAM and HDD. <br><br>  Here is a reference to the names used: <br><br><ul><li>  01-spark-patched - RPM Spark 1.6.1 with two SPARK-4563 and SPARK-19019 patches applied. <br></li><li>  02-validator - parcel validator <br></li><li>  03-anaconda-dsai-parcel-1.0 - parcel Anaconda with the necessary Python (2, 3.5 and 3.6) <br></li><li>  04-cloudera-manager-api - libraries Cloudera Manager API <br></li><li>  05-dsai1.2-offline - final image <br></li></ul><br>  Alas, the assembly may fall for reasons that we did not fix (for example, parcel <a href="https://github.com/docker/hub-feedback/issues/727">crashes tar</a> . In this case, as a rule, you just need to restart the assembly, but this does not always help (for example, the Spark assembly depends on external resources Cloudera, which may cease to be available, etc.). <br><br>  Another drawback - the assembly of parcel is not reproducible.  Since the libraries are constantly updated, the repetition of the assembly can give a result different from the previous one. <br><br><h2>  Solemn finale </h2><br>  Now users are successfully using tools, their number has exceeded several dozen and continues to grow.  In the future, we plan to try JupyterLab and think of connecting the GPU to the cluster, as now the computing resources of two fairly powerful application servers are no longer enough. </div><p>Source: <a href="https://habr.com/ru/post/443294/">https://habr.com/ru/post/443294/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../443278/index.html">Cryptobirge Coinbase loses users due to the purchase of a startup by the creators of spyware software from Hacking Team</a></li>
<li><a href="../443280/index.html">A tale about how I assembled a 120-inch home theater from pipes, ropes, a folding screen and black velvet</a></li>
<li><a href="../443286/index.html">TDMS Fairway. Autocompletion mechanism of basic inscriptions on drawings and details of documents</a></li>
<li><a href="../443290/index.html">Zen Erlanga [and Elixir - approx. translator]</a></li>
<li><a href="../443292/index.html">Flexible preloader using em units</a></li>
<li><a href="../443298/index.html">Wireless charger. How does it work in practice</a></li>
<li><a href="../443300/index.html">How is the development of United Traders</a></li>
<li><a href="../443302/index.html">How Apple is preparing for the era after the iPhone</a></li>
<li><a href="../443304/index.html">Being a technophobe is pointless, even if technophobia is justified.</a></li>
<li><a href="../443306/index.html">Eight named laws in UX design (part 1)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>