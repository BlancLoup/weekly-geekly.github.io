<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Pipeline with spark.ml</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Today I would like to tell you about a new package that has appeared in version 1.2, called spark.ml. It is designed to provide a single high-level AP...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Pipeline with spark.ml</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/bfe/8ff/e85/bfe8ffe85616429fa7d23ed976c761e4.png" align="left">  Today I would like to tell you about a new package that has appeared in version 1.2, called spark.ml.  It is designed to provide a single high-level API for machine learning algorithms that will help simplify the creation and customization, as well as the integration of several algorithms in a single pipeline or workflow.  Now we have version 1.4.1, and the developers claim that the package came out of alpha, although many components are still labeled as Experimental or DeveloperApi. <br><br>  Well, let's check that the new package can and how good it is. <a name="habracut"></a>  First we need to get acquainted with the basic concepts introduced in spark.ml. <br><br>  1. <a href="http://spark.apache.org/docs/latest/ml-guide.html">ML Dataset</a> - spark.ml uses for working with data <a href="http://spark.apache.org/docs/latest/api/scala/index.html">DataFrame</a> from the spark.sql package.  A DataFrame is a distributed collection in which data is stored as named columns.  Conceptually, a DataFrame is equivalent to a table in a relational database or a type of data like frame in R or Python, but with richer optimization under the hood.  (Examples and ways of working will be given below in the article). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      2. <a href="http://spark.apache.org/docs/latest/ml-guide.html">Transformer</a> (modifier) ‚Äã‚Äãis just any algorithm that can convert one DataFrame into another.  For example: any trained model is a modifier, because it converts a set of characteristics (features) into a prediction (prediction) <br><br>  3. <a href="http://spark.apache.org/docs/latest/ml-guide.html">Estimator</a> (estimation algorithm) is an algorithm that can perform the conversion from a DataFrame to a Transformer.  For example, any learning algorithm is also an evaluation algorithm, since it accepts a set of data for training and creates a trained model as its output. <br><br>  4. <a href="http://spark.apache.org/docs/latest/ml-guide.html">Pipeline</a> - a pipeline that combines any number of modifiers and evaluation algorithms to create machine learning workflow. <br><br>  5. <a href="http://spark.apache.org/docs/latest/ml-guide.html">Param</a> is a general type used by modifiers and estimation algorithms to set parameters. <br><br>  According to the interface described, each Estimator must have a fit method that accepts a DataFrame and returns a Transformer.  In turn, the Transformer must have a transform method that transforms one DataFrame to another. <br><br>  In the course of <a href="https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x">Scalable Machine Learning,</a> in one of the laboratory works, teachers, talking about linear regression, solved the problem ‚Äúabout determining the year of creation of a song based on the audio characteristics‚Äù.  It was implemented quite a lot of methods for data processing, and for assessing and finding the best model.  This was done to acquaint students in more detail with the main processes in machine learning, but let's check how much spark.ml will make life easier for us. <br><br>  In the laboratory, we were provided with already prepared and slightly cropped data.  But since we are interested in going all the way, I propose to take a <a href="">raw data set</a> .  Each line of the form: <br><pre><code class="scala hljs"><span class="hljs-number"><span class="hljs-number">2007</span></span>, <span class="hljs-number"><span class="hljs-number">45.17809</span></span> <span class="hljs-number"><span class="hljs-number">46.34234</span></span> <span class="hljs-number"><span class="hljs-number">-40.65357</span></span> <span class="hljs-number"><span class="hljs-number">-2.47909</span></span> <span class="hljs-number"><span class="hljs-number">1.21253</span></span> <span class="hljs-number"><span class="hljs-number">-0.65302</span></span> <span class="hljs-number"><span class="hljs-number">-6.95536</span></span> <span class="hljs-number"><span class="hljs-number">-12.20040</span></span> <span class="hljs-number"><span class="hljs-number">17.02512</span></span> <span class="hljs-number"><span class="hljs-number">2.00002</span></span> <span class="hljs-number"><span class="hljs-number">-1.87785</span></span> <span class="hljs-number"><span class="hljs-number">9.85499</span></span> <span class="hljs-number"><span class="hljs-number">25.59837</span></span> <span class="hljs-number"><span class="hljs-number">1905.18577</span></span> <span class="hljs-number"><span class="hljs-number">3676.09074</span></span> <span class="hljs-number"><span class="hljs-number">1976.85531</span></span> <span class="hljs-number"><span class="hljs-number">913.11216</span></span> <span class="hljs-number"><span class="hljs-number">1957.52415</span></span> <span class="hljs-number"><span class="hljs-number">955.98525</span></span> <span class="hljs-number"><span class="hljs-number">942.72667</span></span> <span class="hljs-number"><span class="hljs-number">439.85991</span></span> <span class="hljs-number"><span class="hljs-number">591.66138</span></span> <span class="hljs-number"><span class="hljs-number">493.40770</span></span> <span class="hljs-number"><span class="hljs-number">496.38516</span></span> <span class="hljs-number"><span class="hljs-number">33.94285</span></span> <span class="hljs-number"><span class="hljs-number">-255.90134</span></span> <span class="hljs-number"><span class="hljs-number">-762.28079</span></span> <span class="hljs-number"><span class="hljs-number">-66.10935</span></span> <span class="hljs-number"><span class="hljs-number">-128.02217</span></span> <span class="hljs-number"><span class="hljs-number">198.12908</span></span> <span class="hljs-number"><span class="hljs-number">-34.44957</span></span> <span class="hljs-number"><span class="hljs-number">176.00397</span></span> <span class="hljs-number"><span class="hljs-number">-140.80069</span></span> <span class="hljs-number"><span class="hljs-number">-22.56380</span></span> <span class="hljs-number"><span class="hljs-number">12.77945</span></span> <span class="hljs-number"><span class="hljs-number">193.30164</span></span> <span class="hljs-number"><span class="hljs-number">314.20949</span></span> <span class="hljs-number"><span class="hljs-number">576.29519</span></span> <span class="hljs-number"><span class="hljs-number">-429.58643</span></span> <span class="hljs-number"><span class="hljs-number">-72.20157</span></span> <span class="hljs-number"><span class="hljs-number">59.59139</span></span> <span class="hljs-number"><span class="hljs-number">-5.12110</span></span> <span class="hljs-number"><span class="hljs-number">-182.15958</span></span> <span class="hljs-number"><span class="hljs-number">31.80120</span></span> <span class="hljs-number"><span class="hljs-number">-10.67380</span></span> <span class="hljs-number"><span class="hljs-number">-8.13459</span></span> <span class="hljs-number"><span class="hljs-number">-122.96813</span></span> <span class="hljs-number"><span class="hljs-number">208.69408</span></span> <span class="hljs-number"><span class="hljs-number">-138.66307</span></span> <span class="hljs-number"><span class="hljs-number">119.52244</span></span> <span class="hljs-number"><span class="hljs-number">-17.48938</span></span> <span class="hljs-number"><span class="hljs-number">75.58779</span></span> <span class="hljs-number"><span class="hljs-number">93.29243</span></span> <span class="hljs-number"><span class="hljs-number">85.83507</span></span> <span class="hljs-number"><span class="hljs-number">47.13972</span></span> <span class="hljs-number"><span class="hljs-number">312.85482</span></span> <span class="hljs-number"><span class="hljs-number">135.50478</span></span> <span class="hljs-number"><span class="hljs-number">-32.47886</span></span> <span class="hljs-number"><span class="hljs-number">49.67063</span></span> <span class="hljs-number"><span class="hljs-number">-214.73180</span></span> <span class="hljs-number"><span class="hljs-number">-77.83503</span></span> <span class="hljs-number"><span class="hljs-number">-47.26902</span></span> <span class="hljs-number"><span class="hljs-number">7.58366</span></span> <span class="hljs-number"><span class="hljs-number">-352.56581</span></span> <span class="hljs-number"><span class="hljs-number">-36.15655</span></span> <span class="hljs-number"><span class="hljs-number">-53.39933</span></span> <span class="hljs-number"><span class="hljs-number">-98.60417</span></span> <span class="hljs-number"><span class="hljs-number">-82.37799</span></span> <span class="hljs-number"><span class="hljs-number">45.81588</span></span> <span class="hljs-number"><span class="hljs-number">-16.91676</span></span> <span class="hljs-number"><span class="hljs-number">18.35888</span></span> <span class="hljs-number"><span class="hljs-number">-315.68965</span></span> <span class="hljs-number"><span class="hljs-number">-3.14554</span></span> <span class="hljs-number"><span class="hljs-number">125.45269</span></span> <span class="hljs-number"><span class="hljs-number">-130.18808</span></span> <span class="hljs-number"><span class="hljs-number">-3.06337</span></span> <span class="hljs-number"><span class="hljs-number">42.26602</span></span> <span class="hljs-number"><span class="hljs-number">-9.04929</span></span> <span class="hljs-number"><span class="hljs-number">26.41570</span></span> <span class="hljs-number"><span class="hljs-number">23.36165</span></span> <span class="hljs-number"><span class="hljs-number">-4.36742</span></span> <span class="hljs-number"><span class="hljs-number">-87.55285</span></span> <span class="hljs-number"><span class="hljs-number">-70.79677</span></span> <span class="hljs-number"><span class="hljs-number">76.57355</span></span> <span class="hljs-number"><span class="hljs-number">-7.71727</span></span> <span class="hljs-number"><span class="hljs-number">3.26926</span></span> <span class="hljs-number"><span class="hljs-number">-298.49845</span></span> <span class="hljs-number"><span class="hljs-number">11.49326</span></span> <span class="hljs-number"><span class="hljs-number">-89.21804</span></span> <span class="hljs-number"><span class="hljs-number">-15.09719</span></span></code> </pre>  where the year goes first, then the 12 numbers are the average timbres, and the last 78 are the covariations of timbres. <br><br>  First of all, we need to tighten this data into a DataFrame, but first we will slightly convert the data format: <br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> sc = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SparkContext</span></span>(<span class="hljs-string"><span class="hljs-string">"local[*]"</span></span>, <span class="hljs-string"><span class="hljs-string">"YearPrediction"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> rawData: <span class="hljs-type"><span class="hljs-type">RDD</span></span>[(<span class="hljs-type"><span class="hljs-type">Double</span></span>, linalg.<span class="hljs-type"><span class="hljs-type">Vector</span></span>, linalg.<span class="hljs-type"><span class="hljs-type">Vector</span></span>)] = sc.textFile(<span class="hljs-string"><span class="hljs-string">"data/YearPredictionMSD.txt"</span></span>) .map(_.split(',')) .map(x =&gt; ( x.head.toDouble, <span class="hljs-type"><span class="hljs-type">Vectors</span></span>.dense(x.tail.take(<span class="hljs-number"><span class="hljs-number">12</span></span>).map(_.toDouble)), <span class="hljs-type"><span class="hljs-type">Vectors</span></span>.dense(x.takeRight(<span class="hljs-number"><span class="hljs-number">78</span></span>).map(_.toDouble)) ))</code> </pre><br>  Now each RDD element is a tuple containing a year and two characteristics vectors, to get a DataFrame, you need to perform another transformation: <br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> sqlContext = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">SQLContext</span></span>(sc) <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sqlContext.implicits._ <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> rawDF: <span class="hljs-type"><span class="hljs-type">DataFrame</span></span> = labeledPointsRDD.toDF(<span class="hljs-string"><span class="hljs-string">"label"</span></span>, <span class="hljs-string"><span class="hljs-string">"avg"</span></span>, <span class="hljs-string"><span class="hljs-string">"cov"</span></span>)</code> </pre><br>  Notice that we created sqlContext and pulled up implicit conversion methods (in this case, we could write <code>import sqlContext.implicits.rddToDataFrameHolder <br></code> <code>import sqlContext.implicits.rddToDataFrameHolder <br></code>  ) to use the toDF method.  We also specified the column names, and now the data structure will look like this: <br><pre> <code class="scala hljs"> label | avg | cov -------|-----------------------------------------|--------------------------------------------- <span class="hljs-number"><span class="hljs-number">2001</span></span> | [<span class="hljs-number"><span class="hljs-number">49.94357</span></span> <span class="hljs-number"><span class="hljs-number">21.47114</span></span> <span class="hljs-number"><span class="hljs-number">73.07750</span></span> <span class="hljs-number"><span class="hljs-number">8.74861</span></span>... | [<span class="hljs-number"><span class="hljs-number">10.20556</span></span> <span class="hljs-number"><span class="hljs-number">611.10913</span></span> <span class="hljs-number"><span class="hljs-number">951.08960</span></span> <span class="hljs-number"><span class="hljs-number">698.11428</span></span>... -------|-----------------------------------------|--------------------------------------------- <span class="hljs-number"><span class="hljs-number">2007</span></span> | [<span class="hljs-number"><span class="hljs-number">50.57546</span></span> <span class="hljs-number"><span class="hljs-number">33.17843</span></span> <span class="hljs-number"><span class="hljs-number">50.53517</span></span> <span class="hljs-number"><span class="hljs-number">11.5521</span></span>... | [<span class="hljs-number"><span class="hljs-number">44.38997</span></span> <span class="hljs-number"><span class="hljs-number">2056.93836</span></span> <span class="hljs-number"><span class="hljs-number">605.40696</span></span> <span class="hljs-number"><span class="hljs-number">457.4117</span></span>...</code> </pre><br>  The gradient method, which is used in linear regression, is sensitive to the variation of the characteristic values, so the data before training should be normalized or standardized.  For these purposes, the spark.ml.feature package has two classes: StandardScaler and Normalizer. <br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.feature.{<span class="hljs-type"><span class="hljs-type">Normalizer</span></span>, <span class="hljs-type"><span class="hljs-type">StandardScalerModel</span></span>, <span class="hljs-type"><span class="hljs-type">StandardScaler</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> scalerAvg: <span class="hljs-type"><span class="hljs-type">StandardScalerModel</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">StandardScaler</span></span>() .setWithMean(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setWithStd(<span class="hljs-literal"><span class="hljs-literal">true</span></span>) .setInputCol(<span class="hljs-string"><span class="hljs-string">"avg"</span></span>) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>) <span class="hljs-comment"><span class="hljs-comment">//    ,    //   (    ) .fit(rawDF) val normAvg: Normalizer = new Normalizer() .setP(2.0) .setInputCol("avg") .setOutputCol("features")</span></span></code> </pre><br>  Please note that StandardScaler is an Estimator, which means we need to call the fit method to get a Transformer, in this case StandardScalerModel.  All classes that work with DataFrame have two general methods: <br>  <b>setInputCol</b> - set the name of the column from which to read data <br>  <b>setOutputCol</b> - specify the name of the column into which you want to write the converted data. <br><br>  The differences in the result of the work of these classes in this case will be that the scaler will return data in the range from -1 to 1, and Normalizer in the range from 0 to 1. You can read more about the algorithms of work <a href="https://en.wikipedia.org/wiki/Feature_scaling">here</a> and <a href="https://ru.wikipedia.org/wiki/Lp_(%25D0%25BF%25D1%2580%25D0%25BE%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D1%2581%25D1%2582%25D0%25B2%25D0%25BE)">here</a> . <br><br>  We prepared the training sample (or rather, received modifiers, which we will use for data processing); now we need to create an estimation algorithm (Estimator), which will give us a trained model as a result.  We set almost standard settings, at this stage they are not particularly interesting. <br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.regression.<span class="hljs-type"><span class="hljs-type">LinearRegression</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> linReg = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">LinearRegression</span></span>() .setFeaturesCol(<span class="hljs-string"><span class="hljs-string">"features"</span></span>) .setLabelCol(<span class="hljs-string"><span class="hljs-string">"label"</span></span>) .setElasticNetParam(<span class="hljs-number"><span class="hljs-number">0.5</span></span>) .setMaxIter(<span class="hljs-number"><span class="hljs-number">500</span></span>) .setRegParam(<span class="hljs-number"><span class="hljs-number">1e-10</span></span>) .setTol(<span class="hljs-number"><span class="hljs-number">1e-6</span></span>)</code> </pre><br>  Now we have everything we need to build a simple conveyor: <br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.<span class="hljs-type"><span class="hljs-type">Pipeline</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> pipeline = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( normAvg, linReg ))</code> </pre><br>  Pipeline has a setStages method that accepts an array of steps that will be performed in the specified order when entering a training set.  Now, all we have to do is not to forget to divide the data into a training and test sample: <br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> splitedData = rawDF.randomSplit(<span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">0.8</span></span>, <span class="hljs-number"><span class="hljs-number">0.2</span></span>), <span class="hljs-number"><span class="hljs-number">42</span></span>).map(_.cache()) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> trainData = splitedData(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> testData = splitedData(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br>  Let's launch the pipeline we created and evaluate the result of its work: <br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> pipelineModel = pipeline.fit(trainData) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> fullPredictions = pipelineModel.transform(testData) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> predictions = fullPredictions.select(<span class="hljs-string"><span class="hljs-string">"prediction"</span></span>).map(_.getDouble(<span class="hljs-number"><span class="hljs-number">0</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> labels = fullPredictions.select(<span class="hljs-string"><span class="hljs-string">"label"</span></span>).map(_.getDouble(<span class="hljs-number"><span class="hljs-number">0</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> rmseTest = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">RegressionMetrics</span></span>(predictions.zip(labels)).rootMeanSquaredError &gt; (<span class="hljs-number"><span class="hljs-number">2003.0</span></span>,<span class="hljs-number"><span class="hljs-number">1999.6153819348176</span></span>) (<span class="hljs-number"><span class="hljs-number">1997.0</span></span>,<span class="hljs-number"><span class="hljs-number">2000.9207184703566</span></span>) (<span class="hljs-number"><span class="hljs-number">1996.0</span></span>,<span class="hljs-number"><span class="hljs-number">2000.4171327880172</span></span>) (<span class="hljs-number"><span class="hljs-number">1997.0</span></span>,<span class="hljs-number"><span class="hljs-number">2002.022142263423</span></span>) (<span class="hljs-number"><span class="hljs-number">2000.0</span></span>,<span class="hljs-number"><span class="hljs-number">1997.6327888556184</span></span>) <span class="hljs-type"><span class="hljs-type">RMSE</span></span>: <span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">552024</span></span></code> </pre><br>  At this stage, everything should be clear, note that we used the ready-made <a href="http://spark.apache.org/docs/latest/api/scala/index.html">RegressionMetrics</a> class in which to evaluate the model, along with the other RMSE estimate already familiar to us, other basic estimates were also implemented. <br><br>  Moving on: in the course of Scalable Machine Learning, we created new characteristics by converting the initial ones into a polynomial with degree 2. The developers of spark.ml took care of this: now it‚Äôs enough for us to create another modifier and add it to the pipeline;  most importantly, in this process, do not get confused and correctly specify the name of the columns. <br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.feature.<span class="hljs-type"><span class="hljs-type">PolynomialExpansion</span></span> <span class="hljs-comment"><span class="hljs-comment">//  ,      "features"       "polyFeatures" val polynomAvg = new PolynomialExpansion() .setInputCol("features") .setOutputCol("polyFeatures") .setDegree(2) //         linReg.setFeaturesCol("polyFeatures") //       val pipeline = new Pipeline().setStages(Array( normAvg, polynomAvg, linReg ))</span></span></code> </pre><br><br>  So far, we have used only 12 characteristics for learning, but I remember that there were 78 more in the raw data, maybe let's try to combine them?  And in this case, spark.ml has a <a href="http://spark.apache.org/docs/latest/ml-features.html">VectorAssembler</a> solution.  Once decided, let's do: <br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.feature.<span class="hljs-type"><span class="hljs-type">VectorAssembler</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> assembler = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">VectorAssembler</span></span>() .setInputCols(<span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-string"><span class="hljs-string">"avg"</span></span>, <span class="hljs-string"><span class="hljs-string">"cov"</span></span>)) .setOutputCol(<span class="hljs-string"><span class="hljs-string">"united"</span></span>) normAvg.setInputCol(<span class="hljs-string"><span class="hljs-string">"united"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> pipeline = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">Pipeline</span></span>().setStages(<span class="hljs-type"><span class="hljs-type">Array</span></span>( assembler, normAvg, polynomAvg, linReg ))</code> </pre><br>  With the preparation of the data, we figured out a little, but the question remained of selecting the optimal parameters for the algorithm, I really do not want to do it manually, and do not!  For this purpose, the spark.ml class is implemented <a href="http://spark.apache.org/docs/latest/ml-guide.html">CrossValidator</a> .  CrossValidator accepts the estimation algorithm (in our case, it is linReg), the set of parameters that we would like to test and the evaluation tool (when we evaluated the model manually, we used RMSE).  CrossValidator starts its work by breaking the data set into several samples (k by default 3), randomly choosing a training and validation sample (the validation sample will be 1 / k in size from the original one).  Then, for each set of parameters on each of the samples, the model will be trained, its effectiveness will be evaluated and the best model will be selected.  It should be noted that the choice of a model through the CrossValidator is quite a time-consuming operation, but is statistically more reasonable than the heuristic manual selection. <br><br>  For the convenience of creating a set of parameters in spark.ml, there is a ParamGridBuilder utility class, which we use: <br><pre> <code class="scala hljs"> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.tuning.{<span class="hljs-type"><span class="hljs-type">CrossValidator</span></span>, <span class="hljs-type"><span class="hljs-type">ParamGridBuilder</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> paramGrid: <span class="hljs-type"><span class="hljs-type">Array</span></span>[<span class="hljs-type"><span class="hljs-type">ParamMap</span></span>] = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">ParamGridBuilder</span></span>() .addGrid(linReg.maxIter, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">500</span></span>)) .addGrid(linReg.regParam, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">1e-15</span></span>, <span class="hljs-number"><span class="hljs-number">1e-10</span></span>)) .addGrid(linReg.tol, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">1e-9</span></span>, <span class="hljs-number"><span class="hljs-number">1e-6</span></span>, <span class="hljs-number"><span class="hljs-number">1e-3</span></span>)) .addGrid(linReg.elasticNetParam, <span class="hljs-type"><span class="hljs-type">Array</span></span>(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) .build() <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> crossVal = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">CrossValidator</span></span>() .setEstimator(pipeline) .setEvaluator(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">RegressionEvaluator</span></span>) .setEstimatorParamMaps(paramGrid) .setNumFolds(<span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> bestModel = crossVal.fit(trainData) &gt; <span class="hljs-type"><span class="hljs-type">Best</span></span> set of parameters: { linReg_3a964d0300fd-elasticNetParam: <span class="hljs-number"><span class="hljs-number">0.5</span></span>, linReg_3a964d0300fd-maxIter: <span class="hljs-number"><span class="hljs-number">500</span></span>, linReg_3a964d0300fd-regParam: <span class="hljs-number"><span class="hljs-number">1.0E-15</span></span>, linReg_3a964d0300fd-tol: <span class="hljs-number"><span class="hljs-number">1.0E-9</span></span> } <span class="hljs-type"><span class="hljs-type">Best</span></span> cross-validation metric: <span class="hljs-number"><span class="hljs-number">-10.47433119891316</span></span></code> </pre><br>  Well, that's probably all that concerns linear regression, for the classification and clustering algorithms in spark.ml there is also a set of solutions that are ready to help you conveniently organize the workflow. <br><br>  Materials used: <br>  <a href="http://spark.apache.org/docs/latest/ml-guide.html">Official documentation</a> <br>  <a href="http://archive.ics.uci.edu/ml/datasets.html">UCI Machine Learning Repository</a> <br>  <a href="https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x">Scalable Machine Learning</a> </div><p>Source: <a href="https://habr.com/ru/post/264397/">https://habr.com/ru/post/264397/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../264385/index.html">The study "The global state of information security 2015" (GSISS 2015). Part 1</a></li>
<li><a href="../264387/index.html">Announcement Rust 1.2</a></li>
<li><a href="../264389/index.html">We measure power consumption for ASIC digital blocks (even before manufacturing)</a></li>
<li><a href="../264391/index.html">Build USB HID under BeagleBone</a></li>
<li><a href="../264395/index.html">Debugging the rules of RewriteRule, or a little about the intimate life of mod_rewrite</a></li>
<li><a href="../264399/index.html">The digest of interesting materials for the mobile # 115 developer (August 2-9)</a></li>
<li><a href="../264401/index.html">IBM Launches 50 Projects for the Open Source Developer Community</a></li>
<li><a href="../264405/index.html">Ministry of Defense accused of sending sensitive information through public postal services</a></li>
<li><a href="../264407/index.html">Buying the best apartment with R</a></li>
<li><a href="../264409/index.html">Min-plus polynomials, cyclic games, and Hilbert's theorem on zeros</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>