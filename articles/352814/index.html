<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Tutorial: toon-contours in Unreal Engine 4</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When talking about "toon-contours", they mean any technique that renders lines around objects. Like cel shading, the outlines help the game look more ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Tutorial: toon-contours in Unreal Engine 4</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/717/8e6/acc/7178e6acc79f0f0cd460d1ee91507ad0.jpg" alt="image"></div><br>  When talking about "toon-contours", they mean any technique that renders lines around objects.  Like cel shading, the outlines help the game look more stylized.  They can create the feeling that objects are painted or inked.  Examples of this style can be seen in games such as <i>Okami</i> , <i>Borderlands</i> and <i>Dragon Ball FighterZ</i> . <br><br>  In this tutorial you will learn the following: <br><br><ul><li>  Create outlines using an inverted mesh </li><li>  Create paths using post processing and convolutions </li><li>  Create and use material functions </li><li>  Sampling adjacent pixels </li></ul><br><blockquote>  <em>Note:</em> this tutorial assumes that you already know the basics of the Unreal Engine.  If you are new to the Unreal Engine, then I recommend to explore my series of tutorials from ten parts of the <a href="https://habrahabr.ru/post/344394/">Unreal Engine for beginners</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      If you are not familiar with the post-processing of materials, then you should first study my <a href="https://habrahabr.ru/post/350172/">cel shading</a> tutorial.  In this article we will use some of the concepts described in the cel shading tutorial. </blockquote><a name="habracut"></a><br><h2>  Getting Started </h2><br>  To get started, download <a href="">materials from</a> this tutorial.  Unzip them, go to <em>ToonOutlineStarter</em> and open <em>ToonOutline.uproject</em> .  You will see the following scene: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fec/e64/cb8/fece64cb80a5a34815f6c7a1a5663d08.jpg"></div><br>  First we create the outlines using an <em>inverted mesh</em> . <br><br><h2>  Inverted Mesh Contours </h2><br>  The principle of implementing this method is to duplicate the target mesh.  Then the duplicate is assigned a solid color (usually black) and its size is increased so that it is slightly larger than the original mesh.  So we will create a silhouette. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d4f/f50/31e/d4ff5031ee608e850e057fe614db4499.jpg"></div><br>  If you use just a duplicate, it will completely block the source mesh. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/774/ada/32b/774ada32b3481bcad74080c1bb945b86.gif"></div><br>  To fix this, we can invert the duplicate normals.  When the backface culling parameter is on, we will see not the outer, but the inner edges. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/aeb/8ac/10e/aeb8ac10e001cf92a9717f55094e6621.gif"></div><br>  This will allow the source mesh to shine through the duplicate.  And since the duplicate is larger than the original mesh, we get the outline. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d4a/d24/853/d4ad2485321c5e0d12022aa1d2e4e89c.gif"></div><br>  Benefits: <br><br><ul><li>  You will always have clear lines, since the contour consists of polygons. </li><li>  The appearance and thickness of the contour is easy to adjust by moving the vertices </li><li>  With distance, the contours become smaller (this may be a disadvantage). </li></ul><br>  Disadvantages: <br><br><ul><li>  Usually, you cannot create contours of parts inside the mesh in this way. </li><li>  Since the contour consists of polygons, they are prone to clipping.  This can be seen in the example above, where the duplicate intersects with the ground. </li><li>  With this method, speed reduction is possible.  It depends on how many polygons are in the mesh.  Since we use duplicates, we essentially double the number of polygons. </li><li>  Such contours better manifest themselves on smooth and convex meshes.  Sharp edges and concave areas will create holes in the contour.  This can be seen in the image below. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d57/d6b/9ad/d57d6b9ad96d56df6ede3d78bd4a3d04.jpg"></div><br>  Creating an inverted mesh is better in a 3D modeling program, this gives more control over the silhouette.  When working with skeletal meshes, this also allows skin duplication of the duplicate with the original skeleton.  Due to this, the duplicate will be able to move along with the original mesh. <br><br>  For this tutorial, we will create a mesh not in a 3D editor.  and in Unreal.  The method is slightly different, but the concept remains the same. <br><br>  First we need to create a duplicate material. <br><br><h3>  Creating Inverted Mesh Material </h3><br>  For this method, we will mask the polygons with the faces out, and as a result we will have polygons with the faces inwards. <br><br><blockquote>  <em>Note:</em> due to masking, this method is a bit more expensive than manually creating a mesh. </blockquote><br>  Navigate to the <em>Materials</em> folder and open <em>M_Inverted</em> .  Then go to the Details panel and change the following parameters: <br><br><ul><li>  <em>Blend Mode</em> : select <em>Masked</em> for it.  This will allow us to mark areas as visible or invisible.  The threshold value can be changed by editing the <em>Opacity Mask Clip Value</em> . </li><li>  <em>Shading Model:</em> select the <em>Unlit</em> value.  Due to this, the mesh will not be affected by the lighting. </li><li>  <em>Two Sided:</em> select the value <em>enabled</em> .  By default, Unreal cuts back edges.  Turning on this option <em>disables the</em> clipping of the rear edges.  If you leave clipping of the back edges turned on, then we will not be able to see the polygons with the faces inside. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e84/8e9/060/e848e9060f01eced95aa5d03a9b144e3.jpg"></div><br>  Next, create a <em>Vector Parameter</em> and name it <em>OutlineColor</em> .  It will control the color of the contour.  Connect it with <em>Emissive Color</em> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b72/d07/111/b72d07111a1769d7189392de75377061.jpg"></div><br>  To mask out polygons, create <em>TwoSidedSign</em> and multiply it by <em>-1</em> .  Attach the result with the <em>Opacity Mask</em> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f18/98e/111/f1898e111390b924cebd32f26c8ae19b.jpg"></div><br>  <em>TwoSidedSign</em> displays <em>1</em> for the front faces and <em>-1</em> for the back faces.  This means that the front faces will be visible, while the rear faces will be invisible.  However, we need the opposite effect.  To do this, we change the signs by multiplying by <em>-1</em> .  Now the front faces will give <em>-1</em> at the output, and the rear ones <em>1</em> . <br><br>  Finally, we need a way to control the thickness of the contour.  To do this, add the selected nodes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c9f/5e5/7e9/c9f5e57e9d4a98f79551bdc3e263df14.jpg"></div><br>  In the Unreal engine, we can change the position of each vertex using <em>World Position Offset</em> .  <em>By</em> multiplying the vertex normal by <em>OutlineThickness</em> , we make the mesh thicker.  Here is a demo using the original mesh: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5da/a0a/d67/5daa0ad671daf9a8f752d09973c24100.gif"></div><br>  At this point we finished preparing the material.  Click <em>Apply</em> and close <em>M_Inverted</em> . <br><br>  Now we need to duplicate the mesh and apply the newly created material. <br><br><h3>  Duplicate mesh </h3><br>  Go to the <em>Blueprints</em> folder and open <em>BP_Viking</em> .  Add the <em>Static Mesh</em> component as a <em>Mesh</em> child and name it <em>Outline</em> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/470/854/292/470854292cc2271dc61152e685b668d1.jpg"></div><br>  Select <em>Outline</em> and select <em>SM_Viking</em> for <em>Static Mesh</em> .  Then select <em>MI_Inverted</em> for its <em>material</em> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c5b/51c/955/c5b51c95576f1559505a32694a513f10.jpg"></div><br>  <em>MI_Inverted</em> is an instance of <em>M_Inverted</em> .  It will allow us to change the <em>OutlineColor</em> and <em>OutlineThickness parameters</em> without recompiling. <br><br>  Click on <em>Compile</em> and close <em>BP_Viking</em> .  Now the viking will have an outline.  We can change the color and thickness of the contour by opening <em>MI_Inverted</em> and adjusting its parameters. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8de/65d/afb/8de65dafb0ae958b5b24f2b3ad61b4b7.gif"></div><br>  And on this we are done with this way!  Try to create an inverted mesh in a 3D editor and then transfer it to Unreal. <br><br>  If you want to create contours differently, you can use <em>post-processing</em> for this. <br><br><h2>  Creating contours by post processing </h2><br>  You can create post-processing contours using <em>edge recognition</em> .  This is a technique that recognizes breaks in image areas.  Here are some types of breaks you can look for: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dd6/1bd/7b3/dd61bd7b3051c88abee70afe943180f6.jpg"></div><br>  Benefits: <br><br><ul><li>  The method is easily applicable to the whole scene. </li><li>  Constant computational cost, since the shader is always executed for each pixel. </li><li>  The thickness of the line always remains the same regardless of the distance (this may be a disadvantage). </li><li>  Lines are not clipped by geometry, since this is a post-processing effect. </li></ul><br>  Disadvantages: <br><br><ul><li>  Usually for recognition of all edges it is required several edge recognizers.  This reduces speed. </li><li>  The method is subject to noise.  This means that the edges will appear in areas where there is great variation. </li></ul><br>  Usually, edge recognition is performed by <em>convolving</em> each pixel. <br><br><h3>  What is a "convolution"? </h3><br>  In the image processing area, convolution is an operation on two groups of numbers to calculate a single number.  First we take a grid of numbers (known as the <em>core</em> ) and position the center above each pixel.  Below is an example of how the kernel moves over two lines of an image: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6c5/8f3/35d/6c58f335db201b8016c1307ac185a880.gif"></div><br>  For each pixel, each element of the core is multiplied by the corresponding pixel.  To demonstrate this, let's take a pixel from the upper left edge of the mouth.  Also, to simplify the calculations, we convert the image to grayscale. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/665/917/73e/66591773e9074da8493c37d6a9273673.jpg"></div><br>  First, we locate the core (we take the same one that was used above) so that the target pixel is in the center.  Then multiply each element of the core by the pixel on which it is superimposed. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6d1/141/650/6d1141650f6d249e4e4e164196c3504e.jpg"></div><br>  Finally, add the results together.  This will be the new value for the center pixel.  In our case, the new value is <em>0.5 + 0.5</em> or <em>1</em> .  Here is what the image looks like after performing a convolution for each pixel: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a1c/175/36c/a1c17536cc81daa61686b2e756ee5601.jpg"></div><br>  The resulting effect depends on the core used.  The kernel from the examples shown above is used to recognize edges.  Here are some examples of other edges: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1da/a87/dee/1daa87deedc39e52f98634159ab2363a.jpg"></div><br>  <em>Note:</em> You may notice that they are used as filters in image editors.  In fact, many operations with filters in image editors are performed using convolutions.  In Photoshop, you can even perform convolutions based on your own kernels! <br><br>  To recognize the edges of the image, you can use <em>Laplace</em> edge recognition. <br><br><h3>  Laplace edge recognition </h3><br>  First, what will be the core for Laplace edge recognition?  In fact, we have already seen this core in the examples of the previous section! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c48/b59/31f/c48b5931f78b04ba31db9f46f701dce2.jpg"></div><br>  This core performs edge recognition because the Laplacian measures changes in steepness.  Areas with large variations deviate from zero and report that this is an edge. <br><br>  To understand this, let's look at the Laplacian in one dimension.  The core for it will be as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/234/ee2/d6f/234ee2d6fe0b2da48960e80e4314bd3b.jpg"></div><br>  First position the core above the edge pixel, and then perform the convolution. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f38/710/c32/f38710c328a1ca0295405979d9a95566.jpg"></div><br>  This will give us a value of <em>1</em> , which indicates that there has been a big change.  That is, the target pixel is likely to be an edge. <br><br>  Next, let's perform a convolution of a region with less variability. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a63/9a3/031/a639a303170d1fb01bddcff754d23495.jpg"></div><br>  Even though the pixels have different values, the gradient is linear.  That is, there is no change in slope and the target pixel is not an edge. <br><br>  Below is the image after convolution and a graph of all values.  You can see that the pixels on the edge more strongly deviate from zero. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/79c/550/dc0/79c550dc078d4eb2362975bb41e106d5.jpg"></div><br>  Yes, we considered quite a lot of theory, but do not worry - now the interesting begins.  In the next section, we will create a post-processing material that will perform Laplace edge recognition in the depth buffer. <br><br><h2>  Build a Laplace Rib Detector </h2><br>  Go to the <em>Maps</em> folder and open <em>PostProcess</em> .  You will see a black screen.  This happened because the map contains Post Process Volume, which uses blank post-processing material. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ab0/d84/387/ab0d84387e6e8bcdd0001f7ccfaf41ab.jpg"></div><br>  It is this material that we will modify to build the edge recognizer.  The first step is that we need to figure out how to sample the adjacent pixels. <br><br>  To get the position of the current pixel, we can use <em>TextureCoordinate</em> .  For example, if the current pixel is in the middle, it will return <em>(0.5, 0.5)</em> .  This two component vector is called <em>UV</em> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a9f/447/fd9/a9f447fd9024c82a3df9c4a3611c6e0a.jpg"></div><br>  To sample another pixel, we just need to add an offset to TextureCoordinate.  In an image of 100 √ó 100, each pixel in the UV space has a size of <em>0.01</em> .  To sample the pixel on the right, add 0.01 along <em>the X axis</em> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/58b/644/d87/58b644d877489df3e98cbbd98c49f636.jpg"></div><br>  However, there is a problem here.  When the image resolution changes, the pixel size also changes.  If we use the same offset <em>(0.01, 0)</em> for the image 200 √ó 200, then <em>two</em> pixels will be sampled on the right. <br><br>  To fix this, we can use the <em>SceneTexelSize</em> node, which returns the pixel size.  To apply it, you need to do something like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/34a/34d/a76/34a34da76f4b40121240be3814dc8874.jpg"></div><br>  Since we are going to sample several pixels, we need to create it several times. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/afc/34c/cb9/afc34ccb93dfa25114e2af8bc169bc8a.jpg"></div><br>  Obviously, the graph will quickly become confused.  Fortunately, we can use the <em>functions of the materials</em> to keep the graph legible. <br><br><blockquote>  <em>Note:</em> The material function is similar to the functions used in Blueprints or in C ++. </blockquote><br>  In the next section, we will insert duplicate nodes into the function and create an input for the offset. <br><br><h3>  Create pixel sampling function </h3><br>  To get started, go to the <em>Materials \ PostProcess folder</em> .  To create a material function, click on <em>Add New</em> and select <em>Materials &amp; Textures \ Material Function</em> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c8d/f54/13f/c8df5413f2d7c818ffbdcd0266b39d64.jpg"></div><br>  Rename it to <em>MF_GetPixelDepth</em> and open it.  There will be one <em>FunctionOutput</em> node in the graph.  This is where we will append the value of the sample pixel. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/027/f25/eb2/027f25eb292fc974ffea3775247d35e3.jpg"></div><br>  First we need to create an input that will receive an offset.  To do this, create a <em>FunctionInput</em> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a4e/d60/063/a4ed6006328e86105ce1878182295a00.jpg"></div><br>  When we continue to use the function, it will be the input contact. <br><br>  Now we need to set several parameters for the input.  Select <em>FunctionInput</em> and go to the Details panel.  Change the following parameters: <br><br><ul><li>  <em>InputName:</em> Offset </li><li>  <em>InputType:</em> Function Input Vector 2. Since the depth buffer is a 2D image, the offset must be of type <em>Vector 2</em> . </li><li>  <em>Use Preview Value as Default:</em> Enabled.  If you do not pass the input value, the function will use the value from the <em>Preview Value</em> . </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/842/59f/0cd/84259f0cda8516ce66267da4720a126d.jpg"></div><br>  Next, we need to multiply the offset by the pixel size.  Then you need to add the result to TextureCoordinate.  To do this, add the selected nodes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/30a/905/7e7/30a9057e73681a1c2a0079859deda2b8.jpg"></div><br>  Finally, we need to sample using a UV depth buffer.  Add <em>SceneDepth</em> and connect everything as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/98e/0b2/fef/98e0b2fefe0d8b1cb08e962ae42bfa02.jpg"></div><br><blockquote>  <em>Note:</em> You can also use <em>SceneTexture</em> instead with a <em>SceneDepth</em> value. </blockquote><br>  Summarize: <br><br><ol><li>  <em>Offset</em> gets <em>Vector 2</em> and multiplies it by <em>SceneTexelSize</em> .  This gives us a shift in UV space. </li><li>  Add an offset to <em>TextureCoordinate</em> to get a pixel located <em>(x, y)</em> pixels from the current one. </li><li>  <em>SceneDepth</em> will use the transmitted UVs to sample the corresponding pixel, and then output it. </li></ol><br>  And on this work with the function of the material is finished.  Click on <em>Apply</em> and close <em>MF_GetPixelDepth</em> . <br><br><blockquote>  <em>Note:</em> You can see an error in the <em>Stats</em> panel telling you that only translucent or post-processing materials can read from the back of the scene.  You can safely ignore this error.  Since we will use the function in the post-processing material, everything will work. </blockquote><br>  Next we need to use the function to perform the convolution of the depth buffer. <br><br><h3>  Convolution </h3><br>  First we need to create offsets for each pixel.  Since the corners of the core are always zero, we can skip them.  That is, we have left, right, upper and lower pixels. <br><br>  Open <em>PP_Outline</em> and create four <em>Constant2Vector</em> nodes.  Give them the following parameters: <br><br><ul><li>  <em>(-ten)</em> </li><li>  <em>(ten)</em> </li><li>  <em>(0, -1)</em> </li><li>  <em>(0, 1)</em> </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6d9/5ad/83c/6d95ad83c31411f0fe424b0ce9ce7d7b.jpg"></div><br>  Next we need to sample five pixels in the core.  Create five <em>MaterialFunctionCall</em> nodes and select <em>MF_GetPixelDepth</em> for each.  Then connect each offset with its own function. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e45/dc3/c50/e45dc3c5090d77ce6f93ee03410fcfe6.jpg"></div><br>  So we get the depth values ‚Äã‚Äãfor each pixel. <br><br>  The next step is the multiplication stage.  Since the multiplier for neighboring pixels is <em>1</em> , we can skip the multiplication.  However, we still need to multiply the central pixel (lower function) by <em>-4</em> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/995/a51/3de/995a513de4013764d3f40ac7cc3c89ba.jpg"></div><br>  Next, we need to summarize all the values.  Create four <em>Add</em> nodes and connect them as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/04b/7b5/232/04b7b52329653094d56634af5e6fd458.jpg"></div><br>  If you remember a plot of pixel values, you will notice that some of them are negative.  If you use the material as it is, negative pixels will appear black, because they are less than zero.  To fix this, we can get <em>an absolute</em> value that converts all input data to a positive value.  Add <em>Abs</em> and mix it up like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/75e/ffd/39a/75effd39afe31022551275e2e6bd2a42.jpg"></div><br>  Summarize: <br><br><ol><li>  <em>MF_GetPixelDepth nodes</em> get the depth value from the center, left, right, top and bottom pixels. </li><li>  Multiply each pixel by its corresponding core value.  In our case, it is enough to multiply only the central pixel. </li><li>  Calculate the sum of all pixels. </li><li>  We get the absolute value of the amount.  This will not allow pixels with negative values ‚Äã‚Äãto appear in black. </li></ol><br>  Click on <em>Apply</em> and return to the main editor.  All lines now appear on the image! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/455/607/16c/45560716c5bbebfed9b85ff19cfe40fd.gif"></div><br>  However, here we have some problems.  First, there are edges in which the difference in depth is insignificant.  Secondly, there are circular lines against the background, because it is a sphere.  This is not a problem if you limit the recognition of the edges only meshes.  However, if you want to create lines in the whole scene, then these circles are undesirable. <br><br>  To correct this, you can use threshold values. <br><br><h2>  Implementation thresholds </h2><br>  First we correct the lines that appear due to minor differences in depths.  Return to the material editor and create the diagram shown below.  Set <em>Threshold</em> to <em>4</em> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d84/c81/499/d84c81499f63deff655fe720a0222d7d.jpg"></div><br>  Later we connect the edge recognition result with <em>A.</em>  It will output a value of <em>1</em> (indicating an edge) if the pixel value is higher than <em>4</em> .  Otherwise, it will output <em>0</em> (no edge). <br><br>  Next we get rid of the lines in the background.  Create the schema shown below.  Set the <em>DepthCutoff</em> value to <em>9000</em> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6b2/cda/5ff/6b2cda5ff7dbc6b33e5087d863ee08ad.jpg"></div><br>  In this case, the output will be transferred to the value <em>0</em> (no edge), if the depth of the current pixel is more than <em>9000</em> .  Otherwise, the value from <em>A &lt;B</em> will be transmitted to the output. <br><br>  Finally, connect everything as follows: <br><br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/102/5fb/e24/1025fbe24603a1f39f75886fef06db23.jpg"></a> </div><br>  Now the lines will be displayed only when the pixel value is greater than <em>4</em> ( <em>Threshold</em> ) <i>and</i> its depth is less than <em>9000</em> ( <em>DepthCutoff</em> ). <br><br>  Click on <em>Apply</em> and return to the main editor.  Small lines and lines in the background is no more! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8af/cf1/e48/8afcf1e48605955f7fdb685afebf3b9e.gif"></div><br><blockquote>  <em>Note: You</em> can create an instance of <em>PP_Outline</em> material to control <em>Threshold</em> and <em>DepthCutoff</em> . </blockquote><br>  Edge recognition works quite well.  But what if we need thicker lines?  For this we need to increase the size of the kernel. <br><br><h2>  Creating thicker lines </h2><br>  In general, the larger the kernel size, the more it affects the speed, because we need to sample more pixels.  But is there a way to enlarge the cores while maintaining the same speed as with a 3 √ó 3 core?  This is where the <em>extended convolution</em> comes in handy. <br><br>  With an expanded convolution, we simply spread the offsets further.  To do this, we multiply each offset by a scalar, called the <em>coefficient of expansion</em> .  It determines the distance between the elements of the core. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5e3/d3f/c47/5e3d3fc4761b6715a6858f36241d704f.gif"></div><br>  As you can see, this allows us to increase the size of the core, while sampling the same number of pixels. <br><br>  Now let's implement the extended convolution.  Return to the material editor and create a <em>ScalarParameter</em> called <em>DilationRate</em> .  Set it to <em>3</em> .  Then multiply each offset by <em>DilationRate</em> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b4f/225/90b/b4f22590b610bd9c640721dc6796278e.jpg"></div><br>  So we will shift each offset a distance of <em>3</em> pixels from the central pixel. <br><br>  Click on <em>Apply</em> and return to the main editor.  You will see that the lines are much thicker.  Here is a comparison of lines with different expansion factors: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/21d/3de/20c/21d3de20c9a159b37ac6ba0f0acbd225.gif"></div><br>  If you are not making a line-style game, then most likely you need the original scene to be visible.  In the last section, we will add lines to the image of the original scene. <br><br><h2>  Adding lines to the original image </h2><br>  Return to the material editor and create the diagram shown below.  Order is important here! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/671/372/3b3/6713723b3fcf66a1c9eb5b2d7465a816.jpg"></div><br>  Next, connect everything as follows: <br><br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/76a/bbf/991/76abbf991a775ed6d51008768bcc74b2.jpg"></a> </div><br>  Now <em>Lerp</em> will display the image of the scene if alpha reaches zero (black color).  Otherwise, it displays <em>LineColor</em> . <br><br>  Click on <em>Apply</em> and close <em>PP_Outline</em> .  Now the original scene has contours! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/717/8e6/acc/7178e6acc79f0f0cd460d1ee91507ad0.jpg"></div><br><br><h2>  Where to go next? </h2><br>  The finished project can be downloaded <a href="">here</a> . <br><br>  If you want to work with edge recognition, try creating a recognition that works with the normal buffer.  This will give you some edges that do not appear in the edge resolver by depth.  Then you can combine both types of edge recognition together. <br><br>  Convolutions is an extensive topic that finds active use, including in artificial intelligence and sound processing.  I recommend to study the convolutions, creating other effects, such as sharpening and blurring.  For some of them, simply changing the kernel values ‚Äã‚Äãis enough!  See an interactive explanation of the <a href="http://setosa.io/ev/image-kernels/">bundles</a> in <a href="http://setosa.io/ev/image-kernels/">Images Kernels explained visually</a> .  It also describes the kernel for some other effects. <br><br>  I also strongly recommend to watch the presentation with GDC on the <a href="https://www.youtube.com/watch%3Fv%3DyhGjCzxJV3E">Guilty Gear Xrd graphic style</a> .  For external lines, this game also uses an inverted mesh method.  However, for internal lines, developers have created a simple, but ingenious technique using textures and UV. </div><p>Source: <a href="https://habr.com/ru/post/352814/">https://habr.com/ru/post/352814/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../352804/index.html">MobileNet: smaller, faster, more accurate</a></li>
<li><a href="../352806/index.html">Arduino for beginners. Part 1</a></li>
<li><a href="../352808/index.html">As we did backups in the ISPsystem. Part two</a></li>
<li><a href="../352810/index.html">Mutual transformations of JSON, YAML, XML</a></li>
<li><a href="../352812/index.html">Data Analysis - Basics and Terminology</a></li>
<li><a href="../352820/index.html">Avito at GopherCon Russia 2018</a></li>
<li><a href="../352822/index.html">How to expand the functionality of Zabbix and not pay for licenses</a></li>
<li><a href="../352824/index.html">Data Center for Technopark: from ‚ÄúConcrete‚Äù to Tier Facility Certification</a></li>
<li><a href="../352826/index.html">We are preparing a project in Sparx Enterprise Architect. Our recipe</a></li>
<li><a href="../352828/index.html">JavaScript Web Workers: Secure Concurrency</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>