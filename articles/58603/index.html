<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to download from Rapidshare.com "free user" using curl or wget</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Appeared almost a year ago, an article about downloading Rapidshare.com with a favorite treasure trove of almost legal information that you had saved ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to download from Rapidshare.com "free user" using curl or wget</h1><div class="post__text post__text-html js-mediator-article"> Appeared almost a year ago, <a href="http://habrahabr.ru/blogs/linux/28400/">an article</a> about downloading Rapidshare.com with a favorite treasure trove of <s>almost legal information</s> that you had saved up aroused public approval by Habr.  Lately, Rapid has removed the captcha from herself, made waiting not so long between downloads, in general, with her whole appearance shows that it is very pleasant to work with her.  And if it can be done more for free ... so why not ?! <br><a name="habracut"></a><br>  In view of the <a href="http://habrahabr.ru/blogs/lenta/58504/">latest</a> events, it is recommended to approach the process of receiving and distributing information to other network users very intelligently.  And the author of this post is not going to bear any responsibility for the violation by you, dear habrovchane, licensing agreements, copyright, etc. <br><br>  A little bit of lyrics at the beginning.  Interest in automating some processes, for example, automatically saving audio pronunciation files that were pulled from <a href="http://www.google.com/dictionary%3Faq%3Df%26langpair%3Den%7Cru">translate.google.com</a> when learning new words;  or getting a direct link from the video storage, for viewing later by mplayer on the tablet nokia N810;  forced to look for ways to communicate with web servers via the console, if possible, without resorting to user intervention, i.e.  full automation.  Perhaps the most popular tool is wget.  But they use it most often for banal direct download links to files.  We will try to correct this article.  But besides wget there are also slightly less known programs, such as for example curl.  The latter, say, is missing from the default Linux installation from Canonical Ltd., widely known in narrow circles. <br><br>  It is with curl and start.  Just a few words to show what he is capable of. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Examples of using the automation of download can be found on the <a href="http://emkay.unpointless.com/Blog/%3Fp%3D63">English</a> open spaces of all Internet.  Let's try on their base to describe the process itself. <br><br>  Let the first be: <br><br> <code>curl -s</code> <br> <br>  The option simply ‚Äúshuts up‚Äù the zyrl, so that it wouldn‚Äôt be too much ... The option is really useful if you don‚Äôt want to understand the grief of status information.  The same option in full form: <br><br>  <code>curl --silent</code> . <br><br>  You can send a POST request to the HTTP server using: <br><br> <code>curl -d DATA_TO_SEND <br> curl --data DATA_TO_SEND</code> <br> <br>  The post is used by the browser when sending form values ‚Äã‚Äãon the HTML page when the user clicks the submit button.  And now we can tell this server which button we pressed, or what we entered in the field on the page, etc. with this parameter. <br><br>  Immediately for example, I will give a method using curl to get a direct link on rapid: <br><br> <code>#!/bin/bash <br> while read urlline; do <br> <br> pageurl=$(curl -s $urlline | grep "&lt;form id=\"ff\" action=\"" | grep -o 'http://[^"]*rar') <br> fileurl=$(curl -s -d "dl.start=Free" "$pageurl" | grep "document.dlf.action=" | grep -o 'http://[^"]*rar' | head -n 1) <br> sleep 60 <br> wget $fileurl <br> <br> done &lt; URLS.txt <br></code> <br><br>  A little about this bash script - URLs are read line by line from the URLS.txt file, in the pageurl variable we pull the link to the page with the premium / free user selection.  A direct link to the file is thrown into the fileurl variable.  We receive it by sending to the server that we want to receive everything from life for free, filtering grep'om legal entities, and since there may be several of them, we leave only the first line in the head.  We wait 60 seconds and download the file.  Here is such a script. <br><br><h4>  And now the banana ... </h4><br>  Let's try to portray it all with wget. <br><br>  Script in the studio: <br><br> <code><a href="http://emkay.unpointless.com/Blog/%3Fp%3D63"></a> #!/bin/bash <br> <br> ################################################ <br> #Purpose: Automate the downloading of files from rapidshare using the free account <br> #using simple unix tools. <br> #Date: 14-7-2008 <br> #Authors: Slith, Tune, Itay <br> #Improvements, Feedback, comments: Please go to emkay.unpointless.com/Blog/?p=63 <br> #Notes: To use curl instead of wget use 'curl -s' and 'curl -s -d' <br> #Version: 1.? <br> ################################################ <br> <br> #! -      <br> <br> # Tune   curl-  ,    <br> #    ,   .rar <br> <br> #TODO:     <br> #TODO:    ,      <br> <br> ### <br> echo "test" <br> in=input.txt <br> <br> timer() <br> { <br> TIME=${1:-960} <br> /bin/echo -ne "${2:-""}\033[s" <br> for i in `seq $TIME -1 1`; do <br> /bin/echo -ne "\033[u $(printf "%02d" `expr $i / 60`)m$(printf "%02d" `expr $i % 60`)s ${3:-""}" <br> sleep 1 <br> done <br> /bin/echo -ne "\033[u 00m00s" <br> echo <br> } <br> <br> while [ `wc -l $in | cut -d " " -f 1` != 0 ]; do <br> read line &lt; $in <br> URL=$(wget -q -O - $line | grep "&lt;form id=\"ff\" action=\"" | grep -o 'http://[^"]*'); <br> output=$(wget -q -O - --post-data "dl.start=Free" "$URL"); <br> <br> #     <br> serverbusy=$(echo "$output" | egrep "Currently a lot of users are downloading files. Please try again in.*minutes" | grep -o "[0-9]{1,0}") <br> if [ "$serverbusy" != "" ]; then <br> timer `expr $serverbusy '*' 60` " . ." " ..." <br> continue; # try again <br> fi <br> <br> #         ( ) <br> longtime=$(echo "$output" | egrep "Or try again in about.*minutes" | egrep -o "[0-9]*") <br> if [ "$longtime" != "" ]; then <br> timer `expr '(' $longtime + 1 ')' '*' 60` "." "(   ) ..." <br> URL=$(wget -q -O - $line | grep "&lt;form id=\"ff\" action=\"" | grep -o 'http://[^"]*'); <br> output=$(wget -q -O - --post-data "dl.start=Free" "$URL"); <br> fi <br> <br> #       ( ,  ) <br> time=$(echo "$output" | grep "var c=[0-9]*;" | grep -o "[0-9]\{1,3\}"); <br> <br> time=$(echo "$time" | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//') # trim ws <br> if [ "$time" = "" ]; then <br> echo " \"`basename "$line"`\"  ". <br> echo $line &gt;&gt; fail.txt <br> sed -i '1 d' $in; #   input  <br> continue <br> fi <br> ourfile=$(echo "$output" | grep "document.dlf.action=" | grep checked | grep -o "http://[^\\]*"); <br> timer $time "" "  `basename "$ourfile"`"; <br> if ! wget -c $ourfile; then <br> echo '  .      .' <br> else <br> sed -i '1 d' $in; #   input  <br> fi <br> done <br> <br> if [ -e fail.txt ]; then <br> mv fail.txt $in #       . <br> fi <br></code> <br><br>  Very nice script.  I found for myself a couple of interesting implementations.  So leave it as it is, with the indication of the authors.  Made only full translation of comments. <br><br>  We throw the list of links in input.txt and run the script - it will tell us what it does.  If the file could not be written, it is sent to the fail.txt file.  When all input.txt is passed, the fail.txt file is written back to the input, and the downloaded links are deleted. <br><br>  Good luck downloading your backups. <br><br>  <b>PS:</b> <ul><li>  Russified phrases in the script; </li><li>  I noticed a bug: if the link is thrown into the file, without moving to a new line - the script does not want to read such a line.  Exit: add an empty line to the end of the file. </li><li>  For especially lazy copy-paste - I post the script <a href="">separately</a> .  Yesterday (27 Oct 09) it was not available, because  on the server rose django framework.  Under the new link the file will be available for a long time. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/58603/">https://habr.com/ru/post/58603/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../58597/index.html">Rumix - create a virtual room and furnish it with 3D furniture models!</a></li>
<li><a href="../58598/index.html">Hivext - Web application development platform</a></li>
<li><a href="../58599/index.html">Extensive Vulnerability (DoS) on PIX / ASA</a></li>
<li><a href="../58601/index.html">CMS systems for highly visited projects. Are there any ?!</a></li>
<li><a href="../58602/index.html">IP colors: Internet colors</a></li>
<li><a href="../58606/index.html">Tracking the spread of swine flu through Live Earth</a></li>
<li><a href="../58607/index.html">Planning a multimedia day. Handbook for the media</a></li>
<li><a href="../58614/index.html">Interview with Program Manager in the team of Internet Information Services Ruslan Yakushev</a></li>
<li><a href="../58615/index.html">Torment with Ubuntu</a></li>
<li><a href="../58619/index.html">T-shirt Viva La Distribuci√≥n!</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>