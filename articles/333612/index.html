<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Deep Learning, now in OpenCV</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article is a brief overview of the capabilities of dnn, an OpenCV module designed to work with neural networks. If you are wondering what it is, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Deep Learning, now in OpenCV</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/web/fef/4e3/8a6/fef4e38a6dd34826b877f2212588b1fa.png"><br><br>  This article is a brief overview of the capabilities of dnn, an OpenCV module designed to work with neural networks.  If you are wondering what it is, what it can do and how fast it works, welcome to cat. <br><a name="habracut"></a><br>  Perhaps many would agree that OpenCV is the most well-known library of computer vision.  For a long time of its existence, it has gained an extensive audience of users and has become, de facto, the standard in the field of computer vision.  Many out-of-the-box algorithms, open source code, great support, a large community of users and developers, the ability to use the library in C, C ++, Python (as well as Matlab, C #, Java) under various operating systems is far from complete a list of what allows OpenCV to remain relevant.  But OpenCV does not stand still - the functionality is constantly added.  And today I want to talk about the new features of OpenCV in the field of Deep Learning. <br><br>  Download and retrieve results (predictions) using models created in any of the three popular frameworks (Caffe, TensorFlow, Torch), fast work on the CPU, support for the main layers of neural networks and, as always, cross-platform, open source code and support In this I am going to tell in this article. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      First of all, I would like to introduce myself.  My name is Rybnikov Alexander.  I am an Intel engineer and I implement Deep Learning functionality in the OpenCV library. <br><br>  A few words about how OpenCV works.  This library is a set of modules, each of which is associated with a specific area of ‚Äã‚Äãcomputer vision.  There is a standard set of modules - ‚Äúmust have‚Äù, so to speak, for any computer vision task.  Implementing well-known algorithms, these modules are well developed and tested.  All of them are represented <a href="https://github.com/opencv/opencv">in the main OpenCV repositories</a> .  There is also a <a href="https://github.com/opencv/opencv_contrib">repository</a> with additional modules that implement experimental or new functionality.  The requirements for experimental modules, for obvious reasons, are softer.  And, as a rule, when one of these modules becomes sufficiently developed, formed and in demand, it can be transferred to the main repository. <br><br>  This article is associated with one of the modules, which recently took an honorable place in the main repository - with the dnn module (hereinafter simply dnn). <br><br><h2>  <font color="#0071c5">(N + 1) -th framework for deep learning, this is why?</font> </h2><br>  Why did you need deep learning in opencv?  In recent years, in many areas, deep learning (in some sources, deep learning) has shown results far exceeding those of classical algorithms.  This also applies to the field of computer vision, where the mass of problems is solved using neural networks.  In light of this fact, it seems logical to give OpenCV users the ability to work with neural networks. <br><br>  Why was the way of writing something of their own, instead of using existing implementations?  There are several reasons for this. <br><br>  First, it is possible to achieve the lightness of the solution.  Leaving only the ability to perform a forward pass over the network, you can simplify the code, speed up the installation and assembly process. <br><br>  Secondly, with its implementation, it is possible to reduce external dependencies to a minimum.  This will simplify the distribution of applications using dnn.  And, if previously the project used the OpenCV library, it would not be difficult to add support for deep networks to such a project. <br><br>  Also, developing your solution, it is possible to make it universal, not tied to any particular framework, its limitations and disadvantages.  If you have your own implementation, all ways are available to optimize and speed up the code. <br><br>  A proprietary module for launching deep networks greatly simplifies the procedure for creating hybrid algorithms that combine the speed of classical computer vision and the remarkable generalizing ability of deep neural networks. <br><br>  It is worth noting that the module is not, strictly speaking, a full-fledged framework for deep learning.  At the moment, the module presents only the possibility of obtaining the results of the network. <br><br><h2>  <font color="#0071c5">Main features</font> </h2><br>  The main possibility of dnn is, of course, loading and running neural networks (inference).  In this case, the model can be created in any of the three deep learning frameworks - Caffe, TensorFlow or Torch;  the way it is loaded and used is preserved regardless of where it was created. <br><br>  By supporting three popular frameworks at once, we can simply combine the results of the models loaded from them without the need to create everything anew in one single framework. <br><br>  When loading, the models are converted into an internal representation that is close to that used in Caffe.  This happened for historical reasons - the support of Caffe was added the very first.  However, there is no one-to-one correspondence between the representations. <br><br>  All basic layers are supported: from basic (Convolution and Fully connected) to more specialized ones - more than 30 in total. <br><br><div class="spoiler">  <b class="spoiler_title">List of supported layers</b> <div class="spoiler_text">  Absval <br>  AveragePooling <br>  Batch normalization <br>  Concatenation <br>  Convolution (with dilation) <br>  Crop <br>  DetectionOutput <br>  Dropout <br>  Eltwise <br>  Flatten <br>  FullConvolution <br>  Fullyconnected <br>  LRN <br>  Lstm <br>  MaxPooling <br>  MaxUnpooling <br>  MVN <br>  NormalizeBBox <br>  Padding <br>  Permute <br>  Power <br>  PReLU <br>  PriorBox <br>  Relu <br>  Rnn <br>  Scale <br>  Shift <br>  Sigmoid <br>  Slice <br>  Softmax <br>  Split <br>  Tanh <br><br>  If you have not found in this list the layer that is required for you, do not despair.  You can create a <a href="https://github.com/opencv/opencv/issues/new">request</a> to add support for the layer of interest to you (and our team will try to help you in the near future), or implement everything yourself and submit a pull request. <br></div></div><br>  In addition to supporting individual layers, support for specific neural network architectures is also important.  The module contains examples for classification ( <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> , <a href="https://arxiv.org/pdf/1409.4842.pdf">GoogLeNet</a> , <a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a> , <a href="https://arxiv.org/pdf/1602.07360.pdf">SqueezeNet</a> ), segmentation ( <a href="https://arxiv.org/pdf/1411.4038.pdf">FCN</a> , <a href="https://arxiv.org/pdf/1606.02147.pdf">ENet</a> ), object detection ( <a href="https://arxiv.org/pdf/1512.02325.pdf">SSD</a> );  Many of these models are tested on the original datasets, but more on that later. <br><br><h2>  <font color="#0071c5">Assembly</font> </h2><br>  If you are an experienced OpenCV user, feel free to skip this section.  If not, then I will try as briefly as possible to talk about how to get working examples from source code for Linux or Windows. <br><br><div class="spoiler">  <b class="spoiler_title">Brief assembly instructions</b> <div class="spoiler_text">  You first need to install git (or Git Bash for Windows), [cmake] (http://cmake.org) and the C ++ compiler (Visual Studio for Windows, Xcode on Mac, clang or gcc for Linux).  If you are going to use OpenCV from Python, then you must also install Python itself (the latest versions 2.7.x or 3.x will do) and the corresponding numpy version. <br><br>  Let's start with repository cloning: <br><br><pre><code class="bash hljs">mkdir git &amp;&amp; <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> git git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/opencv/opencv.git</code> </pre> <br>  On Windows, repository cloning can also be performed, for example, using TortoiseGit or SmartGit.  Next, let's start generating files for the assembly: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> .. mkdir build &amp;&amp; <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> build cmake ../git/opencv -DBUILD_EXAMPLES=ON</code> </pre> <br>  (for Windows, hereinafter, you need to replace cmake with the full path to the cmake file to be launched, for example, ‚ÄúC: \ Program Files \ CMake \ bin \ cmake.exe‚Äù or use the cmake GUI) <br><br>  Now directly build: <br><br><pre> <code class="bash hljs">make -j5 (Linux) cmake --build . --config Release -- /m:5 (Windows)</code> </pre> <br>  After that dnn is ready to use. <br>  The above instructions are quite brief, so I‚Äôll also provide links to step-by-step instructions for installing OpenCV on <a href="http://docs.opencv.org/3.2.0/d3/d52/tutorial_windows_install.html">Windows</a> and <a href="http://docs.opencv.org/3.2.0/d7/d9f/tutorial_linux_install.html">Linux</a> . <br></div></div><br><h2>  <font color="#0071c5">Examples of using</font> </h2><br>  According to a good tradition, each OpenCV module includes usage examples.  dnn is not an exception, C ++ and Python examples are available in the <a href="https://github.com/opencv/opencv/tree/master/samples/dnn">samples</a> subdirectory in the source code repository.  In the examples there are comments, and in general everything is quite simple. <br><br>  I will give here a brief example that performs image classification using the GoogLeNet model.  In Python, our example will look like this: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cv <span class="hljs-comment"><span class="hljs-comment"># read names of classes with open('synset_words.txt') as f: classes = [x[x.find(' ') + 1:] for x in f] image = cv.imread('space_shuttle.jpg') # create tensor with 224x224 spatial size and subtract mean values (104, 117, 123) # from corresponding channels (R, G, B) input = cv.dnn.blobFromImage(image, 1, (224, 224), (104, 117, 123)) # load model from caffe net = cv.dnn.readNetFromCaffe('bvlc_googlenet.prototxt', 'bvlc_googlenet.caffemodel') # feed input tensor to the model net.setInput(input) # perform inference and get output out = net.forward() # get indices with the highest probability indexes = np.argsort(out[0])[-5:] for i in reversed(indexes): print('class:', classes[i], ' probability:', out[0][i])</span></span></code> </pre> <br>  This code loads a picture, conducts a little preprocessing and gets a network output for the image.  The preprocessing consists in scaling the image so that the smallest of the sides becomes equal to 224, cutting out the central part and subtracting the average value from the elements of each channel.  These operations are necessary, since the model was trained on images of a given size (224 x 224) with just such preprocessing. <br><br>  The output tensor is interpreted as the vector of probabilities that an image belongs to a particular class, and the names for the 5 classes with the highest probabilities are output to the console. <br><br>  Looks easy, right?  If you write the same thing in C ++, the code will get a bit longer.  However, the most important thing - the names of the functions and the logic of working with the module - will remain the same. <br><br><h2>  <font color="#0071c5">Accuracy</font> </h2><br>  How to understand that one trained model is better than another?  It is necessary to compare quality metrics for both models.  Very often, the struggle at the top of the ranking of the best models goes for a fraction of percent quality.  Since dnn reads and converts models from various frameworks into its internal representation, there are questions of preserving the quality after the model is transformed: did the model not ‚Äúspoil‚Äù after loading?  Without answers to these questions, which means without checking it is difficult to talk about the full use of dnn. <br><br>  I have tested models from the available examples for various frameworks and various tasks: AlexNet (Caffe), GoogLeNet (Caffe), GoogLeNet (TensorFlow), ResNet-50 (Caffe), SqueezeNet v1.1 (Caffe) for the problem of classifying objects;  FCN (Caffe), ENet (Torch) for the task of semantic segmentation.  The results are shown in Tables 1 and 2. <br><table><tbody><tr><td>  Model (source framework) <br></td><td>  Published value acc @ top-5 <br></td><td>  The measured value acc @ top-5 in the source framework <br></td><td>  Measured value acc @ top-5 in dnn <br></td><td>  The average difference per element between the output tensors of the framework and dnn <br></td><td>  The maximum difference between the output tensors of the framework and dnn <br></td></tr><tr><td>  <a href="http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel">AlexNet</a> (Caffe) <br></td><td>  <a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet">80.2%</a> <br></td><td>  79.1% <br></td><td>  79.1% <br></td><td>  6.5E-10 <br></td><td>  3.01E-06 <br></td></tr><tr><td>  <a href="http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel">GoogLeNet</a> (Caffe) <br></td><td>  <a href="https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet">88.9%</a> <br></td><td>  88.5% <br></td><td>  88.5% <br></td><td>  1.18E-09 <br></td><td>  1.33E-05 <br></td></tr><tr><td>  <a href="">GoogLeNet</a> (TensorFlow) <br></td><td>  - </td><td>  89.4% <br></td><td>  89.4% <br></td><td>  1.84E-09 <br></td><td>  1.47E-05 <br></td></tr><tr><td>  <a href="https://onedrive.live.com/%3Fauthkey%3D%2521AAFW2-FVoxeVRck%26id%3D4006CBB8476FF777%252117887%26cid%3D4006CBB8476FF777">ResNet-50</a> <br>  (Caffe) <br></td><td>  <a href="https://github.com/KaimingHe/deep-residual-networks">92.2%</a> <br></td><td>  91.8% <br></td><td>  91.8% <br></td><td>  8.73E-10 <br></td><td>  4.29E-06 <br></td></tr><tr><td>  <a href="">SqueezeNet v1.1</a> <br>  (Caffe) <br></td><td>  <a href="">80.3%</a> <br></td><td>  80.4% <br></td><td>  80.4% <br></td><td>  1.91E-09 <br></td><td>  6.77E-06 </td></tr></tbody></table>  <i>Table 1. Results of quality assessment for the classification task.</i>  <i>The measurements were performed on the ImageNet 2012 validation kit (ILSVRC2012 val, 50,000 examples).</i> <br><table><tbody><tr><td>  Model (framework) <br></td><td>  Published mean IOU value <br></td><td>  Measured mean IOU value in the source framework <br></td><td>  Measured mean IOU in dnn <br></td><td>  The average difference per element between the output tensors of the framework and dnn <br></td><td>  The maximum difference between the output tensors of the framework and dnn <br></td></tr><tr><td>  <a href="http://dl.caffe.berkeleyvision.org/fcn8s-heavy-pascal.caffemodel">FCN</a> (Caffe) <br></td><td>  <a href="">65.5%</a> <br></td><td>  60.402874% <br></td><td>  60.402879% <br></td><td>  3.1E-7 <br></td><td>  1.53E-5 <br></td></tr><tr><td>  <a href="https://www.dropbox.com/sh/dywzk3gyb12hpe5/AAD5YkUa8XgMpHs2gCRgmCVCa">ENet</a> (Torch) <br></td><td>  <a href="https://arxiv.org/pdf/1606.02147.pdf">58.3%</a> <br></td><td>  59.1368% <br></td><td>  59.1369% <br></td><td>  3.2E-5 <br></td><td>  1.20 <br></td></tr></tbody></table>  <i>Table 2. Quality assessment results for the semantic segmentation task.</i>  <i>The explanation of the large maximum difference for ENet is hereinafter referred to.</i> <br><br>  The results for FCN are calculated for the validation set of the <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/">PASCAL VOC 2012</a> segmentation part (736 examples).  The results for ENet are calculated on the <a href="https://www.cityscapes-dataset.com/">Cityscapes</a> validation set (500 examples). <br><br>  A few words should be said about the meaning of the above numbers.  For classification problems of the generally accepted quality metric of models is accuracy for the top-5 network responses (accuracy @ top-5, [1]): if the correct answer is among the 5 network responses with the maximum confidence indicators (confidence), then this network response is counted as correct .  Accordingly, accuracy is the ratio of the number of correct answers to the number of examples.  This method of measurement makes it possible to take into account not always the correct marking of data, when, for example, an object is marked that is far from central to the frame. <br><br>  For semantic segmentation problems, several metrics are used - pixel accuracy and pixel mean over intersection (mean IOU) [5].  Pixel accuracy is the ratio of the number of correctly classified pixels to the number of all pixels.  mean IOU is a more complex characteristic: it is the class-averaged ratio of correctly marked pixels to the sum of the number of pixels of a given class and the number of pixels marked as a given class. <br><br>  From the tables, it follows that for classification and segmentation problems, there is no difference in accuracy between model launches in the original framework and in dnn.  This remarkable fact means that the module can be safely used without fear of unpredictable results.  All scripts for testing are also available <a href="https://github.com/opencv/opencv/tree/master/modules/dnn/test">here</a> , so you can verify for yourself that the results obtained are correct. <br><br>  The difference between the numbers published and obtained in experiments can be explained by the fact that the authors of the models carry out all the calculations using the GPU, while I used the CPU implementations.  It was also noted that different libraries can decode jpeg format in different ways.  This could have an impact on the results for FCN, since the PASCAL VOC 2012 datasets contain images of this particular format, and the models for semantic segmentation are quite sensitive to changes in the distribution of input data. <br><br>  As you noticed, in Table 2 there is an abnormally large maximum difference between the dnn and Torch outputs for the ENet model.  I was also interested in this fact and then I will briefly describe the reasons for its occurrence. <br><br><div class="spoiler">  <b class="spoiler_title">Why is there a big difference between dnn and torch for eet?</b> <div class="spoiler_text">  The ENet model uses several MaxPooling operations.  This operation selects the maximum element in the vicinity of each position and writes this maximum value to the output tensor, and also passes on the indices of the selected maximum elements.  These indices are then used by the operation, in a sense, the inverse of the given - MaxUnpooling.  This operation writes the elements of the input tensor in the output position, corresponding to the indices.  In this place, a big error occurs: in a certain neighborhood, the MaxPooling operation selects an element with the wrong index;  the difference between the correct Torch output and the dnn output for this layer lies within the computational error (10E-7), and the difference in the indices corresponds to the neighboring elements of the neighborhood.  That is, as a result of a small fluctuation, the neighboring element became somewhat larger than the element with the correct index.  The result of the MaxUnpooling operation, however, depends not only on the output of the previous layer, but also on the indices of the corresponding MaxPooling operation, which is located much earlier (at the beginning of the computational graph of the model).  Thus, MaxUnpooling writes the element with the correct value to the wrong position.  As a result, an error accumulates. <br><br>  Unfortunately, it is not possible to eliminate this error, since the root causes of its occurrence are most likely related to slightly different implementations of the algorithms used during training and in inference and are not related to the presence of an error in the implementation. <br>  However, it is fair to say that the average error on the element of the output tensor remains low (see Table 2) - that is, errors in the indices are quite rare.  Moreover, the presence of this error does not lead to a deterioration in the quality of the model, as evidenced by the numbers in the same Table 2. <br></div></div><br><h2>  <font color="#0071c5">Performance</font> </h2><br>  One of the goals that we set for ourselves when developing dnn is to achieve decent module performance on various architectures.  Not so long ago, optimization was carried out under the CPU, with the result that now dnn shows good results in terms of speed. <br><br>  I spent working time for different models when using them - the results in Table 3. <br><table><tbody><tr><td>  Model (source framework) <br></td><td>  Image resolution <br></td><td>  The performance of the original framework, CPU (acceleration library);  memory consumption <br></td><td>  Performance dnn, CPU (acceleration relative to the source framework);  memory consumption <br></td></tr><tr><td>  <a href="http://dl.caffe.berkeleyvision.org/bvlc_alexnet.caffemodel">AlexNet</a> (Caffe) <br></td><td>  227x227 <br></td><td>  23.7 ms (MKL);  945 MB <br></td><td>  14.7 ms (1.6x);  713 MB <br></td></tr><tr><td>  <a href="http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel">GoogLeNet</a> (Caffe) <br></td><td>  224x224 <br></td><td>  44.6 ms (MKL);  197 MB <br></td><td>  20.1 ms (2.2x);  172 MB <br></td></tr><tr><td>  <a href="https://onedrive.live.com/%3Fauthkey%3D%2521AAFW2-FVoxeVRck%26id%3D4006CBB8476FF777%252117887%26cid%3D4006CBB8476FF777">ResNet-50</a> (Caffe) <br></td><td>  224x224 <br></td><td>  70.2 ms (MKL);  386 MB <br></td><td>  58.8 ms (1.2x);  224 MB <br></td></tr><tr><td>  <a href="https://github.com/DeepScale/SqueezeNet/blob/master/SqueezeNet_v1.1/squeezenet_v1.1.caffemodel">SqueezeNet v1.1</a> <br>  (Caffe) <br></td><td>  227x227 <br></td><td>  12.4 ms (MKL);  113 MB <br></td><td>  5.3 ms (2.3x);  38 MB <br></td></tr><tr><td>  <a href="">GoogLeNet</a> (TensorFlow) <br></td><td>  224x224 <br></td><td>  17.9 ms (Eigen);  310 MB <br></td><td>  21.1 ms (0.8x);  135 MB <br></td></tr><tr><td>  <a href="http://dl.caffe.berkeleyvision.org/fcn8s-heavy-pascal.caffemodel">FCN</a> (Caffe) <br></td><td>  various (500x350 on average) <br></td><td>  3873.6 ms (MKL); <br>  4453 MB <br></td><td>  1229.8 ms (3.1x); <br>  1332 MB <br></td></tr><tr><td>  <a href="https://www.dropbox.com/sh/dywzk3gyb12hpe5/AAD5YkUa8XgMpHs2gCRgmCVCa">ENet</a> (Torch) <br></td><td>  1024x512 <br></td><td>  1105.0 ms;  828 MB <br></td><td>  218.7 ms (5.1x);  190 MB <br></td></tr></tbody></table>  <i>Table 3. The results of measurements of the operating time of various models.</i>  <i>Experiments were conducted using an Intel Core i7-6700k.</i> <br><br>  Time measurements were made with averaging over 50 starts and performed as follows: for dnn, a timer built into OpenCV was used;  caffe time was used for caffe;  For Torch and TensorFlow, existing timekeeping functions were used. <br><br>  As follows from Table 3, dnn in most cases exceeds the performance of the original frameworks.  Actual dnn performance data from OpenCV on various models in comparison with other frameworks can also be found <a href="https://github.com/opencv/opencv/wiki/DNN-efficiency-measurements">here</a> . <br><br><h2>  <font color="#0071c5">Future plans</font> </h2><br>  Deep learning has taken a significant place in computer vision and, accordingly, we have big plans to develop this functionality in OpenCV.  They relate to improving usability, processing the internal architecture of the module itself and improving performance. <br><br>  In improving the user experience, we focus primarily on the wishes of the users themselves.  We strive to add the functionality that developers and researchers need in real-world tasks.  In addition, the plans include the addition of network visualization, as well as the expansion of the set of supported layers. <br><br>  As for performance, despite the many optimizations we have completed, we still have ideas on how to improve the results.  One of these ideas is to reduce the bitness of the calculations.  This procedure is called quantization.  Roughly speaking, throw out a part of the digits at the input and the layer weights before calculating convolutions (fp32 ‚Üí fp16), or calculate the scaling factors that convert the range of input numbers to int or short range.  This will increase the speed (due to the use of faster operations with integers), but perhaps the accuracy will suffer a little.  However, <a href="https://arxiv.org/pdf/1510.00149.pdf">publications</a> and experiments in this area show that even sufficiently strong quantization in certain cases does not lead to a noticeable drop in quality. <br><br>  Parallel execution of layers is another optimization idea.  In the current implementation, only one layer works at a time.  Each layer uses paralleling as much as possible during the calculations.  However, in some cases, the computation graph can be parallelized at the level of the layers themselves.  This could potentially give each thread more work, thereby reducing overhead. <br><br>  Now for the release is preparing something quite interesting.  I think few have heard of the programming language <a href="http://halide-lang.org/">Halide</a> .  It is not Turing-complete ‚Äî some constructs cannot be implemented on it;  maybe that's why he is not popular.  However, this disadvantage is at the same time its advantage - the source code written on it can be automatically turned into highly optimized for different hardware: CPU, GPU, DSP.  And there is no need to be an optimization guru - a special compiler will do everything for you.  Already, Halide allows you to get some models to accelerate - and, for example, semantic segmentation with the ENet model works 25 fps for a resolution of 512x256 on the Intel Core i7-6700k (versus 22 fps for dnn without Halide).  And, best of all, without rewriting the code, you can use the integrated processor GPU, receiving an additional couple of frames per second. <br><br>  In fact, we have high hopes for Halide.  Due to its unique characteristics, it will allow to receive acceleration of work, without requiring additional manipulations from the user.  We strive to ensure that the use of Halide with OpenCV, the user does not have to install additional software to use Halide - the principle of "out of the box" should be maintained.  And, as our experiments show, we have every chance to realize this. <br><br><h2>  <font color="#0071c5">Conclusion</font> </h2><br>  Already, dnn has everything to be useful.  And every day a growing number of users are discovering its capabilities.  However, we still have work to do.  I will continue my work on the module, expanding its capabilities and improving the functionality.  I hope that this article was interesting and useful for you. <br><br>  If you have questions, suggestions, problems or you want to contribute by submitting a pull request - welcome to the <a href="https://github.com/opencv">github repository</a> , as well as to our <a href="http://answers.opencv.org/">forum</a> , where my colleagues and I will try to help you.  If none of these methods come up, on our <a href="http://opencv.org/">website</a> you can find additional ways of communication.  I will always be glad to cooperate, constructive comments and suggestions.  Thanks for attention! <br><br>  PS I express my deep gratitude to my colleagues for their help in working and writing this article. <br><br><h2>  <font color="#0071c5">Links</font> </h2><br><ol><li>  <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolution Neural Networks</a> <br></li><li>  <a href="https://arxiv.org/pdf/1409.4842.pdf">Going deeper with convolutions</a> <br></li><li>  <a href="https://arxiv.org/pdf/1512.03385.pdf">Deep Residual Learning for Image Recognition</a> <br></li><li>  <a href="https://arxiv.org/pdf/1602.07360.pdf">SqueezeNet: AlexNet-level accuracy model with &lt;0.5MB model size</a> <br></li><li>  <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf">Fully Convolutional Networks for Semantic Segmentation</a> <br></li><li>  <a href="https://arxiv.org/pdf/1606.02147.pdf">ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation</a> <br></li><li>  <a href="https://arxiv.org/pdf/1512.02325.pdf">SSD: Single Shot MultiBox Detector</a> <br></li><li>  <a href="https://arxiv.org/pdf/1510.00149.pdf">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a> <br></li><li>  <a href="https://github.com/opencv/">Opencv github</a> <br></li><li>  <a href="http://opencv.org/">OpenCV official website</a> <br></li><li>  <a href="http://answers.opencv.org/">OpenCV Forum</a> <br></li><li>  <a href="http://halide-lang.org/">Halide</a> <br></li><li>  <a href="http://caffe.berkeleyvision.org/">Caffe</a> <br></li><li>  <a href="https://www.tensorflow.org/">Tensorflow</a> <br></li><li>  <a href="http://torch.ch/">Torch</a> <br></li></ol></div><p>Source: <a href="https://habr.com/ru/post/333612/">https://habr.com/ru/post/333612/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../333602/index.html">Time of miracles, or Brakes for the end of the world</a></li>
<li><a href="../333604/index.html">What's new in ECMAScript 2017 (ES8)</a></li>
<li><a href="../333606/index.html">7 best device farm for mobile app testing</a></li>
<li><a href="../333608/index.html">MBLTdev 2017 is coming. Registration is open</a></li>
<li><a href="../333610/index.html">TamTam: how we made a new messenger</a></li>
<li><a href="../333614/index.html">Why Python Networker</a></li>
<li><a href="../333616/index.html">Transition to embedded PostgreSQL in unit tests</a></li>
<li><a href="../333618/index.html">Solid RealTime on React and Socket.io</a></li>
<li><a href="../333622/index.html">Dereferencing null pointers is no longer a problem.</a></li>
<li><a href="../333624/index.html">Apple will spend a billion on Chinese data center</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>