<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How we reduced the time to develop scoring models five times by switching to Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Now everyone is talking a lot about artificial intelligence and its application in all areas of the company. However, there are some areas where one t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How we reduced the time to develop scoring models five times by switching to Python</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/-z/kh/d3/-zkhd3-g2bfo-piumvjuva0iei4.png" alt="image"><br><br>  Now everyone is talking a lot about artificial intelligence and its application in all areas of the company.  However, there are some areas where one type of model has dominated since ancient times, the so-called ‚Äúwhite box‚Äù - logistic regression.  One such area is bank credit scoring. <br><a name="habracut"></a><br>  There are several reasons for this: <br><br><ul><li>  Regression coefficients can be easily explained in contrast to ‚Äúblack boxes‚Äù like boosting, which can include more than 500 variables. </li><li>  Machine learning is still not credible to management because of the difficulty in interpreting models. </li><li>  There are unwritten requirements of the regulator for the interpretability of models: at any time, for example, the Central Bank may ask for an explanation - why the loan was denied to the borrower </li><li>  Companies use external data mining programs (for example, rapid miner, SAS Enterprise Miner, STATISTICA or any other package), which allow you to quickly learn how to build models, even without programming skills </li></ul><br>  These reasons make it almost impossible to use complex machine learning models in some areas, so it is important to be able to ‚Äúsqueeze the most‚Äù out of a simple logistic regression that is easy to explain and interpret. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In this post, we will describe how, when building scoring, we abandoned external data mining packages in favor of an open source solution in the form of Python, increased development speed several times, and also improved the quality of all models. <br><br><h3>  Scoring process </h3><br>  The classic process of building scoring models on a regression is as follows: <br><br><img src="https://habrastorage.org/webt/jn/e2/da/jne2da4ifmsjuhgui2piws8kbsi.png" alt="image"><br><br>  It may vary from company to company, but the main stages remain constant.  We always need to produce binning variables (as opposed to the machine learning paradigm, where in most cases only categorical coding is needed), eliminating them by the Information Value (IV), and manually unloading all coefficients and bins for DSL integration. <br>  This approach to building scoring cards worked fine in the 90s, but the technology of classical data mining packages is very outdated and does not allow the use of new techniques, such as, for example, L2 regularization in regression, which can significantly improve the quality of models. <br><br>  At one point, as a study, we decided to reproduce all the steps that analysts do in building scoring, supplement them with knowledge of Data Scientists, and also automate the entire process as much as possible. <br><br><h3>  Python Upgrade </h3><br>  As a development tool, we chose Python for its simplicity and good libraries, and began to reproduce all the steps in order. <br><br>  The first step is to collect data and generate variables - this stage is a significant part of the work of analysts. <br><br>  In Python, you can load the collected data from the database using pymysql. <br><br><div class="spoiler">  <b class="spoiler_title">Code to download from the database</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">con</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> conn = pymysql.connect( host=<span class="hljs-string"><span class="hljs-string">'10.100.10.100'</span></span>, port=<span class="hljs-number"><span class="hljs-number">3306</span></span>, user=<span class="hljs-string"><span class="hljs-string">'******* '</span></span>, password=<span class="hljs-string"><span class="hljs-string">'*****'</span></span>, db=<span class="hljs-string"><span class="hljs-string">'mysql'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> conn; df = pd.read_sql(<span class="hljs-string"><span class="hljs-string">''' SELECT * FROM idf_ru.data_for_scoring '''</span></span>, con=con())</code> </pre> <br></div></div><br>  Next, we replace the rare and missing values ‚Äã‚Äãwith a separate category to prevent ovefitting, select the target, delete the extra columns, and also divide by the train and test. <br><br><div class="spoiler">  <b class="spoiler_title">Data preparation</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">filling</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df)</span></span></span><span class="hljs-function">:</span></span> cat_vars = df.select_dtypes(include=[object]).columns num_vars = df.select_dtypes(include=[np.number]).columns df[cat_vars] = df[cat_vars].fillna(<span class="hljs-string"><span class="hljs-string">'_MISSING_'</span></span>) df[num_vars] = df[num_vars].fillna(np.nan) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> df <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">replace_not_frequent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df, cols, perc_min=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">5</span></span></span></span><span class="hljs-function"><span class="hljs-params">, value_to_replace = </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">"_ELSE_"</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> else_df = pd.DataFrame(columns=[<span class="hljs-string"><span class="hljs-string">'var'</span></span>, <span class="hljs-string"><span class="hljs-string">'list'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> cols: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i != <span class="hljs-string"><span class="hljs-string">'date_requested'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> i != <span class="hljs-string"><span class="hljs-string">'credit_id'</span></span>: t = df[i].value_counts(normalize=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) q = list(t[t.values &lt; perc_min/<span class="hljs-number"><span class="hljs-number">100</span></span>].index) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> q: else_df = else_df.append(pd.DataFrame([[i, q]], columns=[<span class="hljs-string"><span class="hljs-string">'var'</span></span>, <span class="hljs-string"><span class="hljs-string">'list'</span></span>])) df.loc[df[i].value_counts(normalize=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)[df[i]].values &lt; perc_min/<span class="hljs-number"><span class="hljs-number">100</span></span>, i] =value_to_replace else_df = else_df.set_index(<span class="hljs-string"><span class="hljs-string">'var'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> df, else_df cat_vars = df.select_dtypes(include=[object]).columns df = filling(df) df, else_df = replace_not_frequent_2(df, cat_vars) df.drop([<span class="hljs-string"><span class="hljs-string">'credit_id'</span></span>, <span class="hljs-string"><span class="hljs-string">'target_value'</span></span>, <span class="hljs-string"><span class="hljs-string">'bor_credit_id'</span></span>, <span class="hljs-string"><span class="hljs-string">'bchg_credit_id'</span></span>, <span class="hljs-string"><span class="hljs-string">'last_credit_id'</span></span>, <span class="hljs-string"><span class="hljs-string">'bcacr_credit_id'</span></span>, <span class="hljs-string"><span class="hljs-string">'bor_bonuses_got'</span></span> ], axis=<span class="hljs-number"><span class="hljs-number">1</span></span>, inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) df_train, df_test, y_train, y_test = train_test_split(df, y, test_size=<span class="hljs-number"><span class="hljs-number">0.33</span></span>, stratify=df.y, random_state=<span class="hljs-number"><span class="hljs-number">42</span></span>)</code> </pre> <br></div></div><br>  Now begins the most important stage in scoring for regression - you need to write WOE-binning for numeric and categorical variables.  In the public domain, we did not find good and suitable options for us and decided to write it ourselves.  They took <a href="https://www.researchgate.net/publication/322520135_Monotone_optimal_binning_algorithm_for_credit_risk_modeling">this</a> article of 2017 as the basis for numerical binning, as well as <a href="https://medium.com/%40sundarstyles89/weight-of-evidence-and-information-value-using-python-6f05072e83eb">this</a> , categorical, they wrote from scratch themselves.  The results were impressive (Gini rose by 3‚Äì5 on the test compared with the binning algorithms of external data mining programs). <br><br>  After that, you can look at the graphs or tables (which we later write to excel), how the variables were broken into groups and check the monotony: <br><br><img src="https://habrastorage.org/webt/da/ij/2u/daij2uewkyujfn3jhdapc-yyfia.png" alt="image"><br><br><img src="https://habrastorage.org/webt/c6/if/3u/c6if3uqy--eqewru4nm9au1gwgw.png" alt="image"><br><br><div class="spoiler">  <b class="spoiler_title">Drawing Bean Charts</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_bin</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(ev, for_excel=False)</span></span></span><span class="hljs-function">:</span></span> ind = np.arange(len(ev.index)) width = <span class="hljs-number"><span class="hljs-number">0.35</span></span> fig, ax1 = plt.subplots(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>)) ax2 = ax1.twinx() p1 = ax1.bar(ind, ev[<span class="hljs-string"><span class="hljs-string">'NONEVENT'</span></span>], width, color=(<span class="hljs-number"><span class="hljs-number">24</span></span>/<span class="hljs-number"><span class="hljs-number">254</span></span>, <span class="hljs-number"><span class="hljs-number">192</span></span>/<span class="hljs-number"><span class="hljs-number">254</span></span>, <span class="hljs-number"><span class="hljs-number">196</span></span>/<span class="hljs-number"><span class="hljs-number">254</span></span>)) p2 = ax1.bar(ind, ev[<span class="hljs-string"><span class="hljs-string">'EVENT'</span></span>], width, bottom=ev[<span class="hljs-string"><span class="hljs-string">'NONEVENT'</span></span>], color=(<span class="hljs-number"><span class="hljs-number">246</span></span>/<span class="hljs-number"><span class="hljs-number">254</span></span>, <span class="hljs-number"><span class="hljs-number">115</span></span>/<span class="hljs-number"><span class="hljs-number">254</span></span>, <span class="hljs-number"><span class="hljs-number">109</span></span>/<span class="hljs-number"><span class="hljs-number">254</span></span>)) ax1.set_ylabel(<span class="hljs-string"><span class="hljs-string">'Event Distribution'</span></span>, fontsize=<span class="hljs-number"><span class="hljs-number">15</span></span>) ax2.set_ylabel(<span class="hljs-string"><span class="hljs-string">'WOE'</span></span>, fontsize=<span class="hljs-number"><span class="hljs-number">15</span></span>) plt.title(list(ev.VAR_NAME)[<span class="hljs-number"><span class="hljs-number">0</span></span>], fontsize=<span class="hljs-number"><span class="hljs-number">20</span></span>) ax2.plot(ind, ev[<span class="hljs-string"><span class="hljs-string">'WOE'</span></span>], marker=<span class="hljs-string"><span class="hljs-string">'o'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'blue'</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Legend plt.legend((p2[0], p1[0]), ('bad', 'good'), loc='best', fontsize=10) #Set xticklabels q = list() for i in range(len(ev)): try: mn = str(round(ev.MIN_VALUE[i], 2)) mx = str(round(ev.MAX_VALUE[i], 2)) except: mn = str((ev.MIN_VALUE[i])) mx = str((ev.MAX_VALUE[i])) q.append(mn + '-' + mx) plt.xticks(ind, q, rotation='vertical') for tick in ax1.get_xticklabels(): tick.set_rotation(60) plt.savefig('{}.png'.format(ev.VAR_NAME[0]), dpi=500, bbox_inches = 'tight') plt.show() def plot_all_bins(iv_df): for i in [x.replace('WOE_','') for x in X_train.columns]: ev = iv_df[iv_df.VAR_NAME==i] ev.reset_index(inplace=True) plot_bin(ev)</span></span></code> </pre> <br></div></div><br>  A separate function was written for manual binning, which is useful, for example, in the case of the variable "OS version", where all phones on Android and iOS were grouped manually. <br><br><div class="spoiler">  <b class="spoiler_title">Manual binning function</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">adjust_binning</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df, bins_dict)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(bins_dict)): key = list(bins_dict.keys())[i] <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> type(list(bins_dict.values())[i])==dict: df[key] = df[key].map(list(bins_dict.values())[i]) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-comment"><span class="hljs-comment">#Categories labels categories = list() for j in range(len(list(bins_dict.values())[i])): if j == 0: categories.append('&lt;'+ str(list(bins_dict.values())[i][j])) try: categories.append('(' + str(list(bins_dict.values())[i][j]) +'; '+ str(list(bins_dict.values())[i][j+1]) + ']') except: categories.append('(' + str(list(bins_dict.values())[i][j])) elif j==len(list(bins_dict.values())[i])-1: categories.append(str(list(bins_dict.values())[i][j]) +'&gt;') else: categories.append('(' + str(list(bins_dict.values())[i][j]) +'; '+ str(list(bins_dict.values())[i][j+1]) + ']') values = [df[key].min()] + list(bins_dict.values())[i] + [df[key].max()] df[key + '_bins'] = pd.cut(df[key], values, include_lowest=True, labels=categories).astype(object).fillna('_MISSING_').astype(str) df[key] = df[key + '_bins']#.map(df.groupby(key + '_bins')[key].agg('median')) df.drop([key + '_bins'], axis=1, inplace=True) return df bins_dict = { 'equi_delinquencyDays': [ 200,400,600] 'loan_purpose': {'medicine':'1_group', 'repair':'1_group', 'helpFriend':'2_group'} } df = adjust_binning(df, bins_dict)</span></span></code> </pre> <br></div></div><br>  The next step is the selection of variables on the Information Value.  The standard value is a cut-off of 0.1 (all variables below do not have good predictive power). <br><br>  After that a correlation check was performed.  Of the two correlating variables, you need to remove the one that has IV less.  Cut-off removal was taken 0.75. <br><br><img src="https://habrastorage.org/webt/sv/7g/f0/sv7gf0nt_7sayq8jgjsjo_bfmpy.png" alt="image"><br><br><div class="spoiler">  <b class="spoiler_title">Removing correlations</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">delete_correlated_features</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df, cut_off=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.75</span></span></span></span><span class="hljs-function"><span class="hljs-params">, exclude = [])</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Create correlation matrix corr_matrix = df.corr().abs() # Select upper triangle of correlation matrix upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)) # Plotting All correlations f, ax = plt.subplots(figsize=(15, 10)) plt.title('All correlations', fontsize=20) sns.heatmap(X_train.corr(), annot=True) # Plotting highly correlated try: f, ax = plt.subplots(figsize=(15, 10)) plt.title('High correlated', fontsize=20) sns.heatmap(corr_matrix[(corr_matrix&gt;cut_off) &amp; (corr_matrix!=1)].dropna(axis=0, how='all').dropna(axis=1, how='all'), annot=True, linewidths=.5) except: print ('No highly correlated features found') # Find index of feature columns with correlation greater than cut_off to_drop = [column for column in upper.columns if any(upper[column] &gt; cut_off)] to_drop = [column for column in to_drop if column not in exclude] print ('Dropped columns:', to_drop, '\n') df2 = df.drop(to_drop, axis=1) print ('Features left after correlation check: {}'.format(len(df.columns)-len(to_drop)), '\n') print ('Not dropped columns:', list(df2.columns), '\n') # Plotting final correlations f, ax = plt.subplots(figsize=(15, 10)) plt.title('Final correlations', fontsize=20) sns.heatmap(df2.corr(), annot=True) plt.show() return df2</span></span></code> </pre> <br></div></div><br>  In addition to the IV selection, we added a recursive search for the optimal number of variables using the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html">RFE</a> method from sklearn. <br>  As we see in the graph, after 13 variables, the quality does not change, which means that the extra ones can be removed.  For regression, more than 15 variables in scoring are considered bad tone, which in most cases is corrected using RFE. <br><br><img src="https://habrastorage.org/webt/4y/sm/2c/4ysm2cgo50qcscs2rigxeksod6y.png" alt="image"><br><div class="spoiler">  <b class="spoiler_title">RFE</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">RFE_feature_selection</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(clf_lr, X, y)</span></span></span><span class="hljs-function">:</span></span> rfecv = RFECV(estimator=clf_lr, step=<span class="hljs-number"><span class="hljs-number">1</span></span>, cv=StratifiedKFold(<span class="hljs-number"><span class="hljs-number">5</span></span>), verbose=<span class="hljs-number"><span class="hljs-number">0</span></span>, scoring=<span class="hljs-string"><span class="hljs-string">'roc_auc'</span></span>) rfecv.fit(X, y) print(<span class="hljs-string"><span class="hljs-string">"Optimal number of features : %d"</span></span> % rfecv.n_features_) <span class="hljs-comment"><span class="hljs-comment"># Plot number of features VS. cross-validation scores f, ax = plt.subplots(figsize=(14, 9)) plt.xlabel("Number of features selected") plt.ylabel("Cross validation score (nb of correct classifications)") plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_) plt.show() mask = rfecv.get_support() X = X.ix[:, mask] return X</span></span></code> </pre> <br></div></div><br>  Next, a regression was constructed and its metrics were evaluated for cross-validation and test sampling.  Usually everyone looks at the Gini coefficient (a good article about him is <a href="https://habr.com/company/ods/blog/350440/">here</a> ). <br><br><img src="https://habrastorage.org/webt/kb/mm/uf/kbmmufj5bxr4jybxad88d1pyggg.png" alt="image"><br><br><div class="spoiler">  <b class="spoiler_title">Simulation results</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_score</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(clf, X_test, y_test, feat_to_show=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">30</span></span></span></span><span class="hljs-function"><span class="hljs-params">, is_normalize=False, cut_off=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.5</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#cm = confusion_matrix(pd.Series(clf.predict_proba(X_test)[:,1]).apply(lambda x: 1 if x&gt;cut_off else 0), y_test) print ('ROC_AUC: ', round(roc_auc_score(y_test, clf.predict_proba(X_test)[:,1]), 3)) print ('Gini: ', round(2*roc_auc_score(y_test, clf.predict_proba(X_test)[:,1]) - 1, 3)) print ('F1_score: ', round(f1_score(y_test, clf.predict(X_test)), 3)) print ('Log_loss: ', round(log_loss(y_test, clf.predict(X_test)), 3)) print ('\n') print ('Classification_report: \n', classification_report(pd.Series(clf.predict_proba(X_test)[:,1]).apply(lambda x: 1 if x&gt;cut_off else 0), y_test)) skplt.metrics.plot_confusion_matrix(y_test, pd.Series(clf.predict_proba(X_test)[:,1]).apply(lambda x: 1 if x&gt;cut_off else 0), title="Confusion Matrix", normalize=is_normalize,figsize=(8,8),text_fontsize='large') display(eli5.show_weights(clf, top=20, feature_names = list(X_test.columns))) clf_lr = LogisticRegressionCV(random_state=1, cv=7) clf_lr.fit(X_train, y_train) plot_score(clf_lr, X_test, y_test, cut_off=0.5)</span></span></code> </pre> <br></div></div><br>  When we are satisfied that the quality of the model suits us, we need to record all the results (regression coefficients, groups of beans, Gini stability graphs and variables, etc.) in excel.  For this, it is convenient to use xlsxwriter, which can work with both data and pictures. <br><br>  Exel sheet examples: <br><br><img src="https://habrastorage.org/webt/um/fg/fn/umfgfnwv9fo7hxo8lqq8fwyppes.png" alt="image"><br><br><img src="https://habrastorage.org/webt/xw/ym/1y/xwym1ynrybf7uhvlagzjrjshz_y.png" alt="image"><br><br><div class="spoiler">  <b class="spoiler_title">Write to excel</b> <div class="spoiler_text"><pre> <code class="python hljs"> <span class="hljs-comment"><span class="hljs-comment">#WRITING writer = pd.ExcelWriter('PDL_Score_20180815-3.xlsx', engine='xlsxwriter') workbook = writer.book worksheet = workbook.add_worksheet('Sample information') bold = workbook.add_format({'bold': True}) percent_fmt = workbook.add_format({'num_format': '0.00%'}) worksheet.set_column('A:A', 20) worksheet.set_column('B:B', 15) worksheet.set_column('C:C', 10) # Sample worksheet.write('A2', 'Sample conditions', bold) worksheet.write('A3', 1) worksheet.write('A4', 2) worksheet.write('A5', 3) worksheet.write('A6', 4) # Model worksheet.write('A8', 'Model development', bold) worksheet.write('A9', 1) #labels worksheet.write('C8', 'Bads') worksheet.write('D8', 'Goods') worksheet.write('B9', 'Train') worksheet.write('B10', 'Valid') worksheet.write('B11', 'Total') # goods and bads worksheet.write('C9', y_train.value_counts()[1]) worksheet.write('C10', y_test.value_counts()[1]) worksheet.write('D9', y_train.value_counts()[0]) worksheet.write('D10', y_test.value_counts()[0]) worksheet.write('C11', y.value_counts()[1]) worksheet.write('D11', y.value_counts()[0]) # NPL worksheet.write('A13', 2) worksheet.write('B13', 'NPL') worksheet.write('C13', (y.value_counts()[1]/(y.value_counts()[1]+y.value_counts()[0])), percent_fmt) worksheet.write('A16', 3) worksheet.write('C15', 'Gini') worksheet.write('B16', 'Train') worksheet.write('B17', 'Valid') worksheet.write('B18', 'CV Scores') worksheet.write('C18', str([round(sc, 2) for sc in scores])) worksheet.write('C16', round(2*roc_auc_score(y_train, clf_lr.predict_proba(X_train)[:,1]) - 1, 3)) worksheet.write('C17', round(2*roc_auc_score(y_test, clf_lr.predict_proba(X_test)[:,1]) - 1, 3)) # Regreesion coefs feat.to_excel(writer, sheet_name='Regression coefficients', index=False) worksheet2 = writer.sheets['Regression coefficients'] worksheet2.set_column('A:A', 15) worksheet2.set_column('B:B', 50) #WOE ivs[['VAR_NAME', 'Variable range', 'WOE', 'COUNT', 'WOE_group']].to_excel(writer, sheet_name='WOE', index=False) worksheet3 = writer.sheets['WOE'] worksheet3.set_column('A:A', 50) worksheet3.set_column('B:B', 60) worksheet3.set_column('C:C', 30) worksheet3.set_column('D:D', 20) worksheet3.set_column('E:E', 12) for num, i in enumerate([x.replace('WOE_','') for x in X_train.columns]): ev = iv_df[iv_df.VAR_NAME==i] ev.reset_index(inplace=True) worksheet3.insert_image('G{}'.format(num*34+1), '{}.png'.format(i)) df3.to_excel(writer, sheet_name='Data', index=False) table.to_excel(writer, sheet_name='Scores by buckets', header = True, index = True) worksheet4 = writer.sheets['Scores by buckets'] worksheet4.set_column('A:A', 20) worksheet4.insert_image('J1', 'score_distribution.png') Ginis.to_excel(writer, sheet_name='Gini distribution', header = True, index = True) worksheet5 = writer.sheets['Gini distribution'] worksheet5.insert_image('E1', 'gini_stability.png') writer.save()</span></span></code> </pre> <br></div></div><br>  The final excel at the end once again looks like management, and then given to IT to embed the model in production. <br><br><h3>  Total </h3><br>  As we have seen, almost all stages of scoring can be automated so that analysts do not need programming skills to build models.  In our case, after creating this framework, the analyst is only required to collect data and specify several parameters (specify the target variable, which columns to delete, the minimum number of bins, the cut-off coefficient for correlation of variables, etc.), after which you can run the python script, who builds a model and gives excel with the desired results. <br>  Of course, sometimes you have to correct the code for the needs of a specific project, and you will not manage to run a single script button while modeling, but even now we see the quality better than the packages used on data mining market thanks to such techniques as optimal and monotonous binning, correlation testing , RFE, regularized version of regression, etc. <br><br>  Thus, thanks to the use of Python, we have significantly reduced the development time for scoring cards, as well as reduced labor costs for analysts. </div><p>Source: <a href="https://habr.com/ru/post/421091/">https://habr.com/ru/post/421091/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../421081/index.html">Too few people pay attention to this economic trend.</a></li>
<li><a href="../421083/index.html">Forgotten art of decorating packaging for video cards</a></li>
<li><a href="../421085/index.html">Ilon Musk is not the future</a></li>
<li><a href="../421087/index.html">How to configure the web application warm on Go for Gitlab on VDS</a></li>
<li><a href="../421089/index.html">Russian providers have figured out how to pass on to Google part of the cost of the ‚ÄúSpring package‚Äù</a></li>
<li><a href="../421093/index.html">How I study the Spring framework - part 2 (help for beginners is the work of the beginners themselves)</a></li>
<li><a href="../421095/index.html">Under the new bill on pre-trial blocking can get 19 million sites</a></li>
<li><a href="../421097/index.html">UIViewController composition and navigation between them (and not only)</a></li>
<li><a href="../421099/index.html">Is it hard to concentrate? Maybe it's not your fault</a></li>
<li><a href="../421101/index.html">"Calendar of the tester" for August. Read the book</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>