<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Data Warehouse Testing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Published on behalf of IvanovAleksey . 



 There is little information on testing the Data Warehouse on the Internet. 
 You can find general requirem...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Data Warehouse Testing</h1><div class="post__text post__text-html js-mediator-article"><p>  Published on behalf of <a href="https://habrahabr.ru/users/ivanovaleksey/" class="user_link">IvanovAleksey</a> . </p><br><img src="https://habrastorage.org/files/f04/b51/ced/f04b51cedfe54f30a75e43e382ea732f.jpg"><br><p>  There is little information on testing the Data Warehouse on the Internet. <br>  You can find general requirements: data completeness, quality, etc. <br>  But nowhere is there a description of the organization of the process, and with what checks it is possible to cover these requirements. <br>  In this article I will try to tell you: how we test the data warehouse in <b><a href="https://habrahabr.ru/company/tcsbank/">Tinkoff Bank</a></b> . </p><a name="habracut"></a><br><h3>  Our Data Warehouse </h3><br><p>  First, I will briefly describe our Vault. <br>  The processes of loading and processing data are implemented as jobs.  Jobs are started on the scheduler usually at night, when no one uses external systems (the load on them is minimal) and after the business day is closed. <br>  The base structure is a set of tables of different sizes (from several rows to 3 billion and more), with different numbers of columns (from 1 to 200), with and without historicity.  The base size is about 35TB. <br>  External data sources are used: Oracle, CSV / XLS files, MySql, Informix, BIN files, CDH - Cloudera (Hadoop).  We upload data to external systems also through Oracle, SAS tables and as CSV / XLS files. <br>  Storage objects (descriptions of databases, jobs, tables, views, etc.) are developed in SAS Data Integration Studio and stored on the SAS server as metadata.  Physically, the tables are in the Greenplum and SAS databases. <br>  To run Jobs, a code is generated by metadata and transferred to the deployment server.  After which they can be run on the scheduler. <br>  Changes to the environment are transferred in the form of packages consisting of metadata and scripts (to create / edit physics, data).  For ease of transfer, the developers have written a special program "Authors". <br>  For manual start of jobs there is a web portal.  There you can also see the running schedulers and the status of the jobs that work on them. </p><br><div class="spoiler">  <b class="spoiler_title">You can read more in our previous articles.</b> <div class="spoiler_text"><p>  <a href="https://habrahabr.ru/company/tcsbank/blog/155763/">SAS integration with Greenplum</a> <br>  <a href="https://habrahabr.ru/company/tcsbank/blog/205454/">Data replication.</a>  <a href="https://habrahabr.ru/company/tcsbank/blog/205454/">Attunity Replicate and Greenplum</a> <br>  <a href="https://habrahabr.ru/company/tcsbank/blog/267733/">Greenplum DB as DWH core</a> </p></div></div><br><h3>  Test objects </h3><br><p>  Any revision of DWH is the creation / modification of the physics and metadata of tables and jobs.  Or it is the correction scripts already loaded data. <br>  For example, a new storefront is being created.  The package will have metadata of the new jobb and target and a script for creating physics of the new table in the database. <br>  Thus, a test object is a modified / created job, data in its target table and data in target tables of dependent jobs (if any). </p><br><h3>  Levels and types of testing </h3><br><p>  Our test loop repeats the productive one completely: the same iron, the same data of the same volume, is loaded and processed using the same processes.  Considering this feature, as well as the fact that tasks are developed when sources already have all the necessary data, it is possible to reduce the volume of checks without loss of quality. <br>  We perform load testing (Performance testing) and testing on large amounts of data (Volume testing) only when we transfer the task to the test: we check the work time of the job, the load on the stand (for example, using <a href="https://habrahabr.ru/company/tcsbank/blog/252907/">Grafana</a> ) and the amount of work (I will not describe these checks in detail in this article). <br>  At the system level, jobs and data quality is checked automatically.  Jobs are monitored by the night planner itself and the script for controlling the volumes of work.  And the data after loading is checked using Data Quality Daemon (about it below).  If something is wrong with the data, the responsible will receive letters with errors. <br>  From the ‚Äúwhite box‚Äù we look only at the correct indication of the environment (there were errors with the hardcode of the test and dev circuits).  In the future, it is planned to check it automatically when the package is published by the developer. <br>  The main ones are functional testing (‚Äúblack box‚Äù) and regression testing, at levels: components and integration. <br>  Taking into account the objects of testing defined in the previous paragraph, the full set of checks looks like this: </p><br><ul><li>  <b>Unit testing</b>  We check the new / modified job and the data in its target table. <br>  <u>Functional testing:</u> we build a prototype and compare it with the values ‚Äã‚Äãin the new / modified columns of the target tables. <br>  <u>Regression testing: we</u> perform a comparison with backup, the data not affected by the revision should be the same. </li><li>  <b>Integration testing</b> .  We check the quality of data at the output of dependent jobs (which have not been changed) and in external systems. <br>  <u>Functional testing: the</u> quality of the data affected by the refinement must comply with the TOR. <br>  <u>Regression testing: the</u> quality of unaffected data refinement should not change. </li></ul><br><p>  Under the "comparison with backup" refers to the reconciliation of the results of the new and previous versions of the job.  That is, the old and new jobs are run on the same data.  And their target tables are compared. <br>  A ‚Äúprototype‚Äù is a data set collected from a TOR, and which should be in the storefront after revision.  This may be the layout of either a completely new table, or just changed columns in the old table. <br>  Depending on the task, some of these checks may be redundant.  By determining the types of improvements, you can get rid of redundant checks and reduce the testing time. </p><br><h3>  Types of changes and checks </h3><br><p>  The Data Warehouse project in the bank is constantly evolving - new windows are being created, old windows are being finalized, and loading processes are being optimized.  But in reality, all tasks can be divided into four groups with their own set of tests: </p><br><ol><li>  <b>Technical.</b> <br>  Optimization, migration, etc.  - that is, a task where the algorithm does not change.  And the data in the target table should not change either. <br>  It is enough to perform a regression check: compare the target of the modified job with the backup.  You can not check dependent Jobs, because  if the target matches the backup, the dependent job will also process it. </li><li>  <b>Change the old functionality.</b> <br>  The algorithm changes, filters (the number of lines changes), new fields are added, sources.  That is, the data set in the target table is changing. <br>  All checks should be performed: compare data in the target table of the modified job with backup and prototype, check the quality of the data in the target tables of dependent jobs and external systems. </li><li>  <b>Development of new windows.</b> <br>  New tables and Jobs are created that load them. <br>  We perform only functional testing: we compare target tables with prototypes. <br>  If the upload goes to the external system, we additionally check the integration - we look at how the loaded data is displayed in the external system. </li><li>  <b>Edit data.</b> <br>  Delete duplicate, old records, fix versioning, write correct values. <br>  Checking these changes is quite complicated and it will not work in two sentences.  I will tell you in detail in the next article. </li></ol><br><p>  If within the project / task there are several types of changes, then in the test set we take checks for each of them. </p><br><p>  These checks are sufficient to ensure that the requirements and the results of development are met in most tasks. </p><br><h3>  Blitz check </h3><br><p>  Building a prototype and performing the comparison can take a long time (depending on the performance of the environment and the amount of data).  We encountered this problem when the test circuit was weaker than the productive one.  In order not to waste time on comparison and immediately catch critical defects, quick checks were used. <br>  Suitable for any type of task and performed before testing: </p><br><ul><li>  The task came to the test and the job executes (yes, it is sometimes forgotten to check) </li><li>  There should not be null and duplicates by key (unless otherwise indicated in the statement of work).  Versioning of the table must be respected. </li><li>  New fields receive data: select count (new_field) from table1 must be greater than 0. </li><li>  New records are loaded into the table: compare the number of records in backup and target. </li></ul><br><p>  If they failed, you can immediately return the task to the development / start defect Critical. </p><br><h3>  Instruments </h3><br><p>  The main actions during testing are: rolling tasks onto a test, comparing tables. <br>  As already said, the transfer uses its own development, the program "Authors".  That allows you to save time and avoid errors during manual transfer. </p><br><p>  The work of the program looks like this: </p><br><ul><li>  Removed backups of physics tables and metadata of modified objects. </li><li>  Running scripts that must run before importing new metadata </li><li>  Import metadata. </li><li>  Scripts are executed, which must be executed after the metadata is imported and before the jobs are launched on the scheduler. </li><li>  Deploy job and run on the scheduler. </li><li>  Scripts are executed that must be executed after the job is completed. </li></ul><br><p>  Runs through the console.  The task number, environment name and additional parameters (for example, pause after each step) are input. </p><br><p>  To compare the tables (target with prototype / backup), a macro is used which compares the values ‚Äã‚Äãin the rows to the specified key and compares the occupancy of the fields. <br>  The macro receives the table names and the comparison key as input. <br>  Example of the result of the work: </p><br><p>  <em>The number of differences in columns.</em>  <em>See the differences for yourself.</em> </p><br><table><thead><tr><th>  Obs </th><th>  column_name </th><th>  differ_base_to_comp </th></tr></thead><tbody><tr><td>  one </td><td>  column_1 </td><td>  0 </td></tr><tr><td>  2 </td><td>  column_2 </td><td>  20 </td></tr><tr><td>  3 </td><td>  column_3 </td><td>  0 </td></tr></tbody></table><br><p>  <em>The number of groupings in _cd and _flg fields.</em> </p><br><table><thead><tr><th>  Obs </th><th>  column_name </th><th>  column_group </th><th>  base_groups </th><th>  compare_groups </th><th>  diff </th><th>  base_group_pct </th><th>  compare_group_pct </th><th>  diff_group_pct </th></tr></thead><tbody><tr><td>  one </td><td>  column_2 </td><td>  A </td><td>  18743 </td><td>  63 </td><td>  18680 </td><td>  0.0021 </td><td>  0.0024 </td><td>  -0.0003 </td></tr><tr><td>  2 </td><td>  column_2 </td><td>  B </td><td>  4451740 </td><td>  17756 </td><td>  4433984 </td><td>  0.4897 </td><td>  0.6877 </td><td>  -0.1980 </td></tr><tr><td>  3 </td><td>  column_2 </td><td>  C </td><td>  4619311 </td><td>  7813 </td><td>  4611498 </td><td>  0.5082 </td><td>  0.3026 </td><td>  0.2056 </td></tr><tr><td>  four </td><td>  column_2 </td><td>  null </td><td>  191 </td><td>  188 </td><td>  3 </td><td>  0.0000 </td><td>  0.0073 </td><td>  -0.0073 </td></tr></tbody></table><br><p>  To check the quality of the data, a profiling macro is used.  Which counts the number and percentage of entries with null for each column, duplicates and null by key, rows in grouping by flags and values, min / max / avg by sums in columns. <br>  The input is the name of the table and key. <br>  At the output we get a report, with signs for each of the calculations. <br>  Example: </p><br><p>  <em>The number of missings in columns.</em> </p><br><table><thead><tr><th>  Obs </th><th>  column_name </th><th>  base_nulls </th><th>  nulls_pct </th></tr></thead><tbody><tr><td>  one </td><td>  column_1 </td><td>  0 </td><td>  0.00 </td></tr><tr><td>  2 </td><td>  column_2 </td><td>  0 </td><td>  0.00 </td></tr><tr><td>  3 </td><td>  column_3 </td><td>  7 </td><td>  0.03 </td></tr><tr><td>  four </td><td>  column_4 </td><td>  0 </td><td>  0.00 </td></tr><tr><td>  five </td><td>  column_5 </td><td>  0 </td><td>  0.00 </td></tr></tbody></table><br><p>  There is also a macro for comparing the two different table profiling between each other (or with a table on the productive level).  The principle of operation is the same: profiling is performed for each table, and the results are compared. <br>  The output report is similar to ordinary profiling, with only data from two tables. </p><br><h3>  Data quality control </h3><br><p>  To control the quality of data in the entire repository, a self-signed Data Quality Daemon (DQD) is used, which checks all tables for compliance with the rules compiled by analysts and specialists of the data quality control department. <br>  DQD is a process that every 10 minutes finds updated tables and executes specified SQL queries.  It compares results with benchmarks (presets) and sends a report with deviations. <br>  Report example: </p><br><table><thead><tr><th>  Constraint Definition </th><th>  SQL Script </th><th>  Corrupted Rows Cnt </th></tr></thead><tbody><tr><td>  test_schema.table1 / Unique Entity Key [id] </td><td>  select sum (cnt) as cnt from (select 0 as cnt union all select 1 as cnt from test_schema.table1 group by id having count (*)&gt; 1 union all select 1 as cnt from test_schema.table1 where not ((id) is not null)) sq </td><td>  15 </td></tr></tbody></table><br><h3>  Making test cases </h3><br><p> In our bank, testing lives in Zephyr (Jira add-on).  The tasks for finalization are issued as tickets in Jira, and test cases and test cases in Zephyr. <br>  Having tried several options, we stopped at the fact that we are starting a case for each changed job.  We call the case: ‚Äú&lt;task number in jira&gt;: &lt;job name&gt;‚Äù.  And link to the ticket. <br>  The main advantages of this approach are: </p><br><ol><li>  in the task you can see case coverage (which Jobs will be checked) </li><li>  you can easily calculate the percentage of run / pass / failed </li><li>  A simple search by job name returns all the written cases, their status, who wrote it and when, performed and for what task. </li><li>  Again, from the name of the case, you can find out the number of the task for revision.  And opening it to go into it on the link. </li></ol><br><h3>  Conclusion </h3><br><p>  Testing DWH - a difficult process with its own specifics.  If you adhere to the classical methodology, it turns out to be very cumbersome. <br>  Our approach allows you to test quickly (on average, one tester does the task in three days) and the number of missed errors is reduced to zero.  For six months, more than 400 tasks were brought to the production. <br>  We are not going to stop on our laurels.  The next goal is the automation of most checks. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/302670/">https://habr.com/ru/post/302670/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../302660/index.html">Asus automatically updates BIOS / UEFI over HTTP without verification</a></li>
<li><a href="../302662/index.html">Regular Expression Crossword Algorithm</a></li>
<li><a href="../302664/index.html">Angler EK exploit kit specializes in circumventing EMET security mechanisms</a></li>
<li><a href="../302666/index.html">Google Developers Agency Pro: certification for the best developers of Android applications</a></li>
<li><a href="../302668/index.html">Interview with Baruch Sadogursky: the perfect stack of technologies for enterprise development</a></li>
<li><a href="../302672/index.html">Analysis of the tasks of the third qualifying round of the RCC 2016</a></li>
<li><a href="../302674/index.html">Machine Learning Boot Camp - how it was and how it will be</a></li>
<li><a href="../302676/index.html">Docker meetup in badoo</a></li>
<li><a href="../302678/index.html">RegionSoft CRM: a business that works for business</a></li>
<li><a href="../302680/index.html">Combining 3CX Phone System with Asterisk (FreePBX)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>