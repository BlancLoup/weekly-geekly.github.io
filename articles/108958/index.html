<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Accelerating the distribution of pictures</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The problem of slow return of static content every sysadmin sooner or later faces. 

 This appears approximately as follows: sometimes a 3Kb picture i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Accelerating the distribution of pictures</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/593/d3c/fe3/593d3cfe3facae127bd9c3d8be3e6ba5.jpg" height="270" width="253" align="left"><br>  The problem of slow return of static content every sysadmin sooner or later faces. <br><br>  This appears approximately as follows: sometimes a 3Kb picture is loaded as if it weighs 3Mb, out of the blue they start to ‚Äústick‚Äù (give up very slowly) css and javascript.  You press ctrl + reload - and, it seems, there is no problem, then after only a few minutes everything repeats again. <br><br>  The true cause of the ‚Äúbrakes‚Äù is not always obvious, and we look askance at either nginx, or at the hoster, or at the ‚Äúclogged‚Äù channel, or at the ‚Äúbrake‚Äù or ‚Äúbuggy‚Äù browser :) 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In fact, the problem is the imperfection of the modern hard drive, which has not yet parted with the mechanical subsystems of spindle rotation and head positioning. <br><br>  In this article I will offer you my solution to this problem, based on practical experience of using SSD drives in conjunction with the nginx web server. <br><a name="habracut"></a><br><br><h2>  How to understand that hard brakes? </h2><br>  In Linux, problems with the speed of the disk system are directly related to the <b>iowait</b> parameter (the percentage of CPU idle while waiting for I / O operations).  In order to monitor this parameter there are several commands: <b>mpstat</b> , <b>iostat</b> , <b>sar</b> .  I usually run <b>iostat 5</b> <i>(measurements will be taken every 5 sec.)</i> <br>  I am calm for the server, whose average <b>iowait is</b> up to <b>0.5%</b> .  Most likely on your server "distribution" this parameter will be higher.  It makes sense not to postpone optimization, if <b>iowait&gt; 10%</b> Your system spends a lot of time moving the heads through the hard drive instead of reading information, this can lead to "braking" and other processes on the server. <br><br><h2>  How to deal with big iowait? </h2><br>  Obviously, if you reduce the number of disk I / O operations, the hard drive is lighter and iowait will fall. <br>  Here are some recommendations: <br><ul><li>  Disable <b>access_log</b> </li><li>  Turn off updating the date of the last access to the file and directory, and also allow the system to cache disk writes.  To do this, mount the file system with the following options: <b>async, noatime, barrier = 0</b> .  <i>('barrier = 0' unjustified risk, if the database is on the same section)</i> </li><li>  You can increase the timeout between flushing dirty buffers <b>vm.dirty_writeback_centisecs</b> in <b>/etc/sysctl.conf</b> .  I have installed <b>vm.dirty_writeback_centisecs = 15000</b> <br></li><li>  Did you happen to forget about the <a href="http_headers_module.html">expires max</a> directive? </li><li>  It will not be superfluous to turn on the <a href="http_core_module.html">caching of file descriptors</a> . </li><li>  Application of client optimization: css sprites, all css in one file, all js in one file </li></ul><br>  This will help a bit and will give time to last until the upgrade.  If the project grows, <b>iowait</b> will remind you soon.  :) <br><br><h2>  Upgrade iron </h2><br><ul><li>  <b>We reinstall RAM</b> <br>  Perhaps you can start with RAM.  Linux uses all the ‚Äúfree‚Äù RAM and places the disk cache there. </li><li>  <b>Old verified RAID</b> <br>  You can build software or hardware RAID from multiple HDDs.  In some cases, it makes sense to increase the number of hard drives, but do not collect them in a RAID (for example, distributing iso-disk images, large video files, ...) </li><li>  <b>Solid-state drive: try something new</b> <br>  Well, in my opinion, the cheapest version of the upgrade is to install one or several SSD disks into the system.  Today, as you have already guessed, it will be about this method of acceleration. </li></ul><br>  Absolutely will not affect the speed of distribution of the upgrade CPU, because it does not slow down!  :) <br><br><h2>  Why SSD </h2><br>  A year and a half ago, when I wrote the article <a href="http://habrahabr.ru/blogs/nginx/56497/">‚ÄúTuning nginx‚Äù</a> , one of the nginx acceleration options I suggested was the use of SSD hard drives.  <a href="http://habrahabr.ru/blogs/nginx/56497/">Habrasoobshchestvu restrained showed interest in this technology</a> , there was information about the possible inhibition of SSD over time and fear for a small number of rewriting cycles. <br>  Very soon after the publication of the article in our company appeared Kingston SNE125-S2 / 64GB based on <a href="http://www.thg.ru/storage/intel_x25-e/index.html">Intel x25e SSD,</a> which is still used on one of the most <a href="http://www.thg.ru/storage/intel_x25-e/index.html">heavily</a> distributed servers. <br><br>  After a year of experiments, a number of flaws emerged, which I would like to tell you: <ul><li>  Advertising trick: if the SSD ad says that the <i>maximum reading speed, 250 MB / s</i> , then this means that the average reading speed will be ~ 75% (~ 190Mb / s) of the stated maximum.  I was so with MLC and SLC, expensive and cheap </li><li>  The larger the volume of one SSD, the higher the cost of 1 MB on this disk </li><li>  Most file systems are not adapted for use on SSDs and can create an uneven write load on the disk. </li><li>  Only the most modern (and, accordingly, the most expensive) RAID controllers are adapted to connect SSDs to them. </li><li>  SSD is still expensive technology </li></ul><br>  Why I use SSD: <ul><li>  Advertising does not lie - seek-to-seek really tends to 0. That can significantly reduce <b>iowait</b> with a parallel "distribution" of a large number of files. </li><li>  Yes, indeed, the number of rewriting cycles is limited, but we know about it and can minimize the amount of rewritable information using the technique described below </li><li>  Now drives are available that use SLC (Single-Level Cell) technology with a ‚Äúsmart‚Äù controller, the number of rewriting cycles that are an order of magnitude higher than the usual MLC SSD </li><li>  Modern file systems (for example, <a href="http://ru.wikipedia.org/wiki/Btrfs">btrfs</a> ) already know how to work properly with SSD </li><li>  As a rule, a caching server requires a small amount of cache space (we have 100-200G), which can fit on 1 SSD.  It turns out that it is significantly cheaper than a solution based on a hardware RAID array with several SAS disks. </li></ul><br><br><h2>  Configuring SSD cache </h2><br>  <b>File system selection</b> <br>  At the beginning of the experiment, ext4 was installed on the Kingston SNE125-S2 / 64GB.  On the Internet, you will find many recommendations on how to ‚Äúchop off‚Äù logging, the last file access dates, etc.  Everything worked perfectly and for a long time.  The most important thing that didn‚Äôt suit was that with a large number of small photographs 1-5K on 64G SSD less than half was placed - ~ 20G.  I began to suspect that my SSD is not being used rationally. <br><br>  Upgraded the kernel to 2.6.35 and decided to try (still experimental) btrfs, there is an opportunity to specify when mounting that ssd is mounted.  The disk can not be divided into sections, as is customary, but format as a whole. <br><br>  Example: <br><pre><code class="bash hljs">mkfs.btrfs /dev/sdb</code> </pre> <br>  When mounting, you can disable many features that we do not need and enable compression of files and metadata.  (In fact, jpeg-and will not be compressed, btrfs smart, only metadata will be compressed).  Here is what my <b>fstab</b> mount line looks like (all in one line): <br><br>  UUID = 7db90cb2-8a57-42e3-86bc-013cc0bcb30e / var / www / ssd btrfs device = / dev / sdb, device = / dev / sdc, device = / dev / sdd, noatime, ssd, nobarrier, compress, nodatacow, nodatasow , noacl, notreelog 1 2 <br><br>  You can get the formatted disk UUID using the command: <br><pre> <code class="bash hljs">blkid /dev/sdb</code> </pre> <br><br>  As a result, the disc ‚Äúgot into‚Äù more than 41G (2 times more than on ext4).  At the same time, the speed of distribution did not suffer (since iowait did not increase). <br><br>  <b>We collect RAID from SSD</b> <br>  The moment came when 64G SSD was not enough, I wanted to collect several SSDs into one large section and at the same time there was a desire to use not only expensive SLCs, but also ordinary MLC SSDs.  Here you need to insert a bit of theory: <br><br>  <i>Btrfs saves 3 types of data on a disk: data about the file system itself, addresses of metadata blocks (there are always 2 copies of metadata on the disk) and, in fact, the data itself (file contents).</i>  <i>Experimentally, I found that in our directory structure ‚Äúcompressed‚Äù metadata occupies ~ 30% of all data in the section.</i>  <i>Metadata is the most intensely variable block, since</i>  <i>any addition of a file, transfer of a file, change of access rights entails overwriting a block of metadata.</i>  <i>The area where the data is stored is simply overwritten less often.</i>  <i>Here we come to the most interesting possibility of btrfs: it is to create software RAID-masyvy and explicitly indicate on which drives to save data on which metadata.</i> <br><br>  Example: <br><pre> <code class="bash hljs">mkfs.btrfs -m single /dev/sdc -d raid0 /dev/sdb /dev/sdd</code> </pre> <br>  as a result, the metadata will be created on / dev / sdc and the data on / dev / sdb and / dev / sdd, which will be collected in the stripped raid.  Moreover, <a href="https://btrfs.wiki.kernel.org/index.php/Using_Btrfs_with_Multiple_Devices">you can connect more disks</a> to the existing system <a href="https://btrfs.wiki.kernel.org/index.php/Using_Btrfs_with_Multiple_Devices">, perform data balancing, etc.</a> <br><br>  To find out the UUID btrfs RAID-run: <br><pre> <code class="bash hljs">btrfs device scan</code> </pre> <br>  <i>Attention: feature of working with btrfs-rayd: before each mount the RAID array and after loading the btrfs module it is necessary to run the command: <b>btrfs device scan</b> .</i>  <i>To automatically mount via fstab, you can do without 'btrfs device scan' by adding the <b>device</b> options to the mount line.</i>  <i>Example:</i> <br><pre> <code class="bash hljs">/dev/sdb /mnt btrfs device=/dev/sdb,device=/dev/sdc,device=/dev/sdd,device=/dev/sde</code> </pre> <br><br><h2>  Caching on nginx without proxy_cache </h2><br>  I assume that you have a storage-server on which all the content is located, there is a lot of space on it and the usual "floppy" SATA hard drives that are not able to hold a large share of access. <br>  Between the storage server and site users, there is a ‚Äúdistribution‚Äù server, the task of which is to take the load off the storage server and ensure uninterrupted distribution of statics to any number of clients. <br><br>  Install one or more SSDs with btrfs on board to the distribution server.  This is where the proxy_cache-based nginx configuration comes to mind.  But she has a few drawbacks for our system: <br><ul><li>  at each restart, progin_cache begins to gradually scan the entire contents of the cache.  For several hundred thousand, this is perfectly acceptable, but if we put a large number of files in the cache, then this behavior of nginx is an unjustified expenditure of disk operations </li><li>  for proxy_cache, there is no native cache ‚Äúcleansing‚Äù system, and third-party modules allow cleaning the cache only one file at a time. </li><li>  there is a small overhead in terms of CPU consumption, since  at each return, MD5 hashing is performed on the line specified in the proxy_cache_key directive </li><li>  But the most important thing for you is that proxy_cache does not care about updating the cache with the least amount of information rewriting cycles.  If the file ‚Äúflies‚Äù out of the cache, then it is deleted and, if it is requested again, it is re-recorded in the cache. </li></ul><br>  We will take another approach to caching.  The idea flashed on one of the conferences on hiload.  Create 2 cache0 and cache1 directories in the cache section.  When proxying, all files are saved in cache0 (using proxy_store).  nginx make the file check (and give the file to the client) first in cache0 and then in cache1 and if the file is not found, go to the storage server behind the file, then save it to cache0. <br>  After some time (week / month / quarter), delete cache1, rename cache0 to cache1, and create an empty cache0.  We analyze the logs of access to the cache1 section and those files that are requested from this section are interlinked into cache0. <br><br>  This method allows to significantly reduce write operations on SSD, since  file relinking is still less than full file overwriting.  In addition, you can collect a raid of several SSDs, 1 of which will be SLC for metadata and MLC SSD for regular data.  <i>(On our system, metadata takes up about 30% of the total data)</i> .  When relinking, only metadata will be overwritten! <br><br>  <b>Nginx configuration example</b> <br><pre> <code class="bash hljs">log_format cache0 <span class="hljs-string"><span class="hljs-string">'$request'</span></span>; <span class="hljs-comment"><span class="hljs-comment"># ... server { expires max; location / { root /var/www/ssd/cache0/ ; try_files $uri @cache1; access_log off; } location @cache1 { root /var/www/ssd/cache1; try_files $uri @storage; access_log /var/www/log_nginx/img_access.log cache0; } location @storage { proxy_pass http://10.1.1.1:8080/$request_uri; proxy_store on; proxy_store_access user:rw group:rw all:r; proxy_temp_path /var/www/img_temp/; #    SSD! root /var/www/ssd/cache0/; access_log off; } # ...</span></span></code> </pre><br><br>  <b>Scripts for cache0 and cache1 rotation</b> <br>  I wrote <a href="">several scripts on bash</a> to help you implement the previously described rotation scheme.  If the size of your cache is measured in hundreds of gigabytes and the amount of content in the cache is in millions, then it is advisable to run the <b>ria_ssd_cache_mover.sh</b> script several times in a row after the rotation with the following command: <br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> `seq 1 10`; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> ria_ssd_cache_mover.sh; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span>;</code> </pre>  Time for which this command will be executed install experimentally.  She worked for me almost a day.  On the next.  day set launch ria_ssd_cache_mover.sh on cron every hour. <br><br>  <b>DOS protection and storage server</b> <br>  If the storage server is hilovat and there are ill-wishers thirsting for your system, you can use the <a href="http_secure_link_module.html">secure_link module</a> together with the described solution <a href="http_secure_link_module.html">.</a> <br><br><h2>  useful links </h2><br><ul><li>  <a href="https://btrfs.wiki.kernel.org/index.php/Main_Page">The most complete btrfs documentation (eng)</a> </li><li>  <a href="http://xgu.ru/wiki/Btrfs">About btrfs in Russian</a> </li><li>  Well complements the theme of the <a href="http://zfconf.org.ua/conf-2010/topics/static-and-zend_file-starlook-experience/">report of Cyril Mokevnin "Storage, processing and return of statics"</a> </li></ul><br><br>  <b>UPD1:</b> Still, I advise you to use the kernel&gt; = <b>2.6.37</b> and <b>later</b> , because  I recently had a large crash cache at 2.6.35 due to an overflow of space on the SSD with metadata.  As a result, the incoming formatted several SSDs and reassembled btrfs-raid.  :( <br><br></div><p>Source: <a href="https://habr.com/ru/post/108958/">https://habr.com/ru/post/108958/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../108948/index.html">Writing Magic 8-Ball for Android</a></li>
<li><a href="../108950/index.html">Migrating physical Linux servers to Microsoft Hyper-V hypervisor virtual environment</a></li>
<li><a href="../108951/index.html">We draw a fluorescent acrylic on a solid surface</a></li>
<li><a href="../108955/index.html">How to trick NET.Reflector</a></li>
<li><a href="../108957/index.html">Modified firmware ports from Nokia C6 to 5530/5800</a></li>
<li><a href="../108960/index.html">A new version of Dropbox is being prepared.</a></li>
<li><a href="../108961/index.html">Heuristic investment portfolio generation algorithms</a></li>
<li><a href="../108963/index.html">Everyone in childhood mocked the "soldiers" with a magnifying glass?</a></li>
<li><a href="../108965/index.html">Debian GNU / Linux 5.0.7 released</a></li>
<li><a href="../108967/index.html">Ethernet wiring, TP-Link router and OnLime provider</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>