<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neuroplasticity in artificial neural networks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr, long time no see. In this post I would like to talk about such a relatively new concept in machine learning, like transfer learning . Since ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neuroplasticity in artificial neural networks</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/5e6/c14/474/5e6c14474d2c47a28d1688a28ab00c6f.jpg" align="right" width="320">  Hi, Habr, long time no see.  In this post I would like to talk about such a relatively new concept in machine learning, like <a href="http://en.wikipedia.org/wiki/Inductive_transfer">transfer learning</a> .  Since I have not found any well-established translation of this term, the title of the post also contains a different, but close in meaning, term that seems to be a biological prerequisite for formalizing the theory of knowledge transfer from one model to another.  So, the plan is this: for a start, consider the biological prerequisites;  after touching on the difference of transfer learning from a very similar idea of <a href="http://habrahabr.ru/post/163819/">pre-</a> learning of a <a href="http://habrahabr.ru/post/163819/">deep neural network</a> ;  and at the end we will discuss the real problem of semantic image hashing.  To do this, we will not be shy and take the deep (19 layers) convolutional neural network of the winners of the <a href="http://image-net.org/challenges/LSVRC/2014/results">imagenet</a> competition <a href="http://image-net.org/challenges/LSVRC/2014/results">in 2014</a> in the section ‚ÄúLocalization and Classification‚Äù ( <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">Visual Geometry Group</a> , University of Oxford), make it a little trepanning, extract some of the layers and use them in our purposes.  Go. <br><a name="habracut"></a><br><h1>  Neuroplasticity </h1><br>  To begin, consider the definition.  Neuroplasticity is the property of the brain to change under the influence of experience or after injury.  Changes include both the creation of <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B8%25D0%25BD%25D0%25B0%25D0%25BF%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D0%25BF%25D0%25BB%25D0%25B0%25D1%2581%25D1%2582%25D0%25B8%25D1%2587%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C">synaptic connections</a> and the <a href="https://ru.wikipedia.org/wiki/%25D0%259D%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25B3%25D0%25B5%25D0%25BD%25D0%25B5%25D0%25B7">creation of new neurons</a> .  Until relatively recently, until the 70s of the twentieth century, it was believed that part of the brain, in particular the <a href="https://ru.wikipedia.org/wiki/%25D0%259D%25D0%25BE%25D0%25B2%25D0%25B0%25D1%258F_%25D0%25BA%25D0%25BE%25D1%2580%25D0%25B0">neocortex</a> (and this is just the entire motility, speech, thinking, etc.), remained static after a certain period of maturation, only forces of connections between neurons.  But later, more thorough studies confirmed the presence of neuroplasticity of the brain as a whole.  I suggest to look at a small video: <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/Sg4Y2gyh3wY%3Ffeature%3Doembed&amp;xid=25657,15700002,15700019,15700186,15700190,15700253&amp;usg=ALkJrhhomC8oUE4MGyqjIfs4OfCnsLUzGg" frameborder="0" allowfullscreen=""></iframe><br><br>  To truly experience the full power of our brain, let's take a look at the experiment of the neurophysiologist <a href="http://en.wikipedia.org/wiki/Paul_Bach-y-Rita">Paul Bach-y-Rita</a> , whose work has largely influenced the recognition of neuroplasticity by the scientific community.  An important factor influencing the scientist‚Äôs motivation was that his father was paralyzed.  Together with their brother-physicist, they were able to raise their father to his feet by the age of 68, which allowed him to even engage in extreme sports.  Their story has shown that even at a later age, the human brain is capable of rehabilitation.  But this is a completely different story, back to the experience of 1969.  The goal was serious: to give blind people (even from birth) the opportunity to see.  For this, a dental chair was taken and re-equipped as follows: a television camera was placed next to the chair, and a manipulator was brought to the chair, which allowed changing the scale and position of the camera.  In the back of the chair 400 stimulants were built in, which formed a grid: the image coming from the camera was compressed to a size of 20 by 20;  The stimulants were located 12 mm apart.  A small millimeter tip was attached to each stimulator, which vibrated in proportion to the current supplied to the solenoid located inside the stimulator. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/5f1/66e/208/5f166e208047488fbceea36c322b4875.jpg"><br><br>  With the help of an oscilloscope it was possible to visualize the image created by the vibration of stimulants. <br><br><img src="https://habrastorage.org/files/11e/906/049/11e906049a6a4329b124adebb0dbffd5.jpg"><br><br>  A year later, Paul developed a mobile version of his system: <br><div class="spoiler">  <b class="spoiler_title">1970s mobile visor</b> <div class="spoiler_text">  The man looks like Mavrodi, but this is not him. <br><br><img src="https://habrastorage.org/files/0bb/4f5/b0b/0bb4f5b0bc9e4daba038f462a2d37f1f.jpg"><br></div></div><br>  Nowadays, instead of tactile signal transmission, a ‚Äúshort‚Äù path is used through a more sensitive organ - the language.  As they write in the articles, a few hours are enough to begin to perceive the image by the receptors of the tongue. <br><div class="spoiler">  <b class="spoiler_title">modern option</b> <div class="spoiler_text">  Think he eats noodles?  But no, he looks with his tongue. <br><br><img src="https://habrastorage.org/files/20b/45d/157/20b45d1579474b3cb948e19ed93d855b.jpg"><br><br><img src="https://habrastorage.org/files/a1c/919/090/a1c919090ccf4e8c8d4367342e271d17.jpg"><br><br>  And it is not surprising that they <a href="http://www.wicab.com/en_eu/v100.html">are trying to monetize</a> this <a href="http://www.wicab.com/en_eu/v100.html">discovery</a> : <br><img src="https://habrastorage.org/files/71e/956/410/71e9564106a14f36b1376d7caacf2f4e.jpg"><br></div></div><br>  So how does it work?  Speaking in simple language (not biologists and neurophysiologists, but rather in the language of data analysis), neurons are trained to efficiently extract features and draw conclusions on them.  <a href="https://en.wikipedia.org/wiki/Sensory_substitution">Replacing the usual signal with some other one</a> , our neural network still extracts good low-level features (for example, computer vision, given below, these will be different gradient transitions or patterns) on layers that are located close to the sensor (eye, tongue, ear, etc.). d.).  Deeper layers try to extract high-level signs (wheel, window).  And probably, if we look for a wheel from a car in a sound signal, most likely we will not find it.  Unfortunately, we will not be able to see which signs are extracted by neurons, but thanks to the article " <a href="http://ftp.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf">Visualizing and Understanding Convolutional Networks</a> " it is possible to look at the low-level and high-level signs extracted by the deep convolutional neural network.  This, of course, is not a biological network, and perhaps everything is wrong in reality.  At a minimum, it gives some kind of intuitive understanding of the reasons for seeing even the receptors of the tongue. <br><br>  The large picture in the spoiler shows several features of each convolutional layer and parts of the original images that activate them.  As you can see, the deeper, the less abstract the signs become. <br><br><div class="spoiler">  <b class="spoiler_title">Fully trained model</b> <div class="spoiler_text">  Each gray square corresponds to a visualization of the filter (which is used for convolution) or the weights of one neuron, and each color image is the part of the original image that activates the corresponding neuron.  For clarity, neurons within a single layer are grouped into subject groups. <br><br><img src="https://habrastorage.org/files/812/051/b71/812051b7114040dca73543223477b9bc.png"><br></div></div><br>  It can be assumed that, replacing the sensors, our brain does not need to completely retrain all neurons, it is enough to retrain only those that extract the high-level signs ‚Äî and the rest are already able to extract quality features.  Practice shows that this is approximately what it is.  It is unlikely that in a couple of hours of walking with a plate in the tongue, the brain has time to remove and then grow new synaptic connections without losing the ability to feel the taste of food by the receptors of the tongue. <br><br>  And now let's remember how the history of artificial neural networks began.  In 1949, Donald Hebb publishes the book <a href="https://ru.wikipedia.org/wiki/The_Organization_of_Behavior">The Organization of Behavior</a> , in which he describes the <a href="http://en.wikipedia.org/wiki/Hebbian_theory">first principles of ANN training</a> .  It is worth noting that modern learning algorithms are not far from these principles. <br><ul><li>  if nearby neurons are activated <i>synchronously</i> , the connection between them is <i>enhanced</i> ; </li><li>  if nearby neurons are activated <i>asynchronously</i> , the connection between them <i>weakens</i> (in fact, Hebb did not have this rule, he added it as an addition later). </li></ul><br>  After nine years, <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25BE%25D0%25B7%25D0%25B5%25D0%25BD%25D0%25B1%25D0%25BB%25D0%25B0%25D1%2582%25D1%2582,_%25D0%25A4%25D1%2580%25D1%258D%25D0%25BD%25D0%25BA">Frank Rosenblatt</a> creates the first model for teaching with a teacher, the <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D1%2586%25D0%25B5%25D0%25BF%25D1%2582%25D1%2580%25D0%25BE%25D0%25BD">perceptron</a> .  The author of the first artificial neural network did not pursue the goal of creating a universal approximator.  He was a neurophysiologist, and his task was to create a device that could be trained like a human being.  Just take a look at what journalists write about the perceptron.  IMHO, this is charming: <br><br><div class="spoiler">  <b class="spoiler_title">New York Time, July 8, 1958</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/34b/e3d/156/34be3d156aab4e8681138e361b63edfb.png"><br>  ... <br><img src="https://habrastorage.org/files/19d/f4b/b71/19df4bb71d904a009f695fdc7e859f18.png"><br></div></div><br>  In perceptron, the Hebba learning rules were implemented.  As we can see, synaptic plasticity in the rules has already been taken into account to some extent.  And in principle, <a href="http://en.wikipedia.org/wiki/Online_machine_learning">online learning</a> gives us some plasticity - a neural network can constantly be trained on a continuous data stream, and its predictions in this regard will change over time, constantly taking into account changes in data.  But there are no recommendations on other aspects of neuroplasticity, such as sensory replaceability or neurogenesis.  But this is not surprising, just a little more than 20 years remain before the adoption of neuroplasticity.  Given the subsequent history of ANN, the scientist was not in the mood for imitation of neuroplasticity, the question was about the survival rate of the theory in principle.  Only after another renaissance of neural networks in the two thousand years, thanks to people like <a href="http://www.cs.toronto.edu/~hinton/">Hinton</a> , <a href="http://yann.lecun.com/">LeKun</a> , <a href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html">Bengio</a> and <a href="http://people.idsia.ch/~juergen/onlinepub.html">Schmidthuber</a> , other scientists had the opportunity to comprehensively approach machine learning and come to the concept of transfer learning. <br><br><h1>  Transfer learning </h1><br>  We define the goals of transfer learning.  The authors of the publication of 2009 with the <a href="">same name</a> identify three main goals: <br><ul><li>  higher start - improving the quality of education already at the initial iterations due to a more thorough selection of the initial parameters of the model or some other a priori information; </li><li>  higher slope - acceleration of the convergence of the learning algorithm; </li><li>  higher asymptote - improving the upper reachable quality boundary. </li></ul><br><img src="https://habrastorage.org/files/b61/040/576/b610405766fc4a91ab7749639d52f194.png" width="480"><br><br>  If you are familiar with the <a href="http://habrahabr.ru/post/163819/">pre</a> -training of <a href="http://habrahabr.ru/post/163819/">deep networks</a> with the help of auto-encoders or limited Boltzmann machines, then you will immediately think: ‚ÄúSo this is, I already turned out to practice transfer learning or, at least, know how to do it.‚Äù  But it turns out, no.  The authors draw a clear distinction between standard machine learning and transfer learning.  In the standard approach, you only have a goal and a data set, and the task is to achieve this goal by any means.  As part of solving the problem, you can build a deep network, pre-train it with a greedy algorithm, build about a dozen of them and make an ensemble of them in some way.  But all this will be within the framework of solving a single problem, and the time spent on solving such a problem will be comparable to the total time spent on training each model and its pre-training. <br><br>  Now imagine that there are two tasks, and perhaps even different people have solved them.  One of them uses part of the model of the other (source task) to reduce the time spent on creating a model from scratch and improve the performance of its model (target task).  The process of transferring knowledge from one problem to another is transfer learning.  And our brain probably does that.  As in the example above, his real task is to feel the taste of the receptors of the tongue and see with his eyes.  The challenge is to perceive the visual information by the receptors of the language.  And instead of growing new neurons or losing weight of old ones and training them anew, the brain simply slightly adjusts the existing neural network to achieve the result. <br><br><img src="https://habrastorage.org/files/ede/a16/0ee/edea160eed1f45ed81431348b29bc62f.png" width="480"><br><br>  Another feature of transfer learning is that information can only be transferred from the old model to the new one, because the old problem has long been solved.  While with the standard approach, different models involved in solving a problem can exchange information with each other. <br><br><img src="https://habrastorage.org/files/20d/bdb/691/20dbdb691dbc45db955c596022b965b4.png" width="480"><br><br>  This post applies only to the part of transfer learning, which relates to the task of learning with a teacher, but I recommend interested readers to read the original.  From there, you will learn that, say, in training with reinforcements or when training Bayesian networks, transfer learning was used earlier than in neural networks. <br><br>  So, learning with a teacher is learning from labeled examples, while the learning process with examples is sometimes called inductive learning (transition from particular to general), and the generalizing ability is called <a href="http://en.wikipedia.org/wiki/Inductive_bias">inductive bias</a> .  Hence the second name of transfer learning - <a href="http://en.wikipedia.org/wiki/Inductive_transfer">inductive transfer</a> .  Then we can say that the task of transferring knowledge in inductive learning is to allow the knowledge accumulated in the learning process of the old model to influence the generalizing ability of the new model (even when solving a different task). <br><br>  Knowledge transfer can also be considered as some regularization, which limits the search space to a certain set of valid and good hypotheses. <br><br><img src="https://habrastorage.org/files/d53/2d1/98b/d532d198b4e043b68e627ec638a25957.png" width="480"><br><br><h1>  Practice </h1><br>  I hope that by this time you are imbued with this, at first glance, unremarkable approach, as a transfer of knowledge.  After all, you can simply say that no this is not a transfer of knowledge, neuroplasticity is far-fetched, and the best name for this method is copy-paste.  Then you are just a pragmatist.  This, of course, is also not bad, and then at least you will like the third section.  Let's try to repeat something similar to that described in the first section, but on an artificial neural network. <br><br>  First, we formulate the problem.  Suppose you have a very large set of images.  It is necessary to organize a search for similar images.  This raises two problems.  First, the measure of similarity between the images is not obvious, and if you just take the Euclidean distance from the n * m-dimensional vector, the result will not be very satisfactory.  Secondly, even if there is a quality measure, it will not allow us to avoid a full scan of the base, and the base may contain billions of images. <br><br>  To solve this problem, you can use semantic hashing, one of these methods is described by <a href="http://www.cs.toronto.edu/~rsalakhu/">Salakhudinov</a> and <a href="http://www.cs.toronto.edu/~hinton/">Hinton</a> in the article <a href="http://www.cs.toronto.edu/~fritz/absps/sh.pdf">Semantic Hashing</a> .  Their idea is that the original data vector is encoded by a binary vector of small dimension (in our case, this is an image, and in the original article this is a binary vector of <a href="http://www.cs.toronto.edu/~fritz/absps/sh.pdf">bag of words</a> ).  With this encoding, you can search for similar images for a linear time from the code length using the <a href="https://en.wikipedia.org/wiki/Hamming_distance">Hamming distance</a> .  Such encoding is called semantic, since images (texts, music, etc.) that are close in meaning and content in the new feature space are located close to each other.  To implement this idea, they used a <a href="http://en.wikipedia.org/wiki/Deep_belief_network">deep network of trust</a> , <a href="http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">the learning algorithm of which</a> was developed by Hinton and the company back in 2006: <br><ul><li>  the input network generally takes real values ‚Äã‚Äã(gaussian-bernoulli rbm) and encodes them into a long binary vector, each next layer tries to reduce the length of the binary code (berboulli-bernoulli rbm); </li><li>  such a network is taught consistently from bottom to top by its own limited Boltzmann machine, where each next one uses the output of the previous one as its input data; </li><li>  then the network unfolds in the reverse order and learns as a <a href="http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf">deep auto-encoder</a> , which receives a slightly noisy vector at the entrance, and it tries to restore the original, non-noisy image;  this step is called fine turning; </li><li>  after all, the binary image in the middle of the auto-encoder will be the binary hash from the input image. </li></ul><br><img src="https://habrastorage.org/files/67f/aea/b54/67faeab541464e3cb8a5c1545890c0f6.png" width="640"><br><br>  It seems to me that this is a brilliant model, and I decided that, in principle, the problem was solved, thanks to Hinton.  I ran pre-training on the NVIDIA Tesla K20, waited a couple of days, and it turned out that everything is not as rosy as Hinton describes.  Whether because the pictures are large, or because I used gaussian-bernoulli rbm, and the article uses poisson-bernoulli rbm, or because of the specificity of the data, and maybe, in general, I taught little .  But I didn‚Äôt want to wait any longer.  And then I remembered the advice of <a href="http://www.milakov.org/">Maxim</a> Milakov - use convolutional networks, as well as the term transfer learning from one of his presentations.  There were, of course, other options, ranging from compressing the dimension of pictures and quantifying colors, to classic signs of computer vision and combining them into <a href="http://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision">bags of visual words</a> .  But once I entered the deep learning path, it‚Äôs not so easy to wind up from it, and the bonuses that transfer learning promises (especially time savings) deceive me. <br><br>  It turned out that the VGG group mentioned at the beginning, which won ImageNet competition in 2014, put its trained neural network <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">into free access for non-commercial use</a> , so I downloaded it purely for research purposes. <br><br>  In general, <a href="http://www.image-net.org/">ImageNet</a> is not only a competition, but also a database of images, which contains a little more than one million real images.  Each image is assigned to one of 1000 classes.  The set is class-balanced, that is, just over 1000 images per class.  The guys from Oxford won localization and classification in the nomination.  Since there may be more than one objects in the images, the assessment is based on whether the correct answer is in the top 5 most likely options according to the model version.  In the image below you can see an example of one of the models in the pictures from imagenet.  Pay attention to the funny mistake with the Dalmatian, the model, unfortunately, did not find a cherry there. <br><br><img src="https://habrastorage.org/files/623/f02/5fe/623f025fee03434993daefcee57417cc.png"><br><br>  With such variability of dataset, it is logical to assume that somewhere in the network there is an effective feature extractor, as well as a classifier, which decides which class the image belongs to.  I would like to get this very extractor, separate it from the classifier, and use it to teach deep autoencoder from the article on semantic hashing. <br><br>  VGG neural network is trained and saved in <a href="http://caffe.berkeleyvision.org/">Caffe</a> format.  Very cool library for deep learning, and most importantly - easy to learn, I recommend to read.  Before trepanning the VGG network using caffe, I suggest to get a glimpse of the network itself.  If the details are interesting, then I recommend the original article - <a href="http://arxiv.org/pdf/1409.1556.pdf">Very Deep Convolutional Networks for Large-Scale Image Recognition</a> (the name already hints at the depth of the network).  And those who are not familiar with convolutional networks at all, you should at least read the <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">Russian Wikipedia</a> before moving on, it will not take more than 5-10 minutes (or there <a href="http://geektimes.ru/post/74326/">is a small description on Habr√©</a> ). <br><br>  So, for the article and the contest, the authors have trained several models: <br><br><img src="https://habrastorage.org/files/acf/d11/c68/acfd11c68f9843ea99f1a163073e7528.png" width="640"><br><br>  On their page they laid out the variants D and E. For the experiment we take the 19-layer variant E, in which the first 16 layers are convolutional, and the last three are fully connected.  The last three layers are sensitive to the size of the images, so for the experiment, without thinking twice, I threw them out and left the first 16 layers, considering that I deleted the high-level features. <br><br>  The caffe library uses the Google <a href="https://github.com/google/protobuf/">Protocol Buffers</a> for model description, and the full description of the network is as follows. <br><br><div class="spoiler">  <b class="spoiler_title">19-layer model</b> <div class="spoiler_text"><pre><code class="javascript hljs">name: <span class="hljs-string"><span class="hljs-string">"VGG_ILSVRC_19_layers"</span></span> input: <span class="hljs-string"><span class="hljs-string">"data"</span></span> input_dim: <span class="hljs-number"><span class="hljs-number">10</span></span> input_dim: <span class="hljs-number"><span class="hljs-number">3</span></span> input_dim: <span class="hljs-number"><span class="hljs-number">224</span></span> input_dim: <span class="hljs-number"><span class="hljs-number">224</span></span> layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"data"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv1_1"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv1_1"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">64</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv1_1"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv1_1"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu1_1"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv1_1"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv1_2"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv1_2"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">64</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv1_2"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv1_2"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu1_2"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv1_2"</span></span> top: <span class="hljs-string"><span class="hljs-string">"pool1"</span></span> name: <span class="hljs-string"><span class="hljs-string">"pool1"</span></span> type: POOLING pooling_param { <span class="hljs-attr"><span class="hljs-attr">pool</span></span>: MAX kernel_size: <span class="hljs-number"><span class="hljs-number">2</span></span> stride: <span class="hljs-number"><span class="hljs-number">2</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"pool1"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv2_1"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv2_1"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">128</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv2_1"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv2_1"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu2_1"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv2_1"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv2_2"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv2_2"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">128</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv2_2"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv2_2"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu2_2"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv2_2"</span></span> top: <span class="hljs-string"><span class="hljs-string">"pool2"</span></span> name: <span class="hljs-string"><span class="hljs-string">"pool2"</span></span> type: POOLING pooling_param { <span class="hljs-attr"><span class="hljs-attr">pool</span></span>: MAX kernel_size: <span class="hljs-number"><span class="hljs-number">2</span></span> stride: <span class="hljs-number"><span class="hljs-number">2</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"pool2"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv3_1"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv3_1"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">256</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv3_1"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv3_1"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu3_1"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv3_1"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv3_2"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv3_2"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">256</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv3_2"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv3_2"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu3_2"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv3_2"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv3_3"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv3_3"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">256</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv3_3"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv3_3"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu3_3"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv3_3"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv3_4"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv3_4"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">256</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv3_4"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv3_4"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu3_4"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv3_4"</span></span> top: <span class="hljs-string"><span class="hljs-string">"pool3"</span></span> name: <span class="hljs-string"><span class="hljs-string">"pool3"</span></span> type: POOLING pooling_param { <span class="hljs-attr"><span class="hljs-attr">pool</span></span>: MAX kernel_size: <span class="hljs-number"><span class="hljs-number">2</span></span> stride: <span class="hljs-number"><span class="hljs-number">2</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"pool3"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv4_1"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv4_1"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">512</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv4_1"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv4_1"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu4_1"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv4_1"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv4_2"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv4_2"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">512</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv4_2"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv4_2"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu4_2"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv4_2"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv4_3"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv4_3"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">512</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv4_3"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv4_3"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu4_3"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv4_3"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv4_4"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv4_4"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">512</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv4_4"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv4_4"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu4_4"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv4_4"</span></span> top: <span class="hljs-string"><span class="hljs-string">"pool4"</span></span> name: <span class="hljs-string"><span class="hljs-string">"pool4"</span></span> type: POOLING pooling_param { <span class="hljs-attr"><span class="hljs-attr">pool</span></span>: MAX kernel_size: <span class="hljs-number"><span class="hljs-number">2</span></span> stride: <span class="hljs-number"><span class="hljs-number">2</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"pool4"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv5_1"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv5_1"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">512</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv5_1"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv5_1"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu5_1"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv5_1"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv5_2"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv5_2"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">512</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv5_2"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv5_2"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu5_2"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv5_2"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv5_3"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv5_3"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">512</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv5_3"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv5_3"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu5_3"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv5_3"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv5_4"</span></span> name: <span class="hljs-string"><span class="hljs-string">"conv5_4"</span></span> type: CONVOLUTION convolution_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">512</span></span> pad: <span class="hljs-number"><span class="hljs-number">1</span></span> kernel_size: <span class="hljs-number"><span class="hljs-number">3</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv5_4"</span></span> top: <span class="hljs-string"><span class="hljs-string">"conv5_4"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu5_4"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"conv5_4"</span></span> top: <span class="hljs-string"><span class="hljs-string">"pool5"</span></span> name: <span class="hljs-string"><span class="hljs-string">"pool5"</span></span> type: POOLING pooling_param { <span class="hljs-attr"><span class="hljs-attr">pool</span></span>: MAX kernel_size: <span class="hljs-number"><span class="hljs-number">2</span></span> stride: <span class="hljs-number"><span class="hljs-number">2</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"pool5"</span></span> top: <span class="hljs-string"><span class="hljs-string">"fc6"</span></span> name: <span class="hljs-string"><span class="hljs-string">"fc6"</span></span> type: INNER_PRODUCT inner_product_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">4096</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"fc6"</span></span> top: <span class="hljs-string"><span class="hljs-string">"fc6"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu6"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"fc6"</span></span> top: <span class="hljs-string"><span class="hljs-string">"fc6"</span></span> name: <span class="hljs-string"><span class="hljs-string">"drop6"</span></span> type: DROPOUT dropout_param { <span class="hljs-attr"><span class="hljs-attr">dropout_ratio</span></span>: <span class="hljs-number"><span class="hljs-number">0.5</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"fc6"</span></span> top: <span class="hljs-string"><span class="hljs-string">"fc7"</span></span> name: <span class="hljs-string"><span class="hljs-string">"fc7"</span></span> type: INNER_PRODUCT inner_product_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">4096</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"fc7"</span></span> top: <span class="hljs-string"><span class="hljs-string">"fc7"</span></span> name: <span class="hljs-string"><span class="hljs-string">"relu7"</span></span> type: RELU } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"fc7"</span></span> top: <span class="hljs-string"><span class="hljs-string">"fc7"</span></span> name: <span class="hljs-string"><span class="hljs-string">"drop7"</span></span> type: DROPOUT dropout_param { <span class="hljs-attr"><span class="hljs-attr">dropout_ratio</span></span>: <span class="hljs-number"><span class="hljs-number">0.5</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"fc7"</span></span> top: <span class="hljs-string"><span class="hljs-string">"fc8"</span></span> name: <span class="hljs-string"><span class="hljs-string">"fc8"</span></span> type: INNER_PRODUCT inner_product_param { <span class="hljs-attr"><span class="hljs-attr">num_output</span></span>: <span class="hljs-number"><span class="hljs-number">1000</span></span> } } layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"fc8"</span></span> top: <span class="hljs-string"><span class="hljs-string">"prob"</span></span> name: <span class="hljs-string"><span class="hljs-string">"prob"</span></span> type: SOFTMAX }</code> </pre> <br></div></div><br><br>  In order to make the described trepanation, it is enough to delete all layers in the model description, starting from <b>fc6</b> (full connected layer).  But it is worth noting that then the output of the network will be unlimited from above, since  The activation function is the <a href="http://en.wikipedia.org/wiki/Rectifier_(neural%2520networks)">rectified linear unit</a> : <br><br><img src="https://habrastorage.org/files/64b/98f/0dd/64b98f0dd17b4ddebfa36f7cb0d0649a.gif"><br><br>  Such a question is conveniently solved by taking a <a href="http://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> from the network output.  We can hope that many neurons will either be 0 or some large number.  Then after taking the sigmoid, we get a lot of units and 0.5 (sigmoid from 0 is 0.5).  If we normalize the obtained values ‚Äã‚Äãin the interval from 0 to 1, then they can be interpreted as the probabilities of activation of neurons, and almost all of them will be in the region of zero or one.  The probability of activation of a neuron is interpreted as the probability of the presence of a trait in the image (for example, is there a human eye on it). <br><br><img src="https://habrastorage.org/files/6ce/f51/749/6cef517499c54f108638661754479ba7.gif"><br><br>  Here is the typical answer of such a network in my case: <br><br><div class="spoiler">  <b class="spoiler_title">normalized sigmoid from the last convolutional layer</b> <div class="spoiler_text">  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  1.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.994934 <br>  0.0 <br>  0.0 <br>  0.999047 <br>  0.829219 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.997255 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  1.0 <br>  1.0 <br>  0.0 <br>  0.999382 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.988762 <br>  0.0 <br>  0.0 <br>  1.0 <br>  1.0 <br>  0.0 <br>  1.0 <br>  1.0 <br>  0.0 <br>  1.0 <br>  1.0 <br>  1.0 <br>  1.0 <br>  1.0 <br>  0.0 <br>  1.0 <br>  1.0 <br>  0.0 <br>  0.0 <br>  1.0 <br>  1.0 <br>  0.0 <br>  1.0 <br>  1.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.847886 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.957379 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  1.0 <br>  0.999998 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  1.0 <br>  0.999999 <br>  0.0 <br>  0.54814 <br>  0.739735 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.912179 <br>  0.0 <br>  0.0 <br>  0.78984 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.681776 <br>  0.0 <br>  0.0 <br>  0.991501 <br>  0.0 <br>  0.999999 <br>  0.999152 <br>  0.0 <br>  0.0 <br>  1.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.999996 <br>  1.0 <br>  0.0 <br>  1.0 <br>  1.0 <br>  0.0 <br>  0.880588 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.836756 <br>  0.995515 <br>  0.0 <br>  0.999354 <br>  0.0 <br>  1.0 <br>  1.0 <br>  0.0 <br>  1.0 <br>  1.0 <br>  0.0 <br>  0.999897 <br>  0.0 <br>  0.953126 <br>  0.0 <br>  0.0 <br>  0.999857 <br>  0.0 <br>  0.0 <br>  0.937695 <br>  0.999983 <br>  1.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  0.0 <br>  1.0 <br>  0.0 <br>  0.0 <br>  0.0 <br></div></div><br><br>  In caffe, this question is resolved as follows: <br><br><pre> <code class="javascript hljs"> layers { <span class="hljs-attr"><span class="hljs-attr">bottom</span></span>: <span class="hljs-string"><span class="hljs-string">"pool5"</span></span> top: <span class="hljs-string"><span class="hljs-string">"sigmoid1"</span></span> name: <span class="hljs-string"><span class="hljs-string">"sigmoid1"</span></span> type: SIGMOID }</code> </pre><br><br>  In caffe, a python wrapper is implemented, and the following code initializes the network and makes normalization: <br><br><pre> <code class="python hljs">caffe.set_mode_gpu() net = caffe.Classifier(<span class="hljs-string"><span class="hljs-string">'deploy.prototxt'</span></span>, <span class="hljs-string"><span class="hljs-string">'VGG_ILSVRC_19_layers.caffemodel'</span></span>, channel_swap=(<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>), raw_scale=<span class="hljs-number"><span class="hljs-number">255</span></span>, image_dims=(options.width, options.height)) <span class="hljs-comment"><span class="hljs-comment">#.................. out = net.forward_all(**{net.inputs[0]: caffe_in}) out = out['sigmoid1'].reshape(out['sigmoid1'].shape[0], np.prod(out['sigmoid1'].shape[1:])) out = (out - 0.5)/0.5</span></span></code> </pre><br><br>  So, at the moment we have a binary (well or almost) representation of images with high-dimensional vectors, in my case 4608, it remains to train the Deep Befief Network to compress these representations.  The resulting model has become an even deeper network.  Without waiting for DBN to study for a few days, let's conduct an experiment to search: we will choose a random image and look for the next image for it, in terms of the Hamming distance.  Notice, this is all on raw features, on a vector of high dimension, without any weighting of signs. <br><br>  Explanations: The first picture is a picture request, a random image from the database;  the rest are closest to her. <br><br><img src="https://habrastorage.org/files/af0/113/3cd/af01133cd2454eeab38261c10163c201.png"><br><br><div class="spoiler">  <b class="spoiler_title">other examples in the spoiler</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/bf0/715/dbe/bf0715dbef10421f811feeac61f67a4b.png"><br><hr><br><img src="https://habrastorage.org/files/ded/6df/aff/ded6dfaff3a44965a03765562518974e.png"><br><hr><br><img src="https://habrastorage.org/files/8c5/aff/7af/8c5aff7afc3d4b759bb7b202a24d851b.png"><br><hr><br><img src="https://habrastorage.org/files/f88/60d/f0a/f8860df0ad484ca48744b6ba73d95ae5.png"><br><hr><br><img src="https://habrastorage.org/files/26f/397/b31/26f397b312ac41c08791323cce4a56b7.png"><br><hr><br><img src="https://habrastorage.org/files/68d/2b5/bf9/68d2b5bf925145f6806e533d57b1973a.png"><br><hr><br><img src="https://habrastorage.org/files/715/eae/643/715eae6439324bdd961aa63344f481a0.png"><br><hr><br><img src="https://habrastorage.org/files/452/e22/01d/452e2201d2c94a30a9473b01dc6b3eb0.png"><br><hr><br><img src="https://habrastorage.org/files/c48/065/92e/c4806592ea3943289dce2739d93c599e.png"><br><hr><br><img src="https://habrastorage.org/files/c23/d0a/5b3/c23d0a5b3f0b4f558540d00ff53377fe.png"><br><hr><br><img src="https://habrastorage.org/files/83d/d1f/650/83dd1f6506b342b3ae4c9d28f757b95a.png"><br></div></div><br><br><h1>  Conclusion and links </h1><br>  I think you can easily come up with an example in which you will not just transfer static layers, but also train them in your own way.  Let's say you can initialize part of your network using another network and then retrain in your classes. <br><br><img src="https://habrastorage.org/files/b5b/7db/a7d/b5b7dba7da6946c4a90fdc0ec9252729.png" width="480"><br><br>  And if you try something closer to the experiment described in the first part?  Please: <a href="http://ai.stanford.edu/~ang/papers/nips09-AudioConvolutionalDBN.pdf">Here is an example of an article</a> using convolutional deep belief networks to extract features from audio signals.  Why not use trained convolutions to initialize cDBN weights?  What is such a spectrogram not an image, and why should low-level features not work on it: <br><br><img src="https://habrastorage.org/files/ecb/02c/953/ecb02c953ea74781a1ee5e0831678be7.png" width="480"><br><br>  If you want to experiment with the processing of natural language and try transfer learning, then <a href="http://arxiv.org/pdf/1502.01710v1.pdf">here is a suitable article</a> for <a href="http://arxiv.org/pdf/1502.01710v1.pdf">LeKun</a> to start.  And yes, there, too, the text appears as an image. <br><br>  In general, transfer learning is a great thing, and <a href="http://caffe.berkeleyvision.org/">caffe</a> is a cool library for deep learning. <br><br>  There are a lot of links in the text, here are some of them: <br><ul><li>  <a href="http://spectrum.ieee.org/consumer-electronics/portable-devices/loser-tongue-vision">Tongue Vision</a> </li><li>  <a href="http://www.cs.stanford.edu/people/ang//papers/icml07-selftaughtlearning.pdf">Self-taught Learning: Transfer Learning from Unlabeled Data</a> </li><li>  <a href="http://ftp.cs.wisc.edu/machine-learning/shavlik-group/torrey.handbook09.pdf">Transfer learning</a> </li><li>  <a href="http://image-net.org/challenges/LSVRC/2014/index">Large Scale Visual Recognition Challenge 2014 (ILSVRC2014)</a> </li><li>  <a href="http://www.joostrekveld.net/%3Fp%3D383">electricity for eyes</a> </li><li>  <a href="http://ftp.cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf">Visualizing and Understanding Convolutional Networks</a> </li><li>  <a href="http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf">Stacked Denoising Autoencoders: Learning Useful Representations in the Deep Network with a Local Denoising Criterion</a> </li><li>  <a href="http://www.cs.toronto.edu/~fritz/absps/sh.pdf">Semantic hashing</a> </li><li>  <a href="http://www.robots.ox.ac.uk/~vgg/">Visual geometry group</a> </li><li>  <a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">Very Deep Convolutional Networks for Large-Scale Visual Recognition (model)</a> </li><li>  <a href="http://caffe.berkeleyvision.org/">Caffe</a> </li><li>  <a href="http://arxiv.org/pdf/1409.1556.pdf">Very Deep Convolutional Networks for Large-Scale Visual Recognition (article)</a> </li><li>  <a href="http://www.qualcomm.com/media/documents/files/research-ar-lecture-series-slides-learning-better-image-features.pdf">Image representations for large-scale visual recognition (A. Vedaldi from VGG)</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/252965/">https://habr.com/ru/post/252965/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../252949/index.html">Google Code closes and invites everyone to go to GitHub</a></li>
<li><a href="../252955/index.html">Vesta CP: Installing Ruby and Python Web Applications</a></li>
<li><a href="../252957/index.html">Powershell for testers</a></li>
<li><a href="../252959/index.html">Microsoft has released EMET 5.2</a></li>
<li><a href="../252963/index.html">How to steal money that is not. Or something new about cryptocurrencies</a></li>
<li><a href="../252967/index.html">How not to configure antifraud rules on user geography</a></li>
<li><a href="../252969/index.html">Localization of Android applications using Google Sheets</a></li>
<li><a href="../252971/index.html">Building a Nodejs Modular System</a></li>
<li><a href="../252973/index.html">Proper increase in disk size in a virtual machine</a></li>
<li><a href="../252975/index.html">Create your Amazon-like navigation menu</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>