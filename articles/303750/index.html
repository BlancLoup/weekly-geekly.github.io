<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Methodical notes on the selection of informative features (feature selection)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 

 My name is Alexei Burnakov. I am a Data Scientist at Align Technology. In this article I will tell you about the approaches to the feature s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Methodical notes on the selection of informative features (feature selection)</h1><div class="post__text post__text-html js-mediator-article">  Hello! <br><br>  My name is Alexei Burnakov.  I am a Data Scientist at Align Technology.  In this article I will tell you about the approaches to the feature selection, which we practice in the course of experiments on data analysis. <br><br>  In our company, statistics and machine learning engineers analyze large volumes of clinical information related to the treatment of patients.  In a nutshell, the meaning of this article can be reduced to extracting valuable bits of knowledge contained in a small fraction of noisy and redundant gigabytes of data available to us. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  This article is intended for statisticians, machine learning engineers and specialists who are interested in finding dependencies in datasets.  Also, the material presented in the article may be of interest to a wide circle of readers who are not indifferent to data mining.  The material will not address the issues of feature engineering and, in particular, the application of such methods as the analysis of the main components. </h5><br><img src="https://habrastorage.org/files/483/ed9/1aa/483ed91aa1a24e88a1dcd8c7cb9c75e9.png" alt="image"><br>  <a href="https://www.npmjs.com/package/gl-plot3d"><i>A source.</i></a> <br><br><a name="habracut"></a><br><br>  It has been established that sets with a large number of input variables may contain redundant information and lead to the re-learning of machine learning models if the regularizer is not built into the models.  The stage of selection of informative features ( <b>IPR</b> here and below) is often a necessary step in preprocessing data during an experiment. <br><br><ul><li>  In the first part of this article, we will review some of the methods for selecting attributes and consider the theoretical points in this process.  This section is rather a systematization of our views. </li><li>  In the second part of the article, we will experiment with an example of an artificial data set with the selection of informative features and make a comparison of the results. </li><li>  In the third part, we will highlight the theory and practice of using measures from information theory as applied to the problem under discussion.  The method presented in this section is new, but it still requires numerous checks on various data sets. </li></ul><br><br>  The experiments carried out in the article do not pretend to scientific completeness due to the fact that analytical justifications for the optimality of a particular method will not be given, and the reader is referred to the original sources for more detailed and mathematically accurate presentation.  In addition, the disclaimer is that on other data the value of a particular method will change, which makes the task as a whole intellectually attractive. <br><br>  At the end of the article, the results of the experiments will be summarized and a reference to the full R code on Git will be made. <br><br>  <i>I express my gratitude to all the people who read the material before publication and made it better, in particular, to Vlad Shcherbinin and Alexei Seleznev.</i> <br><br><h5>  <b>1) Methods and methods for the selection of informative features.</b> </h5><br><br>  Let's look at the general approach to classifying IPR methods by referring to the wiki: <br><br><blockquote>  Selection algorithms for informative features can be represented by the following groups: Wrappers (wrapping), Filters (filtering), and Embedded (built-in machines).  (I will leave these terms without exact translation in view of the vagueness of their sound for the Russian-speaking community - my comment.) <br><br>  Wrapping algorithms create subsets using a search in the space of possible input variables and evaluate the obtained input subsets by training the complete model on the available data.  Wrapping algorithms can be very expensive and risk retraining the model.  (If you do not use a validation sample - note my.) <br><br>  Filtering algorithms are similar to wrapping in that they also search for subsets of input data, but instead of running the full model, the importance of the subset for the output variable is estimated using a simpler (filtering) algorithm. <br><br>  The algorithms built into the machines evaluate the importance of the input features using heuristics previously embedded in the training. </blockquote><br><br>  <i><a href="https://en.wikipedia.org/wiki/Feature_selection">A source.</a></i> <br><br>  Examples <br><br>  The wrapping algorithm of the IPR can be called a combination of methods, including searching for a subset of input variables followed by training, for example, random forest, on selected data and evaluating its error in cross-training.  That is, for each iteration, we will train the whole machine (already actually ready for further use). <br><br>  Filtering algorithm IPRs can be called enumeration of input variables, complemented by conducting a statistical test for the relationship between the selected variables and the output.  If our inputs and output are categorical, then it is possible to conduct a chi-square test for independence between an input (or a combined set of inputs) and an output with a p-value estimate and, as a consequence, a binary conclusion about the significance or insignificance of the selected set of features.  Other examples of filtering algorithms include: <br><ul><li>  linear input and output correlation; </li><li>  statistical test for difference in average in the case of categorical inputs and continuous output; </li><li>  F-criterion (analysis of variance). </li></ul><br>  The built-in IPR algorithm is, for example, p-values ‚Äã‚Äãcorresponding to linear regression coefficients.  In this case, p-value also allows you to make a binary conclusion about a significant difference in the coefficient from zero.  If you scale all the inputs of the model, the modules of the scales can be interpreted as indicators of importance.  You can also use the R ^ 2 model - a measure of explaining the variance of a process with simulated values.  Another example is the function of assessing the importance of input variables, built-in random forest.  In addition, weights modules can be used that correspond to the inputs in an artificial neural network.  This list is not exhausted. <br><br>  At this stage, it is important to understand that this distinction, in fact, indicates a difference in the <u>fitness functions of the</u> IPR, that is, a measure of the relevance of the found subset of input features in relation to the problem being solved.  Subsequently, we will return to the question of choosing a fitness function. <br><br>  Now, when we are a little oriented in the main groups of methods of IPRs, I suggest paying attention to what methods are used to iterate over the subsets of input variables.  Turn again to the wiki page: <br><br><blockquote>  Approaches to search include: <br><ul><li>  Brute force </li><li>  First best candidate </li><li>  Annealing imitation </li><li>  Genetic algorithm </li><li>  Greedy search for inclusion </li><li>  Greedy Exception Search </li><li>  Particle swarm optimization </li><li>  Targeted projection pursuit </li><li>  Scatter search </li><li>  Variable Neighborhood Search </li></ul></blockquote><br><br>  <i><a href="https://en.wikipedia.org/wiki/Feature_selection">A source.</a></i> <br><br>  I deliberately did not translate the names of some algorithms, since I was not familiar with their Russian-language interpretation. <br><br>  The search for a subset of predictors is a discrete problem, since at the output we get a vector of integers, denoting input indices, of the form: <br><br>  inputs: 1 2 3 4 5 6 ... 1000 <br>  Selection: 0 0 1 1 1 0 ... 1 <br><br>  We will return to this feature later and illustrate where it leads in practice. <br><br>  The result of the whole experiment strongly depends on how the search for a subset of input features is configured.  In order to intuitively understand the main difference in these approaches, I suggest the reader to divide them into two groups: greedy (non-greedy) and non-greedy. <br><br><h6>  <b>Greedy search algorithms.</b> </h6><br><br>  They are used often, as they are fast and give a good result in many tasks.  The greed of the algorithm is that if one of the candidates for entry into the final subset was chosen (or excluded), then it remains in it (in the case of a greedy inclusion) or forever absent (in the case of a greedy exception).  Thus, if candidate A was selected at early iterations, at later iterations a subset will always include him and other candidates, which, together with A, show an improvement in the metric of importance of the subset for the output variable.  The opposite situation is for exclusion: if candidate A was removed, because after his exclusion, the importance metric is least affected or improved, the researcher will not receive information about the importance metric, where A is in the subset and other candidates excluded later. <br><br>  If we draw a parallel with the search for a maximum (minimum) in multidimensional space, the greedy algorithm will get stuck in a local minimum, if there is one, or quickly find the optimal solution, if there is a single minimum, and it is global. <br><br>  On the other hand, enumeration of all greedy variants is carried out relatively quickly and allows to take into account some <u>interactions</u> between inputs. <br><br>  Examples of greedy algorithms: greedy inclusion (forward selection; step forward) and greedy exception (backward elimination; step backward).  The list is not limited to this. <br><br><h6>  <b>Unexpected search algorithms.</b> </h6><br><br>  The principle of operation of non-greedy algorithms implies the ability to discard all or partly formed subsets of features, combinations of subsets among themselves and make random changes to the subsets in order to avoid a local minimum. <br><br>  If we draw an analogy with the search for the maximum (minimum) values ‚Äã‚Äãof the fitness function in a multidimensional space, non-greedy algorithms consider much more neighboring points and can even make large jumps to random areas. <br><br>  It is possible to present graphically the operation of these types of algorithms. <br><br>  First, the greedy inclusion: <br><img src="https://habrastorage.org/files/c5d/f7d/393/c5df7d393e674358b2a14215ccd53531.png"><br><br>  Now non-greedy - stochastic - search: <br><img src="https://habrastorage.org/files/7e1/9d3/5e6/7e19d35e6a0e4f6c8bacbb7c1850ce4d.png"><br><br>  In both cases, you need to select one of the best combination of two inputs, whose indices are plotted along the axes of the graph.  The greedy algorithm begins by selecting the one best entry, going through the indexes horizontally.  And then adds a second input to the selected input so that their total relevance is maximum.  But it turns out that of all possible combinations, it completely passes only 1/37 part.  When you add another dimension, the number of cells traversed will become even smaller: approximately 1/3 ^ 2. <br><br>  In this case, a practical situation is possible when the wrong combination is not the one that the greedy algorithm found.  This can occur if each of the two inputs separately does not show the best relevance to the task (and they will not be selected in the first step), but their interaction maximizes the relevance metric. <br><br>  The rash algorithm seeks much longer: <br><br><blockquote>  (O) = 2 ^ n </blockquote><br><br>  and checks for more possible combinations of inputs.  But he has a chance to find, perhaps, an even better subset of inputs due to a sweeping search at once in all changes of the task. <br><br>  <b>There are search algorithms that go beyond the established greedy / non-greedy dichotomy.</b> <br><br>  An example of such a separate search would be to iterate through the inputs individually and evaluate their individual importance for the output variable.  With this, by the way, the first wave begins in the greedy variable inclusion algorithm.  But what is good with such a search, except that it is very fast?  Each input variable begins to exist "in a vacuum", that is, without taking into account the influence of other inputs on the connection between the selected input and output.  At the same time, after the completion of the algorithm, the resulting list of inputs indicating their individual importance for the output only gives information about the individual significance of each predictor.  When combining a number of the most important predictors, according to this list, you can get a number of problems: <br><br><ul><li>  redundancy (in the case of the correlation of predictors among themselves); </li><li>  lack of information due to the neglect of predictor interactions at the selection stage; </li><li>  blurring the boundaries above which you need to take the predictors. </li></ul><br><br>  As you can see, the problems are not the most trivial. <br><br>  <b>The main question in the problem of the IPR is formulated as the optimal combination of the subset search method and the fitness function.</b> <br><br>  Let us consider this statement in more detail.  Our problem of IPR can be described by two hypotheses: <br><br>  a) the surface of the error is simple or complex; <br>  b) there are simple or complex dependencies in the data. <br><br>  Depending on the answers to these questions, the choice should be made on a specific combination of the search method and the method for determining the relevance of the selected characteristics. <br><br>  Surface error. <br><br>  An example of a simple surface: <br><img src="https://habrastorage.org/files/eef/31d/66c/eef31d66c9144ff698465219451c7a53.jpg" alt="image"><br><br>  <i><a href="http://www.engram9.info/excel-2007-vba-methods/nonlinear-leastsquares-curve-fitting.html">A source.</a></i> <br><br>  Here we choose a combination of two inputs, determining their relevance to the output and descend along a smooth surface in the direction of the gradient, almost certainly at the optimum point. <br><br>  An example of a complex surface: <br><img src="https://habrastorage.org/files/807/03d/400/80703d40010f4baa88bf28f2c3a4357d.gif" alt="image"><br><br>  <a href="http://see.library.utoronto.ca/SEED/Vol3-3/Castro.htm">A source.</a> <br><br>  In this case, solving the same problem, we encounter a set of local minima, the greedy algorithm cannot get out of which.  At the same time, the algorithm with a stochastic search has an increased chance of finding a more accurate solution. <br><br>  We mentioned earlier that finding a subset of predictors is a discrete task.  If the dependence of the output on the inputs includes interactions, when moving from one point of space to the next one, one can observe sharp jumps in the value of the fitness function.  The surface error in our case is often not smooth, not differentiable: <br><img src="https://habrastorage.org/files/a6c/32d/58a/a6c32d58a9474b3c94fb4821531add32.png" alt="image"><br><br>  This is an example of finding a subset of two inputs and the corresponding value of the relevance function of a subset of the output variable.  It can be seen that the surface is far from smooth, with peaks, and also includes a rugged plateau of approximately the same values.  Nightmare for methods of greedy gradient descent. <br><br>  Dependencies <br><br>  As the number of measurements of a task grows, the theoretical chance increases that the dependence of the output variable has a very complex structure and involves many inputs.  In addition, the dependence can be either linear or non-linear.  If the dependence implies the interaction of predictors and a non-linear form, it will be possible to find it only taking into account both these points, applying, for example, random forest or neural network training.  If the dependence is simple, linear, includes only a small part of all the predictors, the approach to finding it - and, as a result, to the IPR - can be reduced to alternate inclusion in the linear regression model with 1 or more inputs with a model quality rating. <br><br>  An example of a simple dependency: <br><img src="https://habrastorage.org/files/2c5/aa2/682/2c5aa26829f94830b57b33df07567be3.png" alt="image"><br><br>  In this case, the dependence of the value along the output axis on the values ‚Äã‚Äãinput1 and input2 is described by a plane in space: <br><blockquote>  output = input1 * 10 + input2 * 10 </blockquote><br>  The model of such dependence is very simple and can be approximated by linear regression. <br><br>  An example of a complex dependency: <br><img src="https://habrastorage.org/files/03c/032/787/03c032787093465cbe7c76e22207a313.png" alt="image"><br><br>  This non-linear relationship can no longer be detected by the construction of a linear model.  Her appearance is: <br><blockquote>  output = input1 ^ 2 + input2 ^ 2 </blockquote><br><br>  It is also necessary to take into account the dimension of the problem. <br><br>  If the number of input variables is large, the search for the optimal subset by stochastic (non-greasy) methods can be very expensive due to the fact that the total number of all possible subsets is given by <br><blockquote>  m = 2 ^ n, <br>  where n is the number of all input features. </blockquote><br><br>  Accordingly, the search for a minimum in such a variety can be very long.  On the other hand, the use of a greedy search will make it possible to make a first approximation in a reasonable time, even if it is a local minimum and the researcher will know about it. <br><br>  In the absence of objective knowledge about the phenomenon under study, it is impossible to say in advance how complex the dependence of input variables and output will be and how many inputs will be optimal for finding an approximate or exact solution to the problem of selecting the optimal subset of inputs.  It is also difficult to predict whether the surface of the error during the IPR is smooth and simple or complex and rugged. <br><br>  We are also always limited in resources and must make the most optimal decisions.  As a small help in developing an approach to IPR, you can use the following table: <br><img src="https://habrastorage.org/files/0f0/57b/75d/0f057b75dc274166b457f9e969454cdc.JPG" alt="image"><br><br>  Thus, we always have the opportunity to consider several combinations of methods for finding a subset of inputs and the fitness function of relevance.  The most expensive and probably the most effective combination is the greedy search and wrapper fitness wrapping.  It allows you to avoid a local minimum, while providing the most accurate measure of the relevance of the selected inputs, since at each iteration there is a trained model (and its accuracy for validation). <br><br>  The least expensive, but not always the least effective approach, is a greedy search paired with a filter function, which can be a statistical test, a correlation coefficient, or a quantity of mutual information. <br><br>  In addition, the embedded (embedded) methods allow immediately after training the model to weed out a number of inputs that are unnecessary from the algorithm algorithm point of view, without losing significantly the accuracy of the simulation. <br><br>  A good way is to try to solve a problem several times in different ways and choose the best one. <br><br>  In general, the selection of informative features is the selection of the best combination of the multi-dimensional search method and the most optimal fitness function of the relevance of the selected subset with respect to the output variable. <br><br><img src="https://habrastorage.org/files/564/e00/c38/564e00c38f4e454cb3b5cca5c664da8f.png" alt="image"><br><br>  <a href="http://www.ministryinsights.com/interlocking-pieces/"><i>A source.</i></a> <br><br><h5>  <b>2) Experiments on the selection of informative features on synthetic data.</b> </h5><br><br>  Our experimental data set ("Stanford rabbit"): <br><img src="https://habrastorage.org/files/fd0/763/ce9/fd0763ce9a6649f7bda201b0c0cacf6e.JPG" alt="image"><br><br>  I just love rabbits. <br><br>  We will look at the dependence of the height of a point (Z axis) on latitude and longitude.  In this case, I add 10 noise variables to the set with a distribution approximately corresponding to the mixture of two informative inputs (X and Y), but not related to the variable Z. <br><br>  Let's look at histograms of the density of distribution for the variables X, Y, Z and one of the noise variables. <br><img src="https://habrastorage.org/files/28e/ab2/62f/28eab262f4ef4975bc952a8036b51420.JPG" alt="image"><br><br><img src="https://habrastorage.org/files/0df/562/450/0df562450b524db0ae81de696e5ea8a3.JPG" alt="image"><br><br><img src="https://habrastorage.org/files/917/27b/e26/91727be260a9486eb656e91c16cdacd8.JPG" alt="image"><br><br><img src="https://habrastorage.org/files/26f/4ca/60e/26f4ca60efc945b1b569dcd7b587c9c9.JPG" alt="image"><br><br>  It is seen that the distribution with arbitrary parameters.  In this case, all noise variables are distributed in such a way that they have a small peak in a certain range of values. <br><br>  Further, the data set will be randomly divided into two parts: training and validation. <br><br>  Data preparation. <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre><code class="python hljs">library(onion) data(bunny) <span class="hljs-comment"><span class="hljs-comment">#XYZ point plot open3d() points3d(bunny, col=8, size=0.1) bunny_dat &lt;- as.data.frame(bunny) inputs &lt;- append(as.numeric(bunny_dat$x), as.numeric(bunny_dat$y)) for (i in 1:10){ naming &lt;- paste('input_noise_', i, sep = '') bunny_dat[, eval(naming)] &lt;- inputs[sample(length(inputs), nrow(bunny_dat), replace = T)] }</span></span></code> </pre> <br></div></div><br><br>  <b>Experiment ‚Ññ1: greedy search for a subset of inputs with a linear function of assessing importance (as the fitness function, the variant wrapper will be used - an estimate of the coefficient of determination of a trained model on a validation sample).</b> <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">### greedy search with filter function library(FSelector) sampleA &lt;- bunny_dat[sample(nrow(bunny_dat), nrow(bunny_dat) / 2, replace = F), c("x", "y", "input_noise_1", "input_noise_2", "input_noise_3", "input_noise_4", "input_noise_5", "input_noise_6", "input_noise_7", "input_noise_8", "input_noise_9", "input_noise_10", "z")] sampleB &lt;- bunny_dat[!row.names(bunny_dat) %in% rownames(sampleA), c("x", "y", "input_noise_1", "input_noise_2", "input_noise_3", "input_noise_4", "input_noise_5", "input_noise_6", "input_noise_7", "input_noise_8", "input_noise_9", "input_noise_10", "z")] linear_fit &lt;- function(subset){ dat &lt;- sampleA[, c(subset, "z")] lm_m &lt;- lm(formula = z ~., data = dat, model = T) lm_predict &lt;- predict(lm_m, newdata = sampleB) r_sq_validate &lt;- 1 - sum((sampleB$z - lm_predict) ^ 2) / sum((sampleB$z - mean(sampleB$z)) ^ 2) print(subset) print(r_sq_validate) return(r_sq_validate) } #subset &lt;- backward.search(attributes = names(sampleA)[1:(ncol(sampleA) - 1)], eval.fun = linear_fit) subset &lt;- forward.search(attributes = names(sampleA)[1:(ncol(sampleA) - 1)], eval.fun = linear_fit)</span></span></code> </pre><br></div></div><br><br>  Result: <br><br><pre> <code class="python hljs">&gt; subset &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"x"</span></span> <span class="hljs-string"><span class="hljs-string">"y"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_2"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_5"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_6"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_8"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_9"</span></span>&lt;/b&gt;</code> </pre><br><br>  It turned out to include noise variables. <br><br>  Let's look at the trained model: <br><br><pre> <code class="python hljs">&gt; summary(lm_m) Call: lm(formula = z ~ ., data = dat, model = T) Residuals: Min <span class="hljs-number"><span class="hljs-number">1</span></span>Q Median <span class="hljs-number"><span class="hljs-number">3</span></span>Q Max <span class="hljs-number"><span class="hljs-number">-0.060613</span></span> <span class="hljs-number"><span class="hljs-number">-0.022650</span></span> <span class="hljs-number"><span class="hljs-number">-0.000173</span></span> <span class="hljs-number"><span class="hljs-number">0.024939</span></span> <span class="hljs-number"><span class="hljs-number">0.048544</span></span> Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) <span class="hljs-number"><span class="hljs-number">0.0232453</span></span> <span class="hljs-number"><span class="hljs-number">0.0005581</span></span> <span class="hljs-number"><span class="hljs-number">41.651</span></span> &lt; <span class="hljs-number"><span class="hljs-number">2e-16</span></span> *** &lt;b&gt;x <span class="hljs-number"><span class="hljs-number">-0.0257686</span></span> <span class="hljs-number"><span class="hljs-number">0.0052998</span></span> <span class="hljs-number"><span class="hljs-number">-4.862</span></span> <span class="hljs-number"><span class="hljs-number">1.17e-06</span></span> *** y <span class="hljs-number"><span class="hljs-number">-0.1572786</span></span> <span class="hljs-number"><span class="hljs-number">0.0052585</span></span> <span class="hljs-number"><span class="hljs-number">-29.910</span></span> &lt; <span class="hljs-number"><span class="hljs-number">2e-16</span></span> ***&lt;/b&gt; input_noise_2 <span class="hljs-number"><span class="hljs-number">-0.0017249</span></span> <span class="hljs-number"><span class="hljs-number">0.0027680</span></span> <span class="hljs-number"><span class="hljs-number">-0.623</span></span> <span class="hljs-number"><span class="hljs-number">0.533</span></span> input_noise_5 <span class="hljs-number"><span class="hljs-number">-0.0027391</span></span> <span class="hljs-number"><span class="hljs-number">0.0027848</span></span> <span class="hljs-number"><span class="hljs-number">-0.984</span></span> <span class="hljs-number"><span class="hljs-number">0.325</span></span> input_noise_6 <span class="hljs-number"><span class="hljs-number">0.0032417</span></span> <span class="hljs-number"><span class="hljs-number">0.0027907</span></span> <span class="hljs-number"><span class="hljs-number">1.162</span></span> <span class="hljs-number"><span class="hljs-number">0.245</span></span> input_noise_8 <span class="hljs-number"><span class="hljs-number">0.0044998</span></span> <span class="hljs-number"><span class="hljs-number">0.0027723</span></span> <span class="hljs-number"><span class="hljs-number">1.623</span></span> <span class="hljs-number"><span class="hljs-number">0.105</span></span> input_noise_9 <span class="hljs-number"><span class="hljs-number">0.0006839</span></span> <span class="hljs-number"><span class="hljs-number">0.0027808</span></span> <span class="hljs-number"><span class="hljs-number">0.246</span></span> <span class="hljs-number"><span class="hljs-number">0.806</span></span> --- Signif. codes: <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-string"><span class="hljs-string">'***'</span></span> <span class="hljs-number"><span class="hljs-number">0.001</span></span> <span class="hljs-string"><span class="hljs-string">'**'</span></span> <span class="hljs-number"><span class="hljs-number">0.01</span></span> <span class="hljs-string"><span class="hljs-string">'*'</span></span> <span class="hljs-number"><span class="hljs-number">0.05</span></span> <span class="hljs-string"><span class="hljs-string">'.'</span></span> <span class="hljs-number"><span class="hljs-number">0.1</span></span> <span class="hljs-string"><span class="hljs-string">' '</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> Residual standard error: <span class="hljs-number"><span class="hljs-number">0.02742</span></span> on <span class="hljs-number"><span class="hljs-number">17965</span></span> degrees of freedom Multiple R-squared: <span class="hljs-number"><span class="hljs-number">0.04937</span></span>, Adjusted R-squared: <span class="hljs-number"><span class="hljs-number">0.049</span></span> F-statistic: <span class="hljs-number"><span class="hljs-number">133.3</span></span> on <span class="hljs-number"><span class="hljs-number">7</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-number"><span class="hljs-number">17965</span></span> DF, p-value: &lt; <span class="hljs-number"><span class="hljs-number">2.2e-16</span></span></code> </pre><br><br>  We see that in fact, only our original inputs and the free term of the equation take statistical significance. <br><br>  And now let's get the greedy exception of variables. <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs">subset &lt;- backward.search(attributes = names(sampleA)[<span class="hljs-number"><span class="hljs-number">1</span></span>:(ncol(sampleA) - <span class="hljs-number"><span class="hljs-number">1</span></span>)], eval.fun = linear_fit) <span class="hljs-comment"><span class="hljs-comment">#subset &lt;- forward.search(attributes = names(sampleA)[1:(ncol(sampleA) - 1)], eval.fun = linear_fit)</span></span></code> </pre><br></div></div><br><br>  Result: <br><pre> <code class="python hljs">&gt; subset &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"x"</span></span> <span class="hljs-string"><span class="hljs-string">"y"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_2"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_5"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_6"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_8"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_9"</span></span>&lt;/b&gt;</code> </pre><br><br>  The model also included noises. <br><br>  Let's look at the trained model: <br><br><pre> <code class="python hljs">&gt; summary(lm_m) Call: lm(formula = z ~ ., data = dat, model = T) Residuals: Min <span class="hljs-number"><span class="hljs-number">1</span></span>Q Median <span class="hljs-number"><span class="hljs-number">3</span></span>Q Max <span class="hljs-number"><span class="hljs-number">-0.060613</span></span> <span class="hljs-number"><span class="hljs-number">-0.022650</span></span> <span class="hljs-number"><span class="hljs-number">-0.000173</span></span> <span class="hljs-number"><span class="hljs-number">0.024939</span></span> <span class="hljs-number"><span class="hljs-number">0.048544</span></span> Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) <span class="hljs-number"><span class="hljs-number">0.0232453</span></span> <span class="hljs-number"><span class="hljs-number">0.0005581</span></span> <span class="hljs-number"><span class="hljs-number">41.651</span></span> &lt; <span class="hljs-number"><span class="hljs-number">2e-16</span></span> *** &lt;b&gt;x <span class="hljs-number"><span class="hljs-number">-0.0257686</span></span> <span class="hljs-number"><span class="hljs-number">0.0052998</span></span> <span class="hljs-number"><span class="hljs-number">-4.862</span></span> <span class="hljs-number"><span class="hljs-number">1.17e-06</span></span> *** y <span class="hljs-number"><span class="hljs-number">-0.1572786</span></span> <span class="hljs-number"><span class="hljs-number">0.0052585</span></span> <span class="hljs-number"><span class="hljs-number">-29.910</span></span> &lt; <span class="hljs-number"><span class="hljs-number">2e-16</span></span> ***&lt;/b&gt; input_noise_2 <span class="hljs-number"><span class="hljs-number">-0.0017249</span></span> <span class="hljs-number"><span class="hljs-number">0.0027680</span></span> <span class="hljs-number"><span class="hljs-number">-0.623</span></span> <span class="hljs-number"><span class="hljs-number">0.533</span></span> input_noise_5 <span class="hljs-number"><span class="hljs-number">-0.0027391</span></span> <span class="hljs-number"><span class="hljs-number">0.0027848</span></span> <span class="hljs-number"><span class="hljs-number">-0.984</span></span> <span class="hljs-number"><span class="hljs-number">0.325</span></span> input_noise_6 <span class="hljs-number"><span class="hljs-number">0.0032417</span></span> <span class="hljs-number"><span class="hljs-number">0.0027907</span></span> <span class="hljs-number"><span class="hljs-number">1.162</span></span> <span class="hljs-number"><span class="hljs-number">0.245</span></span> input_noise_8 <span class="hljs-number"><span class="hljs-number">0.0044998</span></span> <span class="hljs-number"><span class="hljs-number">0.0027723</span></span> <span class="hljs-number"><span class="hljs-number">1.623</span></span> <span class="hljs-number"><span class="hljs-number">0.105</span></span> input_noise_9 <span class="hljs-number"><span class="hljs-number">0.0006839</span></span> <span class="hljs-number"><span class="hljs-number">0.0027808</span></span> <span class="hljs-number"><span class="hljs-number">0.246</span></span> <span class="hljs-number"><span class="hljs-number">0.806</span></span> --- Signif. codes: <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-string"><span class="hljs-string">'***'</span></span> <span class="hljs-number"><span class="hljs-number">0.001</span></span> <span class="hljs-string"><span class="hljs-string">'**'</span></span> <span class="hljs-number"><span class="hljs-number">0.01</span></span> <span class="hljs-string"><span class="hljs-string">'*'</span></span> <span class="hljs-number"><span class="hljs-number">0.05</span></span> <span class="hljs-string"><span class="hljs-string">'.'</span></span> <span class="hljs-number"><span class="hljs-number">0.1</span></span> <span class="hljs-string"><span class="hljs-string">' '</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> Residual standard error: <span class="hljs-number"><span class="hljs-number">0.02742</span></span> on <span class="hljs-number"><span class="hljs-number">17965</span></span> degrees of freedom Multiple R-squared: <span class="hljs-number"><span class="hljs-number">0.04937</span></span>, Adjusted R-squared: <span class="hljs-number"><span class="hljs-number">0.049</span></span> F-statistic: <span class="hljs-number"><span class="hljs-number">133.3</span></span> on <span class="hljs-number"><span class="hljs-number">7</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-number"><span class="hljs-number">17965</span></span> DF, p-value: &lt; <span class="hljs-number"><span class="hljs-number">2.2e-16</span></span></code> </pre><br><br>  Similarly, inside the model, we see that only the original inputs are important. <br><br>  If we train the model only on variables X and Y, we get: <br><br><pre> <code class="python hljs">&gt; print(subset) &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"x"</span></span> <span class="hljs-string"><span class="hljs-string">"y"</span></span>&lt;/b&gt; &gt; print(r_sq_validate) &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-number"><span class="hljs-number">0.05185492</span></span>&lt;/b&gt; &gt; summary(lm_m) Call: lm(formula = z ~ ., data = dat, model = T) Residuals: Min <span class="hljs-number"><span class="hljs-number">1</span></span>Q Median <span class="hljs-number"><span class="hljs-number">3</span></span>Q Max <span class="hljs-number"><span class="hljs-number">-0.059884</span></span> <span class="hljs-number"><span class="hljs-number">-0.022653</span></span> <span class="hljs-number"><span class="hljs-number">-0.000209</span></span> <span class="hljs-number"><span class="hljs-number">0.024955</span></span> <span class="hljs-number"><span class="hljs-number">0.048238</span></span> Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) <span class="hljs-number"><span class="hljs-number">0.0233808</span></span> <span class="hljs-number"><span class="hljs-number">0.0005129</span></span> <span class="hljs-number"><span class="hljs-number">45.590</span></span> &lt; <span class="hljs-number"><span class="hljs-number">2e-16</span></span> *** &lt;b&gt;x <span class="hljs-number"><span class="hljs-number">-0.0257813</span></span> <span class="hljs-number"><span class="hljs-number">0.0052995</span></span> <span class="hljs-number"><span class="hljs-number">-4.865</span></span> <span class="hljs-number"><span class="hljs-number">1.15e-06</span></span> *** y <span class="hljs-number"><span class="hljs-number">-0.1573098</span></span> <span class="hljs-number"><span class="hljs-number">0.0052576</span></span> <span class="hljs-number"><span class="hljs-number">-29.920</span></span> &lt; <span class="hljs-number"><span class="hljs-number">2e-16</span></span> ***&lt;/b&gt; --- Signif. codes: <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-string"><span class="hljs-string">'***'</span></span> <span class="hljs-number"><span class="hljs-number">0.001</span></span> <span class="hljs-string"><span class="hljs-string">'**'</span></span> <span class="hljs-number"><span class="hljs-number">0.01</span></span> <span class="hljs-string"><span class="hljs-string">'*'</span></span> <span class="hljs-number"><span class="hljs-number">0.05</span></span> <span class="hljs-string"><span class="hljs-string">'.'</span></span> <span class="hljs-number"><span class="hljs-number">0.1</span></span> <span class="hljs-string"><span class="hljs-string">' '</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> Residual standard error: <span class="hljs-number"><span class="hljs-number">0.02742</span></span> on <span class="hljs-number"><span class="hljs-number">17970</span></span> degrees of freedom Multiple R-squared: <span class="hljs-number"><span class="hljs-number">0.04908</span></span>, Adjusted R-squared: <span class="hljs-number"><span class="hljs-number">0.04898</span></span> F-statistic: <span class="hljs-number"><span class="hljs-number">463.8</span></span> on <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-number"><span class="hljs-number">17970</span></span> DF, p-value: &lt; <span class="hljs-number"><span class="hljs-number">2.2e-16</span></span></code> </pre><br><br>  The fact is that the validation of R ^ 2 was higher when the noise variables were turned off. <br><br>  Strange result!  Probably, because of the data structure, the noise does not have a detrimental effect on the model. <br><br>  But we have not tried to take into account the interaction of predictors. <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs">lm_m &lt;- lm(formula = z ~ x * y, data = dat, model = T)</code> </pre><br></div></div><br><br>  It turned out well: <br><br><pre> <code class="python hljs">&gt; summary(lm_m) Call: lm(formula = z ~ x * y, data = dat, model = T) Residuals: Min <span class="hljs-number"><span class="hljs-number">1</span></span>Q Median <span class="hljs-number"><span class="hljs-number">3</span></span>Q Max <span class="hljs-number"><span class="hljs-number">-0.057761</span></span> <span class="hljs-number"><span class="hljs-number">-0.023067</span></span> <span class="hljs-number"><span class="hljs-number">-0.000119</span></span> <span class="hljs-number"><span class="hljs-number">0.024762</span></span> <span class="hljs-number"><span class="hljs-number">0.049747</span></span> Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) <span class="hljs-number"><span class="hljs-number">0.0196766</span></span> <span class="hljs-number"><span class="hljs-number">0.0006545</span></span> <span class="hljs-number"><span class="hljs-number">30.062</span></span> &lt;<span class="hljs-number"><span class="hljs-number">2e-16</span></span> *** x <span class="hljs-number"><span class="hljs-number">-0.1513484</span></span> <span class="hljs-number"><span class="hljs-number">0.0148113</span></span> <span class="hljs-number"><span class="hljs-number">-10.218</span></span> &lt;<span class="hljs-number"><span class="hljs-number">2e-16</span></span> *** y <span class="hljs-number"><span class="hljs-number">-0.1084295</span></span> <span class="hljs-number"><span class="hljs-number">0.0075183</span></span> <span class="hljs-number"><span class="hljs-number">-14.422</span></span> &lt;<span class="hljs-number"><span class="hljs-number">2e-16</span></span> *** x:y <span class="hljs-number"><span class="hljs-number">1.3771299</span></span> <span class="hljs-number"><span class="hljs-number">0.1517363</span></span> <span class="hljs-number"><span class="hljs-number">9.076</span></span> &lt;<span class="hljs-number"><span class="hljs-number">2e-16</span></span> *** --- Signif. codes: <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-string"><span class="hljs-string">'***'</span></span> <span class="hljs-number"><span class="hljs-number">0.001</span></span> <span class="hljs-string"><span class="hljs-string">'**'</span></span> <span class="hljs-number"><span class="hljs-number">0.01</span></span> <span class="hljs-string"><span class="hljs-string">'*'</span></span> <span class="hljs-number"><span class="hljs-number">0.05</span></span> <span class="hljs-string"><span class="hljs-string">'.'</span></span> <span class="hljs-number"><span class="hljs-number">0.1</span></span> <span class="hljs-string"><span class="hljs-string">' '</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> Residual standard error: <span class="hljs-number"><span class="hljs-number">0.02736</span></span> on <span class="hljs-number"><span class="hljs-number">17969</span></span> degrees of freedom Multiple R-squared: <span class="hljs-number"><span class="hljs-number">0.05342</span></span>, Adjusted R-squared: <span class="hljs-number"><span class="hljs-number">0.05327</span></span> F-statistic: <span class="hljs-number"><span class="hljs-number">338.1</span></span> on <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-number"><span class="hljs-number">17969</span></span> DF, p-value: &lt; <span class="hljs-number"><span class="hljs-number">2.2e-16</span></span></code> </pre><br><br>  The interaction of X and Y is significant.  What about R ^ 2 on validation? <br><br><pre> <code class="python hljs">&gt; lm_predict &lt;- predict(lm_m, + newdata = sampleB) &gt; <span class="hljs-number"><span class="hljs-number">1</span></span> - sum((sampleB$z - lm_predict) ^ <span class="hljs-number"><span class="hljs-number">2</span></span>) / sum((sampleB$z - mean(sampleB$z)) ^ <span class="hljs-number"><span class="hljs-number">2</span></span>) &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-number"><span class="hljs-number">0.05464066</span></span>&lt;/b&gt;</code> </pre><br><br>  This is the highest value we have seen.  Unfortunately, it was the interaction option that was not incorporated into the fitness function and we missed this combination of inputs. <br><br>  <b>Experiment number 2: the greedy search for a subset of inputs with a linear function of assessing the importance (as the fitness function will be used the option embedded - f-statistics of the trained model on the training set).</b> <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs">linear_fit_f &lt;- function(subset){ dat &lt;- sampleA[, c(subset, <span class="hljs-string"><span class="hljs-string">"z"</span></span>)] lm_m &lt;- lm(formula = z ~., data = dat, model = T) print(subset) print(summary(lm_m)$fstatistic[[<span class="hljs-number"><span class="hljs-number">1</span></span>]]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>(summary(lm_m)$fstatistic[[<span class="hljs-number"><span class="hljs-number">1</span></span>]]) } <span class="hljs-comment"><span class="hljs-comment">#subset &lt;- backward.search(attributes = names(sampleA)[1:(ncol(sampleA) - 1)], eval.fun = linear_fit_f) subset &lt;- forward.search(attributes = names(sampleA)[1:(ncol(sampleA) - 1)], eval.fun = linear_fit_f)</span></span></code> </pre><br></div></div><br><br>  The result of the sequential inclusion of variables is only one predictor Y. For it, F-Statistic was maximized.  That is, this variable is very important.  But for some reason, the variable X is forgotten. <br><br>  And now the sequential elimination of variables. <br><br>  The result is similar - only one variable Y. <br><br>  It can be noted that while maximizing the F-Statistic multi-variable model, all the noise turned out to be overboard, and the model turned out to be robust: the coefficient of determination for validation is almost as good as the best model from experiment No. 1: <br><br><pre> <code class="python hljs">&gt; r_sq_validate &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-number"><span class="hljs-number">0.05034534</span></span>&lt;/b&gt;</code> </pre><br><br>  <b>Experiment No. 3: sequential assessment of the individual significance of predictors using the Pearson correlation coefficient (this option is the simplest, it does not take into account the interaction, the fitness function is also simple ‚Äî it evaluates only the linear relationship).</b> <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs">correlation_arr &lt;- data.frame() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">12</span></span>){ correlation_arr[i, <span class="hljs-number"><span class="hljs-number">1</span></span>] &lt;- colnames(sampleA)[i] correlation_arr[i, <span class="hljs-number"><span class="hljs-number">2</span></span>] &lt;- cor(sampleA[, i], sampleA[, <span class="hljs-string"><span class="hljs-string">'z'</span></span>]) }</code> </pre><br></div></div><br><br>  Result: <br><br><pre> <code class="python hljs">&gt; correlation_arr V1 V2 &lt;b&gt;<span class="hljs-number"><span class="hljs-number">1</span></span> x <span class="hljs-number"><span class="hljs-number">0.0413782832</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> y <span class="hljs-number"><span class="hljs-number">-0.2187061876</span></span>&lt;/b&gt; <span class="hljs-number"><span class="hljs-number">3</span></span> input_noise_1 <span class="hljs-number"><span class="hljs-number">-0.0097719425</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> input_noise_2 <span class="hljs-number"><span class="hljs-number">-0.0019297383</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span> input_noise_3 <span class="hljs-number"><span class="hljs-number">0.0002143946</span></span> <span class="hljs-number"><span class="hljs-number">6</span></span> input_noise_4 <span class="hljs-number"><span class="hljs-number">-0.0142325764</span></span> <span class="hljs-number"><span class="hljs-number">7</span></span> input_noise_5 <span class="hljs-number"><span class="hljs-number">-0.0048206943</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> input_noise_6 <span class="hljs-number"><span class="hljs-number">0.0090877674</span></span> <span class="hljs-number"><span class="hljs-number">9</span></span> input_noise_7 <span class="hljs-number"><span class="hljs-number">-0.0152897433</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span> input_noise_8 <span class="hljs-number"><span class="hljs-number">0.0143477495</span></span> <span class="hljs-number"><span class="hljs-number">11</span></span> input_noise_9 <span class="hljs-number"><span class="hljs-number">0.0027560459</span></span> <span class="hljs-number"><span class="hljs-number">12</span></span> input_noise_10 <span class="hljs-number"><span class="hljs-number">-0.0079526578</span></span></code> </pre><br><br>  The largest correlation is Z with Y, and in second place is X. However, correlation X is not explicitly expressed and requires a statistical test for the significance of the difference in the correlation coefficient from zero for each of the variables. <br><br>  On the other hand, in all 3 experiments we did not take into account the interaction of predictors (X * Y) at all.  This may explain the fact that the estimation of single significance or the linear inclusion of predictors in the equation does not give us an unequivocal answer. <br><br>  Such an experiment: <br><br><pre> <code class="python hljs">&gt; cor(sampleA$x * sampleA$y, sampleA$z) &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-number"><span class="hljs-number">0.1211382</span></span>&lt;/b&gt;</code> </pre><br><br>  Indicates that the interaction of X and Y correlates with Z quite strongly. <br><br>  <b>Experiment number 4: assessment of the importance of predictors by the algorithm embedded in the machine (the variant of the greedy search and the embedded fitness function of the importance of inputs into GBM).</b> <br><br>  We will train Gradient Boosted Trees (gbm) and look at the importance of variables.  Good article detailing aspects of using GBM: <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/"><i>Gradient boosting machines, a tutorial.</i></a> <br><br>  Take the learning parameters from the ceiling, set a very low learning rate to avoid strong retraining.  Note that any decision trees are greedy, and improving the model by adding many models is achieved by sampling observations and inputs. <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs">library(gbm) gbm_dat &lt;- bunny_dat[, c(<span class="hljs-string"><span class="hljs-string">"x"</span></span>, <span class="hljs-string"><span class="hljs-string">"y"</span></span>, <span class="hljs-string"><span class="hljs-string">"input_noise_1"</span></span>, <span class="hljs-string"><span class="hljs-string">"input_noise_2"</span></span>, <span class="hljs-string"><span class="hljs-string">"input_noise_3"</span></span>, <span class="hljs-string"><span class="hljs-string">"input_noise_4"</span></span>, <span class="hljs-string"><span class="hljs-string">"input_noise_5"</span></span>, <span class="hljs-string"><span class="hljs-string">"input_noise_6"</span></span>, <span class="hljs-string"><span class="hljs-string">"input_noise_7"</span></span>, <span class="hljs-string"><span class="hljs-string">"input_noise_8"</span></span>, <span class="hljs-string"><span class="hljs-string">"input_noise_9"</span></span>, <span class="hljs-string"><span class="hljs-string">"input_noise_10"</span></span>, <span class="hljs-string"><span class="hljs-string">"z"</span></span>)] gbm_fit &lt;- gbm(formula = z ~., distribution = <span class="hljs-string"><span class="hljs-string">"gaussian"</span></span>, data = gbm_dat, n.trees = <span class="hljs-number"><span class="hljs-number">500</span></span>, interaction.depth = <span class="hljs-number"><span class="hljs-number">12</span></span>, n.minobsinnode = <span class="hljs-number"><span class="hljs-number">100</span></span>, shrinkage = <span class="hljs-number"><span class="hljs-number">0.0001</span></span>, bag.fraction = <span class="hljs-number"><span class="hljs-number">0.9</span></span>, train.fraction = <span class="hljs-number"><span class="hljs-number">0.7</span></span>, n.cores = <span class="hljs-number"><span class="hljs-number">6</span></span>) gbm.perf(object = gbm_fit, plot.it = TRUE, oobag.curve = F, overlay = TRUE) summary(gbm_fit)</code> </pre><br></div></div><br><br>  Result: <br><br><pre> <code class="python hljs">&gt; summary(gbm_fit) var rel.inf &lt;b&gt;yy <span class="hljs-number"><span class="hljs-number">69.7919</span></span> xx <span class="hljs-number"><span class="hljs-number">30.2081</span></span>&lt;/b&gt; input_noise_1 input_noise_1 <span class="hljs-number"><span class="hljs-number">0.0000</span></span> input_noise_2 input_noise_2 <span class="hljs-number"><span class="hljs-number">0.0000</span></span> input_noise_3 input_noise_3 <span class="hljs-number"><span class="hljs-number">0.0000</span></span> input_noise_4 input_noise_4 <span class="hljs-number"><span class="hljs-number">0.0000</span></span> input_noise_5 input_noise_5 <span class="hljs-number"><span class="hljs-number">0.0000</span></span> input_noise_6 input_noise_6 <span class="hljs-number"><span class="hljs-number">0.0000</span></span> input_noise_7 input_noise_7 <span class="hljs-number"><span class="hljs-number">0.0000</span></span> input_noise_8 input_noise_8 <span class="hljs-number"><span class="hljs-number">0.0000</span></span> input_noise_9 input_noise_9 <span class="hljs-number"><span class="hljs-number">0.0000</span></span> input_noise_10 input_noise_10 <span class="hljs-number"><span class="hljs-number">0.0000</span></span></code> </pre><br><br>  This approach perfectly coped with the task and singled out noiseless inputs, making all other inputs absolutely insignificant. <br><br>  Moreover, it should be noted that the setup of this experiment is very fast, everything works almost out of the box.  More careful planning of this experiment, including cross-training to obtain optimal learning parameters, is more difficult, but this is necessary when preparing a real model in production. <br><br>  <b>Experiment # 5: assessing the importance of predictors using a stochastic search with a linear importance rating function (this is a <u>non-greasy</u> search in the input space, and the wrapper variant will be used as the fitness function ‚Äî an estimate of the determination coefficient of the trained model on the validation sample).</b> <br><br>  <u>This time, the learning linear model involves pairwise interactions between the predictors.</u> <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">### simulated annealing search with linear model interactions stats library(scales) library(GenSA) sampleA &lt;- bunny_dat[sample(nrow(bunny_dat), nrow(bunny_dat) / 2, replace = F), c("x", "y", "input_noise_1", "input_noise_2", "input_noise_3", "input_noise_4", "input_noise_5", "input_noise_6", "input_noise_7", "input_noise_8", "input_noise_9", "input_noise_10", "z")] sampleB &lt;- bunny_dat[!row.names(bunny_dat) %in% rownames(sampleA), c("x", "y", "input_noise_1", "input_noise_2", "input_noise_3", "input_noise_4", "input_noise_5", "input_noise_6", "input_noise_7", "input_noise_8", "input_noise_9", "input_noise_10", "z")] #calculate parameters predictor_number &lt;- dim(sampleA)[2] - 1 sample_size &lt;- dim(sampleA)[1] par_v &lt;- runif(predictor_number, min = 0, max = 1) par_low &lt;- rep(0, times = predictor_number) par_upp &lt;- rep(1, times = predictor_number) ############### the fitness function sa_fit_f &lt;- function(par){ indexes &lt;- c(1:predictor_number) for (i in 1:predictor_number){ if (par[i] &gt;= threshold) { indexes[i] &lt;- i } else { indexes[i] &lt;- 0 } } local_predictor_number &lt;- 0 for (i in 1:predictor_number){ if (indexes[i] &gt; 0) { local_predictor_number &lt;- local_predictor_number + 1 } } if (local_predictor_number &gt; 0) { sampleAf &lt;- as.data.frame(sampleA[, c(indexes[], dim(sampleA)[2])]) lm_m &lt;- lm(formula = z ~ .^2, data = sampleAf, model = T) lm_predict &lt;- predict(lm_m, newdata = sampleB) r_sq_validate &lt;- 1 - sum((sampleB$z - lm_predict) ^ 2) / sum((sampleB$z - mean(sampleB$z)) ^ 2) } else { r_sq_validate &lt;- 0 } return(-r_sq_validate) } #stimating threshold for variable inclusion threshold &lt;- 0.5 # do it simply #run feature selection start &lt;- Sys.time() sao &lt;- GenSA(par = par_v, fn = sa_fit_f, lower = par_low, upper = par_upp , control = list( #maxit = 10 max.time = 300 , smooth = F , simple.function = F)) trace_ff &lt;- data.frame(sao$trace)$function.value plot(trace_ff, type = "l") percent(- sao$value) final_vector &lt;- c((sao$par &gt;= threshold), T) names(sampleA)[final_vector] final_sample &lt;- as.data.frame(sampleA[, final_vector]) Sys.time() - start # check model lm_m &lt;- lm(formula = z ~ .^2, data = sampleA[, final_vector], model = T) summary(lm_m)</span></span></code> </pre><br></div></div><br><br>  What happened? <br><br><pre> <code class="python hljs">&lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"5.53%"</span></span>&lt;/b&gt; &gt; final_vector &lt;- c((sao$par &gt;= threshold), T) &gt; names(sampleA)[final_vector] &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"x"</span></span> <span class="hljs-string"><span class="hljs-string">"y"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_7"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_8"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_9"</span></span> <span class="hljs-string"><span class="hljs-string">"z"</span></span> &lt;/b&gt; &gt; summary(lm_m) Call: lm(formula = z ~ .^<span class="hljs-number"><span class="hljs-number">2</span></span>, data = sampleA[, final_vector], model = T) Residuals: Min <span class="hljs-number"><span class="hljs-number">1</span></span>Q Median <span class="hljs-number"><span class="hljs-number">3</span></span>Q Max <span class="hljs-number"><span class="hljs-number">-0.058691</span></span> <span class="hljs-number"><span class="hljs-number">-0.023202</span></span> <span class="hljs-number"><span class="hljs-number">-0.000276</span></span> <span class="hljs-number"><span class="hljs-number">0.024953</span></span> <span class="hljs-number"><span class="hljs-number">0.050618</span></span> Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) <span class="hljs-number"><span class="hljs-number">0.0197777</span></span> <span class="hljs-number"><span class="hljs-number">0.0007776</span></span> <span class="hljs-number"><span class="hljs-number">25.434</span></span> &lt;<span class="hljs-number"><span class="hljs-number">2e-16</span></span> *** &lt;b&gt;x <span class="hljs-number"><span class="hljs-number">-0.1547889</span></span> <span class="hljs-number"><span class="hljs-number">0.0154268</span></span> <span class="hljs-number"><span class="hljs-number">-10.034</span></span> &lt;<span class="hljs-number"><span class="hljs-number">2e-16</span></span> *** y <span class="hljs-number"><span class="hljs-number">-0.1148349</span></span> <span class="hljs-number"><span class="hljs-number">0.0085787</span></span> <span class="hljs-number"><span class="hljs-number">-13.386</span></span> &lt;<span class="hljs-number"><span class="hljs-number">2e-16</span></span> ***&lt;/b&gt; input_noise_7 <span class="hljs-number"><span class="hljs-number">-0.0102894</span></span> <span class="hljs-number"><span class="hljs-number">0.0071871</span></span> <span class="hljs-number"><span class="hljs-number">-1.432</span></span> <span class="hljs-number"><span class="hljs-number">0.152</span></span> input_noise_8 <span class="hljs-number"><span class="hljs-number">-0.0013928</span></span> <span class="hljs-number"><span class="hljs-number">0.0071508</span></span> <span class="hljs-number"><span class="hljs-number">-0.195</span></span> <span class="hljs-number"><span class="hljs-number">0.846</span></span> input_noise_9 <span class="hljs-number"><span class="hljs-number">0.0026736</span></span> <span class="hljs-number"><span class="hljs-number">0.0071910</span></span> <span class="hljs-number"><span class="hljs-number">0.372</span></span> <span class="hljs-number"><span class="hljs-number">0.710</span></span> &lt;b&gt;x:y <span class="hljs-number"><span class="hljs-number">1.3098676</span></span> <span class="hljs-number"><span class="hljs-number">0.1515268</span></span> <span class="hljs-number"><span class="hljs-number">8.644</span></span> &lt;<span class="hljs-number"><span class="hljs-number">2e-16</span></span> ***&lt;/b&gt; x:input_noise_7 <span class="hljs-number"><span class="hljs-number">0.0352997</span></span> <span class="hljs-number"><span class="hljs-number">0.0709842</span></span> <span class="hljs-number"><span class="hljs-number">0.497</span></span> <span class="hljs-number"><span class="hljs-number">0.619</span></span> x:input_noise_8 <span class="hljs-number"><span class="hljs-number">0.0653103</span></span> <span class="hljs-number"><span class="hljs-number">0.0714883</span></span> <span class="hljs-number"><span class="hljs-number">0.914</span></span> <span class="hljs-number"><span class="hljs-number">0.361</span></span> x:input_noise_9 <span class="hljs-number"><span class="hljs-number">0.0459939</span></span> <span class="hljs-number"><span class="hljs-number">0.0716704</span></span> <span class="hljs-number"><span class="hljs-number">0.642</span></span> <span class="hljs-number"><span class="hljs-number">0.521</span></span> y:input_noise_7 <span class="hljs-number"><span class="hljs-number">0.0512392</span></span> <span class="hljs-number"><span class="hljs-number">0.0710949</span></span> <span class="hljs-number"><span class="hljs-number">0.721</span></span> <span class="hljs-number"><span class="hljs-number">0.471</span></span> y:input_noise_8 <span class="hljs-number"><span class="hljs-number">0.0563148</span></span> <span class="hljs-number"><span class="hljs-number">0.0707809</span></span> <span class="hljs-number"><span class="hljs-number">0.796</span></span> <span class="hljs-number"><span class="hljs-number">0.426</span></span> y:input_noise_9 <span class="hljs-number"><span class="hljs-number">-0.0085022</span></span> <span class="hljs-number"><span class="hljs-number">0.0710267</span></span> <span class="hljs-number"><span class="hljs-number">-0.120</span></span> <span class="hljs-number"><span class="hljs-number">0.905</span></span> input_noise_7:input_noise_8 <span class="hljs-number"><span class="hljs-number">0.0129156</span></span> <span class="hljs-number"><span class="hljs-number">0.0374855</span></span> <span class="hljs-number"><span class="hljs-number">0.345</span></span> <span class="hljs-number"><span class="hljs-number">0.730</span></span> input_noise_7:input_noise_9 <span class="hljs-number"><span class="hljs-number">0.0519535</span></span> <span class="hljs-number"><span class="hljs-number">0.0376869</span></span> <span class="hljs-number"><span class="hljs-number">1.379</span></span> <span class="hljs-number"><span class="hljs-number">0.168</span></span> input_noise_8:input_noise_9 <span class="hljs-number"><span class="hljs-number">0.0128397</span></span> <span class="hljs-number"><span class="hljs-number">0.0379640</span></span> <span class="hljs-number"><span class="hljs-number">0.338</span></span> <span class="hljs-number"><span class="hljs-number">0.735</span></span> --- Signif. codes: <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-string"><span class="hljs-string">'***'</span></span> <span class="hljs-number"><span class="hljs-number">0.001</span></span> <span class="hljs-string"><span class="hljs-string">'**'</span></span> <span class="hljs-number"><span class="hljs-number">0.01</span></span> <span class="hljs-string"><span class="hljs-string">'*'</span></span> <span class="hljs-number"><span class="hljs-number">0.05</span></span> <span class="hljs-string"><span class="hljs-string">'.'</span></span> <span class="hljs-number"><span class="hljs-number">0.1</span></span> <span class="hljs-string"><span class="hljs-string">' '</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> Residual standard error: <span class="hljs-number"><span class="hljs-number">0.0274</span></span> on <span class="hljs-number"><span class="hljs-number">17957</span></span> degrees of freedom Multiple R-squared: <span class="hljs-number"><span class="hljs-number">0.05356</span></span>, Adjusted R-squared: <span class="hljs-number"><span class="hljs-number">0.05277</span></span> F-statistic: <span class="hljs-number"><span class="hljs-number">67.75</span></span> on <span class="hljs-number"><span class="hljs-number">15</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-number"><span class="hljs-number">17957</span></span> DF, p-value: &lt; <span class="hljs-number"><span class="hljs-number">2.2e-16</span></span></code> </pre><br><br>  We see that we miss.  Noise included. <br><br>  As you can see, the best value of the coefficient of determination for validation is achieved with the inclusion of noise variables.  In this case, the convergence of the search algorithm is exhaustive: <br><img src="https://habrastorage.org/files/445/6d8/b17/4456d8b17a434b38adab19a0b2e68d7f.JPG" alt="image"><br><br>  Let's try to change the fitness function, save the search method. <br><br>  <b>Experiment # 6: assessing the importance of predictors using a stochastic search with a linear function of assessing importance (this is a <u>non-greasy</u> search in the space of inputs, the fitness function is the embedded p-values ‚Äã‚Äãcorresponding to the coefficients of the model).</b> <br><br>  We will select a set of predictors that minimize the average p-value of the coefficients in the model. <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># fittness based on p-values sa_fit_f2 &lt;- function(par){ indexes &lt;- c(1:predictor_number) for (i in 1:predictor_number){ if (par[i] &gt;= threshold) { indexes[i] &lt;- i } else { indexes[i] &lt;- 0 } } local_predictor_number &lt;- 0 for (i in 1:predictor_number){ if (indexes[i] &gt; 0) { local_predictor_number &lt;- local_predictor_number + 1 } } if (local_predictor_number &gt; 0) { sampleAf &lt;- as.data.frame(sampleA[, c(indexes[], dim(sampleA)[2])]) lm_m &lt;- lm(formula = z ~ .^2, data = sampleAf, model = T) mean_val &lt;- mean(summary(lm_m)$coefficients[, 4]) } else { mean_val &lt;- 1 } return(mean_val) } #stimating threshold for variable inclusion threshold &lt;- 0.5 # do it simply #run feature selection start &lt;- Sys.time() sao &lt;- GenSA(par = par_v, fn = sa_fit_f2, lower = par_low, upper = par_upp , control = list( #maxit = 10 max.time = 60 , smooth = F , simple.function = F)) trace_ff &lt;- data.frame(sao$trace)$function.value plot(trace_ff, type = "l") percent(- sao$value) final_vector &lt;- c((sao$par &gt;= threshold), T) names(sampleA)[final_vector] final_sample &lt;- as.data.frame(sampleA[, final_vector]) Sys.time() - start</span></span></code> </pre><br></div></div><br><br>  Result: <br><br><pre> <code class="python hljs">&gt; percent(- sao$value) &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"-4.7e-208%"</span></span>&lt;/b&gt; &gt; final_vector &lt;- c((sao$par &gt;= threshold), T) &gt; names(sampleA)[final_vector] &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"y"</span></span> <span class="hljs-string"><span class="hljs-string">"z"</span></span>&lt;/b&gt;</code> </pre><br><br>  This time everything worked out.  Only original predictors were selected, since their p-values ‚Äã‚Äãare really small. <br><br>  The algorithm convergence is good in 60 seconds: <br><img src="https://habrastorage.org/files/8d1/171/492/8d11714925a44330a7ba45697bb3514c.JPG" alt="image"><br><br>  <b>Experiment number 7: assessing the importance of predictors using a greedy search with an assessment of the importance of the quality of the trained model (this is a greedy search in the input space, the fitness function is a wrapper corresponding to the determination coefficient on the validation of the boosted decision trees model).</b> <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">### greedy search with wrapper GBM validation fitness library(FSelector) library(gbm) sampleA &lt;- bunny_dat[sample(nrow(bunny_dat), nrow(bunny_dat) / 2, replace = F), c("x", "y", "input_noise_1", "input_noise_2", "input_noise_3", "input_noise_4", "input_noise_5", "input_noise_6", "input_noise_7", "input_noise_8", "input_noise_9", "input_noise_10", "z")] sampleB &lt;- bunny_dat[!row.names(bunny_dat) %in% rownames(sampleA), c("x", "y", "input_noise_1", "input_noise_2", "input_noise_3", "input_noise_4", "input_noise_5", "input_noise_6", "input_noise_7", "input_noise_8", "input_noise_9", "input_noise_10", "z")] gbm_fit &lt;- function(subset){ dat &lt;- sampleA[, c(subset, "z")] gbm_fit &lt;- gbm(formula = z ~., distribution = "gaussian", data = dat, n.trees = 50, interaction.depth = 10, n.minobsinnode = 100, shrinkage = 0.1, bag.fraction = 0.9, train.fraction = 1, n.cores = 7) gbm_predict &lt;- predict(gbm_fit, newdata = sampleB, n.trees = 50) r_sq_validate &lt;- 1 - sum((sampleB$z - gbm_predict) ^ 2) / sum((sampleB$z - mean(sampleB$z)) ^ 2) print(subset) print(r_sq_validate) return(r_sq_validate) } subset &lt;- backward.search(attributes = names(sampleA)[1:(ncol(sampleA) - 1)], eval.fun = gbm_fit) subset &lt;- forward.search(attributes = names(sampleA)[1:(ncol(sampleA) - 1)], eval.fun = gbm_fit)</span></span></code> </pre><br></div></div><br><br>  Result with greedy predictors: <br><br><pre> <code class="python hljs">&gt; subset &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"x"</span></span> <span class="hljs-string"><span class="hljs-string">"y"</span></span>&lt;/b&gt; &gt; r_sq_validate &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-number"><span class="hljs-number">0.2363794</span></span>&lt;/b&gt;</code> </pre><br><br>  Got the point! <br><br>  Result with the greedy <u>exception of</u> predictors: <br><br><pre> <code class="python hljs">&gt; subset &lt;b&gt; [<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"x"</span></span> <span class="hljs-string"><span class="hljs-string">"y"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_1"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_2"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_3"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_4"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_5"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_6"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_7"</span></span> [<span class="hljs-number"><span class="hljs-number">10</span></span>] <span class="hljs-string"><span class="hljs-string">"input_noise_9"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_10"</span></span>&lt;/b&gt; &gt; r_sq_validate &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-number"><span class="hljs-number">0.2266737</span></span>&lt;/b&gt;</code> </pre><br><br>  It's getting worse.  The inclusion of noise predictors in the model did not greatly impair the quality of the prediction on validation.  And there is an explanation for this: random decision forests have a built-in regularizer and can themselves ignore non-informative inputs in the learning process. <br><br>  This concludes the section of experiments on IPRs on standard methods.  And in the next section, I will substantiate and show practical application, in my opinion, statistically reliable and doing a good job, based on information metrics. <br><br><h5>  <b>3) The use of information theory for the construction of fitness functions of IPR.</b> </h5><br><br><h5>  The key question of this section is how to describe the concept of dependence and formulate it in the information-theoretical sense. </h5><br><br><img src="https://habrastorage.org/files/26d/804/781/26d8047814774df3b0709592df9dd449.jpg" alt="image"><br><br>  <i><a href="https://xenophilius.wordpress.com/2012/06/15/test-your-psychic-powers-understand-the-odds-of-guessing-20-coin-tosses-correctly-in-a-row/">A source.</a></i> <br><br>  We need to start with the concept of informational entropy.  Entropy (Shannon) is a synonym for uncertainty.  The more uncertain we are about the value of a random variable, the more entropy (another synonym for information) is the realization of this value.  If we consider the example of tossing a coin, then the symmetric coin of all any other variants of coins will have the greatest entropy, because we have the greatest uncertainty in the next outcome of a toss. <br><br>  Formula for Shannon's entropy: <br><br><img src="https://habrastorage.org/files/4d6/af8/750/4d6af87509784cadba1cb0e3bf9dfdbd.png" alt="image"><br><br><h5>  What is addiction? </h5><br><br>  Suppose we flip a coin a few times.  Can we say that our uncertainty about the next outcome of the throw diminished after we saw the result of the previous throw? <br><br>  Suppose we have a coin that lands with an eagle with a probability of 2/3 after falling heads, and vice versa - after falling of an eagle, it lands on heads with a probability of 2/3.  At the same time, the unconditional frequency of the fallouts of the eagle and tails remains 50/50. <br><br>  For such a coin after a falling eagle, the frequency of tails is no longer 1/2, and vice versa.  So, our uncertainty about the next outcome of the throw has decreased (we don‚Äôt expect 50/50). <br><br>  To understand the phenomenon of dependence, we recall that probability theory defines independence as follows: <br><blockquote>  p (x, y) == p (x) * p (y) </blockquote><br>  thus, the probability of the joint realization of events is equal to the product of the probabilities of their own realizations. <br><br>  If this is observed, the events are independent in a mathematical sense.  If <br><blockquote>  p (x, y)! = p (x) * p (y) </blockquote><br>  then events are not mathematically independent. <br><br>  The same principle underlies the formula for measuring the relationship between two (and more) random variables in information theory. <br><br>  We emphasize that here dependence is understood in a probabilistic sense.  The analysis of causality requires a more comprehensive review, including both the analysis of false correlations and redundancy (through the use of mutual information), and the involvement of expert knowledge about the object under study. <br><br><h6>  Mutual Information </h6><br><img src="https://habrastorage.org/files/0cc/528/832/0cc5288328e740adad6702f8f0e00884.png" alt="image"><br><br>  You can also output mutual information through entropies: <br><br><img src="https://habrastorage.org/files/f58/0ab/0b6/f580ab0b6669428dafcb39cf58f040c2.png" alt="image"><br><br>  In simple words, mutual information is the amount of entropy (uncertainty) that leaves the system, if there is a predictive variable (or several variables).  For example, the entropy of a random variable is 3 bits.  Mutual information is 2 bits.  Hence, by 2/3, the uncertainty about the implementation of a random variable is compensated by the presence of a predictor. <br><br>  Mutual information has the following properties: <br><br><ul><li>  symmetry </li><li>  can be defined for discrete and continuous variables </li><li>  vanishes if X and Y are independent </li><li>  the normalized measure of mutual information takes the value 1 if the two variables completely determine each other </li></ul><br><br>  Mutual information satisfies some of the requirements for the ideal measure of dependence, formulated in: <br><br><blockquote>  Granger, C, E. Maasoumi e J. Racine, 'A Dependence Metric for Possibly Nonlinear Processes', Journal of Time Series Analysis 25, 2004, 649-669. </blockquote><br><br>  There is one property of mutual information ( <b>VI</b> ) that is useful to know when applying this measure: <br><br><ul><li>  A CI can reach its maximum equal to the smaller of the entropy values ‚Äã‚Äãfor entry and exit (predictor and dependent variable). </li></ul><br><br>  This means that if our input variable has an entropy of 10 bits, and the output variable has 3 bits, then the maximum information that the input variable can transmit to the output is 3 bits.  This is the maximum VI, which can be in such a system. <br><br>  Another option is that the input variable has an entropy of 3 bits, and the output is 10 bits.  The maximum information that an input can carry to the output is 3 bits, and this is the possible maximum of the VI. <br><br>  If the VI value is divided by the entropy of the dependent variable, we get a value in the range [0, 1], which, by analogy with the correlation coefficient, indicates how, without taking into account the scale of values, the input variable determines the dependent variable. <br><br>  <b>There are a number of arguments for using VIs, although it involves a number of tradeoffs:</b> <br><br><ul><li>  The method allows to find dependencies of an arbitrary type (linear and non-linear) in a space of arbitrary dimension; </li><li>  The method will exclude all input variables if their total or individual information content is lower than the statistically significant one; </li><li>  The weak side of the method is the fact that weak dependencies - which barely exceed the level of statistical noise - will be hidden from the eyes of the researcher after applying the numerically calculated quantile of the VI metric; </li><li>  With a small number of significant discrete input variables, the researcher is able to build a human-readable set of rules that allow interpretation of the dependencies found; </li><li>  It is not necessary to compose randomized committees of models (by analogy with random forest and busting machines), since the method in itself tries many possible levels of "tree depth" for all the selected predictors and returns the most significant, in a statistical sense, combination of informative features; </li><li>  The method is <b>very</b> expensive, especially in comparison with other methods combining greedy gradient descent and a simpler fitness function that does not require simulation corrections; </li><li>   ,         ,      ,         . </li></ul><br><br><h5>   . </h5><br><br>       ,  ,           .    ,     ,     ,        . <br><br>            ‚Äî  (Multiinformation)    (Total Correlation). <br><br>  A source: <br><br>  : <br><br><img src="https://habrastorage.org/files/2b7/1f7/319/2b71f731994b437f9ddb091bba3619a6.png" alt="image"><br><br>      : <br><br><blockquote> Watanabe S (1960). Information theoretical analysis of multivariate correlation, IBM Journal of Research and Development 4, 66‚Äì82 </blockquote><br><br>  ( <b></b> )     ,              .     ,      ( 1  n)       ( 1  m),           .    ,  -       . <br><br>   ,         . <br><br>         ,   ,        .       : <br><br><blockquote><ul><li> 1)   . ,     ‚Äî ,  ,  ,      ‚Äî         .  ,         ,        2.           ,    ,   . </li><li> 2)         .      ,      ,     . ,  ,           ,      ‚Äî      . </li><li> 3)   .      ,           . </li></ul></blockquote><br><br> <i><a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D1%2582%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B8%25D1%258F_%25D1%2581%25D0%25B2%25D1%258F%25D0%25B7%25D0%25B8_(%25D1%2581%25D1%2582%25D0%25B0%25D1%2582%25D1%258C%25D1%258F)">.</a></i> <br><br>              , ,  : <br><blockquote>   ,   .   .  ,   . ., , 1973 ‚Äî 512 . </blockquote><br><blockquote>  .. ‚Äî ¬´    ¬ª. </blockquote><br><br><h5>     ,    . </h5><br><br>      ,           ,  ,           ,          . <br><br>       ,        n       N. <br><br>       .  ,      . <br><br>          100 . <br><img src="https://habrastorage.org/files/2fe/a33/366/2fea33366daf428e8a3f3e17723e3ae1.png" alt="image"><br><br>          1000 . <br><img src="https://habrastorage.org/files/7ed/97a/82c/7ed97a82cba54d04826b031e5e6ca3a4.png" alt="image"><br><br>      ,     ,        . <br><br>  ,      ,      .     ,           . .: <br><br> <a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.487.1512%26rep%3Drep1%26type%3Dpdf"> 1.</a> <br> <a href="http://www.stat.berkeley.edu/~binyu/summer08/L2P2.pdf"> 2.</a> <br><br>         ,   ,    .          .          ,      (  ).      ,           . <br><br> <b>          ,      .</b> <br><br>            .                 .        ,    ,      .       . <br><br>          (   ),                      . <br><br>     .     .   ,    ,    ,  .  ,    1 000     51% ,        -         .        . <i>        .</i> <br><br><h5> -            . </h5><br><br><ul><li> )          . </li><li> )     ,   , ,     (     )       ¬´ ¬ª    . </li><li> )    numeric         ,     [0, 1]    (0)   (1)     ‚Äî    SA. </li><li> )     ,   , -,   . </li><li> )    -      ;   (, 0.9)      . </li><li> )       .  ,  . </li><li> )  ,  . </li></ul><br><br>  ¬´¬ª  .    ‚Äî   ,    : <br><br><blockquote> optim_var_num &lt; ‚Äî log(x = sample_size / 100, base = round(mean_levels, 0)) </blockquote><br><br>   , , <u>     </u> ,           ,        ,            n ,  n     100.     ,         ,         ,     . <br><br>            ,              ,           . <br><br>  ,  ,              (   ),    : <br><br><blockquote> threshold &lt; ‚Äî 1 ‚Äî optim_var_num / predictor_number </blockquote><br><br>     ,            .       . <br><br> ,   :   ,    . <br><br>   17 973 , 12 ,    5 .       ,      3,226. <br><br>        ,  0,731. <br><br>           ? <br><img src="https://habrastorage.org/files/1be/93e/209/1be93e209508477286aecfa50c47562c.JPG" alt="image"><br><br>    3 .   ,  5 ^ 3,226   178 ,       100 . <br><br>      . <br><br> <b> ‚Ññ8   ,   ()         (     , - ‚Äî       ).</b> <br><br>      0,9   ( )  10  -,   1  1     ( ).    10 . <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">############## simlated annelaing search with mutual information fitness function library(infotheo) # measured in nats, converted to bits library(scales) library(GenSA) sampleA &lt;- bunny_dat[sample(nrow(bunny_dat), nrow(bunny_dat) / 2, replace = F), c("x", "y", "input_noise_1", "input_noise_2", "input_noise_3", "input_noise_4", "input_noise_5", "input_noise_6", "input_noise_7", "input_noise_8", "input_noise_9", "input_noise_10", "z")] sampleB &lt;- bunny_dat[!row.names(bunny_dat) %in% rownames(sampleA), c("x", "y", "input_noise_1", "input_noise_2", "input_noise_3", "input_noise_4", "input_noise_5", "input_noise_6", "input_noise_7", "input_noise_8", "input_noise_9", "input_noise_10", "z")] # discretize all variables dat &lt;- sampleA disc_levels &lt;- 5 for (i in 1:13){ naming &lt;- paste(names(dat[i]), 'discrete', sep = "_") dat[, eval(naming)] &lt;- discretize(dat[, eval(names(dat[i]))], disc = "equalfreq", nbins = disc_levels)[,1] } sampleA &lt;- dat[, 14:26] #calculate parameters predictor_number &lt;- dim(sampleA)[2] - 1 sample_size &lt;- dim(sampleA)[1] par_v &lt;- runif(predictor_number, min = 0, max = 1) par_low &lt;- rep(0, times = predictor_number) par_upp &lt;- rep(1, times = predictor_number) #load functions to memory shuffle_f_inp &lt;- function(x = data.frame(), iterations_inp, quantile_val_inp){ mutins &lt;- c(1:iterations_inp) for (count in 1:iterations_inp){ xx &lt;- data.frame(1:dim(x)[1]) for (count1 in 1:(dim(x)[2] - 1)){ y &lt;- as.data.frame(x[, count1]) y$count &lt;- sample(1 : dim(x)[1], dim(x)[1], replace = F) y &lt;- y[order(y$count), ] xx &lt;- cbind(xx, y[, 1]) } mutins[count] &lt;- multiinformation(xx[, 2:dim(xx)[2]]) } quantile(mutins, probs = quantile_val_inp) } shuffle_f &lt;- function(x = data.frame(), iterations, quantile_val){ height &lt;- dim(x)[1] mutins &lt;- c(1:iterations) for (count in 1:iterations){ x$count &lt;- sample(1 : height, height, replace = F) y &lt;- as.data.frame(c(x[dim(x)[2] - 1], x[dim(x)[2]])) y &lt;- y[order(y$count), ] x[dim(x)[2]] &lt;- NULL x[dim(x)[2]] &lt;- NULL x$dep &lt;- y[, 1] rm(y) receiver_entropy &lt;- entropy(x[, dim(x)[2]]) received_inf &lt;- mutinformation(x[, 1 : dim(x)[2] - 1], x[, dim(x)[2]]) corr_ff &lt;- received_inf / receiver_entropy mutins[count] &lt;- corr_ff } quantile(mutins, probs = quantile_val) } ############### the fitness function fitness_f &lt;- function(par){ indexes &lt;- c(1:predictor_number) for (i in 1:predictor_number){ if (par[i] &gt;= threshold) { indexes[i] &lt;- i } else { indexes[i] &lt;- 0 } } local_predictor_number &lt;- 0 for (i in 1:predictor_number){ if (indexes[i] &gt; 0) { local_predictor_number &lt;- local_predictor_number + 1 } } if (local_predictor_number &gt; 1) { sampleAf &lt;- as.data.frame(sampleA[, c(indexes[], dim(sampleA)[2])]) pred_entrs &lt;- c(1:local_predictor_number) for (count in 1:local_predictor_number){ pred_entrs[count] &lt;- entropy(sampleAf[count]) } max_pred_ent &lt;- sum(pred_entrs) - max(pred_entrs) pred_multiinf &lt;- multiinformation(sampleAf[, 1:dim(sampleAf)[2] - 1]) pred_multiinf &lt;- pred_multiinf - shuffle_f_inp(sampleAf, iterations_inp, quantile_val_inp) if (pred_multiinf &lt; 0){ pred_multiinf &lt;- 0 } pred_mult_perc &lt;- pred_multiinf / max_pred_ent inf_corr_val &lt;- shuffle_f(sampleAf, iterations, quantile_val) receiver_entropy &lt;- entropy(sampleAf[, dim(sampleAf)[2]]) received_inf &lt;- mutinformation(sampleAf[, 1:local_predictor_number], sampleAf[, dim(sampleAf)[2]]) if (inf_corr_val - (received_inf / receiver_entropy) &lt; 0){ fact_ff &lt;- (inf_corr_val - (received_inf / receiver_entropy)) * (1 - pred_mult_perc) } else { fact_ff &lt;- inf_corr_val - (received_inf / receiver_entropy) } } else if (local_predictor_number == 1) { sampleAf&lt;- as.data.frame(sampleA[, c(indexes[], dim(sampleA)[2])]) inf_corr_val &lt;- shuffle_f(sampleAf, iterations, quantile_val) receiver_entropy &lt;- entropy(sampleAf[, dim(sampleAf)[2]]) received_inf &lt;- mutinformation(sampleAf[, 1:local_predictor_number], sampleAf[, dim(sampleAf)[2]]) fact_ff &lt;- inf_corr_val - (received_inf / receiver_entropy) } else { fact_ff &lt;- 0 } return(fact_ff) } ########## estimating threshold for variable inclusion iterations = 10 quantile_val = 0.9 iterations_inp = 1 quantile_val_inp = 1 levels_arr &lt;- numeric() for (i in 1:predictor_number){ levels_arr[i] &lt;- length(unique(sampleA[, i])) } mean_levels &lt;- mean(levels_arr) optim_var_num &lt;- log(x = sample_size / 100, base = round(mean_levels, 0)) if (optim_var_num / predictor_number &lt; 1){ threshold &lt;- 1 - optim_var_num / predictor_number } else { threshold &lt;- 0.5 } #run feature selection start &lt;- Sys.time() sao &lt;- GenSA(par = par_v, fn = fitness_f, lower = par_low, upper = par_upp , control = list( #maxit = 10 max.time = 300 , smooth = F , simple.function = F)) trace_ff &lt;- data.frame(sao$trace)$function.value plot(trace_ff, type = "l") percent(- sao$value) final_vector &lt;- c((sao$par &gt;= threshold), T) names(sampleA)[final_vector] final_sample &lt;- as.data.frame(sampleA[, final_vector]) Sys.time() - start</span></span></code> </pre><br></div></div><br><br>  Result: <br><br><pre> <code class="python hljs">&gt; percent(- sao$value) &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"18.1%"</span></span>&lt;/b&gt; &gt; final_vector &lt;- c((sao$par &gt;= threshold), T) &gt; names(sampleA)[final_vector] &lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"x_discrete"</span></span> <span class="hljs-string"><span class="hljs-string">"y_discrete"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_2_discrete"</span></span> <span class="hljs-string"><span class="hljs-string">"z_discrete"</span></span> &lt;/b&gt; &gt; final_sample &lt;- <span class="hljs-keyword"><span class="hljs-keyword">as</span></span>.data.frame(sampleA[, final_vector]) &gt; &gt; Sys.time() - start Time difference of <span class="hljs-number"><span class="hljs-number">10.00453</span></span> mins</code> </pre><br><br>  .        .   ‚Äî   . <br><br>    20 ,       -  1   . <br><br>  Result: <br><br><pre> <code class="python hljs">&gt; percent(- sao$value) &lt;b&gt;&lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"18.2%"</span></span>&lt;/b&gt;&lt;/b&gt; &gt; final_vector &lt;- c((sao$par &gt;= threshold), T) &gt; names(sampleA)[final_vector] &lt;b&gt;&lt;b&gt;[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-string"><span class="hljs-string">"x_discrete"</span></span> <span class="hljs-string"><span class="hljs-string">"y_discrete"</span></span> <span class="hljs-string"><span class="hljs-string">"input_noise_1_discrete"</span></span> &lt;/b&gt;<span class="hljs-string"><span class="hljs-string">"z_discrete"</span></span> &lt;/b&gt; &gt; final_sample &lt;- <span class="hljs-keyword"><span class="hljs-keyword">as</span></span>.data.frame(sampleA[, final_vector]) &gt; &gt; Sys.time() - start Time difference of <span class="hljs-number"><span class="hljs-number">20.00585</span></span> mins</code> </pre><br><br>     ‚Äî       . <br><br>       18%    .  . <br><br>   (        ): <br><img src="https://habrastorage.org/files/bcc/d64/bca/bccd64bca03b4469902297c5a524fe09.JPG" alt="image"><br><br> <b>Minimum-redundancy-maximum-relevance (mRMR)</b> <br><br>           .       : <br><br> <a href="http://penglab.janelia.org/papersall/docpdf/2005_TPAMI_FeaSel.pdf">¬´Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy,¬ª <br> Hanchuan Peng, Fuhui Long, and Chris Ding <br> IEEE Transactions on Pattern Analysis and Machine Intelligence, <br></a>  <a href="http://penglab.janelia.org/papersall/docpdf/2005_TPAMI_FeaSel.pdf">Vol.</a> <a href="http://penglab.janelia.org/papersall/docpdf/2005_TPAMI_FeaSel.pdf">27, No. 8, pp.1226-1238, 2005</a> <br><br>            ,   (    )    . <a href="https://en.wikipedia.org/wiki/Feature_selection"><i>.</i></a> <br><br>        ()          ,     ()         . <br><br>   ()      ,   ,     ,         .    ,       . <br><br>   ()      ,             .  ,        ( ). <br><br>       ,         ,     ,   ,    ,      (  )    ,     .          , mRMR    ,       . <br><br>  ()      ,                ‚Äî   limited sampling bias. <br><br>       mRMR             . ,                , ,  ,              . <br><br> <b>         .</b> <br><img src="https://habrastorage.org/files/db7/8dd/98a/db78dd98a7be410281bfc1a2a38660ef.JPG" alt="image"><br><br>     ,      ,      GBM,     p-values   . <br><br>            ,      .   ,       ,   ,        . ,        XY-Noise1  .      ,               (  ),         Align Technology.        ,   , ,     Gradient Boosted Trees ( <i><a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">.</a></i> ). <br><br> ,          , ,        .       , ,       ,      ,        . <br><br>      ,    ,     .       .     ,      ,      . <br><br>     .     ,    ,         ,  ,   .       ,      . <br><br>       github: <a href="https://github.com/alexmosc/habr_feature_selection_experiments_R_code/">Git</a> <br><br><h5>   : </h5><br><ul><li> <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">Greedy Function Approximation: A Gradient Boosting Machine by J. Friedman</a> </li><li> <a href="https://en.wikipedia.org/wiki/Feature_selection">   </a> </li><li>   ,   .   .  ,   . ., , 1973 ‚Äî 512 . </li><li>  .. ‚Äî ¬´    ¬ª. </li><li> <a href="http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.487.1512%26rep%3Drep1%26type%3Dpdf">Analytical estimates of limited sampling biases in different information measures by S. Panzeri at.al.</a> </li><li> <a href="http://www.stat.berkeley.edu/~binyu/summer08/L2P2.pdf">Estimation of Entropy and Mutual Information by L. Paninski</a> </li><li> <a href="http://www.cefage.uevora.pt/content/download/750/8442/version/1/file/andreia%2520dionisio.pdf">Entropy-Based Independence Test by A. Dion√≠sio at.al.</a> </li><li> <a href="https://habrahabr.ru/post/127394/">        ,   ,    DJI, . </a> </li><li> <a href="http://arxiv.org/abs/1505.02213">Measuring dependence powerfully and equitably by Y. Reshef et.al.</a> </li><li> <a href="http://penglab.janelia.org/papersall/docpdf/2005_TPAMI_FeaSel.pdf">¬´Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy,¬ª Hanchuan Peng, Fuhui Long, and Chris Ding IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 27, No. 8, pp.1226-1238, 2005</a> </li><li> Watanabe S (1960). Information theoretical analysis of multivariate correlation, IBM Journal of Research and Development 4, 66‚Äì82 </li><li> <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/">Gradient boosting machines, a tutorial.</a> </li><li> Claude E. Shannon, Warren Weaver. The Mathematical Theory of Communication. Univ of Illinois Press, 1949 </li></ul></div><p>Source: <a href="https://habr.com/ru/post/303750/">https://habr.com/ru/post/303750/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../303740/index.html">Solving square equations through derivatives</a></li>
<li><a href="../303742/index.html">Experience in developing management software for the questroom</a></li>
<li><a href="../303744/index.html">Search Postgres using ZomboDb and elasticsearch</a></li>
<li><a href="../303746/index.html">CTO "Medusa" - Samat Galimov briefly about what it is to be the main development in the media</a></li>
<li><a href="../303748/index.html">What hides Array # sort: reverse engineering by improvised means</a></li>
<li><a href="../303752/index.html">Emsisoft experts found another JavaScript extortionist</a></li>
<li><a href="../303754/index.html">Alan Kay, the creator of the PLO, about the development of Lisp and PLO</a></li>
<li><a href="../303756/index.html">How we tested the Russian speedtest</a></li>
<li><a href="../303760/index.html">7 rules for writing world-class technical documentation</a></li>
<li><a href="../303762/index.html">Ansible: testing playbooks (part 1)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>