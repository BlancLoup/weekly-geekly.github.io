<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Testing flash storage. Violin 6232 Series Flash Memory Array</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We continue the topic begun in the articles " Testing Flash Storage. Theoretical Part " and " Testing Flash Storage. IBM RamSan FlashSystem 820" . Tod...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Testing flash storage. Violin 6232 Series Flash Memory Array</h1><div class="post__text post__text-html js-mediator-article">  We continue the topic begun in the articles " <a href="http://itg-td.blogspot.com/2014/04/ibm-ramsan-flashsystem-820.html">Testing Flash Storage. Theoretical Part</a> " and " <a href="http://itg-td.blogspot.com/2014/05/ibm-ramsan-flashsystem-820.html" rel="bookmark">Testing Flash Storage. IBM RamSan FlashSystem 820"</a> .  Today we look at the capabilities of one of the most "mass" models of the company Violin Memory.  The startup, founded by immigrants from <a href="http://en.wikipedia.org/wiki/Fusion-io" title="Fusion io">Fusion-io</a> , became the pioneer and spiritual leader of the ideology of building data storage systems based solely on flash memory.  The <a href="http://www.violin-memory.com/products/6000-flash-memory-array/">Violin 6232</a> array was released in September 2011 and remained the flagship until the 6264 model was released in August 2013. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cd8/69d/68b/cd869d68bf2bd03882bfb20b88c43afb.gif" height="244" width="640"></div><br><br>  We, as technical specialists, were more interested in the architecture of the Violin Memory arrays, which is their distinguishing feature and undoubted advantage in comparison with competitors.  Each component is the <a href="http://www.violin-memory.com/products/technology-architecture/">company's own development</a> : <br><ul><li>  Own flash modules (VIMM); </li><li>  Own VMOS operating system optimized for working with flash; </li><li>  Own patented RAID (vRAID), devoid of the disadvantages of standard RAID 5.6; </li></ul><br>  A system without a single point of failure, where all components are duplicated.  Where the replacement of components or firmware upgrade does not only require stopping work, but does not reduce performance: 4 controllers, no internal cache, writing full ‚Äústripes‚Äù, optimal algorithms for ‚Äúgarbage collection‚Äù.  This architecture allows you to get the highest performance, minimize delays and side effects (Write Cliff), ensures the availability of data level 99.9999 and eliminates the loss of performance with the possible failure of components.  A rich, thought-out management interface harmoniously adds to the convenience of working with Violin equipment.  Many technological advantages are achieved through joint development with Toshiba, which is the company's main investor. <br><a name="habracut"></a><br><h2>  <b>Testing method</b> </h2><br>  During testing, the following tasks were solved: <br><ul><li>  explore the process of storage performance degradation during long-term write write and read; </li><li>  explore the performance of Violin 6232 storage systems under various load profiles; </li><li>  study the effect of LUN block size on performance. </li></ul><habracut><br><h4>  Testbed Configuration </h4><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3e6/0b1/3ac/3e60b13ac7eaa646addf58567cd036e8.gif" height="178" width="640"></div></td></tr><tr><td>  Figure 1. Block diagram of the test bench </td></tr></tbody></table><br>  The test bench consists of a server connected through an FC factory using four 8Gb FC connections to the Violin 6232 storage system. The server and array specifications are as follows: <abbr title="Processor: 2x Intel (R) Xeon (R) CPU E5-2470 0 @ 2.30GHz RAM: 96GB DDR3 1333 FC adapter: 2x Qlogic QLE2562 fw 5.08.00 OS: RHEL 6.4 x86-64">IBM 3630 M4 server (7158-AC1)</abbr> ;  <abbr title="Raw capacity: 32TiB Formatted capacity: 18.56TiB Connection interfaces: 8x FC 4-8Gb Number of flash modules (data + HS): 60 + 4 Firmware version: G5.6.0">Storage System Violin Memory 6232</abbr> <br>  As additional software, Symantec Storage Foundation 6.1 is installed on the test server, which implements: <br><ul><li>  functional logical volume manager (Veritas Volume Manager); </li><li>  functionality of fault-tolerant connection to disk arrays (Dynamic Multi Pathing).  (For tests of groups 1 and 2. For tests of group 3 native Linux DMP is used) </li></ul><br><div class="spoiler">  <b class="spoiler_title">See tiresome details and all sorts of clever words.</b> <div class="spoiler_text">  On the test server, settings were made to reduce the disk I / O latency: <br><ul><li> changed the I / O scheduler from <code>cfq</code> to <code>noop</code> by assigning the value of noop to the parameter <code>/sys/&lt;___Symantec_VxVM&gt;/queue/scheduler</code> </li><li>  Added a parameter to <code>/etc/sysctl.conf</code> minimizes the queue size at the level of the Symantec logical volume manager: <code>vxvm.vxio.vol_use_rq = 0</code> ; </li><li>  The limit of simultaneous I / O requests to the device is increased to 1024 by setting the value of 1024 to the parameter <code>/sys/&lt;___Symantec_VxVM&gt;/queue/nr_requests</code> </li><li>  Disabling the check of the possibility of merging I / O operations (iomerge) by setting the value 1 to <code>/sys/&lt;___Symantec_VxVM&gt;/queue/nomerges</code> </li><li>  increased queue size on FC adapters by adding options <code>ql2xmaxqdepth=64 (options qla2xxx ql2xmaxqdepth=64)</code> <code>/etc/modprobe.d/modprobe.conf</code> configuration file <code>ql2xmaxqdepth=64 (options qla2xxx ql2xmaxqdepth=64)</code> ; </li></ul><br>  The storage configuration of the disk space is made on the storage system: for all tests 8 LUNs of the same size are created.  Their total volume covers the entire usable capacity of the disk array.  For group 2 tests, the LUN block size is set to 512B, for group 3 tests, the LUN block size is set to 4KB.  Created LUNs are presented to the test server. </div></div><br><h4>  Software used in the testing process </h4><br>  To create a synthetic load (performance of synthetic tests) on the storage system, the Flexible IO Tester (fio) version 2.1.4 utility is used.  All synthetic tests use the following fio configuration parameters of the [global] section: <br><ul><li>  thread = 0 </li><li>  direct = 1 </li><li>  group_reporting = 1 </li><li>  norandommap = 1 </li><li>  time_based = 1 </li><li>  randrepeat = 0 </li><li>  ramp_time = 10 </li></ul><br>  The following utilities are used to remove performance indicators under synthetic load: <br><ul><li>  iostat, part of the sysstat version 9.0.4 package with <code>txk</code> keys; </li><li>  vxstat, which is part of Symantec Storage Foundation 6.1 with <code>svd</code> keys; </li><li>  vxdmpadm, part of Symantec Storage Foundation 6.1 with the <code>-q iostat</code> keys; </li><li>  fio version 2.1.4, to generate a summary report for each load profile. </li></ul><br>  The removal of performance indicators during the test with the utilities iostat, vxstat, vxdmpstat is performed at intervals of 5 seconds. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Testing program. </h3><br>  Testing consisted of 3 groups of tests.  The tests were performed by creating a synthetic load with the fio program on a block device, which is a stripe-type logical volume with a bundle of 8 disks, a data block size of 1MiB, created using Veritas Volume Manager or Native Linux LVM (in group 3 ) of 8 LUNs presented from the system under test. <br><br><div class="spoiler">  <b class="spoiler_title">Ask for details</b> <div class="spoiler_text"><h5>  Group 1: Tests that implement a continuous load of random write type with a change in the size of the block I / O operations (I / O). </h5><br>  When creating a test load, the following additional parameters of the fio program are used: <br><ul><li>  rw = randwrite </li><li>  blocksize = 4K </li><li>  numjobs = 64 </li><li>  iodepth = 64 </li></ul><br>  A test group consists of four tests that differ in the total volume of LUNs presented with the tested storage system, the size of the block of I / O operations and the I / O direction (write or read): <br><ul><li>  a test for recording performed on a fully-marked storage system ‚Äî the total volume of the presented LUNs is equal to the usable storage capacity of the storage system, the test duration is 7.5 hours; </li><li>  write tests with varying block size (4,32,1024K), performed on a fully-marked storage system, the duration of each test is 4.5 hours.  Pause between tests - 2 hours. </li></ul><br>  Based on the test results, based on the data output by the vxstat command, graphs are generated that combine the test results: <br><ul><li>  IOPS as a function of time; </li><li>  Latency as a function of time. </li></ul><br>  The analysis of the received information is carried out and conclusions are drawn about: <br><ul><li>  the presence of performance degradation during long-term load on the record and reading; </li><li>  the performance of the service processes storage (Garbage Collection), limiting the performance of the disk array to write during a long peak load; </li><li>  the degree of influence of the size of the block of I / O operations on the performance of the storage service processes; </li><li>  the amount of space reserved for storage for leveling storage service processes. </li></ul><br><h5>  Group 2: Disk array performance tests with different types of load, executed at the block device level created by Symantec Volume Manager (VxVM) with a LUN block size of 512 bytes. </h5><br>  During testing, the following types of loads are investigated: <br><ul><li>  load profiles (changeable software parameters fio: randomrw, rwmixedread): </li></ul><ol><li>  random recording 100%; </li><li>  random write 30%, random read 70%; </li><li>  random read 100%. </li></ol><ul><li>  block sizes: 1KB, 8KB, 16KB, 32KB, 64KB, 1MB (changeable software parameter fio: blocksize); </li><li>  methods of processing I / O operations: synchronous, asynchronous (variable software parameter fio: ioengine); </li><li>  the number of load generating processes: 1, 2, 4, 8, 16, 32, 64, 128, 160, 192 (changeable software parameter fio: numjobs); </li><li>  queue depth (for asynchronous I / O operations): 32, 64 (changeable software parameter fio: iodepth). </li></ul><br>  A test group consists of a set of tests representing all possible combinations of the above types of load.  To level the impact of the Garbage Collection service processes on the test results, between the tests a pause is realized equal to the ratio of the amount of information recorded during the test to the performance of the storage service processes (determined by the results of the first group of tests). <br>  Based on the test results, the following graphs are generated for each combination of the following load types based on the data output by the fio software after each of the tests: load profile, method of processing I / O operations, queue depth, which combine tests with different I / O block values : <br><ul><li>  IOPS - as a function of the number of load generating processes; </li><li>  Bandwidth - as a function of the number of processes that generate the load; </li><li>  Latitude (clat) - as a function of the number of processes that generate the load; </li></ul><br>  The analysis of the obtained results is carried out, conclusions are drawn on the load characteristics of the disk array at latency &lt;1ms. <br><br><h5>  Group 3: disk array performance tests with a synchronous I / O method, different types of load, executed at the level of a block device created using Linux LVM, with a block size of LUN of 4KiB. </h5><br>  Tests are conducted similarly to tests of group 2, but only the synchronous method of iv is investigated because of the limited testing time.  At the end of each test, graphs are constructed showing the difference in% of the obtained performance indicators (iops, latency) from those obtained during testing with a block size of LUN 512 bytes (test group 2).  Conclusions are made about the effect of the size of the LUN block on the performance of the disk array. <br><br></div></div><br><h2>  <b>Test results</b> </h2><br><h5>  Group 1: Tests that implement a continuous load of random write type with a change in the size of the block I / O operations (I / O). </h5><br>  1. With a long load on the recording at a certain point in time, a significant degradation of storage performance is recorded.  A drop in performance is expected and is a feature of SSD (Write Cliff) work related to the inclusion of Garbage Collection (GC) processes and limited performance of the indicated processes.  The performance of the disk array, fixed with running GC processes, can be considered as the maximum average performance of the disk array. <br><div class="spoiler">  <b class="spoiler_title">Charts</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/802/d7e/8d1/802d7e8d1d10a3500cfb549580ec78ec.jpg" height="470" width="640"></div></td></tr><tr><td>  Changing the speed of I / O operations (iops) and delays (Latency) during long-term 4K recording </td></tr></tbody></table><br></div></div><br>  2. The block size with long write load does not affect the performance of the GC process.  CG operates at a speed of about 600Mib / s. <br><br>  3. The difference in the values ‚Äã‚Äãof the maximum storage operation time at peak performance recorded during the first long test and the subsequent equivalent test with the 4K unit is caused by the incomplete storage system before testing. <br><div class="spoiler">  <b class="spoiler_title">Schedule</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d77/d9e/9b5/d77d9e9b5d192bf47072cc1dbd09497f.jpg" height="390" width="640"></div></td></tr><tr><td>  Change of the I / O speed (iops) during long-term recording in 4K, 32K blocks </td></tr></tbody></table><br></div></div><br>  4. The maximum storage time for peak performance is significantly different with a 4K block and all other blocks, which is most likely due to the architectural optimization of the storage for the designated block (Violin storage systems always write full 4K stripe using the configuration of RAID5 flash modules (4 + P) , stripe unit size 1K). <br><div class="spoiler">  <b class="spoiler_title">Chart and table</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f94/a59/841/f94a59841c0d96025844b886c7715a73.jpg" height="390" width="640"></div></td></tr><tr><td>  Change of data transmission speed (bandwidth) during long recording with different block sizes. </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/435/2f2/0fc/4352f20fcbff1d14c0d110ecd3283084.png" height="212" width="640"></div></td></tr><tr><td>  The dependence of storage performance on the block size during long-term recording load. </td></tr></tbody></table><br></div></div><br><h5>  Group 2: Disk array performance tests with different types of load, executed at the block device level created by Symantec Volume Manager (VxVM) with a LUN block size of 512 bytes. </h5><br><div class="spoiler">  <b class="spoiler_title">Block device performance tables.</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fbf/64d/925/fbf64d9259dd581d90bfc6059ecd7f97.png" height="536" width="640"></div></td></tr><tr><td>  Storage performance with one load generating process (jobs = 1) </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a82/d88/885/a82d888852695a0ea8f398912004e85f.png" height="538" width="640"></div></td></tr><tr><td>  Maximum storage performance with delays less than 1ms </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dee/95a/8cb/dee95a8cb8715d55d1e02b6b076937a8.png" height="536" width="640"></div></td></tr><tr><td>  Maximum storage performance at a different load profile. </td></tr></tbody></table></div></div><br><div class="spoiler">  <b class="spoiler_title">Block device performance graphs.</b> <div class="spoiler_text">  (All pictures are clickable) <br><table border="1" cellpadding="2" cellspacing="2"><tbody><tr><td><br></td><td>  Synchronous way in / in </td><td>  Asynchronous way in / in with a queue depth of 32 </td><td>  Asynchronous way in / in with a queue depth of 64 </td></tr><tr><td>  Random reading </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ca1/cf7/54a/ca1cf754aa5c0b9c3934a16d37485f7a.jpg" height="320" width="226"></div><br><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/240/490/2e8/2404902e8000cb04ccb34b05ea933f3b.jpg" height="320" width="226"></div><br><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/73c/1c3/fb8/73c1c3fb8d25fbf9ecd721a93a6fb629.jpg" height="320" width="226"></div><br><br></td></tr><tr><td>  With random recording </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f22/f02/f76/f22f02f7609806497016e1b15a77d152.jpg" height="320" width="226"></div><br><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ae/fa2/545/5aefa2545869dcc8e638cb91e73aeb00.jpg" height="320" width="226"></div><br><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d62/cdd/042/d62cdd0420aa695ce666642290463a06.jpg" height="320" width="226"></div><br><br></td></tr><tr><td>  With mixed load (70% read, 30% write) </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1a9/156/e80/1a9156e80f04f4684f362453e4f1ba4c.jpg" height="320" width="226"></div><br><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/db1/b2c/a71/db1b2ca7148843473c59dac7e1202898.jpg" height="320" width="226"></div><br><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/080/e6c/0dc/080e6c0dc954654b6ec537a1231fb6e6.jpg" height="320" width="226"></div><br><br></td></tr></tbody></table><br></div></div><br><ul><li>  Approximately identical array read and write performance was obtained. </li><li>  Failed to get the manufacturer declared performance on read operations (maximum 500 000IOPS is claimed). </li><li>  With mixed I / O, the array shows less performance than separately when writing and reading with almost any I / O profile. </li><li>  A significant performance degradation is recorded with an 8K block on a mixed load profile with an increase in the number of I / O threads.  (The cause of the detected phenomenon is currently not clear). </li></ul><br><h4>  Maximum recorded performance parameters for Violin 6232 </h4><br>  Record: <br><ul><li>  307000 IOPS with latency 0.8ms (4KB async qd32 block) </li><li>  Bandwidth: 2224MB / c for large blocks </li></ul><br>  Reading: <br><ul><li>  256000 IOPS with latency 0.7ms (4KB sync block); </li><li>  300000 IOPS with latency 6,7ms (4KB async qd 32 block); </li><li>  Bandwidth: 2750MB / s for medium blocks (16-32K). </li></ul><br>  Mixed load (70/30 rw) <br><ul><li>  256000 IOPS with latency 0.7ms (4KB sync block); </li><li>  305,000 IOPS with latency 6,7ms (4KB async qd 64 block); </li><li>  Bandwidth 2700MB / s for medium blocks (16-32K) </li></ul><br>  Minimal latency fixed: <br><ul><li>  When recording - 0.146ms for 4K jobs = 1 block </li><li>  When reading - 0.21ms for 4K jobs = 1 </li></ul><br><h5>  Group 3: disk array performance tests with a synchronous I / O method, different types of load, executed at the level of a block device created by Linux LVM with a block size of LUN - 4KiB. </h5><br><div class="spoiler">  <b class="spoiler_title">Charts</b> <div class="spoiler_text">  (All pictures are clickable) <br><table><tbody><tr><td> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/0aa/33c/c09/0aa33cc095ec948fc47e8947eea21256.jpg" height="502" width="640"></a> </td></tr><tr><td>  The difference between IOPS and Latency between a device with a LUN block size of 4KB and 512B for random reading (figures for block size LUN = 512B are taken as 0) </td></tr></tbody></table><br><table><tbody><tr><td> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/ebb/908/042/ebb908042f141d6e43bd824932d492c2.jpg" height="492" width="640"></a> </td></tr><tr><td>  The difference between IOPS and Latency between a device with a LUN block size of 4KB and 512B for random recording (figures for block size LUN = 512B are taken as 0) </td></tr></tbody></table><br><table><tbody><tr><td> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/c8a/585/0d6/c8a5850d64d71f00c98e123dc79268e2.jpg" height="482" width="640"></a> </td></tr><tr><td>  The difference between IOPS and Latency between a device with a block size of LUN 4KB and 512B with a mixed load (70/30 r / w) (figures for block size LUN = 512B are taken as 0) </td></tr></tbody></table><br></div></div><br><ol><li>  The effect of LUN block size on performance with no more than 64 jobs. </li><li>  With jobs&gt; 64 on write operations, an increase in performance is observed up to 20% compared with the block size of LUN 512B </li><li>  With jobs&gt; 64 on read operations with medium and large blocks, there is a decrease in performance up to 10-15% </li><li>  With a mixed load of small and medium blocks (up to 32K), the array shows the same performance for both sizes of the LUN block.  But with large blocks (64K and 1M), performance improves by up to 50% when using the LUN 4KiB block </li></ol><br><h2>  <b>findings</b> </h2><br>  In general, the array made an impression of a high-end high-grade device.  We managed to get very good results; nevertheless, the impression was left that it was still not possible to choose the entire system resource.  To create the load, a single server with two processors was used, which were overloaded during the testing process.  Most likely, we can say that we have rather reached the limit of the load server capacity than the storage system under test.  Separately, it should be noted: <br><ul><li>  Very good IOPS / rack ratio (3U).  As compared with traditional disk arrays, in fact, it is a competitive solution capable of replacing a set of high-end system cabinets with several Violin shelves with a significant increase in performance. </li><li>  The presence of Enterprise functionality, such as Snapshot, can be useful for combining Test / Development and Production tasks within a single disk system. </li><li>  The absence of a write penalty when forming a RAID-5 (recording with only full stripe) leads to better results in write operations. </li><li>  The presence of 4 RAID controllers and the lack of cache (in the SSD it is not needed) ensures stable performance in case of failures.  On traditional, 2-controller Mid-range systems, if one controller fails, the performance can decline by a factor of 3-4, since  controller failure turns off the entire write cache. </li></ul><br><br><br>  <font color="green">PS The author expresses cordial thanks to Pavel Katasonov, Yuri Rakitin and all other company employees who participated in the preparation of this material.</font> </habracut></div><p>Source: <a href="https://habr.com/ru/post/231057/">https://habr.com/ru/post/231057/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../231045/index.html">Creating a site funnel and measuring the conversion of a cup of coffee without a line of code</a></li>
<li><a href="../231047/index.html">The second law of thermodynamics -> increase in successful projects</a></li>
<li><a href="../231051/index.html">Game for the sysadmin day: guess the racks in our workshop by the handles?</a></li>
<li><a href="../231053/index.html">Start thinking</a></li>
<li><a href="../231055/index.html">InfoboxCloud cloud infrastructure launched in Amsterdam. Technical details of the installation</a></li>
<li><a href="../231059/index.html">Google gives 90 days of Google Play Music All Access in honor of the anniversary of the launch of Chromecast</a></li>
<li><a href="../231061/index.html">Economics of IT for small businesses: an outsourcer or a staff member?</a></li>
<li><a href="../231063/index.html">Google is going to create a genetic-molecular map of an absolutely healthy person.</a></li>
<li><a href="../231065/index.html">As we did XVM. Part One: Start and Gather a Team</a></li>
<li><a href="../231067/index.html">Heathrow Airport equipped with intelligent toilets on the platform of Intel</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>