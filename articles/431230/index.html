<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Storage for HPC infrastructure, or How we collected 65 PB of storage at the Japanese research center RIKEN</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="datacenterknowledge.com 

 Last year, the largest at the moment installation of storage systems based on RAIDIX was implemented. A system of 11 fault-...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Storage for HPC infrastructure, or How we collected 65 PB of storage at the Japanese research center RIKEN</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/gt/hj/ib/gthjibaqdw82ss1jxo2tmhpa97o.jpeg"><br>  <i><font color="#99999">datacenterknowledge.com</font></i> <br><br>  Last year, the largest at the moment installation of storage systems based on RAIDIX was implemented.  A system of 11 fault-tolerant clusters was deployed at the Center for Computational Sciences of the RIKEN Institute (Japan).  The main purpose of the system is the storage for the HPC infrastructure (HPCI), which is implemented as part of a large-scale national project for the exchange of academic information Academic Cloud (based on the SINET network). <br><br>  A landmark feature of this project is its total volume - 65 PB, of which the useful volume of the system is 51.4 PB.  To more accurately understand this value, we add that this is 6512 disks of 10 TB each (the most modern at the time of installation).  It's a lot. <br><a name="habracut"></a><br>  Work on the project went on for a year, after that the monitoring of the system‚Äôs operation continued for about a year.  The obtained indicators met the stated requirements, and now we can talk about the success of this record and significant project for us. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Supercomputer at the Center for Computational Sciences of the RIKEN Institute </h2><br>  For ICT, the RIKEN institute is known primarily for its legendary ‚ÄúK-computer‚Äù (from Japanese ‚Äúkei‚Äù, which means 10 quadrillion), which at the time of launch (June 2011) was considered the most productive supercomputer in the world. <br><br><div class="spoiler">  <b class="spoiler_title">Read about K-computer</b> <div class="spoiler_text">  <a href="https://www.nytimes.com/2011/06/20/technology/20computer.html">www.nytimes.com/2011/06/20/technology/20computer.html</a> <br></div></div><br>  The supercomputer helps the Center for Computational Sciences in the implementation of the most complex large-scale studies: it allows for modeling climate, weather conditions and molecular behavior, calculating and analyzing reactions in nuclear physics, predicting earthquakes and much more.  Supercomputer powers are also used for more ‚Äúeveryday‚Äù and applied research ‚Äî to search for oil fields and predict trends in stock markets. <br><br>  Similar calculations and experiments generate a huge amount of data, the value and significance of which cannot be overestimated.  To extract the maximum benefit from this, Japanese scientists developed the concept of a single information space in which HPC professionals from different research centers will have access to the HPC resources obtained. <br><br><h2>  High Performance Computing Infrastructure (HPCI) </h2><br>  HPCI operates on the basis of SINET (The Science Information Network) - a backbone network for the exchange of scientific data between Japanese universities and research centers.  Currently, SINET brings together about 850 institutes and universities, creating tremendous opportunities for information exchange in research that affects nuclear physics, astronomy, geodesy, seismology and computer science. <br><br>  HPCI is a unique infrastructure project that forms a unified system for exchanging information in the field of high-performance computing between universities and research centers in Japan. <br><br>  By combining the capabilities of the ‚ÄúK‚Äù supercomputer and other scientific centers in an accessible form, the scientific community receives obvious benefits for working with valuable data generated by supercomputer computing. <br><br>  In order to ensure efficient user sharing of the HPCI environment, the storage facility was subject to high speed access requirements.  And thanks to the ‚Äúhyper-productivity‚Äù of the K-computer, the storage cluster at the Center for Computational Sciences of the RIKEN Institute was calculated to be created with a working volume of at least 50 PB. <br><br>  The HPCI project infrastructure was built on the basis of the Gfarm file system, which made it possible to provide a high level of performance and integrate disparate storage clusters into a single space for sharing. <br><br><h2>  Gfarm file system </h2><br>  Gfarm is an open source distributed file system developed by Japanese engineers.  Gfarm is the fruit of the development of the Institute of Advanced Industrial Science and Technology (AIST), and the name of the system refers to the used architecture of Grid Data Farm. <br><br>  This file system combines a number of seemingly incompatible properties: <br><br><ul><li>  Highly scalable in size and performance </li><li>  Distribution of the network over long distances with the support of a single namespace for several separated research centers </li><li>  POSIX API support </li><li>  High level of performance required for parallel computing </li><li>  Secure data storage </li></ul><br>  Gfarm creates a virtual file system using multiple server storage resources.  The data is distributed by the metadata server, and the distribution scheme itself is hidden from users.  I must say that Gfarm consists not only of a storage cluster, but also a compute grid using the resources of the same servers.  The principle of operation of the system resembles Hadoop: the submitted work is ‚Äúlowered‚Äù to the node where the data lies. <br><br>  The file system architecture is asymmetric.  Explicit roles: Storage Server, Metadata Server, Client.  But at the same time, all three roles can be performed by the same machine.  Storage servers store multiple copies of files, and metadata servers operate in master-slave mode. <br><br><h2>  Project work </h2><br>  Core Micro Systems, a strategic partner and exclusive RAIDIX supplier in Japan, was responsible for implementation at the RIKEN Institute's Computational Science Center.  The project took about 12 months of hard work, in which not only Core Micro Systems employees, but also technical specialists of the Radiks team took an active part. <br><br>  At the same time, the transition to another storage system looked unlikely: the existing system had a lot of technical bindings that complicated the transition to any new brand. <br><br>  In the course of long-term tests, checks and improvements, RAIDIX demonstrated consistently high performance and efficiency when working with such impressive amounts of data. <br><br>  About the improvements should tell a little more.  It was necessary not only to create the integration of storage systems with the Gfarm file system, but also to expand some functional characteristics of the software.  For example, in order to meet the established technical requirements, it was necessary to develop and implement Automatic Write-Through technology in the shortest possible time. <br><br>  The deployment itself took place systematically.  Engineers from Core Micro Systems carefully and accurately conducted each stage of the test, gradually increasing the scale of the system. <br><br>  In August 2017, the first deployment phase was completed, when the volume of the system reached 18 PB.  In October of the same year, the second phase was implemented, at which the volume rose to a record 51 PB. <br><br><h2>  Solution Architecture </h2><br>  The solution was created based on the integration of RAIDIX storage systems and the Gfarm distributed file system.  In conjunction with Gfarm, create scalable storage using 11 dual-controller RAIDIX systems. <br><br>  Connection to Gfarm servers is via 8 x SAS 12G. <br><br><img src="https://habrastorage.org/webt/ii/3x/wk/ii3xwkawuyvwht0pczwonxbumnu.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">1. Image of a cluster with a separate data server for each node</font></i> <br><br>  (1) 48Gbps √ó 8 connections SAN Mesh connection;  bandwidth: 384Gbps <br>  (2) 48Gbps √ó 40 Mesh FABRIC connections;  bandwidth: 1920Gbps <br><br><h3>  Configuration of dual-controller platform </h3><br><table><tbody><tr><td>  CPU </td><td>  Intel Xeon E5-2637 - 4pcs </td></tr><tr><td>  Motherboard </td><td>  Compatible with processor model with support for PCI Express 3.0 x8 / x16 </td></tr><tr><td>  Internal cache </td><td>  256 GB for each node </td></tr><tr><td>  Chassis </td><td>  2U </td></tr><tr><td>  SAS controllers for connecting disk shelves, servers and write cache synchronization </td><td>  Broadcom 9305 16e, 9300 8e </td></tr><tr><td>  HDD </td><td>  HGST Helium 10TB SAS HDD </td></tr><tr><td>  HeartBeat sync </td><td>  Ethernet 1 GbE </td></tr><tr><td>  CacheSync sync </td><td>  6 x SAS 12G </td></tr></tbody></table><br>  Both nodes of the failover cluster are connected to 10 JBODs (60 disks of 10TB each) through 20 SAS 12G ports for each node.  On these disk shelves 58 RAID6 arrays of 10TB each were created (8 data disks (D) + 2 parity disks (P)) and 12 disks were allocated for ‚Äúhot swap‚Äù. <br><br>  10 JBOD =&gt; 58 √ó RAID6 (8 data disks (D) + 2 parity disks (P)), LUN of 580 HDD + 12 HDD for ‚Äúhot swap‚Äù (2.06% of the total) <br><br>  592 HDD (10TB SAS / 7.2k HDD) per cluster * HDD: HGST (MTBF: 2,500,000 hours) <br><br><img src="https://habrastorage.org/webt/qv/vc/0e/qvvc0egbrc1txvsztmx3o6yl918.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">2. Failover Cluster with 10 JBOD Connection Diagrams</font></i> <br><br><h3>  General scheme of the system and connections </h3><br><img src="https://habrastorage.org/webt/6m/ju/xv/6mjuxvlhx5zlcnqb1eopx9nknfu.png"><br><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">3. Image of a single cluster within the HPCI system</font></i> <br><br><h2>  Key project indicators </h2><br><blockquote>  Net capacity per cluster: <b>4.64 PB</b> ((RAID6 / 8D + 2P) LUN √ó 58) <br><br>  Total effective capacity of the entire system: <b>51.04 PB</b> (4.64 PB √ó 11 clusters). <br><br>  Total system capacity: <b>65 PB</b> . <br><br>  The system performance was: <b>17 GB / s</b> for writing, <b>22 GB / s</b> for reading. <br><br>  The total performance of the cluster disk subsystem on 11 RAIDIX storage systems: <b>250 GB / s</b> . </blockquote></div><p>Source: <a href="https://habr.com/ru/post/431230/">https://habr.com/ru/post/431230/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../431216/index.html">Snom D345 IP Phone Review</a></li>
<li><a href="../431218/index.html">How I Made a Lovecraft Comic Game</a></li>
<li><a href="../431220/index.html">Biologist's look at the roots of our aging</a></li>
<li><a href="../431226/index.html">The Snake game for FPGA Cyclone IV (with VGA & SPI joystick)</a></li>
<li><a href="../431228/index.html">Light obstacle run: liquid crystals to help</a></li>
<li><a href="../431232/index.html">We generate beautiful SVG placeholders on Node.js</a></li>
<li><a href="../431234/index.html">December 11, Moscow - Alfa JS MeetUp</a></li>
<li><a href="../431236/index.html">How to write on Objective-C in 2018. Part 1</a></li>
<li><a href="../431238/index.html">Event digest for HR-specialists in IT in December 2018</a></li>
<li><a href="../431240/index.html">The Supreme Court of the Russian Federation clarified what ‚Äúspecial means for secretly obtaining information‚Äù is</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>