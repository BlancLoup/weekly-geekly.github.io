<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Databases and Kubernetes (review and video report)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="On November 8, the report ‚ÄúDatabases and Kubernetes‚Äù was presented in the main conference room of HighLoad ++ 2018 , within the DevOps and Operation s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Databases and Kubernetes (review and video report)</h1><div class="post__text post__text-html js-mediator-article">  On November 8, the report ‚ÄúDatabases and Kubernetes‚Äù was presented in the main conference room of <a href="http://www.highload.ru/moscow/2018">HighLoad ++ 2018</a> , within the DevOps and Operation section.  It tells about high availability of databases and approaches to fault tolerance up to Kubernetes and with it, as well as practical options for deploying DBMS in Kubernetes clusters and existing for this solution (including Stolon for PostgreSQL). <br><br><img src="https://habrastorage.org/webt/oq/mh/kp/oqmhkpy4pxg-olk9yybf_julwvu.jpeg"><br><br>  By tradition, we are glad to present a <a href="https://www.youtube.com/watch%3Fv%3DBnegHj53pW4"><b>video with a report</b></a> (about an hour, <b>much more</b> informative <b>than an</b> article) and the main squeeze in text form.  Go! <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Theory </h2><br>  This report appeared as an answer to one of the most popular questions that over the past years we have been tirelessly asked in different places: comments on Habr√© or YouTube, social networks, etc.  It sounds simple: ‚ÄúIs it possible to launch a base in Kubernetes?‚Äù, And if we usually answered it ‚Äúin general, yes, but ...‚Äù, then the explanations for these ‚Äúin general‚Äù and ‚Äúbut‚Äù clearly did not suffice, but to accommodate them in a short message could not succeed. <br><br>  However, to begin with, I will generalize the question from the ‚Äú[database]‚Äù to the stateful as a whole.  A DBMS is only a special case of stateful solutions, a more complete list of which can be represented as follows: <br><br><img src="https://habrastorage.org/webt/px/ps/2_/pxps2_ff80ru5qfth8bduiu_kdw.png"><br><br>  Before looking at specific cases, I‚Äôll talk about three important features of the work / use of Kubernetes. <br><br><h3>  1. The philosophy of high availability in Kubernetes </h3><br>  Everyone knows the analogy ‚Äúpets <a href="https://www.theregister.co.uk/2013/03/18/servers_pets_or_cattle_cern/">vs cattle</a> ‚Äù and understand that if Kubernetes is a story from the world of the herd, then the classic DBMS is just pets. <br><br>  And how did the architecture of the ‚Äúpets‚Äù look in the ‚Äútraditional‚Äù version?  The classic MySQL installation example is replication on two iron servers with backup power, disk, network ... and everything else (including an engineer and various support tools), which will help us to be sure that the MySQL process will not fall, and if there is a problem with any of the critical ones. for it components, fault tolerance will be respected: <br><br><img src="https://habrastorage.org/webt/0m/qg/s3/0mqgs3e3cco1zukmlah1jg2ubjs.png"><br><br>  How will the same look in Kubernetes?  There are usually much more iron servers, they are simpler and they do not have redundant power and network (in the sense that dropping one machine doesn‚Äôt affect anything) - all of this is clustered together.  Its fault tolerance is ensured by software: if something happens to a node, Kubernetes detects it and runs the necessary instances on another node. <br><br>  What are the mechanisms for high availability in K8s? <br><br><img src="https://habrastorage.org/webt/n2/gw/mh/n2gwmhiogm3uzv1m5igrzqpyifq.png"><br><br><ol><li>  Controllers.  There are many of them, but there are two main ones: <code>Deployment</code> (for stateless applications) and <code>StatefulSet</code> (for stateful applications).  They contain all the logic of actions taken in the event of a node fall (inaccessibility of the pod). </li><li>  <code>PodAntiAffinity</code> - the ability to indicate to certain pods that they are not on the same node. </li><li>  <code>PodDisruptionBudgets</code> - the limit on the number of pod instances that can be simultaneously turned off in case of planned work. </li></ol><br><h3>  2. Assurance of consistency in Kubernetes </h3><br>  How does the familiar one-master failover scheme work?  Two servers (master and standby), one of which is constantly accessed by the application, which in turn is used through the load balancer.  What happens in case of a network problem? <br><br><img src="https://habrastorage.org/webt/6p/1k/ve/6p1kvelnrzyrphtu6sb_agftgaa.gif"><br><br>  The classic <a href="https://en.wikipedia.org/wiki/Split-brain_(computing)"><i>split-brain</i></a> : the application starts to access both instances of the DBMS, each of which considers itself to be the main.  To avoid this, keepalived was replaced with corosync already with three copies of it to achieve a quorum when voting for the master.  However, even in this case, there are problems: if a failed copy of the DBMS tries in every way to ‚Äúsuicide‚Äù (remove the IP address, transfer the database to read-only ...), then another part of the cluster does not know what happened to the master ‚Äî indeed, it can happen that the node actually still works and requests get to it, which means that we cannot yet switch masters. <br><br>  To solve this situation, there is a node isolation mechanism in order to protect the entire cluster from incorrect operation - this process is called <a href="https://en.wikipedia.org/wiki/Fencing_(computing)"><i>fencing</i></a> .  The practical essence comes down to the fact that we are trying by some external means to ‚Äúkill‚Äù the fallen off car.  Approaches can be different: from shutting down the machine via IPMI and blocking the port on the switch to accessing the cloud provider's API, etc.  And only after this operation, you can switch the wizard.  This achieves an <a href="https://doc.akka.io/docs/akka/current/general/message-delivery-reliability.html%3Flanguage%3Dscala"><i>at-most-once</i></a> guarantee that ensures <i><a href="https://en.wikipedia.org/wiki/Consistency_(database_systems)">consistency</a></i> . <br><br><img src="https://habrastorage.org/webt/sl/xb/9r/slxb9rwf-lk8mcdaeqiuankz3z8.png"><br><br>  How to achieve the same in Kubernetes?  To do this, there are already mentioned controllers, whose behavior in the case of node unavailability is different: <br><br><ol><li>  <code>Deployment</code> : "I was told that there should be 3 pods, and now there are only 2 of them - I will create a new one"; </li><li>  <code>StatefulSet</code> : ‚ÄúPod'a gone?  I will wait: either this node will return, or they will tell us to kill him ‚Äù, i.e.  containers themselves (without operator actions) are not re-created.  This is the way the same at-most-once warranty is achieved. </li></ol><br>  However, here, in the latter case, fencing is required: a mechanism is needed that will confirm that this node is definitely no more.  First of all, it is very difficult to make it automatic (many implementations are required), and secondly, even worse, it usually kills nodes slowly (accessing IPMI can take seconds or tens of seconds, or even minutes).  Few people will wait per minute to switch the base to the new master.  But there is another approach that does not require a fencing mechanism ... <br><br>  I‚Äôll start his description outside Kubernetes.  It uses a special load balancer <i>(load balancer)</i> , through which the backends access the DBMS.  Its specificity lies in the fact that it has the property of consistency, i.e.  protection from network failures and split-brain, because it allows you to remove all connections to the current master, wait for synchronization (replica) on another node and switch to it.  I did not find an established term for this approach and called it <i>Consistent Switchover</i> . <br><br><img src="https://habrastorage.org/webt/ct/oq/lk/ctoqlkp3efyctjlvsyiheesk2s4.gif"><br><br>  The main question with him is how to make it universal, providing support to both cloud providers and private installations.  To do this, proxy servers are added to the applications.  Each of them will receive requests from its application (and send them to the DBMS), and quorum will be collected from all together.  As soon as a part of the cluster fails, those proxies that have lost their quorum immediately remove their connections to the DBMS. <br><br><img src="https://habrastorage.org/webt/nj/y0/-t/njy0-tahnwp8uxeyevxot_tlntq.png"><br><br><h3>  3. Data Storage and Kubernetes </h3><br>  The main mechanism is the <i>Network Block Device</i> network disk (aka SAN) in different implementations for the desired cloud options or bare metal.  However, putting a loaded database (for example, MySQL, which requires 50,000 IOPS) to the cloud (AWS EBS <i>) is</i> not possible due to the <i>latency</i> . <br><br><img src="https://habrastorage.org/webt/qq/wp/r7/qqwpr7fae-nbek0y2o6ex9lbzsa.png"><br><br>  In Kubernetes for such cases it is possible to connect a local hard disk - <i>Local Storage</i> .  If a failure occurs (the disk ceases to be available in the pod), then we are forced to repair this machine - by analogy with the classical scheme in case of failure of one reliable server. <br><br>  Both options ( <i>Network Block Device</i> and <i>Local Storage</i> ) belong to the <i>ReadWriteOnce</i> category: the storage cannot be mounted in two places (pods) - for such scaling you need to create a new disk and connect it to the new pod (for this, there is a K8s built-in mechanism) and then fill it with the necessary data (already done by our forces). <br><br>  If we need the <i>ReadWriteMany</i> mode, then the <i>Network File System</i> (or NAS) implementations are available: for the public cloud, this is <code>AzureFile</code> and <code>AWSElasticFileSystem</code> , and for its installations - CephFS and Glusterfs for fans of distributed systems, as well as NFS. <br><br><img src="https://habrastorage.org/webt/eq/y6/gp/eqy6gpf2duz9ljtzc342bj9upg4.png"><br><br><h2>  Practice </h2><br><h3>  1. Standalone </h3><br>  This option is about the case when nothing prevents to launch the DBMS in the mode of a separate server with local storage.  This is not about high availability ... although it may be to some extent (i.e., sufficient for a given application) implemented at the iron level.  There are many cases for this use.  First of all, these are various staging- and dev-environments, but not only: secondary services also fall here, and disabling them for 15 minutes is not critical.  In Kubernetes, this is implemented by <code>StatefulSet</code> with one pod: <br><br><img src="https://habrastorage.org/webt/hl/xk/ha/hlxkhaodepe50imvuobtoilrz6o.png"><br><br>  In general, this is a viable option, which, from my point of view, has no drawbacks compared to installing a DBMS on a separate virtual machine. <br><br><h3>  2. Replicated pair with manual switching </h3><br>  <code>StatefulSet</code> used again, but the general scheme is as follows: <br><br><img src="https://habrastorage.org/webt/vq/_i/to/vq_itonyvigrh0uezqek_lwtlt4.png"><br><br>  If one of the nodes ( <code>mysql-a-0</code> ) fails, the miracle does not occur, but we have a replica ( <code>mysql-b-0</code> ) to which we can switch traffic.  At the same time - even before switching traffic - it is important not to forget not only to remove the requests to the DBMS from the <code>mysql</code> service, but also to go to the DBMS manually and make sure that all connections are completed (kill them), and also to go to the second node from the DBMS and reconfigure reversed. <br><br>  If you are currently using the classic version with two servers (master + standby) without <i>failover</i> , this solution is equivalent to Kubernetes.  Suitable for MySQL, PostgreSQL, Redis and other products. <br><br><h3>  3. Scaling the read load </h3><br>  In fact, this case is not stateful, because we are talking only about reading.  Here, the main DBMS server is outside the considered scheme, and within the Kubernetes framework, a ‚Äúfarm of slave servers‚Äù is created that are available only for reading.  The general mechanism ‚Äî the use of init containers to fill the database with data on each new pod of this farm (using a hot dump or the usual one with additional actions, etc. ‚Äî depends on the database used).  To be sure that each instance is not too far behind the master, you can use liveness samples. <br><br><img src="https://habrastorage.org/webt/nz/pd/uk/nzpdukumat3zbax7vnkcs5jtsvs.png"><br><br><h3>  4. Smart client </h3><br>  If you make a <code>StatefulSet</code> of three memcached, a special service is available in Kubernetes that will not balance the requests, but will create each pod for its own domain.  The client will be able to work with them if he can sharding and replication. <br><br>  You don‚Äôt have to go far for an example: storage of sessions in PHP works out of the box.  For each session request, requests are made simultaneously to all servers, after which the most relevant answer is selected from them (similarly to recording). <br><br><img src="https://habrastorage.org/webt/t8/iz/26/t8iz261adru0mbdw7cd2o5i3y8o.png"><br><br><h3>  5. Cloud Native Solutions </h3><br>  There are many solutions that are initially focused on node failure, i.e.  they themselves are able to do <i>failover</i> and recovery of nodes, provide guarantees of <i>consistency</i> .  This is not a complete list of them, but only some of the popular examples: <br><br><img src="https://habrastorage.org/webt/9u/ah/qz/9uahqzayfbdsokgyod153jwfjfs.png"><br><br>  All of them are simply put in the <code>StatefulSet</code> , after which the nodes find each other and form a cluster.  The products themselves are different in how they implement three things: <br><br><ol><li>  How do nodes learn about each other?  For this, there are methods such as the Kubernetes API, DNS records, static configuration, specialized nodes (seed), third-party service discovery ... </li><li>  How does the client connect?  Through the load balancer, distributing to the hosts, or the client needs to know about all the hosts, and he himself decides how to proceed. </li><li>  How is horizontal scaling done?  No, full or difficult / with limitations. </li></ol><br>  Regardless of the chosen solutions to these issues, all such products work well with Kubernetes, because they were originally created as a ‚Äúherd‚Äù <i>(cattle)</i> . <br><br><h3>  6. Stolon PostgreSQL </h3><br>  <a href="https://github.com/sorintlab/stolon">Stolon</a> actually allows you to turn PostgreSQL DBMS, created as a <i>pet</i> , into a <i>cattle</i> .  How is this achieved? <br><br><img src="https://habrastorage.org/webt/g_/z4/pc/g_z4pcehfw6p985duphcgrbukzo.png"><br><br><ul><li>  First, you need a service discovery, in the role of which you may have <b>etcd</b> (other options are available) - a cluster of them is placed in the <code>StatefulSet</code> . </li><li>  Another part of the infrastructure is <code>StatefulSet</code> with PostgreSQL instances.  In addition to the DBMS itself, a component called <b>keeper</b> , which configures the DBMS, is also placed next to each installation. </li><li>  The other component, <b>sentinel,</b> is deployed as <code>Deployment</code> and monitors the cluster configuration.  It is he who decides who will be the master and standby, writes this information to etcd.  And keeper reads data from etcd and performs actions corresponding to the current status with a PostgreSQL instance. </li><li>  Another component deployed in <code>Deployment</code> and facing PostgreSQL instances, <b>proxy,</b> is an implementation of the <i>Consistent Switchover</i> pattern already mentioned.  These components are connected to etcd, and if this connection is lost, then the proxy immediately kills outgoing connections, because from this moment it does not know the role of its server (now is it master or standby?). </li><li>  Finally, before the proxy instances is the usual <code>LoadBalancer</code> from Kubernetes. </li></ul><br><h2>  findings </h2><br>  So is it possible to base in Kubernetes?  Yes, of course, it is possible, in some cases ... And if it is expedient, then it is done like this (see the work scheme of Stolon) ... <br><br>  Everyone knows that technology develops in waves.  Initially, any new device can be very difficult to use, but over time, everything changes: the technology becomes available.  Where are we going?  Yes, it will remain like this inside, but how it will work, we will not know.  In Kubernetes, <a href="https://habr.com/company/flant/blog/326414/">operators are</a> actively developing.  So far there are not so many of them and they are not so good, but there is a movement in this direction. <br><br><h2>  Video and slides </h2><br>  Video from the performance (about an hour): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/BnegHj53pW4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Presentation of the report: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  PS We also found a very (!) Short <a href="">text message</a> from this report on the web - thanks for her Nikolai Volynkin. <br><br><h2>  Pps </h2><br>  Other reports on our blog: <br><br><ul><li>  ‚Äú <a href="https://habr.com/company/flant/blog/412901/">Monitoring and Kubernetes</a> ‚Äù;  <i>(Dmitry Stolyarov; May 28, 2018 on RootConf)</i> ; </li><li>  " <a href="https://habr.com/company/flant/blog/345116/">Best Practices CI / CD with Kubernetes and GitLab</a> ";  <i>(Dmitry Stolyarov; November 7, 2017 on HighLoad ++)</i> ; </li><li>  ‚Äú <a href="https://habr.com/company/flant/blog/331188/">Our experience with Kubernetes in small projects</a> ‚Äù;  <i>(Dmitry Stolyarov; June 6, 2017 at RootConf)</i> ; </li><li>  ‚Äú <a href="https://habr.com/company/flant/blog/324274/">We assemble Docker images for CI / CD quickly and conveniently with dapp</a> ‚Äù <i>(Dmitry Stolyarov; November 8, 2016 on HighLoad ++)</i> ; </li><li>  ‚Äú <a href="https://habr.com/company/flant/blog/322686/">Practices of Continuous Delivery with Docker</a> ‚Äù <i>(Dmitry Stolyarov; May 31, 2016 at RootConf)</i> . </li></ul><br>  You may also be interested in the following publications: <br><br><ul><li>  ‚Äú <a href="https://habr.com/company/flant/blog/417509/">Kubernetes tips &amp; tricks: speeding up the bootstrap of large databases</a> ‚Äù; </li><li>  <a href="https://habr.com/company/flant/blog/328756/">Orchestration of the CockroachDB DBMS at Kubernetes</a> . </li></ul></div><p>Source: <a href="https://habr.com/ru/post/431500/">https://habr.com/ru/post/431500/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../431488/index.html">Sound modulation</a></li>
<li><a href="../431490/index.html">External - GUI for Golang</a></li>
<li><a href="../431492/index.html">Review of the React quiz competition from the HeadHunter booth at HolyJs 2018</a></li>
<li><a href="../431496/index.html">How technology helps special classes teachers</a></li>
<li><a href="../431498/index.html">WebP will soon capture the web, but the century won't be long</a></li>
<li><a href="../431502/index.html">Conference for iOS developers Kolesa Mobile 3.0. Video report</a></li>
<li><a href="../431504/index.html">Phishing works. Chronicle of theft of the iPhone XS, followed by the theft of iCloud data</a></li>
<li><a href="../431506/index.html">Xcode and advanced debugging in LLDB: Part 1</a></li>
<li><a href="../431508/index.html">Efficient Spring Transaction Management</a></li>
<li><a href="../431514/index.html">Interview for interviewers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>