<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Openshift - red-hat crafts</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Openshift 


1. Openshift sweep. 
2. Configuration after installation. 
3. Create and connect PV. 
4. Creating and deploying a Red Hat Decision Manage...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Openshift - red-hat crafts</h1><div class="post__text post__text-html js-mediator-article"><h2>  Openshift </h2><br><ol><li>  Openshift sweep. </li><li>  Configuration after installation. </li><li>  Create and connect PV. </li><li>  Creating and deploying a Red Hat Decision Manager project (enterprise kie-workbench). </li><li>  Creating and deploying AMQ (red hat active mq) and postgressql projects using persistent storage. </li><li>  Creating separate projects for services, templates for them, pipeline, integration with gitlab, gitlab regestry. </li></ol><a name="habracut"></a><br><h2>  <b>1. Openshift sweep</b> </h2><br>  Server requirements, preparing dns servers, list of server names, server requirements. <br><br>  Minimum requirements are brief - all servers must have at least 16Gb Ram 2 cores and at least 100 gigabytes for docker needs. <br><br>  Bind-based dns should have the following configuration. <br>  dkm - master, dk0 - performing, ifr - infrastructure, bln - balancer, shd - nfs, dkr - control node with which the cluster configuration occurred, was also planned as a separate node under docker regestry. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre><code class="plaintext hljs">db.osh $TTL 1h @ IN SOA test.osh. root.test.osh. ( 2008122601 ; Serial 28800 ; Refresh 14400 ; Retry 604800 ; Expire - 1 week 86400 ) ; Minimum @ IN NS test.osh. @ IN A 127.0.0.1 rnd-osh-dk0-t01 IN A 10.19.86.18 rnd-osh-dk0-t02 IN A 10.19.86.19 rnd-osh-dk0-t03 IN A 10.19.86.20 rnd-osh-dkm-t01 IN A 10.19.86.21 rnd-osh-dkm-t02 IN A 10.19.86.22 rnd-osh-dkm-t03 IN A 10.19.86.23 rnd-osh-ifr-t01 IN A 10.19.86.24 rnd-osh-ifr-t02 IN A 10.19.86.25 rnd-osh-ifr-t03 IN A 10.19.86.26 rnd-osh-bln-t01 IN A 10.19.86.27 rnd-osh-shd-t01 IN A 10.19.86.28 rnd-osh-dkr-t01 IN A 10.19.86.29 lb IN A 10.19.86.27 openshift IN A 10.19.86.27 api-openshift IN A 10.19.86.27 *.apps.openshift IN A 10.19.86.21 *.apps.openshift IN A 10.19.86.22 *.apps.openshift IN A 10.19.86.23</code> </pre> <br><pre> <code class="plaintext hljs">db.rv.osh $TTL 1h @ IN SOA test.osh. root.test.osh. ( 1 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS test.osh. @ IN A 127.0.0.1 18 IN PTR rnd-osh-dk0-t01.test.osh. 19 IN PTR rnd-osh-dk0-t02.test.osh. 20 IN PTR rnd-osh-dk0-t03.test.osh. 21 IN PTR rnd-osh-dkm-t01.test.osh. 22 IN PTR rnd-osh-dkm-t02.test.osh. 23 IN PTR rnd-osh-dkm-t03.test.osh. 24 IN PTR rnd-osh-ifr-t01.test.osh. 25 IN PTR rnd-osh-ifr-t02.test.osh. 26 IN PTR rnd-osh-ifr-t03.test.osh. 27 IN PTR rnd-osh-bln-t01.test.osh. 28 IN PTR rnd-osh-shd-t01.test.osh. 29 IN PTR rnd-osh-dkr-t01.test.osh. 27 IN PTR lb.test.osh. 27 IN PTR api-openshift.test.osh. named.conf.default-zones</code> </pre><br><pre> <code class="plaintext hljs">zone "test.osh" IN { type master; file "/etc/bind/db.osh"; allow-update { none; }; notify no; }; zone "86.19.10.in-addr.arpa" { type master; file "/etc/bind/db.rv.osh"; };</code> </pre><br>  <b>Server Preparation</b> <br><br>  After connecting the subscription.  Enabling repositories and installing the necessary initial packages. <br><br><pre> <code class="bash hljs">rm -rf /etc/yum.repos.d/cdrom.repo subscription-manager repos --<span class="hljs-built_in"><span class="hljs-built_in">disable</span></span>=<span class="hljs-string"><span class="hljs-string">"*"</span></span> subscription-manager repos --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>=<span class="hljs-string"><span class="hljs-string">"rhel-7-server-rpms"</span></span> --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>=<span class="hljs-string"><span class="hljs-string">"rhel-7-server-extras-rpms"</span></span> --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>=<span class="hljs-string"><span class="hljs-string">"rhel-7-server-ose-3.10-rpms"</span></span> --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>=<span class="hljs-string"><span class="hljs-string">"rhel-7-server-ansible-2.4-rpms"</span></span> yum -y install wget git net-tools <span class="hljs-built_in"><span class="hljs-built_in">bind</span></span>-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct yum -y update yum -y install docker</code> </pre> <br>  Docker storage configuration (separate drive). <br><br><pre> <code class="bash hljs">systemctl stop docker rm -rf /var/lib/docker/* <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"STORAGE_DRIVER=overlay2"</span></span> &gt; /etc/sysconfig/docker-storage-setup <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"DEVS=/dev/sdc"</span></span> &gt;&gt; /etc/sysconfig/docker-storage-setup <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"CONTAINER_ROOT_LV_NAME=dockerlv"</span></span> &gt;&gt; /etc/sysconfig/docker-storage-setup <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"CONTAINER_ROOT_LV_SIZE=100%FREE"</span></span> &gt;&gt; /etc/sysconfig/docker-storage-setup <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"CONTAINER_ROOT_LV_MOUNT_PATH=/var/lib/docker"</span></span> &gt;&gt; /etc/sysconfig/docker-storage-setup <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"VG=docker-vg"</span></span> &gt;&gt; /etc/sysconfig/docker-storage-setup systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> docker docker-storage-setup systemctl is-active docker systemctl restart docker docker info | grep Filesystem</code> </pre><br>  Installing the rest of the required packages. <br><br><pre> <code class="bash hljs">yum -y install atomic atomic trust show yum -y install docker-novolume-plugin systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> docker-novolume-plugin systemctl start docker-novolume-plugin yum -y install openshift-ansible</code> </pre> <br>  Create, add user, and also keys. <br><br><pre> <code class="bash hljs">useradd --create-home --groups users,wheel ocp sed -i <span class="hljs-string"><span class="hljs-string">'s/# %wheel/%wheel/'</span></span> /etc/sudoers mkdir -p /home/ocp/.ssh <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"ssh-rsa AAAAB3NzaC........8Ogb3Bv ocp SSH Key"</span></span> &gt;&gt; /home/ocp/.ssh/authorized_keys</code> </pre> <br>  In case of conflict with already used subnets, change the default addressing inside containers. <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'{ "bip": "172.26.0.1/16" }'</span></span> &gt; /etc/docker/daemon.json systemctl restart docker</code> </pre> <br>  Network manager configuration.  (dns should be able to froward to the outside world) <br><br><pre> <code class="bash hljs">nmcli connection modify ens192 ipv4.dns 172.17.70.140 nmcli connection modify ens192 ipv4.dns-search cluster.local +ipv4.dns-search test.osh +ipv4.dns-search cpgu systemctl stop firewalld systemctl <span class="hljs-built_in"><span class="hljs-built_in">disable</span></span> firewalld systemctl restart network</code> </pre> <br>  Edit, if necessary, the name of the machine to the full name. <br><br><pre> <code class="bash hljs">hhh=$(cat /etc/hostname) <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$hhh</span></span></span><span class="hljs-string">"</span></span>.test.osh &gt; /etc/hostname</code> </pre> <br>  After the actions taken restart the server. <br><br><h3>  Preparation of the control node dkr </h3><br>  The difference between the controlling node and the rest is that there is no need to connect the docker to a separate disk. <br><br>  There is also a need to configure ntp. <br><br><pre> <code class="bash hljs">yum install ntp -y systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> ntpd service ntpd start service ntpd status ntpq -p chmod 777 -R /usr/share/ansible/openshift-ansible/</code> </pre> <br>  You also need to add the private key to the user ocp. <br><br>  Log in via ssh as an ocp user on all nodes. <br><br>  <b>Inventory file preparation and cluster scan.</b> <br><br><pre> <code class="plaintext hljs">host-poc.yaml [OSEv3:children] masters nodes etcd lb nfs [OSEv3:vars] ansible_ssh_user=ocp ansible_become=yes openshift_override_hostname_check=True openshift_master_cluster_method=native openshift_disable_check=memory_availability,disk_availability,package_availability openshift_deployment_type=openshift-enterprise openshift_release=v3.10 oreg_url=registry.access.redhat.com/openshift3/ose-${component}:${version} debug_level=2 os_firewall_use_firewalld=True openshift_install_examples=true openshift_clock_enabled=True openshift_router_selector='node-role.kubernetes.io/infra=true' openshift_registry_selector='node-role.kubernetes.io/infra=true' openshift_examples_modify_imagestreams=true os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant' openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider'}] openshift_master_htpasswd_users={'admin': '$apr1$pQ3QPByH$5BDkrp0m5iclRske.M0m.0'} openshift_master_default_subdomain=apps.openshift.test.osh openshift_master_cluster_hostname=api-openshift.test.osh openshift_master_cluster_public_hostname=openshift.test.osh openshift_enable_unsupported_configurations=true openshift_use_crio=true openshift_crio_enable_docker_gc=true # registry openshift_hosted_registry_storage_kind=nfs openshift_hosted_registry_storage_access_modes=['ReadWriteMany'] openshift_hosted_registry_storage_nfs_directory=/exports openshift_hosted_registry_storage_nfs_options='*(rw,root_squash)' openshift_hosted_registry_storage_volume_name=registry openshift_hosted_registry_storage_volume_size=30Gi # cluster monitoring openshift_cluster_monitoring_operator_install=true openshift_cluster_monitoring_operator_node_selector={'node-role.kubernetes.io/master': 'true'} #metrics openshift_metrics_install_metrics=true openshift_metrics_hawkular_nodeselector={"node-role.kubernetes.io/infra": "true"} openshift_metrics_cassandra_nodeselector={"node-role.kubernetes.io/infra": "true"} openshift_metrics_heapster_nodeselector={"node-role.kubernetes.io/infra": "true"} openshift_metrics_storage_kind=nfs openshift_metrics_storage_access_modes=['ReadWriteOnce'] openshift_metrics_storage_nfs_directory=/exports openshift_metrics_storage_nfs_options='*(rw,root_squash)' openshift_metrics_storage_volume_name=metrics openshift_metrics_storage_volume_size=20Gi #logging openshift_logging_kibana_nodeselector={"node-role.kubernetes.io/infra": "true"} openshift_logging_curator_nodeselector={"node-role.kubernetes.io/infra": "true"} openshift_logging_es_nodeselector={"node-role.kubernetes.io/infra": "true"} openshift_logging_install_logging=true openshift_logging_es_cluster_size=1 openshift_logging_storage_kind=nfs openshift_logging_storage_access_modes=['ReadWriteOnce'] openshift_logging_storage_nfs_directory=/exports openshift_logging_storage_nfs_options='*(rw,root_squash)' openshift_logging_storage_volume_name=logging openshift_logging_storage_volume_size=20Gi #ASB ansible_service_broker_install=true openshift_hosted_etcd_storage_kind=nfs openshift_hosted_etcd_storage_nfs_options="*(rw,root_squash,sync,no_wdelay)" openshift_hosted_etcd_storage_nfs_directory=/opt/osev3-etcd openshift_hosted_etcd_storage_volume_name=etcd-vol2 openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"] openshift_hosted_etcd_storage_volume_size=30G openshift_hosted_etcd_storage_labels={'storage': 'etcd'} ansible_service_broker_local_registry_whitelist=['.*-apb$'] #cloudforms #openshift_management_install_management=true #openshift_management_app_template=cfme-template # host group for masters [masters] rnd-osh-dkm-t0[1:3].test.osh # host group for etcd [etcd] rnd-osh-dkm-t0[1:3].test.osh [lb] rnd-osh-bln-t01.test.osh containerized=False [nfs] rnd-osh-shd-t01.test.osh [nodes] rnd-osh-dkm-t0[1:3].test.osh openshift_node_group_name='node-config-master' rnd-osh-ifr-t0[1:3].test.osh openshift_node_group_name='node-config-infra' rnd-osh-dk0-t0[1:3].test.osh openshift_node_group_name='node-config-compute'</code> </pre> <br>  Run alternately playbooks. <br><br><pre> <code class="bash hljs">ansible-playbook -i host-poc.yaml /usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml ansible-playbook -i host-poc.yaml /usr/share/ansible/openshift-ansible/playbooks/openshift-checks/pre-install.yml ansible-playbook -i host-poc.yaml /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml</code> </pre> <br>  If all is well in the end will be about the following. <br><br><img src="https://habrastorage.org/webt/n8/5h/sh/n85hshlwjuugbzqsaottl3foscg.jpeg"><br><br>  Edit the local host file to verify after installation that oprenshift works via the web interface. <br><br><pre> <code class="plaintext hljs">10.19.86.18 rnd-osh-dk0-t01.test.osh 10.19.86.19 rnd-osh-dk0-t02.test.osh 10.19.86.20 rnd-osh-dk0-t03.test.osh 10.19.86.21 rnd-osh-dkm-t01.test.osh 10.19.86.22 rnd-osh-dkm-t02.test.osh 10.19.86.23 rnd-osh-dkm-t03.test.osh 10.19.86.24 rnd-osh-ifr-t01.test.osh 10.19.86.25 rnd-osh-ifr-t02.test.osh 10.19.86.26 rnd-osh-ifr-t03.test.osh 10.19.86.27 rnd-osh-bln-t01.test.osh openshift.test.osh 10.19.86.28 rnd-osh-shd-t01.test.osh 10.19.86.29 rnd-osh-dkr-t01.test.osh</code> </pre><br>  Check by url <a href="https://openshift.test.osh/">openshift.test.osh</a> : 8443 <br><br><h3>  <b>2. Configuration after installation</b> </h3><br>  Go to dkm. <br><br><pre> <code class="bash hljs">oc login oc adm policy add-cluster-role-to-user cluster-admin admin --rolebinding-name=cluster-admin</code> </pre> <br>  Check that it is possible to see a previously hidden project (openshift, for example) in the web interface. <br><br><h3>  <b>3. Creating and connecting PV</b> </h3><br>  Creating a persistent volume on the nfs server. <br><br><pre> <code class="bash hljs">mkdir -p /exports/examplpv chmod -R 777 /exports/examplpv chown nfsnobody:nfsnobody -R /exports/examplpv <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'"/exports/examplpv" *(rw,root_squash)'</span></span> &gt;&gt; /etc/exports.d/openshift-ansible.exports exportfs -ar restorecon -RvF</code> </pre> <br>  Adding pv to openshift. <br><br>  You need to create a project oc new-project examplpv-project. <br><br>  If the project has already been created go to it oc project examplpv-project.  Create a yaml of the following content. <br><br><pre> <code class="python hljs">apiVersion: v1 kind: PersistentVolume metadata: name: examplpv-ts1 spec: capacity: storage: <span class="hljs-number"><span class="hljs-number">20</span></span>Gi accessModes: - ReadWriteOnce nfs: path: /exports/examplpv server: rnd-osh-shd-t01 persistentVolumeReclaimPolicy: Recycle</code> </pre> <br>  And apply.  oc apply -f filename.yaml <br><br>  After performance <br><br><pre> <code class="bash hljs">oc get pv</code> </pre> <br>  the created pv will be visible in the list. <br><br><h3>  <b>4. Creation and deployment of the Red Hat Decision Manager project (enterprise analog kie-workbench)</b> </h3><br>  Check for patterns. <br><br><pre> <code class="bash hljs">oc get imagestreamtag -n openshift | grep rhdm</code> </pre> <br><img src="https://habrastorage.org/webt/qj/ta/tc/qjtatcty6wmkrszyiromshu6zum.png"><br><br>  Adding templates - a link and a more complete description can be found. <br><br><pre> <code class="bash hljs">unzip rhdm-7.2.1-openshift-templates.zip -d ./rhdm-7.2.1-openshift-templates</code> </pre> <br>  Create a new project: <br><br><pre> <code class="bash hljs">oc new-project rhdm72</code> </pre> <br>  Add authorization to docker registry.redhat.io server: <br><br><pre> <code class="bash hljs">docker login registry.redhat.io cat ~/.docker/config.json oc create secret generic pull-secret-name --from-file=.dockerconfigjson=/root/.docker/config.json --<span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=kubernetes.io/dockerconfigjson oc secrets link default pull-secret-name --<span class="hljs-keyword"><span class="hljs-keyword">for</span></span>=pull oc secrets link builder pull-secret-name</code> </pre> <br>  Import imagetream, create keys Decision Server, Decision Central. <br><br><pre> <code class="bash hljs">keytool -genkeypair -<span class="hljs-built_in"><span class="hljs-built_in">alias</span></span> jboss -keyalg RSA -keystore keystore.jks -storepass mykeystorepass --dname <span class="hljs-string"><span class="hljs-string">"CN=STP,OU=Engineering,O=POC.mos,L=Raleigh,S=NC,C=RU"</span></span> oc create -f rhdm72-image-streams.yaml oc create secret generic kieserver-app-secret --from-file=keystore.jks oc create secret generic decisioncentral-app-secret --from-file=keystore.jks</code> </pre> <br>  Creating a persistent volume on the nfs server. <br><br><pre> <code class="bash hljs">mkdir -p /exports/rhdm72 chmod -R 777 /exports/rhdm72 chown nfsnobody:nfsnobody -R /exports/rhdm72 <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'"/exports/rhamq72" *(rw,root_squash)'</span></span> &gt;&gt; /etc/exports.d/openshift-ansible.exports exportfs -ar restorecon -RvF</code> </pre> <br>  Add pv to project: <br><br><pre> <code class="python hljs">apiVersion: v1 kind: PersistentVolume metadata: name: rhdm72-pv1 spec: capacity: storage: <span class="hljs-number"><span class="hljs-number">20</span></span>Gi accessModes: - ReadWriteMany nfs: path: /exports/rhdm72 server: rnd-osh-shd-t01 persistentVolumeReclaimPolicy: Recycle</code> </pre> <br>  Rhdm70 required PV parameters <br>  accessModes: <br>  - ReadWriteOnce <br>  but in 7.2 it is already required <br>  accessModes: <br>  - ReadWriteMany <br>  Apply - oc apply -f filename.yaml <br>  + check that the created pv is available. <br><img src="https://habrastorage.org/webt/yz/jd/kj/yzjdkjxx8h-_hh-ivjflfg6zk0u.png"><br>  create an application from templates according to official documentation. <br><br><pre> <code class="bash hljs">oc new-app -f rhdm-7.2.1-openshift-templates/templates/rhdm72-authoring.yaml -p DECISION_CENTRAL_HTTPS_SECRET=decisioncentral-app-secret -p KIE_SERVER_HTTPS_SECRET=kieserver-app-secret</code> </pre> <br>  The application will automatically deploy upon completion of the Pull images in the docker-registry. <br>  Up to this point the status will be as follows. <br><br><img src="https://habrastorage.org/webt/mq/f_/76/mqf_76tvlkrevnb1i0ko-c5geog.png"><br><br>  In the case of a link to the image will be the following error <br><br><img src="https://habrastorage.org/webt/ja/tt/fk/jattfkbexppthirthvr4olxo3j4.png"><br><br>  It is necessary to change the url of loading images by selecting edit yaml from registry.redhat.io to registry.access.redhat.com <br><br><img src="https://habrastorage.org/webt/i7/au/7q/i7au7qlcag2vyzowpvah_curhhy.png"><br><br>  To go to the deployed service in its web interface, you must add the following url to the hosts file <img src="https://habrastorage.org/webt/eu/_p/mj/eu_pmjasouvshqh4sljl4chcs_k.png"><br>  on any of the infra nodes <br><blockquote>  10.19.86.25 rnd-osh-ifr-t02.test.osh myapp-rhdmcentr-rhdm72.apps.openshift.test.osh </blockquote><br><br><img src="https://habrastorage.org/webt/uq/ce/03/uqce03lnqjk7-snzah8dy1jljhc.png"><br><br><h3>  <b>5. Creating and deploying AMQ projects (red hat active mq) and postgressql using persistent storage</b> </h3><br><br>  <b>Rhamq</b> <br><br>  Create a new project <br><br><pre> <code class="bash hljs">oc new-project rhamq-and-pgsql</code> </pre><br>  We import images in case of their absence. <br><br><pre> <code class="bash hljs">oc replace --force -f https://raw.githubusercontent.com/jboss-container-images/jboss-amq-7-broker-openshift-image/72-1.1.GA/amq-broker-7-image-streams.yaml oc replace --force -f https://raw.githubusercontent.com/jboss-container-images/jboss-amq-7-broker-openshift-image/72-1.1.GA/amq-broker-7-scaledown-controller-image-streams.yaml oc import-image amq-broker-72-openshift:1.1 oc import-image amq-broker-72-scaledown-controller-openshift:1.0</code> </pre> <br>  Installation templates <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> template <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> amq-broker-72-basic.yaml \ amq-broker-72-ssl.yaml \ amq-broker-72-custom.yaml \ amq-broker-72-persistence.yaml \ amq-broker-72-persistence-ssl.yaml \ amq-broker-72-persistence-clustered.yaml \ amq-broker-72-persistence-clustered-ssl.yaml; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> oc replace --force -f \ https://raw.githubusercontent.com/jboss-container-images/jboss-amq-7-broker-openshift-image/72-1.1.GA/templates/<span class="hljs-variable"><span class="hljs-variable">${template}</span></span> <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre> <br>  Add role to service account. <br><br><pre> <code class="bash hljs">oc policy add-role-to-user view -z default</code> </pre> <br>  Creating pv on nfs server <br><br><pre> <code class="bash hljs">mkdir -p /exports/pgmq chmod -R 777 /exports/pgmq chown nfsnobody:nfsnobody -R /exports/pgmq <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'"/exports/pgmq" *(rw,root_squash)'</span></span> &gt;&gt; /etc/exports.d/openshift-ansible.exports exportfs -ar restorecon -RvF</code> </pre><br>  Create yaml <br><br>  pgmq_storage.yaml <br><br><pre> <code class="python hljs">apiVersion: v1 kind: PersistentVolume metadata: name: pgmq-ts1 spec: capacity: storage: <span class="hljs-number"><span class="hljs-number">20</span></span>Gi accessModes: - ReadWriteOnce nfs: path: /exports/pgmq server: rnd-osh-shd-t01 persistentVolumeReclaimPolicy: Recycle</code> </pre> <br>  Apply pv <br><br><pre> <code class="plaintext hljs">oc apply -f pgmq_storage.yaml</code> </pre> <br>  Create from template <br><br><img src="https://habrastorage.org/webt/ft/ue/ot/ftueotubh-sihze42urwj8keoxq.png"><br><br>  is ready <br><br><img src="https://habrastorage.org/webt/3d/tj/6j/3dtj6jdf9dw6ym4bvidbznruig8.png"><br><br>  For other options with ssl clustering, etc.  You can refer to the documentation <a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.2/html/deploying_amq_broker_on_openshift_container_platform/">access.redhat.com/documentation/en-us/red_hat_amq/7.2/html/deploying_amq_broker_on_openshift_container_platform</a> <br><br>  Postgresql <br><br>  Create another PV in the same way as you did for MQ. <br><br><pre> <code class="bash hljs">mkdir -p /exports/pgmq2 chmod -R 777 /exports/pgmq2 chown nfsnobody:nfsnobody -R /exports/pgmq2 <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'"/exports/pgmq2" *(rw,root_squash)'</span></span> &gt;&gt; /etc/exports.d/openshift-ansible.exports exportfs -ar restorecon -RvF</code> </pre> <br>  pgmq_storage.yaml <br><br><pre> <code class="python hljs">apiVersion: v1 kind: PersistentVolume metadata: name: pgmq-ts2 spec: capacity: storage: <span class="hljs-number"><span class="hljs-number">20</span></span>Gi accessModes: - ReadWriteOnce nfs: path: /exports/pgmq2 server: rnd-osh-shd-t01 persistentVolumeReclaimPolicy: Recycle</code> </pre><br><img src="https://habrastorage.org/webt/sc/_b/da/sc_bda3bjxmgtonlqct5ncvzgkc.png"><br><br>  Fill in the required parameters <br><br><img src="https://habrastorage.org/webt/qq/qu/5a/qqqu5aqvvl4kyrocork_6zw_408.png"><br><br><img src="https://habrastorage.org/webt/ii/nj/dd/iinjdd2ye08h1qcew-yuiwum-ss.png"><br><br>  is ready. <br><br><h3>  <b>6. Creating separate projects for services, templates for them, pipeline, integration with gitlab, gitlab regestry</b> </h3><br>  <b>The first thing you need to create a project.</b> <br><br><pre> <code class="bash hljs">oc new-project ttttt</code> </pre> <br><img src="https://habrastorage.org/webt/5j/ho/r9/5jhor9y3veoydqwp7d4wzlxgwa8.png"><br>  Initially, without having a template, you can create a manual application. <br>  <b>There are two ways.</b> <br>  <i>The first one is</i> simply using a ready-made image, but then version version of images, etc. will not be available, but in some cases it will be relevant. <br><br>  First of all, you need to get the data for authentication to the registry.  Using the example of the assembled image in Gitlab, this is done like this. <br><img src="https://habrastorage.org/webt/80/t9/dp/80t9dpkvstrfb3-a-45dypptupo.png"><br><br>  First you need to create secrets to access the docker registry - see the options and syntax. <br><br><pre> <code class="bash hljs">oc create secret docker-registry</code> </pre> <br>  Then create a secret <br><br><pre> <code class="bash hljs">oc create secret docker-registry gitlabreg --docker-server=<span class="hljs-string"><span class="hljs-string">'gitlab.xxx.com:4567'</span></span> --docker-username=<span class="hljs-string"><span class="hljs-string">'gitlab+deploy-token-1'</span></span> --docker-password=<span class="hljs-string"><span class="hljs-string">'syqTBSMjHtT_t-X5fiSY'</span></span> --docker-email=<span class="hljs-string"><span class="hljs-string">'email'</span></span></code> </pre> <br>  Then create our application. <br><br><pre> <code class="bash hljs">oc new-app --docker-image=<span class="hljs-string"><span class="hljs-string">'gitlab.xxx.com:4567/oko/oko-service:latest'</span></span></code> </pre> <br>  If something went wrong, and the image does not stretch in the application's state, specify the secret to our regestry. <br><br><img src="https://habrastorage.org/webt/cl/bh/0_/clbh0_c9htdaqd1rgqvfgearvuw.png"><br><br>  Then we add the necessary environment variables. <br><br><img src="https://habrastorage.org/webt/of/tl/kq/oftlkqo4eq-xizgq1jjngv2p2ae.png"><br><br>  Done - the container is alive. <br><br>  Then click on the right to edit yaml and select the ports. <br><br><img src="https://habrastorage.org/webt/0o/tc/-3/0otc-3ewuzgidcqb2b43hltjeyy.png"><br><br>  Then, to get access to our container, you need to create a route, but you cannot create it without a service, because the first step is to create a service. <br><br>  service.yaml <br><br><pre> <code class="python hljs">kind: Service apiVersion: v1 metadata: name: oko-service spec: type: ClusterIP ports: - port: <span class="hljs-number"><span class="hljs-number">9000</span></span> protocol: TCP targetPort: <span class="hljs-number"><span class="hljs-number">9000</span></span> selector: app: oko-service sessionAffinity: <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> status: loadBalancer: {}</code> </pre> <br><pre> <code class="bash hljs">oc apply -f service.yaml</code> </pre> <br>  Create a Route. <br><br><img src="https://habrastorage.org/webt/tn/tr/st/tntrstju-zih92pce1renqfbkzk.png"><br><br>  we register url in hosts on the machine looking at one of infra nodes. <br><br><img src="https://habrastorage.org/webt/te/xj/51/texj51gumi95cwkxcsnaygudih8.png"><br><br>  we register url in hosts on the machine looking at one of infra nodes. <br><br>  Is done. <br><br>  <b>Template.</b> <br><br>  The template is created by uploading to yaml separately all the components that relate to the service. <br><br>  Namely, in this case it is secrets dc Service Route. <br><br>  You can see all that is done in a specific project. <br><br><pre> <code class="bash hljs">oc get all</code> </pre> <br>  unload interest <br><br><pre> <code class="bash hljs">oc get deploymentconfig.apps.openshift.io oko-service -o yaml</code> </pre> <br>  or <br><br><pre> <code class="bash hljs">oc get d oko-service -o yaml</code> </pre> <br>  Then you can take as a basis any template for opensihft and integrate what was obtained to obtain the template. <br><br>  In this case, the result will look like this: <br><br>  template.yaml <br><br><pre> <code class="python hljs">kind: <span class="hljs-string"><span class="hljs-string">"Template"</span></span> apiVersion: <span class="hljs-string"><span class="hljs-string">"v1"</span></span> metadata: name: oko-service-template objects: - kind: DeploymentConfig apiVersion: v1 metadata: name: oko-service annotations: description: <span class="hljs-string"><span class="hljs-string">"ImageStream Defines how to build the application oko-service"</span></span> labels: app: oko-service spec: replicas: <span class="hljs-number"><span class="hljs-number">1</span></span> revisionHistoryLimit: <span class="hljs-number"><span class="hljs-number">10</span></span> selector: matchLabels: app: oko-service deploymentconfig: oko-service template: metadata: labels: app: oko-service spec: selector: app: oko-service deploymentconfig: oko-service containers: - env: - name: serverPort value: <span class="hljs-string"><span class="hljs-string">"9000"</span></span> - name: storeLogin value: <span class="hljs-string"><span class="hljs-string">"iii"</span></span> - name: storePassword value: <span class="hljs-string"><span class="hljs-string">"trCsm5"</span></span> - name: storeApiUrl value: <span class="hljs-string"><span class="hljs-string">"http://14.75.41.20/custom-api-2.0/CustomWebService2"</span></span> - name: storeWsdlUrl value: <span class="hljs-string"><span class="hljs-string">"http://14.75.41.20/custom-api-2.0/CustomWebService2/CustomWebService2.wsdl"</span></span> - name: logLevel value: <span class="hljs-string"><span class="hljs-string">"INFO"</span></span> - name: logPath value: <span class="hljs-string"><span class="hljs-string">"/var/log/efp-oko.log"</span></span> ports: - containerPort: <span class="hljs-number"><span class="hljs-number">9000</span></span> name: acces protocol: TCP readinessProbe: failureThreshold: <span class="hljs-number"><span class="hljs-number">3</span></span> httpGet: path: / port: <span class="hljs-number"><span class="hljs-number">9000</span></span> scheme: HTTP initialDelaySeconds: <span class="hljs-number"><span class="hljs-number">5</span></span> periodSeconds: <span class="hljs-number"><span class="hljs-number">10</span></span> successThreshold: <span class="hljs-number"><span class="hljs-number">1</span></span> timeoutSeconds: <span class="hljs-number"><span class="hljs-number">1</span></span> image: gitlab.xxx.com:<span class="hljs-number"><span class="hljs-number">4567</span></span>/oko/oko-service imagePullPolicy: Always name: oko-service imagePullSecrets: - name: gitlab.xxx.com type: ImageChange strategy: activeDeadlineSeconds: <span class="hljs-number"><span class="hljs-number">21600</span></span> resources: {} rollingParams: intervalSeconds: <span class="hljs-number"><span class="hljs-number">1</span></span> maxSurge: <span class="hljs-number"><span class="hljs-number">25</span></span>% maxUnavailable: <span class="hljs-number"><span class="hljs-number">25</span></span>% timeoutSeconds: <span class="hljs-number"><span class="hljs-number">600</span></span> updatePeriodSeconds: <span class="hljs-number"><span class="hljs-number">5</span></span> type: Rolling triggers: - type: <span class="hljs-string"><span class="hljs-string">"ImageChange"</span></span> imageChangeParams: automatic: true containerNames: - <span class="hljs-string"><span class="hljs-string">"oko-service"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">from</span></span>: kind: ImageStream name: <span class="hljs-string"><span class="hljs-string">'oko-service:latest'</span></span> - kind: ImageStream apiVersion: v1 metadata: name: oko-service annotations: openshift.io/generated-by: OpenShiftNewApp labels: app: oko-service deploymentconfig: oko-service spec: dockerImageRepository: gitlab.xxx.com:<span class="hljs-number"><span class="hljs-number">4567</span></span>/oko/oko-service tags: - annotations: openshift.io/imported-<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>: gitlab.xxx.com:<span class="hljs-number"><span class="hljs-number">4567</span></span>/oko/oko-service <span class="hljs-keyword"><span class="hljs-keyword">from</span></span>: kind: DockerImage name: gitlab.xxx.com:<span class="hljs-number"><span class="hljs-number">4567</span></span>/oko/oko-service importPolicy: insecure: <span class="hljs-string"><span class="hljs-string">"true"</span></span> name: latest referencePolicy: type: Source forcePull: true - kind: Service apiVersion: v1 metadata: name: oko-service spec: type: ClusterIP ports: - port: <span class="hljs-number"><span class="hljs-number">9000</span></span> protocol: TCP targetPort: <span class="hljs-number"><span class="hljs-number">9000</span></span> selector: app: oko-service sessionAffinity: <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> status: loadBalancer: {} - kind: Route apiVersion: route.openshift.io/v1 metadata: name: oko-service spec: host: oko-service.moxs.ru to: kind: Service name: oko-service weight: <span class="hljs-number"><span class="hljs-number">100</span></span> wildcardPolicy: <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> status: ingress: - conditions: host: oko-service.xxx.com routerName: router wildcardPolicy: <span class="hljs-keyword"><span class="hljs-keyword">None</span></span></code> </pre> <br>  You can also add secrets here, in the following example we will consider a version of the service with an assembly on the openshift side where the secret will be in the template. <br><br>  <i>Second way</i> <br><br>  Creating a project with complete stages of the assembly of images, a simple pipeline and assembly by Push. <br><br>  First of all create a new project. <br><br>  First you need to create a Buildconfig from the gita (in this case there are three docker files in the project, a regular docker file which is designed for docker version 1.17 above using two FROMs, and two separate dockerfiles to build the base image and the target one.) <br><br>  For access to git if it is private, authorization is required.  Create a secret with the following content. <br><br><pre> <code class="bash hljs">oc create secret generic sinc-git --from-literal=username=gitsinc --from-literal=password=Paaasssword123</code> </pre><br>  We give service account builder access to our secret <br><br><pre> <code class="bash hljs">oc secrets link builder sinc-git</code> </pre> <br>  We tie the secret to the url of git <br><br><pre> <code class="bash hljs">oc annotate secret sinc-git <span class="hljs-string"><span class="hljs-string">'build.openshift.io/source-secret-match-uri-1=https://gitlab.xxx.com/*'</span></span></code> </pre> <br>  Finally, we will try to create an application from the gita with the key - allow-missing-images, since we do not have the base image that we have collected yet. <br><br>  oc new-app <a href="">gitlab.xxx.com/OKO/oko-service.git</a> --strategy = docker --allow-missing-images <br>  Then you need to fix the build for the required dockerfile in the created buildconfig. <br><img src="https://habrastorage.org/webt/rf/_k/_m/rf_k_myzec3x1ao-x18cl3oznci.png"><br>  Rule <br><img src="https://habrastorage.org/webt/kd/xy/4o/kdxy4ophl95y8kk0maiyzz65mlo.png"><br><img src="https://habrastorage.org/webt/af/2f/zd/af2fzdlg49hshegumeacnpupdsg.png"><br>  Also we change the parameters in order to make the base container. <br><img src="https://habrastorage.org/webt/yq/tw/1t/yqtw1taajm-0ikv3cxg92l4gvcs.png"><br>  Let's try to make this one Buildcconfig two. Under the base image, unload it into yaml and pick up what is needed. <br><br>  At the exit, you can get two of these patterns. <br><br>  bc-py <br><br><pre> <code class="python hljs">kind: <span class="hljs-string"><span class="hljs-string">"BuildConfig"</span></span> apiVersion: <span class="hljs-string"><span class="hljs-string">"v1"</span></span> metadata: name: <span class="hljs-string"><span class="hljs-string">"oko-service-build-pyton-ml"</span></span> labels: app: oko-service spec: completionDeadlineSeconds: <span class="hljs-number"><span class="hljs-number">2400</span></span> triggers: - type: <span class="hljs-string"><span class="hljs-string">"ImageChange"</span></span> source: type: git git: uri: <span class="hljs-string"><span class="hljs-string">"https://gitlab.xxx.com/OKO/oko-service.git"</span></span> ref: <span class="hljs-string"><span class="hljs-string">"master"</span></span> sourceSecret: name: git-oko strategy: type: Docker dockerStrategy: dockerfilePath: Dockerfile-python-ml forcePull: true output: to: kind: <span class="hljs-string"><span class="hljs-string">"ImageStreamTag"</span></span> name: <span class="hljs-string"><span class="hljs-string">"python-ml:latest"</span></span></code> </pre> <br>  bc-oko <br><br><pre> <code class="python hljs">kind: <span class="hljs-string"><span class="hljs-string">"BuildConfig"</span></span> apiVersion: <span class="hljs-string"><span class="hljs-string">"v1"</span></span> metadata: name: <span class="hljs-string"><span class="hljs-string">"oko-service-build"</span></span> labels: app: oko-service spec: completionDeadlineSeconds: <span class="hljs-number"><span class="hljs-number">2400</span></span> triggers: - type: <span class="hljs-string"><span class="hljs-string">"ImageChange"</span></span> source: type: git git: uri: <span class="hljs-string"><span class="hljs-string">"https://gitlab.xxx.xom/OKO/oko-service.git"</span></span> ref: <span class="hljs-string"><span class="hljs-string">"master"</span></span> sourceSecret: name: git-oko strategy: type: Docker dockerStrategy: dockerfilePath: Dockerfile-oko-service <span class="hljs-keyword"><span class="hljs-keyword">from</span></span>: kind: ImageStreamTag name: <span class="hljs-string"><span class="hljs-string">"python-ml:latest"</span></span> forcePull: true env: - name: serverPort value: <span class="hljs-string"><span class="hljs-string">"9000"</span></span> - name: storeLogin value: <span class="hljs-string"><span class="hljs-string">"iii"</span></span> - name: storePassword value: <span class="hljs-string"><span class="hljs-string">"trCsn5"</span></span> - name: storeApiUrl value: <span class="hljs-string"><span class="hljs-string">"http://14.75.41.20/custom-api-2.0/CustomWebService2"</span></span> - name: storeWsdlUrl value: <span class="hljs-string"><span class="hljs-string">"http://14.75.41.20/custom-api-2.0/CustomWebService2/CustomWebService2.wsdl"</span></span> - name: logLevel value: <span class="hljs-string"><span class="hljs-string">"INFO"</span></span> - name: logPath value: <span class="hljs-string"><span class="hljs-string">"/var/log/efp-oko.log"</span></span> output: to: kind: <span class="hljs-string"><span class="hljs-string">"ImageStreamTag"</span></span> name: <span class="hljs-string"><span class="hljs-string">"oko-service:latest"</span></span></code> </pre> <br><br>  We also need to create deploymentconfig two imagestream and to complete the service and route scan. <br>  I preferred not to separate all configs separately, but to immediately create a template that includes all the components for the service.  based on the previous template for the version without assembly. <br><br><pre> <code class="python hljs">template kind: <span class="hljs-string"><span class="hljs-string">"Template"</span></span> apiVersion: <span class="hljs-string"><span class="hljs-string">"v1"</span></span> metadata: name: oko-service-template objects: - kind: Secret apiVersion: v1 type: kubernetes.io/basic-auth metadata: name: git-oko annotations: build.openshift.io/source-secret-match-uri<span class="hljs-number"><span class="hljs-number">-1</span></span>: https://gitlab.xxx.com/* data: password: R21ZFSw== username: Z2l0cec== - kind: <span class="hljs-string"><span class="hljs-string">"BuildConfig"</span></span> apiVersion: <span class="hljs-string"><span class="hljs-string">"v1"</span></span> metadata: name: <span class="hljs-string"><span class="hljs-string">"oko-service-build-pyton-ml"</span></span> labels: app: oko-service spec: completionDeadlineSeconds: <span class="hljs-number"><span class="hljs-number">2400</span></span> triggers: - type: <span class="hljs-string"><span class="hljs-string">"ImageChange"</span></span> source: type: git git: uri: <span class="hljs-string"><span class="hljs-string">"https://gitlab.xxx.com/OKO/oko-service.git"</span></span> ref: <span class="hljs-string"><span class="hljs-string">"master"</span></span> sourceSecret: name: git-oko strategy: type: Docker dockerStrategy: dockerfilePath: Dockerfile-python-ml forcePull: true output: to: kind: <span class="hljs-string"><span class="hljs-string">"ImageStreamTag"</span></span> name: <span class="hljs-string"><span class="hljs-string">"python-ml:latest"</span></span> - kind: <span class="hljs-string"><span class="hljs-string">"BuildConfig"</span></span> apiVersion: <span class="hljs-string"><span class="hljs-string">"v1"</span></span> metadata: name: <span class="hljs-string"><span class="hljs-string">"oko-service-build"</span></span> labels: app: oko-service spec: completionDeadlineSeconds: <span class="hljs-number"><span class="hljs-number">2400</span></span> triggers: - type: <span class="hljs-string"><span class="hljs-string">"ImageChange"</span></span> source: type: git git: uri: <span class="hljs-string"><span class="hljs-string">"https://gitlab.xxx.com/OKO/oko-service.git"</span></span> ref: <span class="hljs-string"><span class="hljs-string">"master"</span></span> sourceSecret: name: git-oko strategy: type: Docker dockerStrategy: dockerfilePath: Dockerfile-oko-service <span class="hljs-keyword"><span class="hljs-keyword">from</span></span>: kind: ImageStreamTag name: <span class="hljs-string"><span class="hljs-string">"python-ml:latest"</span></span> forcePull: true env: - name: serverPort value: <span class="hljs-string"><span class="hljs-string">"9000"</span></span> - name: storeLogin value: <span class="hljs-string"><span class="hljs-string">"iii"</span></span> - name: storePassword value: <span class="hljs-string"><span class="hljs-string">"trC"</span></span> - name: storeApiUrl value: <span class="hljs-string"><span class="hljs-string">"http://14.75.41.20/custom-api-2.0/CustomWebService2"</span></span> - name: storeWsdlUrl value: <span class="hljs-string"><span class="hljs-string">"http://14.75.41.20/custom-api-2.0/CustomWebService2/CustomWebService2.wsdl"</span></span> - name: logLevel value: <span class="hljs-string"><span class="hljs-string">"INFO"</span></span> - name: logPath value: <span class="hljs-string"><span class="hljs-string">"/var/log/efp-oko.log"</span></span> output: to: kind: <span class="hljs-string"><span class="hljs-string">"ImageStreamTag"</span></span> name: <span class="hljs-string"><span class="hljs-string">"oko-service:latest"</span></span> - kind: DeploymentConfig apiVersion: v1 metadata: name: oko-service annotations: description: <span class="hljs-string"><span class="hljs-string">"ImageStream Defines how to build the application oko-service"</span></span> labels: app: oko-service spec: replicas: <span class="hljs-number"><span class="hljs-number">1</span></span> revisionHistoryLimit: <span class="hljs-number"><span class="hljs-number">10</span></span> selector: matchLabels: app: oko-service deploymentconfig: oko-service template: metadata: labels: app: oko-service spec: selector: app: oko-service deploymentconfig: oko-service containers: - env: - name: serverPort value: <span class="hljs-string"><span class="hljs-string">"9000"</span></span> - name: storeLogin value: <span class="hljs-string"><span class="hljs-string">"iii"</span></span> - name: storePassword value: <span class="hljs-string"><span class="hljs-string">"trCsn5"</span></span> - name: storeApiUrl value: <span class="hljs-string"><span class="hljs-string">"http://14.75.41.20/custom-api-2.0/CustomWebService2"</span></span> - name: storeWsdlUrl value: <span class="hljs-string"><span class="hljs-string">"http://14.75.41.20/custom-api-2.0/CustomWebService2/CustomWebService2.wsdl"</span></span> - name: logLevel value: <span class="hljs-string"><span class="hljs-string">"INFO"</span></span> - name: logPath value: <span class="hljs-string"><span class="hljs-string">"/var/log/efp-oko.log"</span></span> ports: - containerPort: <span class="hljs-number"><span class="hljs-number">9000</span></span> name: acces protocol: TCP readinessProbe: failureThreshold: <span class="hljs-number"><span class="hljs-number">3</span></span> httpGet: path: / port: <span class="hljs-number"><span class="hljs-number">9000</span></span> scheme: HTTP initialDelaySeconds: <span class="hljs-number"><span class="hljs-number">5</span></span> periodSeconds: <span class="hljs-number"><span class="hljs-number">10</span></span> successThreshold: <span class="hljs-number"><span class="hljs-number">1</span></span> timeoutSeconds: <span class="hljs-number"><span class="hljs-number">1</span></span> image: docker-registry.default.svc:<span class="hljs-number"><span class="hljs-number">5000</span></span>/oko-service-p/oko-service imagePullPolicy: Always name: oko-service type: ImageChange strategy: activeDeadlineSeconds: <span class="hljs-number"><span class="hljs-number">21600</span></span> resources: {} rollingParams: intervalSeconds: <span class="hljs-number"><span class="hljs-number">1</span></span> maxSurge: <span class="hljs-number"><span class="hljs-number">25</span></span>% maxUnavailable: <span class="hljs-number"><span class="hljs-number">25</span></span>% timeoutSeconds: <span class="hljs-number"><span class="hljs-number">600</span></span> updatePeriodSeconds: <span class="hljs-number"><span class="hljs-number">5</span></span> type: Rolling triggers: - type: <span class="hljs-string"><span class="hljs-string">"ImageChange"</span></span> imageChangeParams: automatic: true containerNames: - <span class="hljs-string"><span class="hljs-string">"oko-service"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">from</span></span>: kind: ImageStreamTag name: <span class="hljs-string"><span class="hljs-string">'oko-service:latest'</span></span> - kind: ImageStream apiVersion: v1 metadata: name: oko-service annotations: openshift.io/generated-by: OpenShiftNewApp labels: app: oko-service deploymentconfig: oko-service spec: dockerImageRepository: <span class="hljs-string"><span class="hljs-string">""</span></span> tags: - annotations: openshift.io/imported-<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>: oko-service <span class="hljs-keyword"><span class="hljs-keyword">from</span></span>: kind: DockerImage name: oko-service importPolicy: insecure: <span class="hljs-string"><span class="hljs-string">"true"</span></span> name: latest referencePolicy: type: Source - kind: ImageStream apiVersion: v1 metadata: name: python-ml spec: dockerImageRepository: <span class="hljs-string"><span class="hljs-string">""</span></span> tags: - annotations: openshift.io/imported-<span class="hljs-keyword"><span class="hljs-keyword">from</span></span>: oko-service-build <span class="hljs-keyword"><span class="hljs-keyword">from</span></span>: kind: DockerImage name: python-ml importPolicy: insecure: <span class="hljs-string"><span class="hljs-string">"true"</span></span> name: latest referencePolicy: type: Source - kind: Service apiVersion: v1 metadata: name: oko-service spec: type: ClusterIP ports: - port: <span class="hljs-number"><span class="hljs-number">9000</span></span> protocol: TCP targetPort: <span class="hljs-number"><span class="hljs-number">9000</span></span> selector: app: oko-service sessionAffinity: <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> status: loadBalancer: {} - kind: Route apiVersion: route.openshift.io/v1 metadata: name: oko-service spec: host: oko-service.xxx.com to: kind: Service name: oko-service weight: <span class="hljs-number"><span class="hljs-number">100</span></span> wildcardPolicy: <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> status: ingress: - conditions: host: oko-service.xxx.com routerName: router wildcardPolicy: <span class="hljs-keyword"><span class="hljs-keyword">None</span></span></code> </pre><br>  This template is made for the oko-service-p project, so you need to take this into account. <br>  You can use variables to automatically substitute the desired values. <br>  Once again, the basic Yaml can be obtained by unloading data using oc get ... -o yaml <br><br>  You can use this pattern for scanning as follows. <br><br><pre> <code class="bash hljs">oc process -f oko-service-templatebuild.yaml | oc create -f -</code> </pre> <br>  Then create Pipeline <br><br>  oko-service-pipeline.yaml <br><br><pre> <code class="python hljs">kind: <span class="hljs-string"><span class="hljs-string">"BuildConfig"</span></span> apiVersion: <span class="hljs-string"><span class="hljs-string">"v1"</span></span> type: <span class="hljs-string"><span class="hljs-string">"GitLab"</span></span> gitlab: secret: <span class="hljs-string"><span class="hljs-string">"secret101"</span></span> metadata: name: <span class="hljs-string"><span class="hljs-string">"oko-service-sample-pipeline"</span></span> spec: strategy: jenkinsPipelineStrategy: jenkinsfile: |- // path of the template to use // <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">templatePath</span></span></span><span class="hljs-function"> = '</span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">https</span></span></span><span class="hljs-function">:</span></span>//raw.githubusercontent.com/openshift/nodejs-ex/master/openshift/templates/nodejs-mongodb.json<span class="hljs-string"><span class="hljs-string">' // name of the template that will be created def templateName = '</span></span>oko-service-template<span class="hljs-string"><span class="hljs-string">' // NOTE, the "pipeline" directive/closure from the declarative pipeline syntax needs to include, or be nested outside, pipeline { agent any environment { DEV_PROJECT = "oko-service"; } stages { stage('</span></span>deploy<span class="hljs-string"><span class="hljs-string">') { steps { script { openshift.withCluster() { openshift.withProject() { echo "Hello from project ${openshift.project()} in cluster ${openshift.cluster()}" def dc = openshift.selector('</span></span>dc<span class="hljs-string"><span class="hljs-string">', "${DEV_PROJECT}") openshiftBuild(buildConfig: '</span></span>oko-service-build<span class="hljs-string"><span class="hljs-string">', waitTime: '</span></span><span class="hljs-number"><span class="hljs-number">3000000</span></span><span class="hljs-string"><span class="hljs-string">') openshiftDeploy(deploymentConfig: '</span></span>oko-service<span class="hljs-string"><span class="hljs-string">', waitTime: '</span></span><span class="hljs-number"><span class="hljs-number">3000000</span></span><span class="hljs-string"><span class="hljs-string">') } } } } } } // stages } // pipeline type: JenkinsPipeline triggers: - type: GitLab gitlab: secret: ffffffffk</span></span></code> </pre> <br>  After applying the Pipeline configuration by running <br><br><pre> <code class="bash hljs">oc describe buildconfig oko-service-sample-pipeline</code> </pre> <br>  You can get the url for the webhook in gitlab. <br><br><img src="https://habrastorage.org/webt/_l/ee/bk/_leebk_a59h1kbzn8widsczafsk.png"><br><br>  The secret to replace the specified secret in the config. <br><img src="https://habrastorage.org/webt/nt/63/ce/nt63ceczqzpqujqysq0caka5yle.png"><br><br>  Also, after applying the Pipeline, openshift itself will begin installing jenkins in the project to launch Pipeline.  The initial launch is long, so you need to wait some time. <br><br>  Then in Gitlab in our project: <br><br><img src="https://habrastorage.org/webt/nt/63/ce/nt63ceczqzpqujqysq0caka5yle.png"><br><br>  Fill in the Url, secret, remove the Enable SSL verification and our webhook is ready. <br><br>  You can make a test push and look at the progress of assembly <br><br><img src="https://habrastorage.org/webt/c4/sd/at/c4sdatvqowwtgn_rjyzqvjj_hd4.png"><br><br>  Do not forget to register in the host url to get into the same jenkins on infranode. <br><br><img src="https://habrastorage.org/webt/zj/d_/1n/zjd_1nqxegz1jrodji6wmsi8ebm.png"><br><br>  You can also see the progress of the assembly. <br><br><img src="https://habrastorage.org/webt/-o/ru/zq/-oruzqelmtrzyhwairvn8t8am_u.png"><br><br><h4>  PS I hope this article will help many understand how and with what openshift is eaten, clarifies many moments that are not obvious at first glance. <br><br>  PSS some solutions to solve some problems </h4><br>  Problems running build builds, etc. <br>  - create a service account for the project <br><br><pre> <code class="bash hljs">oc create serviceaccount oko-serviceaccount oc adm policy add-scc-to-user privileged system:serviceaccount:__:oko-serviceaccount oc adm policy add-scc-to-group anyuid system:authenticated oc adm policy add-scc-to-user anyuid system:serviceaccount:__:oko-serviceaccount</code> </pre><br>  Problems with the suspension of the project and not deleting it <br>  - forced termination script (congenital disease) <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> $(oc get projects | grep Terminating| awk <span class="hljs-string"><span class="hljs-string">'{print $1}'</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$i</span></span>; oc get serviceinstance -n <span class="hljs-variable"><span class="hljs-variable">$i</span></span> -o yaml | sed <span class="hljs-string"><span class="hljs-string">"/kubernetes-incubator/d"</span></span>| oc apply -f - ; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre> <br>  Problems downloading images. <br><br><pre> <code class="bash hljs">oc adm policy add-role-to-group system:image-puller system:serviceaccounts:__ oc adm policy add-role-to-user system:image-puller system:serviceaccount:__::default oc adm policy add-role-to-group system:image-puller system:serviceaccounts:__ oc policy add-role-to-user system:image-puller system:serviceaccount:__::default oc policy add-role-to-group system:image-puller system:serviceaccounts:__</code> </pre> <br><img src="https://habrastorage.org/webt/ij/zc/kp/ijzckpza-dz2ninhkmak58f0sj0.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Also override permissions on the folder for the registry on nfs. </font><font style="vertical-align: inherit;">(in the registry log errors for writing, the build is hanging on the push).</font></font><br><br><pre> <code class="bash hljs">chmod 777 -r /exports/registry/docker/registry/ chmod -R 777 /exports/registry/docker/registry/ chown nfsnobody:nfsnobody -R /exports/registry/ hown -R 1001 /exports/registry/ restorecon -RvF exportfs -ar</code> </pre> </div><p>Source: <a href="https://habr.com/ru/post/441360/">https://habr.com/ru/post/441360/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../441336/index.html">RTOS or not RTOS is the question</a></li>
<li><a href="../441348/index.html">Direct routing and balancing with NFT vs Nginx</a></li>
<li><a href="../441352/index.html">Patterns and anti-patterns CI / CD. Part 2</a></li>
<li><a href="../441356/index.html">How to understand the "foreign" code and join the new team?</a></li>
<li><a href="../441358/index.html">The first commercial lunar landing apparatus Beresheet was launched.</a></li>
<li><a href="../441368/index.html">What does the invisible moon of Neptune look like?</a></li>
<li><a href="../441370/index.html">Fearless defense. Thread Safety in Rust</a></li>
<li><a href="../441372/index.html">[Friday] How to fry a chicken in terms of physics</a></li>
<li><a href="../441376/index.html">On the other side of purity: what can and what can not reverse osmosis membrane</a></li>
<li><a href="../441378/index.html">Researchers from Google: for the protection of Specter requires a change in processor architecture, software patches will not help</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>