<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Programming low latency sound in iOS</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The article will discuss the features of the low-level API for working with sound in iOS, which had to be encountered during the development of Viber....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Programming low latency sound in iOS</h1><div class="post__text post__text-html js-mediator-article">  The article will discuss the features of the low-level API for working with sound in iOS, which had to be encountered during the development of Viber.  It will be about choosing the size of the hardware buffer and the behavior of AudioUnit when the sample rate changes. <br><br>  For software work with sound in iOS, Apple provides 4 groups of APIs, each of which is designed to solve a specific class of tasks: <br><ul><li>  <b>AVFoundation</b> allows <b>you</b> to play and record files and buffers in memory with the ability to use the platform provided hardware or software implementations of some audio codecs.  It is recommended to use when there are no stringent requirements for low latency playback and playback. </li><li>  <b>OpenAL API is</b> intended for rendering and playback of three-dimensional sound as well as the use of sound effects.  It is used mainly in games.  Provides low latency playback, but does not provide the ability to record sound. </li><li>  <b>AudioQueue is a</b> basic API for recording and playback of audio streams with the ability to use codecs provided by the platform.  Using this API, it will not be possible to get the minimum delay, but using it is extremely simple. </li><li>  Finally, <b>AudioUnit</b> , a powerful and rich API, for working with audio streams.  Compared to Mac OS X on iOS, the programmer is not fully accessible, but it is best suited for recording and playing sound as close as possible to hardware. </li></ul><br><a name="habracut"></a><br><h5>  Audiounit </h5><br>  Quite a lot has been written about initialization and basic use of AudioUnit, including examples in official documentation.  Consider not quite the trivial features of its configuration and use.  The interaction with the sound, as close as possible to the equipment, is the RemoteIO and VoiceProcessingIO modules.  VoiceProcessingIO adds to RemoteIO the ability to control additional sound processing at the OS level to improve the quality of voice reproduction and automatic signal level correction (AGC).  From the point of view of the programmer, both of these modules have ‚Äúinput‚Äù and ‚Äúoutput‚Äù, to which 2 buses are connected. <br><img src="https://habrastorage.org/getpro/habr/post_images/4b6/edb/0d3/4b6edb0d32c2a580bae647c1676814e1.png"><br>  The programmer can set and request the audio stream format on these buses.  By requesting the format of the bus stream 1 at the AudioUnit input, you can find out the parameters of the stream received from the microphone at the hardware level, and by setting the bus format 1 at the output, you can determine in what format the audio stream will be transmitted to the application.  Accordingly, setting the format of the bus 0 input AudioUnit we report the format of the audio stream, which we will provide for playback, and by requesting the format of the bus 0 output - find out what format the equipment uses for playback.  The exchange of buffers with AudioUnit takes place in 2x callbacks with a signature: <br><pre><code class="cpp hljs"><span class="hljs-function"><span class="hljs-function">OSStatus </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">AURenderCallback</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">void</span></span></span></span><span class="hljs-function"><span class="hljs-params"> * inRefCon, AudioUnitRenderActionFlags * ioActionFlags, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">const</span></span></span></span><span class="hljs-function"><span class="hljs-params"> AudioTimeStamp * inTimeStamp, Int32 inBusNumber, UInt32 inNumberFrames, AudioBufferList * ioData)</span></span></span></span>;</code> </pre> <br>  InputCallback is called when the module is ready to provide us with a buffer of data recorded from the microphone.  To get this data into the application, you need to call the AudioUnitRender function in this callback.  RenderCallback is called when a module requests the application for playback data to be written to the ioData buffer.  These callbacks are called in the context of the AudioUnit internal flow and should work as soon as possible.  Ideally, their work should be limited to copying ready-made data buffers.  This introduces additional difficulties in the organization of audio signal processing in terms of stream synchronization.  In addition to buffers, a time stamp is sent to these callbacks in the form: <br><pre> <code class="cpp hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AudioTimeStamp</span></span></span><span class="hljs-class"> {</span></span> Float64 mSampleTime; <span class="hljs-comment"><span class="hljs-comment">//     UInt64 mHostTime; //      //       // ... UInt32 mFlags; //       };</span></span></code> </pre><br>  This time stamp can (and should) be used to detect missing samples during recording and playback.  The main reasons for the loss of samples: <br><ul><li>  Switch recording and playback devices (speaker / headphone / Bluetooth).  It is impossible to get rid of the loss of part of the samples in this case.  The time stamp can be used to correct further audio processing, for example, synchronization with a video stream or recalculation of the ‚ÄúTimestamp‚Äù field of an RTP packet. </li><li>  Too much CPU usage, in which the AudioUnit stream does not have enough time to work, is corrected by optimizing the algorithms or by refusing to support insufficiently powerful devices. </li><li>  Errors in the implementation of stream synchronization when working with audio data buffers.  In this case, correct use of lock-free structures, cyclic buffers, GCD will help (however, GCD is not always a good solution for problems close to real-time).  You can use System Trace from Instruments to identify the causes of problems with thread synchronization. </li></ul><br><h5>  Hardware Buffer Size </h5><br>  In the ideal case, to obtain a minimum delay in sound recording, any intermediate buffering is absent.  However, in the real world, hardware and software are more optimized to work with groups of consecutive samples, rather than with single samples.  At the same time, iOS provides the ability to adjust the size of the hardware buffer.  The property of the PreferredHardwareIOBufferDuration audio session allows you to request the required buffer duration in seconds, and CurrentHardwareIOBufferDuration to get the real one.  Possible durations depend on the currently used sampling rate.  For example, by default, when playing through the built-in speakers and recording through the built-in microphone, the equipment will operate at a sampling frequency of 44100Hz.  The minimum buffer that the audio subsystem operates on is 256 bytes, the size is usually equal to the power of two (the values ‚Äã‚Äãwere obtained experimentally and are not included in the documentation).  Therefore, the buffer may have a duration of: <br>  256/44100 = 5.805ms <br>  512/44100 = 11.61ms <br>  1024/44100 = 23.22ms <br>  If you use a bluetooth headset with a sampling frequency of 16000Hz, then the size of the hardware buffer can be: <br>  256/16000 = 16ms <br>  512/16000 = 32ms <br>  1024/16000 = 64ms <br><br>  The duration of the hardware buffer affects not only the delay, but also the number of samples that AudioUnit exchanges with the application each time a callback is called.  If the frequency of the discrepancy at the input and output of AudioUnit coincides, a buffer equal in duration to the hardware one will be transmitted to the callback and the callback will be called at regular intervals.  Accordingly, if the application algorithms are designed to work with sequences of 10ms, in any case, intermediate buffering on the application side will be necessary, since AudioUnit cannot be configured to work with buffers of arbitrary duration.  The size of the hardware buffer is best chosen experimentally, given the performance of specific devices.  The reduction improves latency, but adds overhead to switching threads when calling callbacks and increases the likelihood of skipping samples when CPU is high. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  Buffering when changing the sampling rate </h5><br>  In applications for VoIP communication, it does not always make sense to process audio with a sampling frequency above 16000Hz.  In addition, it is easier to abstract from the hardware sampling frequency, since it can change at any time when switching the sound source.  Configuring AudioUnit, you can set the sampling rate of the audio stream when exchanging data with AudioUnit.  Consider how this will work for sound recording in the following example: <br><pre> <code class="cpp hljs">SampleRateHW = <span class="hljs-number"><span class="hljs-number">44100</span></span> <span class="hljs-comment"><span class="hljs-comment">//    buffSizeHW = 1024 //    (1024 / 44100 = 23.22ms) mSampleRateAPP = 16000 //    buffSizeAPP = 1024 * 16000/44100 = 371.52 //    </span></span></code> </pre><br>  After resampling, the output will be an integer number of samples, and the fractional remainder will be stored as resampler filter coefficients.  The behavior of AudioUnit is quite different in iOS5 and iOS6. <br><br><h6>  iOS5 </h6><br>  In iOS5, AudioUnit modules exchange buffers that are multiples of powers of two, so the application will receive 256 samples (16ms @ 16kHz) during the first call.  The remaining 371-256 = 115 will remain inside AudioUnit. <br><img src="https://habrastorage.org/getpro/habr/post_images/681/434/f76/681434f76162062b3f70f70b1dccbffb.png"><br><br>  The second time the callback is called, the application will again receive a buffer of 256 samples: some of the data in it will be from the previous hardware buffer, and some from the new one. <br><img src="https://habrastorage.org/getpro/habr/post_images/e42/4a7/e20/e424a7e207ef203c3435e9f0d1643597.png"><br><br>  In the third call, the remainder accumulated after resampling will allow 512 samples to be transferred to the application immediately. <br><img src="https://habrastorage.org/getpro/habr/post_images/01a/3cd/063/01a3cd0635a782b77943a37d43ff921c.png"><br><br>  Then, again, the application receives 256 samples. <br><img src="https://habrastorage.org/getpro/habr/post_images/313/ad4/d14/313ad4d14fdb867c14f1a05b6211576d.png"><br><br>  Thus, when recording with resampling, the callback will be called at regular intervals, and the size of the buffer it receives will not be constant (but equal to the power of two). <br><br><h6>  iOS6 </h6>  In iOS6, the restriction on the size of the buffer transferred between the application and AudioUnit was removed, thus getting rid of intermediate buffering during resampling and, accordingly, reducing the delay.  The application will receive buffers of size 371 and 372 samples alternately. <br><br>  API CoreAudio can hardly be called clear and well documented.  Many features of the work have to learn experimentally, but we must remember that the behavior may differ in different versions of the OS.  For those who are interested in the topic of real-time audio processing, in addition to the Apple documentation, we can recommend the <a href="https://itunes.apple.com/us/book/izotope-ios-audio-programming/id478070883">‚ÄúiZotope iOS Audio Programming Guide‚Äú</a> . </div><p>Source: <a href="https://habr.com/ru/post/171021/">https://habr.com/ru/post/171021/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../171009/index.html">Product Information Support: KANO Special Practice</a></li>
<li><a href="../171011/index.html">We work with the VKontakte API from the Google Chrome extension</a></li>
<li><a href="../171015/index.html">Example - clock on CSS3 without images and javascript</a></li>
<li><a href="../171017/index.html">Installing Windows 7 over the network using Microsoft Windows AIK</a></li>
<li><a href="../171019/index.html">Prediction system based on neural networks in industry</a></li>
<li><a href="../171025/index.html">Practical tips for portfolio building for testers</a></li>
<li><a href="../171029/index.html">‚ÄúFake video work of the smartphone application with the background around‚Äù or ‚ÄúHow to save on the DIY stabilization system‚Äù or ‚ÄúIs it possible to write this down with a thread of software right in the smart?‚Äù</a></li>
<li><a href="../171031/index.html">New Grandmothers. "Innovative" cloud video surveillance</a></li>
<li><a href="../171033/index.html">The history of the GNU operating system, or what happened to Hurd?</a></li>
<li><a href="../171035/index.html">Posting videos on YouTube‚Äôs user‚Äôs channel</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>