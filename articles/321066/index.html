<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Complex neural network based on the Fourier series of a function of many variables</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="There are many tasks for which the direct propagation neural networks with a sigmoidal activation function are not optimal. For example - problems of ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Complex neural network based on the Fourier series of a function of many variables</h1><div class="post__text post__text-html js-mediator-article">  There are many tasks for which the direct propagation neural networks with a sigmoidal activation function are not optimal.  For example - problems of recognition of binary images, with preprocessing using Fourier transform.  During these transformations, the image becomes invariant to offsets, scaling, and rotations.  An example of such transformations is given below. [1]  At the output, such a method produces a vector of complex numbers.  Modern neural networks cannot work with them.  they only work with real numbers. <br><br><img src="https://habrastorage.org/files/d81/272/b2f/d81272b2f11149d280df7c930d39dfd6.png" alt="image"><br><br><a name="habracut"></a><br>  The second such task is the prediction of time series with a given accuracy.  Direct propagation networks with a sigmoidal activation function do not make it possible to predict an error in the number of hidden neurons.  In order to be able to predict this error, it is worth using some series for the rate of convergence of which there are calculation formulas.  Fourier series was chosen as such a series. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>Fourier series</b> <br><br>  Complex Fourier series of one variable. <br><br><img src="https://habrastorage.org/files/404/d41/3ab/404d413ab21d4289b95ae520b2ce41af.png" alt="image"><br><br>  But neural networks often work with functions of many variables.  I propose to consider the Fourier series of two independent variables.  [2] <br><br><img src="https://habrastorage.org/files/dfd/ba1/366/dfdba1366e324686ad041027b9dd339b.png" alt="image"><br><br>  In general, one should use the Fourier series for functions of "k" variables. <br><br><img src="https://habrastorage.org/files/175/ea4/519/175ea451953d4b558608c9f61c04fdca.png" alt="image"><br><br>  <b>Architecture</b> <br><img src="https://habrastorage.org/files/203/60a/7a1/20360a7a16cc48f9b1dd3c3c8432f90e.png"><br>  <i>Figure 1. Integrated network architecture (KINSF)</i> <br><br>  Figure 1 shows the architecture of the complex artificial Fourier neural network (KINSF), which essentially implements formula 9. Here the hidden layer is the mxn neuron matrix where m is the number of Fourier decomposition descriptors and n is the dimension of the input vector.  The weights in the first layer have the physical meaning of the frequencies with the highest energy, and the weights of the second layer have the meaning of the coefficients of the Fourier series.  Thus, the number of inputs for each neuron of the output layer is m * n, which corresponds to the number of coefficients of the Fourier series. <br><br>  Figure 2 shows a diagram of a separate neuron of the hidden layer. <br><img src="https://habrastorage.org/files/511/8f0/582/5118f0582d27487f8d9480b610532e9d.png"><br>  <i>Figure 2. Scheme of the neuron of the hidden layer.</i> <br><br>  Figure 3 is a diagram of the neuron of the output layer. <br><br><img src="https://habrastorage.org/files/d79/187/7f7/d791877f783242098ea516c972a0c593.png"><br>  <i>Figure 3. Diagram of the neuron of the output layer.</i> <br><br>  <b>Architecture optimization</b> <br><br>  Creating a matrix of neurons involves the use of large computational resources.  You can reduce the number of neurons on the hidden layer n times.  In this case, each neuron of the hidden layer has n outputs, if we have n inputs in the network, then with this approach the number of connections decreases n square times without losing the quality of approximation of functions.  Such a neural network is shown in Figure 4. <br><br><img src="https://habrastorage.org/files/013/4f2/f7d/0134f2f7ddea4ff7bda220008c001a62.png"><br>  Figure 4. KINSF, the reduction of the matrix into a vector. <br><br>  <b>The principle of the NA</b> <br><br>  We write the principle of operation of this NA in matrix form. <br><br><img src="https://habrastorage.org/files/371/058/6f3/3710586f34c94033b0f83fb002f512fb.png"><br><br>  Expression 10 describes the output of the hidden layer, where: n is the dimension of the input ‚Äúx‚Äù vector, m is the dimension of the output vector of the hidden layer ‚Äúy1‚Äù, f is the activation vector function, j is the imaginary unit, w is the weights matrix, net1 = w * x, k is the number of outputs of the NA.  Expression 11 in turn shows the operation of the output layer, the vector "y2" is obtained by n-times duplicating the vector y1.  In Appendix A, for clarity of implementation methods, a program listing is presented that works on the basis of expressions 10-11. <br><br>  Reducing the task of learning to the task of learning a single-layer perceptron. <br><br>  Using the physical meaning of the weights of the neurons of the hidden layer, namely that the weights of the neurons of the hidden layer are frequencies.  Assuming that the oscillations are periodic, it is possible to determine the initial cyclic frequencies for each direction, and to initialize the weights in such a way that, passing from a neuron to a neuron, the frequencies will multiply.  So you can reduce the task of learning a complex two-layer network to the task of learning a single-layer perceptron.  The only difficulty is that the inputs and outputs of this perceptron are complex.  But the teaching methods of this network will be discussed in the next article. <br><br>  <b>KINSF modification for recognition tasks</b> <br><br>  In problems of recognition or classification at the output of neural networks, it is necessary to obtain the so-called membership function [3], the value of which lies within, from zero to one.  To obtain it, it is very convenient to use the logistic sigmoid function (Figure 5).  By transforming a few neurons of my network, it is possible to get a network for pattern recognition.  Such a network is well suited for recognizing binary images processed by the method described above (expressions 1-6).  It takes complex numbers at the input, after which it approximates the function of the image belonging to the class. [4] <br>  Figure 6 shows the neuron of the hidden layer, and figure 7 shows the output layer. <br><br>  As we can see, the scheme of the hidden neuron has not changed, which cannot be said about the neurons of the output layer. <br><br><img src="https://habrastorage.org/files/511/8f0/582/5118f0582d27487f8d9480b610532e9d.png"><br>  <i>Figure 6. Scheme of the hidden neuron.</i> <br><br><img src="https://habrastorage.org/files/ac1/2d2/866/ac12d2866dda4a398d4509c92a5eb9ef.png"><br>  <i>Figure 7. Diagram of the output neuron.</i> <br><br>  <b>Experiment</b> <br><br>  For the experiment a number of pictures was chosen, one of them is presented below.  Before recognition, binarization and reduction algorithms were used. <br><br>  Reduction to a general view was performed according to the algorithm presented above (1-6).  After that, the vector, in the case of KINSF, was fed completely ‚Äúas is‚Äù to the artificial neural network, and in the case of NS with a sigmaidal activation function, the real and imaginary components were fed separately, after which the vector was recognized.  The experiment was conducted using two types of NA.  Direct distribution network with sigmoidal activation function and KINSF.  The scope of the training sample was 660 vectors divided into 33 classes.  The program is shown in Figure 8. <br><br><img src="https://habrastorage.org/files/74a/1bd/912/74a1bd9126cc45789b6063c9f751a991.png"><br>  <i>Figure 8. Handwriting recognition software.</i> <br><br>  The network with a sigmoidal activation function serves separately the real and imaginary components of the complex vector.  KINSF and ANN with a sigmoidal activation function were trained using the 500 learning cycles delta rule.  As a result of testing, 400 images of KINSF produced an accuracy of 87%, and an ANN with a sigmoidal activation function of 73%. <br><br>  <b>Code</b> <br><br><pre><code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">KINSF_Simple</span></span> { ComplexVector input; <span class="hljs-comment"><span class="hljs-comment">//  ComplexVector output, // fOut; //   Matrix fL; //     ComplexMatrix C; //     int inpN, outpN, hN, n =0 ; Complex J = new Complex(0,1); //   public KINSF_Simple(int inpCout, int outpCout, int hLCount) { inpN = inpCout; outpN = outpCout; hN = hLCount; //   .  fL = Statistic.rand(inpN, hN); C = Statistic.randComplex(inpN*hN,outpN); } /// &lt;summary&gt; /// -    /// &lt;/summary&gt; /// &lt;param name="compVect"&gt;&lt;/param&gt; /// &lt;returns&gt;&lt;/returns&gt; public ComplexVector FActiv1(ComplexVector compVect) { ComplexVector outp = compVect.Copy(); for(int i = 0; i&lt;outp.N; i++) { outp.Vecktor[i] = Complex.Exp(J*outp.Vecktor[i]); } return outp; } /// &lt;summary&gt; ///    /// &lt;/summary&gt; /// &lt;param name="inp"&gt;&lt;/param&gt; void OutputFirstLayer(ComplexVector inp) { input = inp; fOut = FActiv1(inp*fL); } /// &lt;summary&gt; ///    /// &lt;/summary&gt; void OutputOutLayer() { List&lt;Complex&gt; outList = new List&lt;Complex&gt;(); for(int i = 0; i&lt;inpN; i++) { outList.AddRange(fOut.Vecktor); } ComplexVector outVector = new ComplexVector(outList.ToArray()); output = fOut*C; } /// &lt;summary&gt; ///      /// &lt;/summary&gt; /// &lt;param name="inp"&gt; &lt;/param&gt; /// &lt;returns&gt;&lt;/returns&gt; public ComplexVector NetworkOut(ComplexVector inp) { OutputFirstLayer(inp); OutputOutLayer(); return output; } }</span></span></code> </pre> <br>  <b>Video work</b> <br><br>  Below is a video of the work of this program that was in the test.  I wrote it down as a demonstration of how INS can be applied.  Immediately apologize for the freestyle presentation. <br><iframe width="560" height="315" src="https://www.youtube.com/embed/8q15K8ym_n0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  <b>Results</b> <br><br>  Benefits: <br><br>  This network has several advantages.  Most importantly, this network can work with complex numbers.  It can be used to work with signals specified in a complex form.  Also, it is very well suited for recognizing binary images.  It can approximate a function, leaving all the properties of the Fourier transform, which makes it easy to analyze.  After all, the non-analyzability of the work of neural networks leads to the fact that research projects are increasingly being abandoned by them, solving problems by other methods.  Also, the physical meaning of the weights will allow using this neural network for the numerical decomposition of the function of many variables in a Fourier series. <br><br>  Disadvantages: <br><br>  A significant disadvantage of this neural network is the presence of a large number of connections.  Much more than the number of connections of direct distribution networks of the perceptron type. <br><br>  Further study: <br><br>  In the course of further work on this neural network, it is planned: to develop more advanced training methods, to prove the convergence of training, to prove the absence of retraining error for any number of neurons on the hidden layer, to build a recurrent neural network based on KINF designed for processing speech and other signals. <br><br><div class="spoiler">  <b class="spoiler_title">Literature</b> <div class="spoiler_text">  1. Osovsky C ... Neural networks for information processing / Per.  from Polish I.D.  Rudinsky - Moscow: Finance and Statistics, 2004. <br>  2. Fichtengolts G.M.  Course of differential and integral calculus.  Volume 3. - M .: FIZMATLIT, 2001. <br>  3. Zade L. The concept of a linguistic variable and its application to making approximate decisions.  - M .: Mir, 1976. <br>  4. Rutkovskaya D., Pilinsky M., Rutkovsky L. Neural networks, genetic algorithms and fuzzy systems: Trans.  from polish  I.D. Rudinsky.  - M .: Hotline - Telecom, 2006. </div></div></div><p>Source: <a href="https://habr.com/ru/post/321066/">https://habr.com/ru/post/321066/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../321056/index.html">First steps with STM32 and mikroC compiler for ARM architecture - Part 3 - UART and GSM module</a></li>
<li><a href="../321058/index.html">Welcome to Game Design meetup February 4</a></li>
<li><a href="../321060/index.html">Bypass restrictions in Calabash-Android with UIAutomator</a></li>
<li><a href="../321062/index.html">Using GlusterFS with Docker swarm cluster</a></li>
<li><a href="../321064/index.html">We pump over NES Classic Mini</a></li>
<li><a href="../321068/index.html">Roskomnadzor for the Bitcoin ban in Russia</a></li>
<li><a href="../321070/index.html">Perspectives of HR robots / bots in the field of staff recruitment - current realities, opinions and experience of experts</a></li>
<li><a href="../321072/index.html">85% of employees scores on project management systems. How do we do our</a></li>
<li><a href="../321074/index.html">sudo rm -rf, or Chronicle of the incident with the database GitLab.com from 2017/01/31</a></li>
<li><a href="../321076/index.html">Powershell and Cyrillic in console applications (updated)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>