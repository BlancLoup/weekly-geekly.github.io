<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Web Scraping with python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 Having recently looked at KinoPoisk, I discovered that over the years I managed to leave more than 1000 ratings and thought that it wou...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Web Scraping with python</h1><div class="post__text post__text-html js-mediator-article"><h2>  Introduction </h2><br>  Having recently looked at KinoPoisk, I discovered that over the years I managed to leave more than 1000 ratings and thought that it would be interesting to investigate this data in more detail: did my tastes in the cinema change over time?  Is there an annual / weekly seasonality in activity?  Do my ratings correlate with a KinoPoisk rating, IMDb or film critics? <br>  But before you analyze and build beautiful graphics, you need to get the data.  Unfortunately, many services (and KinoPoisk is not an exception) do not have a public API, so you have to roll up your sleeves and parse html pages.  It is about how to download and parse the website, and I want to tell you in this article. <br>  First of all, the article is intended for those who always wanted to deal with Web Scrapping, but did not reach out or did not know where to start. <br><br>  <strong>Off-topic</strong> : by the way, <a href="http://plus.kinopoisk.ru/">New Film Search</a> under the hood uses queries that return evaluation data in the form of JSON, so the task could be solved in a different way. <a name="habracut"></a><br><br><h2>  Task </h2><br>  The task will be to upload data about the films viewed at KinoPoisk: the name of the film (Russian, English), the date and time of viewing, the rating of the user. <br>  In fact, you can divide the work into 2 stages: <br><ul><li>  <strong>Stage 1:</strong> unload and save html pages </li><li>  <strong>Stage 2:</strong> parse html into a convenient format for further analysis (csv, json, pandas dataframe etc.) </li></ul><br><h2>  Instruments </h2><br>  There are quite a few python libraries to send http requests, the most famous urllib / urllib2 and Requests.  For my taste, <a href="http://docs.python-requests.org/en/master/">Requests are</a> more convenient and more concise, so I will use it. <br>  You also need to select a library for parsing html, a small research gives the following options: <br><ul><li>  <a href="https://docs.python.org/2/library/re.html"><strong>re</strong></a> <br>  Regular expressions, of course, will be useful to us, but to use only them, in my opinion, is too hardcore, and they <a href="http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags">are not a little for that</a> .  More convenient tools for parsing html were invented, so let's move on to them. </li><li>  <a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/"><strong>BeatifulSoup</strong></a> , <a href="http://lxml.de/index.html"><strong>lxml</strong></a> <br>  These are the two most popular libraries for parsing html and the choice of one of them, rather, is due to personal preferences.  Moreover, these libraries are closely intertwined: BeautifulSoup began to use lxml as an internal parser for acceleration, and the module soupparser was added to lxml.  More about the pros and cons of these libraries can be found <a href="http://stackoverflow.com/questions/1922032/parsing-html-in-python-lxml-or-beautifulsoup-which-of-these-is-better-for-wha">in the discussion</a> .  For comparison, I will parse the data using BeautifulSoup and using <a href="http://lxml.de/xpathxslt.html">XPath</a> selectors in the lxml.html module. </li><li>  <a href="http://scrapy.org/"><strong>scrapy</strong></a> <br>  This is no longer just a library, but a whole open-source framework for retrieving data from web pages.  It has many useful functions: asynchronous requests, the ability to use XPath and CSS selectors for data processing, convenient work with encodings and much more (you can read more <a href="http://doc.scrapy.org/en/latest/intro/overview.html">here</a> ).  If my task was not a one-time download, but a production process, then I would choose it.  In the current production, this is overkill. </li></ul><br><h2>  Data loading </h2><br><h3>  First try </h3><br>  Let's start to upload data.  To begin with, let's just try to get the page by url and save it to a local file. <br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests user_id = <span class="hljs-number"><span class="hljs-number">12345</span></span> url = <span class="hljs-string"><span class="hljs-string">'http://www.kinopoisk.ru/user/%d/votes/list/ord/date/page/2/#list'</span></span> % (user_id) <span class="hljs-comment"><span class="hljs-comment"># url    r = requests.get(url) with open('test.html', 'w') as output_file: output_file.write(r.text.encode('cp1251'))</span></span></code> </pre> <br>  Open the resulting file and see that everything is not so simple: the site has recognized the robot in us and is not in a hurry to show the data. <br><img src="https://habrastorage.org/files/5cb/aa1/151/5cbaa115161c43feb7b3f87e47607545.png" alt="image"><br><h3>  We will understand how the browser works </h3><br>  However, the browser is great at getting information from the site.  Let's see how it sends the request.  To do this, use the "Network" panel in the "Developer Tools" in the browser (I use Firebug for this), the request we usually need is the longest. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/118/0ab/1f4/1180ab1f48e74572bfcf4d42ac893331.png" alt="image"><br><br>  As we can see, the browser also passes in headers UserAgent, a cookie and a number of other parameters.  For a start, let's just try to pass the correct UserAgent to the header. <br><pre> <code class="python hljs">headers = { <span class="hljs-string"><span class="hljs-string">'User-Agent'</span></span>: <span class="hljs-string"><span class="hljs-string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:45.0) Gecko/20100101 Firefox/45.0'</span></span> } r = requests.get(url, headers = headers)</code> </pre> <br>  At this time everything turned out, now we are given the necessary data.  It is worth noting that sometimes the site also checks the correctness of the cookie, in which case <a href="http://docs.python-requests.org/en/master/user/advanced/">sessions</a> in the Requests library will help. <br><br><h3>  Download all ratings </h3><br>  Now we can save one page with ratings.  But usually the user has a lot of ratings and you need to iterate through all the pages.  The page number we are interested in can be easily transferred directly to the url.  There remains only the question: "How to understand how many pages with ratings?"  I solved this problem as follows: if you specify an oversized page number, then such a page without a table with films will return to us.  Thus, we can iterate over the pages for as long as there is a block with movie ratings ( <code>&lt;div class = "profileFilmsList"&gt;</code> ). <br><br><img src="https://habrastorage.org/files/582/b17/2f1/582b172f12124b82967d6913ee40f1cf.png" alt="image"><br><div class="spoiler">  <b class="spoiler_title">Full data download code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests <span class="hljs-comment"><span class="hljs-comment"># establishing session s = requests.Session() s.headers.update({ 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:45.0) Gecko/20100101 Firefox/45.0' }) def load_user_data(user_id, page, session): url = 'http://www.kinopoisk.ru/user/%d/votes/list/ord/date/page/%d/#list' % (user_id, page) request = session.get(url) return request.text def contain_movies_data(text): soup = BeautifulSoup(text) film_list = soup.find('div', {'class': 'profileFilmsList'}) return film_list is not None # loading files page = 1 while True: data = load_user_data(user_id, page, s) if contain_movies_data(data): with open('./page_%d.html' % (page), 'w') as output_file: output_file.write(data.encode('cp1251')) page += 1 else: break</span></span></code> </pre> <br></div></div><br><h2>  Parsing </h2><br><h3>  A little bit about XPath </h3><br>  XPath is a language for querying xml and xhtml documents.  We will use XPath selectors when working with the lxml library ( <a href="http://lxml.de/xpathxslt.html">documentation</a> ).  Consider a small example of working with XPath <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> lxml <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> html test = <span class="hljs-string"><span class="hljs-string">''' &lt;html&gt; &lt;body&gt; &lt;div class="first_level"&gt; &lt;h2 align='center'&gt;one&lt;/h2&gt; &lt;h2 align='left'&gt;two&lt;/h2&gt; &lt;/div&gt; &lt;h2&gt;another tag&lt;/h2&gt; &lt;/body&gt; &lt;/html&gt; '''</span></span> tree = html.fromstring(test) tree.xpath(<span class="hljs-string"><span class="hljs-string">'//h2'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#  h2  tree.xpath('//h2[@align]') # h2    align tree.xpath('//h2[@align="center"]') # h2    align  "center" div_node = tree.xpath('//div')[0] # div  div_node.xpath('.//h2') #  h2 ,    div </span></span></code> </pre><br>  You can also read more about XPath syntax at <a href="http://www.w3schools.com/xsl/xpath_intro.asp">W3Schools</a> . <br><br><h3>  Let's return to our task. </h3><br>  Now let's go directly to getting data from html.  The easiest way to understand how the html-page is arranged using the function "Inspect element" in the browser.  In this case, everything is quite simple: the entire table with estimates is enclosed in the <code>&lt;div class = "profileFilmsList"&gt;</code> .  Select this node: <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> bs4 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BeautifulSoup <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> lxml <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> html <span class="hljs-comment"><span class="hljs-comment"># Beautiful Soup soup = BeautifulSoup(text) film_list = soup.find('div', {'class': 'profileFilmsList'}) # lxml tree = html.fromstring(text) film_list_lxml = tree.xpath('//div[@class = "profileFilmsList"]')[0]</span></span></code> </pre> <br>  Each movie is presented as <code>&lt;div class = "item"&gt;</code> or <code>&lt;div class = "item even"&gt;</code> .  Consider how to pull out the Russian title of the film and the link to the film page (also learn how to get the text and the value of the attribute). <br><img src="https://habrastorage.org/files/fb3/b60/ca3/fb3b60ca302141098a888d2d91e7fa5b.png" alt="image"><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Beatiful Soup movie_link = item.find('div', {'class': 'nameRus'}).find('a').get('href') movie_desc = item.find('div', {'class': 'nameRus'}).find('a').text # lxml movie_link = item_lxml.xpath('.//div[@class = "nameRus"]/a/@href')[0] movie_desc = item_lxml.xpath('.//div[@class = "nameRus"]/a/text()')[0]</span></span></code> </pre> <br>  Another small hint for debug: in order to see that inside the selected node in BeautifulSoup, you can simply print it, and in the lxml use the <code>tostring()</code> function of the <code>tostring()</code> module. <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># BeatifulSoup print item #lxml from lxml import etree print etree.tostring(item_lxml)</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Full code for parsing html-files under the cut</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">read_file</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> input_file: text = input_file.read() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> text <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parse_user_datafile_bs</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> results = [] text = read_file(filename) soup = BeautifulSoup(text) film_list = film_list = soup.find(<span class="hljs-string"><span class="hljs-string">'div'</span></span>, {<span class="hljs-string"><span class="hljs-string">'class'</span></span>: <span class="hljs-string"><span class="hljs-string">'profileFilmsList'</span></span>}) items = film_list.find_all(<span class="hljs-string"><span class="hljs-string">'div'</span></span>, {<span class="hljs-string"><span class="hljs-string">'class'</span></span>: [<span class="hljs-string"><span class="hljs-string">'item'</span></span>, <span class="hljs-string"><span class="hljs-string">'item even'</span></span>]}) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> item <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> items: <span class="hljs-comment"><span class="hljs-comment"># getting movie_id movie_link = item.find('div', {'class': 'nameRus'}).find('a').get('href') movie_desc = item.find('div', {'class': 'nameRus'}).find('a').text movie_id = re.findall('\d+', movie_link)[0] # getting english name name_eng = item.find('div', {'class': 'nameEng'}).text #getting watch time watch_datetime = item.find('div', {'class': 'date'}).text date_watched, time_watched = re.match('(\d{2}\.\d{2}\.\d{4}), (\d{2}:\d{2})', watch_datetime).groups() # getting user rating user_rating = item.find('div', {'class': 'vote'}).text if user_rating: user_rating = int(user_rating) results.append({ 'movie_id': movie_id, 'name_eng': name_eng, 'date_watched': date_watched, 'time_watched': time_watched, 'user_rating': user_rating, 'movie_desc': movie_desc }) return results def parse_user_datafile_lxml(filename): results = [] text = read_file(filename) tree = html.fromstring(text) film_list_lxml = tree.xpath('//div[@class = "profileFilmsList"]')[0] items_lxml = film_list_lxml.xpath('//div[@class = "item even" or @class = "item"]') for item_lxml in items_lxml: # getting movie id movie_link = item_lxml.xpath('.//div[@class = "nameRus"]/a/@href')[0] movie_desc = item_lxml.xpath('.//div[@class = "nameRus"]/a/text()')[0] movie_id = re.findall('\d+', movie_link)[0] # getting english name name_eng = item_lxml.xpath('.//div[@class = "nameEng"]/text()')[0] # getting watch time watch_datetime = item_lxml.xpath('.//div[@class = "date"]/text()')[0] date_watched, time_watched = re.match('(\d{2}\.\d{2}\.\d{4}), (\d{2}:\d{2})', watch_datetime).groups() # getting user rating user_rating = item_lxml.xpath('.//div[@class = "vote"]/text()') if user_rating: user_rating = int(user_rating[0]) results.append({ 'movie_id': movie_id, 'name_eng': name_eng, 'date_watched': date_watched, 'time_watched': time_watched, 'user_rating': user_rating, 'movie_desc': movie_desc }) return results</span></span></code> </pre> <br></div></div><br><h2>  Summary </h2><br>  As a result, we learned how to parse web-sites, got acquainted with the libraries of Requests, BeautifulSoup and lxml, and also received data suitable for further analysis about watched films on KinoPoisk. <br><img src="https://habrastorage.org/files/7f4/f35/054/7f4f35054190422d887a832c1ac2d997.png" alt="image"><br>  The complete project code can be found on <a href="https://github.com/miptgirl/kinopoisk_data/blob/master/loading_kp_data.ipynb">github'e</a> . <br><br><h2>  UPD </h2><br>  As noted in the <a href="https://habrahabr.ru/post/280238/">comments</a> , the following topics may be useful in the context of Web Scrapping: <br><ul><li>  <strong>Authentication:</strong> often in order to obtain data from the site you need to pass authentication, in the simplest case it is just HTTP Basic Auth: login and password.  Here the Requests library <a href="http://docs.python-requests.org/en/master/user/authentication/">will help</a> us again.  In addition, oauth2 is widespread: how to use oauth2 in python can be read on <a href="http://stackoverflow.com/questions/9548729/how-to-authenticate-a-site-with-python-using-urllib2">stackoverflow</a> .  Also in the <a href="https://habrahabr.ru/post/280238/">comments</a> there is an example from <a href="https://habrahabr.ru/users/terras/" class="user_link">Terras</a> of how to authenticate in a web form. </li><li>  <strong>Controls:</strong> The site can also have additional web-forms (drop-down lists, check boxes, etc.).  The algorithm for working with them is about the same: we look at what the browser sends and send the same parameters as data to a POST request ( <a href="http://docs.python-requests.org/en/master/user/quickstart/">Requests</a> , <a href="http://stackoverflow.com/questions/8377055/submit-data-via-web-form-and-extract-the-results">stackoverflow</a> ).  I can also recommend to see the 2nd lesson of the <a href="https://www.udacity.com/course/data-wrangling-with-mongodb--ud032">course "Data Wrangling" at Udacity</a> , where an example of scrapping the site of the <a href="http://www.transtats.bts.gov/Data_Elements.aspx%3FData%3D2">US Department of Transportation</a> and sending the data of web forms is described in detail. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/280238/">https://habr.com/ru/post/280238/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../280224/index.html">Caller ID handling and distinctive 3CX Phone System call</a></li>
<li><a href="../280228/index.html">Drupal licensing FAQ</a></li>
<li><a href="../280232/index.html">Microsoft DevCon 2016 - computer vision, SQL Server 2016, Data Science and more</a></li>
<li><a href="../280234/index.html">Rules for working with Tasks API. Part 1</a></li>
<li><a href="../280236/index.html">We make our friGate with anonymity and without advertising</a></li>
<li><a href="../280248/index.html">Vivaldi builds you might not have noticed</a></li>
<li><a href="../280250/index.html">Quick start 1C Trade management for Kazakhstan (Trade and Warehouse)</a></li>
<li><a href="../280254/index.html">Experience starting AHCI in VxWorks653</a></li>
<li><a href="../280256/index.html">Wearable Smart Gateway - wearable device for emergency services</a></li>
<li><a href="../280258/index.html">The digest of interesting materials for the mobile # 146 developer (March 21-27)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>