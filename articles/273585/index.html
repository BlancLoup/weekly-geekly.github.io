<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Parallelizing the Strassen Algorithm on Intel¬Æ Xeon Phi (TM)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Intel Xeon Phi (TM) coprocessors are a PCI Express device and have x86 architecture, providing high peak performance - up to 1.2 teraflops (a trillion...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Parallelizing the Strassen Algorithm on Intel¬Æ Xeon Phi (TM)</h1><div class="post__text post__text-html js-mediator-article">  Intel Xeon Phi (TM) coprocessors are a PCI Express device and have x86 architecture, providing high peak performance - up to 1.2 teraflops (a trillion floating-point operations per second) of double precision per coprocessor.  Xeon Phi (TM) can provide simultaneous operation of up to 244 threads, and this must be considered when programming for maximum efficiency. <br><br>  Recently, together with Intel, we conducted a small study on the effectiveness of the implementation of the Strassen algorithm for the Intel Xeon Phi (TM) coprocessor.  Who are interested in the subtleties of working with this device and just loving parallel programming, please under the cat. <br><br><div style="text-align:center;"><img height="60%" width="60%" src="https://habrastorage.org/files/016/5b4/e34/0165b4e343644704b7db1bd22325cc50.jpg"></div><br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The total number of operations of the standard multiplication algorithm for square matrices of size <i>n</i> (additions and multiplications): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/dd6/8c7/0a1/dd68c70a1b1b4a878741e0dd3a43580a.gif"></div><br>  The Strassen Algorithm (1969), which belongs to the fast matrix multiplication algorithms, requires: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a1a/34b/118/a1a34b1183234b1bbde6bd475de71f7a.gif"></div><br>  Thus, the Strassen algorithm has less asymptotic complexity and is potentially faster on large matrix sizes.  But the recursive nature of this algorithm presents a certain complexity for efficient parallelization on modern computing systems and the use of data from memory. <br><br>  In this paper, we consider a combined approach in which the standard matrix multiplication algorithm is called when the threshold value is reached from the Strassen algorithm (we used Intel MKL DGEMM as the standard algorithm).  This is due to the fact that with small matrix sizes (tens or hundreds, depending on the processor architecture), Strassen‚Äôs algorithm begins to lose to the standard algorithm both in the number of operations and due to a larger number of cash misses.  Theoretical estimates for the matrix size threshold for the transition to the standard algorithm (excluding caching and pipelining) give various estimates - 8 for Hayem and 12 for Hass-Lederman; in practice, the threshold value depends on the architecture of the computing system and should be evaluated experimentally.  For our system with Intel Xeon Phi (TM) 7120D, the value of 1024 turned out to be the most effective. <br><br>  The results of computational experiments were compared with the DGEMM function from the Intel MKL library.  Considered multiplication of square size matrices <img src="https://habrastorage.org/files/393/5c5/01b/3935c501b3614dd89ef120c47e60f334.png">  where <img src="https://habrastorage.org/files/67c/a0d/f25/67ca0df25f8241dcb473dba8bf79e7e2.png">  and <img src="https://habrastorage.org/files/81f/a2e/af5/81fa2eaf53304f04aa28a7fa4a85a4f7.png">  .  M here means the threshold size of the matrices, after which the standard algorithm is called.  The multiplication of two matrices is written as <img src="https://habrastorage.org/files/e80/622/084/e80622084056409b94208309c130230e.gif">  where a, b and c are matrix size <img src="https://habrastorage.org/files/393/5c5/01b/3935c501b3614dd89ef120c47e60f334.png">  .  The Strassen method for multiplying matrices is based on the recursive division of each multiplied matrix into 4 submatrices and performing operations on them.  Matrix size requirement ( <img src="https://habrastorage.org/files/67c/a0d/f25/67ca0df25f8241dcb473dba8bf79e7e2.png">  and <img src="https://habrastorage.org/files/81f/a2e/af5/81fa2eaf53304f04aa28a7fa4a85a4f7.png">  ) it is necessary that it was possible to split each matrix into 4 submatrices at the required recursion depth. <br><br>  The Strassen method is described by the following formula: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/9fa/45a/78d/9fa45a78d48343fdbf358f1e299f356e.gif"></div><br><br>  Where <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/5d3/091/764/5d309176413d43fa955b231a9da03299.gif"></div><br><br>  Below is shown how 4 groups of operations can be distinguished; in each of them, all operations can be performed in parallel. <br><br><div style="text-align:center;"><img width="500" src="https://habrastorage.org/files/d60/b3c/f72/d60b3cf7294d4cc9b7ccb67cc3a2428f.png"></div><br><br>  There is nothing difficult in the implementation of parallelization.  OpenMP and the task mechanism ( <i>omp task</i> ) were used, the load distribution between tasks is shown in the figure.  The first groups of operations were combined into one, so there are fewer synchronization points, since this is a very expensive operation. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a15/e44/be0/a15e44be0efa44c18e65281ed50598bd.png"></div><br><br>  To measure the time, a standard approach was used with a ‚Äúwarming up‚Äù start and averaging the time of several starts.  As a timer, the Intel MKL dsecnd function is used. <br><br><div class="spoiler">  <b class="spoiler_title">function of measuring the operation time of multiplication algorithms</b> <div class="spoiler_text"><pre><code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">double</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">run_method</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(MatrixMultiplicationMethod mm_method, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> n, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params"> *A, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params"> *B, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">double</span></span></span></span><span class="hljs-function"><span class="hljs-params"> *C)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> runs = <span class="hljs-number"><span class="hljs-number">5</span></span>; mm_method(n, A, B, C); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> start_time = dsecnd(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; runs; ++i) mm_method(n, A, B, C); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> end_time = dsecnd(); <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> elapsed_time = (end_time - start_time) / runs; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> elapsed_time; }</code> </pre> <br></div></div><br>  After the implementation of parallelization, we conducted a series of tests.  Tests were conducted in native mode on Intel Xeon Phi (TM) 7120D, 16 GB. <br><br><div class="spoiler">  <b class="spoiler_title">What are the Xeon Phi (TM) and usage modes?</b> <div class="spoiler_text"><table><tbody><tr><td>  Name </td><td>  TDP (WATTS) </td><td>  Number of cores </td><td>  Frequency (GHz) </td><td>  Peak Performance (GFLOP) </td><td>  Peak Bandwidth (GB / s) </td><td>  Memory Capacity (GB) </td></tr><tr><td>  3120A </td><td>  300 </td><td>  57 </td><td>  1.1 </td><td>  1003 </td><td>  240 </td><td>  6 </td></tr><tr><td>  3120P </td><td>  300 </td><td>  57 </td><td>  1.1 </td><td>  1003 </td><td>  240 </td><td>  6 </td></tr><tr><td>  5110P </td><td>  225 </td><td>  60 </td><td>  1.053 </td><td>  1011 </td><td>  320 </td><td>  eight </td></tr><tr><td>  5120D </td><td>  245 </td><td>  60 </td><td>  1.053 </td><td>  1011 </td><td>  352 </td><td>  eight </td></tr><tr><td>  7120P </td><td>  300 </td><td>  61 </td><td>  1.238 </td><td>  1208 </td><td>  352 </td><td>  sixteen </td></tr><tr><td>  7120X </td><td>  300 </td><td>  61 </td><td>  1.238 </td><td>  1208 </td><td>  352 </td><td>  sixteen </td></tr><tr><td>  7120A </td><td>  300 </td><td>  61 </td><td>  1.238 </td><td>  1208 </td><td>  352 </td><td>  sixteen </td></tr><tr><td>  7120D </td><td>  270 </td><td>  61 </td><td>  1.238 </td><td>  1208 </td><td>  352 </td><td>  sixteen </td></tr></tbody></table><br><br><table border="5"><tbody><tr><td width="250"><img src="https://habrastorage.org/files/510/f02/d15/510f02d15b93405bbfc7ff89e4e1f4b8.png" width="253"></td><td width="250"><img src="https://habrastorage.org/files/599/83b/4bd/59983b4bd751492ab70f2e63575ceaab.png" width="250"></td><td width="250"><img src="https://habrastorage.org/files/32e/846/0e1/32e8460e141c452784355b959b0ca9ab.png" width="250"></td></tr><tr><td>  Suitable for high-parallel and vectorized code, sequential code is slow </td><td>  Suitable for serial code with large parallel regions, a potential problem is data transfer over PCIe </td><td>  Suitable for high-parallel code that runs efficiently on both platforms, a potential problem is load imbalance </td></tr></tbody></table><br></div></div><br>  In order not to complicate the article, all tests will be carried out on 8192 √ó 8192 matrix sizes (this size is almost marginal in terms of memory consumption for the Strassen parallelized algorithm ‚Äî about 10GB ‚Äî and is large enough to experience the effect of a smaller asymptotic complexity of the algorithm). <br><table><tbody><tr><td>  Number of threads </td><td>  one </td><td>  four </td><td>  eight </td><td>  sixteen </td><td>  60 </td><td>  120 </td><td>  180 </td><td>  240 </td></tr><tr><td>  Strassen, with </td><td>  <b>114.89</b> </td><td>  <b>29.9</b> </td><td>  <b>15.75</b> </td><td>  <b>8.85</b> </td><td>  3.68 </td><td>  3.58 </td><td>  4.22 </td><td>  8.17 </td></tr><tr><td>  MKL DGEMM, c </td><td>  131.79 </td><td>  34.38 </td><td>  17.27 </td><td>  9 </td><td>  2.61 </td><td>  1.90 </td><td>  2.45 </td><td>  2.54 </td></tr></tbody></table><br>  On a small number of threads, the Strassen algorithm turned out to be faster than the Intel MKL DGEMM.  It was also noted that a large number of threads experience a drop in performance (the task almost does not scale over 60 threads).  For efficient use of Intel Xeon Phi (TM) resources in a multi-threaded application, it is recommended to use the number of threads, multiple of <i>NP-1</i> , where <i>NP</i> is the number of processors in the device (61 in our case).  You can read more <a href="https://software.intel.com/en-us/articles/openmp-thread-affinity-control">here</a> . <br><br>  The idea arose that the use of parallelism within the DGEMM called from Strassen could help further scaling.  There are several functions for managing the number of threads in MKL: <i>mkl_set_num_threads</i> , <i>mkl_set_num_threads_local</i> , <i>mkl_domain_set_num_threads</i> .  When trying to use <i>mkl_set_num_threads,</i> we found that this function does not affect the number of threads in the MKL DGEMM called from the Strassen algorithm (the number of threads in the MKL outside of Stra√üen was affected by this function).  Nested parallelism was enabled ( <i>OMP_NESTED = true</i> ). <br><br>  As it turned out, MKL actively resists nested parallelism.  MKL uses the environment variable <a href="https://software.intel.com/en-us/node/528547"><i>MKL_DYNAMIC</i></a> , which is <i>true</i> by default.  This variable allows MKL to reduce the number of threads that the user sets, in particular, when calling MKL functions from the parallel area, the number of threads will be set to 1. To enable parallelism in MKL DGEMM, we used the <i>mkl_set_dynamic (0)</i> function. <br><br><div class="spoiler">  <b class="spoiler_title">the code of what we got</b> <div class="spoiler_text"><pre> <code class="cpp hljs">mkl_set_dynamic(<span class="hljs-number"><span class="hljs-number">0</span></span>); omp_set_nested(<span class="hljs-number"><span class="hljs-number">1</span></span>); set_num_threads(num_threads); mkl_set_num_threads(mkl_num_threads); strassen( ‚Ä¶ ); ‚Ä¶ mkl_set_num_threads(num_threads * mkl_num_threads); dgemm( ‚Ä¶ );</code> </pre><br></div></div><br><table><tbody><tr><td rowspan="3">  Total number of threads </td><td colspan="3">  Strassen, with </td><td rowspan="3">  MKL DGEMM, with </td></tr><tr><td colspan="3">  Number of threads in MKL DGEMM </td></tr><tr><td>  one </td><td>  2 </td><td>  four </td></tr><tr><td>  60 </td><td>  3.68 </td><td>  3.89 </td><td>  5.19 </td><td>  2.61 </td></tr><tr><td>  120 </td><td>  3.58 </td><td>  <b>3.50</b> </td><td>  3.82 </td><td>  1.90 </td></tr><tr><td>  240 </td><td>  8.17 </td><td>  4.23 </td><td>  3.59 </td><td>  2.54 </td></tr></tbody></table><br>  The results of the Strassen algorithm for 120 threads using the multi-threaded MKL DGEMM became a little better, but we did not get much benefit. <br><br>  Our next step was to study the binding of OpenMP software threads to physical and logical cores (binding).  On Xeon Phi, the environment variables <i>KMP_AFFINITY</i> and <i>KMP_PLACE_THREADS are used</i> to control the binding of threads to the kernels.  A good description found <a href="https://software.intel.com/en-us/articles/openmp-thread-affinity-control">here</a> . <br><br>  The most important among the <i>KMP_AFFINITY</i> parameters is <i>type</i> , which controls the order in which the threads are assigned to the kernels (see figure below). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/cd4/95c/5c9/cd495c5c9c2e49b592b8d9f57a83493d.png"></div><br><br>  The following <i>KMP_AFFINITY</i> values ‚Äã‚Äãwere used: <br><br>  KMP_AFFINITY = granularity = fine, balanced <br><br><table><tbody><tr><td rowspan="3">  Total number of threads </td><td colspan="3">  Strassen, with </td><td rowspan="3">  MKL DGEMM, with </td></tr><tr><td colspan="3">  Number of threads in MKL DGEMM </td></tr><tr><td>  one </td><td>  2 </td><td>  four </td></tr><tr><td>  60 </td><td>  3.67 </td><td>  4.07 </td><td>  5.53 </td><td>  2.64 </td></tr><tr><td>  120 </td><td>  3.54 </td><td>  3.51 </td><td>  3.95 </td><td>  1.51 </td></tr><tr><td>  240 </td><td>  7.11 </td><td>  4.33 </td><td>  <b>3.41</b> </td><td>  1.15 </td></tr></tbody></table><br><br>  KMP_AFFINITY = granularity = fine, compact <br><br><table><tbody><tr><td rowspan="3">  Total number of threads </td><td colspan="3">  Strassen, with </td><td rowspan="3">  MKL DGEMM, with </td></tr><tr><td colspan="3">  Number of threads in MKL DGEMM </td></tr><tr><td>  one </td><td>  2 </td><td>  four </td></tr><tr><td>  60 </td><td>  9.29 </td><td>  9.96 </td><td>  10.27 </td><td>  4.58 </td></tr><tr><td>  120 </td><td>  6.23 </td><td>  6.79 </td><td>  6.04 </td><td>  2.31 </td></tr><tr><td>  240 </td><td>  9.32 </td><td>  5.21 </td><td>  <b>4.21</b> </td><td>  1.15 </td></tr></tbody></table><br>  By default, the variable <i>KMP_AFFINITY = granularity = core, balanced</i> .  When testing, it turned out that the best parameters for matrix multiplication are <i>KMP_AFFINITY = granularity = fine, balanced</i> , and the parameter does not affect the results of the <i>MKL DGEMM</i> as much as the Strassen algorithm, where, compared to <i>KMP_AFFINITY = granularity = fine, compact,</i> there is a two-time reduction work.  Also note that changing the environment variable <i>KMP_AFFINITY</i> from its default value ( <i>KMP_AFFINITY = granularity = core, balanced</i> ) reduced the running time of the <i>MKL DGEMM</i> from the minimum 1.90 s to 1.15 s (approximately 1.5 times).  The <i>MKL DGEMM results</i> with <i>compact</i> and <i>balanced</i> differ in a predictable way, for example, with 120 threads, the <i>compact</i> option uses 30 cores with 4 threads <i>each</i> , and <i>balanced</i> - 60 through 2, due to a larger number of cores and almost double acceleration is obtained in the <i>balanced</i> version. <br><br>  We also tried to profile the program on Xeon Phi, how to do this you can read <a href="https://software.intel.com/en-us/articles/how-to-analyze-xeon-phi-coprocessor-applications-using-intel-vtune-amplifier-xe-2015">here</a> .  To profile only the multiplication algorithm, we used the capabilities of the <a href="https://software.intel.com/ru-ru/node/496602">VTune API</a> . <br><br>  As a result, we could not catch up with the MKL DGEMM on the maximum number of threads, but learned a little more about programming such a powerful calculator like Intel Xeon Phi (TM). </div><p>Source: <a href="https://habr.com/ru/post/273585/">https://habr.com/ru/post/273585/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../273573/index.html">Use the comet server to implement a simple chat.</a></li>
<li><a href="../273575/index.html">Temperature control in the data center: why it is sometimes possible and hotter</a></li>
<li><a href="../273579/index.html">History 30 places in the final Russian AI Cup 2015</a></li>
<li><a href="../273581/index.html">Handlebars. Guide to action</a></li>
<li><a href="../273583/index.html">How the Airbnb engineering team ‚Äúcrashed‚Äù the main project database in a couple of weeks</a></li>
<li><a href="../273589/index.html">Custom EditText with three buttons on the right</a></li>
<li><a href="../273591/index.html">Pro workflow organization</a></li>
<li><a href="../273593/index.html">ACS: from sadness to joy. History of Russian Automation</a></li>
<li><a href="../273595/index.html">How we unpacked the game automated</a></li>
<li><a href="../273597/index.html">"Countdown by simple means - or the rule" 3-2-1 "for disk storages"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>