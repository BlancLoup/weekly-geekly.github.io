<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Brief Introduction to Markov Chains</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In 1998, Lawrence Page, Sergey Brin, Rajiv Motvani and Terry Vinohrad published the article ‚ÄúThe PageRank Citation Ranking: Bringing Order to the Web‚Äù...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Brief Introduction to Markov Chains</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84a/01a/072/84a01a0729fb1a772e89f2fa6c257a7d.gif" alt="image"></div><br>  In 1998, Lawrence Page, Sergey Brin, Rajiv Motvani and Terry Vinohrad published the article ‚ÄúThe PageRank Citation Ranking: Bringing Order to the Web‚Äù, which now describes the famous PageRank algorithm, which became the foundation of Google.  After a little less than two decades, Google has become a giant, and even though its algorithm has evolved greatly, PageRank is still a ‚Äúsymbol‚Äù of Google's ranking algorithms (although only a few people can really tell how much weight it takes today in an algorithm) . <br><br>  From a theoretical point of view, it is interesting to note that one of the standard interpretations of the PageRank algorithm is based on a simple but fundamental concept of Markov chains.  From the article we will see that Markov chains are powerful tools of stochastic modeling, which can be useful to any data analyst expert.  In particular, we will answer such basic questions: what are Markov chains, what good properties do they have, and what can they do with their help? <br><a name="habracut"></a><br><h4>  Short review </h4><br>  In the first section, we give the basic definitions necessary to understand Markov chains.  In the second section, we consider the special case of Markov chains in finite space of states.  In the third section, we will look at some of the elementary properties of Markov chains and illustrate these properties in many small examples.  Finally, in the fourth section, we will connect Markov chains with the PageRank algorithm and see in an artificial example how the Markov chains can be used to rank the nodes of a graph. <br><br><blockquote>  <strong>Note.</strong>  To understand this post requires knowledge of the basics of probability and linear algebra.  In particular, the following concepts will be used: <a href="https://en.wikipedia.org/wiki/Conditional_probability" rel="noopener">conditional probability</a> , <a href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors" rel="noopener">eigenvector</a> and <a href="https://en.wikipedia.org/wiki/Law_of_total_probability" rel="noopener">formula of total probability</a> . </blockquote><br><hr><br><h3>  What is a Markov chain? </h3><br><h4>  Random variables and random processes </h4><br>  Before introducing the concept of Markov chains, let us briefly recall the basic, but important concepts of probability theory. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      First, outside the language of mathematics, a <strong>random variable</strong> X is considered to be a quantity that is determined by the result of a random phenomenon.  Its result can be a number (or ‚Äúsimilarity of a number‚Äù, for example, vectors) or something else.  For example, we can define a random variable as the result of a die roll (number) or as the result of a coin toss (not a number, unless we designate, for example, ‚Äúeagle‚Äù as 0, and ‚Äútails‚Äù as 1).  We also mention that the space of possible outcomes of a random variable can be discrete or continuous: for example, the normal random variable is continuous, and the Poisson random variable is discrete. <br><br>  Next, we can define a <strong>random process</strong> (also called a stochastic <strong>process</strong> ) as a set of random variables indexed by the set T, which often denotes different points in time (in the future we will assume this way).  The two most common cases: T can be either a set of natural numbers (a random process with discrete time) or a set of real numbers (a random process with continuous time).  For example, if we throw a coin every day, we will define a random process with discrete time, and the ever-changing value of the option on the exchange will define a random process with continuous time.  Random variables at different points in time can be independent of each other (an example with a coin toss), or have a certain dependence (an example with an option value);  in addition, they can have a continuous or discrete space of states (the space of possible results at each moment of time). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/31e/ada/2f8/31eada2f80d66f0df4c007ec8da11579.jpg"></div><br>  <i>Different types of random processes (discrete / continuous in space / time).</i> <br><br><h4>  Markov property and Markov chain </h4><br>  There are well-known families of random processes: Gaussian processes, Poisson processes, autoregressive models, moving average models, Markov chains, and others.  Each of these individual cases has certain properties that allow us to better explore and understand them. <br><br>  One of the properties that greatly simplifies the study of a random process is the ‚ÄúMarkov property‚Äù.  If we explain it in a very informal language, the Markov property tells us that if we know the value obtained by some random process at a given point in time, we will not get any additional information about the future behavior of the process, collecting other information about its past.  More mathematical language: at any moment in time, the conditional distribution of future states of a process with given current and past states depends only on the current state, but not on past states (the <strong>absence of memory property</strong> ).  A random process with a Markov property is called a <strong>Markov process</strong> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/44f/1ba/f48/44f1baf48ceb669e0416489eea2bae35.png"></div><br>  <i>Markov property means that if we know the current state at a given point in time, then we do not need any additional information about the future, collected from the past.</i> <br><br>  Based on this definition, we can formulate the definition of ‚Äúhomogeneous Markov chains with discrete time‚Äù (hereinafter, for simplicity, we will call them ‚ÄúMarkov chains‚Äù).  <strong>A Markov chain</strong> is a Markov process with discrete time and a discrete state space.  So, a Markov chain is a discrete sequence of states, each of which is taken from a discrete space of states (finite or infinite), satisfying the Markov property. <br><br>  Mathematically, we can denote a Markov chain like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/114/3b7/fc3/1143b7fc371f3142534c2b886bf3e69c.png"></div><br>  where at each moment of time the process takes its values ‚Äã‚Äãfrom the discrete set E, such that <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/886/a22/d76/886a22d7671798102ee3d94fe9868b81.png"></div><br>  Then the Markov property implies that we have <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/edc/8bf/384/edc8bf38422705e72c9dd7d094b249db.png"></div><br>  Note again that this last formula reflects the fact that for chronology (where I am now and where I was before) the probability distribution of the next state (where I will be further) depends on the current state, but not on past states. <br><br><blockquote>  <strong>Note.</strong>  In this introductory post, we decided to talk only about simple homogeneous Markov chains with discrete time.  However, there are also non-uniform (time-dependent) Markov chains and / or chains with continuous time.  In this article we will not consider such variations of the model.  It is also worth noting that the definition of the Markov property given above is extremely simplified: the true mathematical definition uses the notion of filtering, which goes far beyond our introductory acquaintance with the model. </blockquote><br><h4>  Characterize the dynamics of randomness of a Markov chain </h4><br>  In the previous section, we became acquainted with the general structure corresponding to any Markov chain.  Let's see what we need to set a specific ‚Äúinstance‚Äù of such a random process. <br><br>  First, we note that the complete determination of the characteristics of a random process with a discrete time that does not satisfy the Markov property can be a difficult task: the probability distribution at a given time can depend on one or several moments in the past and / or future.  All of these possible temporal dependencies could potentially complicate the creation of a process definition. <br><br>  However, due to the Markov property, the dynamics of a Markov chain are fairly easy to determine.  And in fact.  we only need to define two aspects: the <strong>initial probability distribution</strong> (that is, the probability distribution at time n = 0), denoted <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/595/90e/140/59590e140cdb348c943d9dcab0ea011d.png"></div><br>  and <strong>the transition probability matrix</strong> (which gives us the probability that the state at time n + 1 is the next for another state at time n for any pair of states), denoted <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/011/aee/574/011aee5747fe7e42fa09bf044c421e26.png"></div><br>  If these two aspects are known, then the full (probabilistic) dynamics of the process are clearly defined.  Indeed, the probability of any outcome of the process can then be calculated cyclically. <br><br>  Example: let's say we want to know the probability that the first 3 states of the process will have values ‚Äã‚Äã(s0, s1, s2).  So we want to calculate the probability <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6c5/991/81b/6c599181b1fd3892391711f311878b72.png"></div><br>  Here we apply the total probability formula, which states that the probability of obtaining (s0, s1, s2) is equal to the probability of obtaining the first s0 multiplied by the probability of obtaining s1, taking into account the fact that earlier we received s0 multiplied by the probability of obtaining s2 taking into account that we got earlier in order of s0 and s1.  Mathematically, this can be written as <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a46/da3/64f/a46da364f28d5b69055700759db02663.png"></div><br>  And then there is a simplification, determined by the Markov assumption.  In fact, in the case of long chains, we obtain strongly conditional probabilities for the last states.  However, in the case of Markov chains, we can simplify this expression by using the fact that <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ab/7f6/568/0ab7f6568e28bb562ebb287252422d51.png"></div><br>  getting this way <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b6c/700/30a/b6c70030a8627d87c39dab96c147513a.png"></div><br>  Since they fully characterize the probabilistic dynamics of the process, many complex events can be calculated only on the basis of the initial probability distribution q0 and the transition probability matrix p.  It is also worth giving one more basic connection: the expression of the probability distribution at time n + 1, expressed relative to the probability distribution at time n <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ae/f31/a8a/5aef31a8ae120a25e5b43d6534dc20ff.png"></div><br><h3>  Markov chains in finite state spaces </h3><br><h4>  Representation in the form of matrices and graphs </h4><br>  Here we assume that in the set E there is a finite number of possible states N: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d57/788/f81/d57788f81d0a92aa1d94ef572bdf25e3.png"></div><br>  Then the initial probability distribution can be described as a <strong>vector-string</strong> q0 of size N, and the transition probabilities can be described as a matrix p of size N by N, such that <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/100/6f8/6ae/1006f86aeff6699711058bd890190917.png"></div><br>  The advantage of such a record is that if we denote the distribution of probabilities at step n by the row vector qn, such that its components are specified <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/006/19b/4ae/00619b4ae1601eacdb8fd7ce248f1738.png"></div><br>  then simple matrix links are saved. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a12/cdc/a7b/a12cdca7b75ef09ceaacef33a3667549.png"></div><br>  (here we will not consider the evidence, but it is very simple to reproduce it). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/23c/3c6/a10/23c3c6a102bd7aa079c36a75f60a5e42.png"></div><br>  <i>If we multiply to the right a row vector describing the distribution of probabilities at a given time, by a matrix of transition probabilities, we get the probability distribution at the next time.</i> <br><br>  So, as we see, the transition of the probability distribution from a given stage to the next is simply defined as the right multiplication of the probability vector row of the original step by the matrix p.  In addition, this implies that we have <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/35b/072/e0c/35b072e0c74397094e5bf6a9dab11417.png"></div><br>  The dynamics of randomness of a Markov chain in finite state space can be easily represented as a normalized oriented graph, such that each node of the graph is a state, and for each pair of states (ei, ej) there is an edge going from ei to ej, if p (ei, ej )&gt; 0.  Then the value of the edge is the same probability p (ei, ej). <br><br><h4>  Example: reader of our site </h4><br>  Let's illustrate it all with a simple example.  Consider the daily behavior of a fictional site visitor.  Every day he has 3 possible states: the reader does not visit the site that day (N), the reader visits the site but does not read the entire post (V) and the reader visits the site and reads the whole post (R).  So, we have the following state space: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/242/b1c/627/242b1c62745a4a13f2a24b8f919c6820.png"></div><br>  Suppose, on the first day, this reader has a 50% chance only to enter the site and a 50% chance to visit the site and read at least one article.  The vector describing the initial probability distribution (n = 0) then looks like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/362/a5a/045/362a5a0455c939f9d855d65605945c90.png"></div><br>  Also imagine that the following probabilities are observed: <br><br><ul><li>  when the reader does not visit one day, he has a 25% chance of not visiting him and the next day, the probability of 50% is only to visit him and 25% to visit and read the article </li><li>  when a reader visits the site one day, but does not read, it has a 50% chance to visit it again the next day and not read the article, and a 50% chance to visit and read </li><li>  when a reader visits and reads an article on one day, he has a 33% chance not to enter the next day <em>(I hope this post will not give such an effect!)</em> , a 33% chance only to visit the site and 34% to visit and read the article again </li></ul><br>  Then we have the following transition matrix: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cf5/7e6/ba5/cf57e6ba5303d4e13cbb736e6115306d.png"></div><br>  From the previous section, we know how to calculate for this reader the probability of each state on the next day (n = 1) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c64/a77/76c/c64a7776cfc56a5af1a0ccf495469ef7.png"></div><br>  The probabilistic dynamics of this Markov chain can be represented graphically as: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/832/797/9a1/8327979a1aa6edd9d462a0b40a4c072d.png"></div><br>  <i>Representation in the form of a graph of a Markov chain, modeling the behavior of our invented site visitor.</i> <br><br><h3>  Markov Chain Properties </h3><br>  In this section, we will discuss only some of the most basic properties or characteristics of Markov chains.  We will not go into mathematical details, but present a brief overview of interesting points that need to be explored in order to use Markov chains.  As we have seen, in the case of a finite space of states, the Markov chain can be represented as a graph.  In the following, we will use a graphical representation to explain some properties.  However, one should not forget that these properties are not necessarily limited to the case of a finite state space. <br><br><h4>  Degradability, periodicity, irrevocability and recoverability </h4><br>  In this subsection, let's start with a few classic ways of characterizing a state or a whole Markov chain. <br><br>  First, we mention that the Markov chain is <strong>indecomposable</strong> if it is possible to reach any state from any other state (it is not necessary that in one time step).  If the state space is finite and the chain can be represented as a graph, then we can say that the graph of an indecomposable Markov chain is strongly connected (graph theory). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cef/a39/05c/cefa3905cced27b4e27a4e9547fbe846.png"></div><br>  <i>Illustration of the property of indecomposability (incompatibility).</i>  <i>The chain on the left cannot be cut: from 3 or 4 we cannot get into 1 or 2. The chain on the right (one edge added) can be cut: each state can be reached from any other.</i> <br><br>  A state has a period k, if, when leaving it, any return to this state requires a number of stages of time, a multiple of k (k is the greatest common divisor of all possible lengths of return paths).  If k = 1, then they say that the state is aperiodic, and the whole Markov chain is <strong>aperiodic</strong> if all its states are aperiodic.  In the case of an irreducible Markov chain, it can also be mentioned that if one state is aperiodic, then all the others are also aperiodic. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cbe/65f/a07/cbe65fa07e7b816d385408824ba0ff39.png"></div><br>  <i>Illustration of the periodicity property.</i>  <i>The chain on the left is periodic with k = 2: when leaving any state, the number of steps in a multiple of 2 is always required to return to it. The chain on the right has a period of 3.</i> <br><br>  A state is <strong>irrevocable</strong> if, upon leaving the state, there is a nonzero probability that we will never return to it.  Conversely, a state is considered <strong>recurring</strong> if we know that after leaving the state we can return to it in the future with a probability of 1 (if it is not irretrievable). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6a0/1ad/7b0/6a01ad7b05e96f5a6606a8a48d127233.png"></div><br>  <i>Illustration of the property of return / irrevocability.</i>  <i>The chain on the left has the following properties: 1, 2 and 3 are non-returnable (when leaving these points we cannot be absolutely sure that we will return to them) and have period 3, and 4 and 5 are returnable (when leaving these points we are absolutely sure that one day we shall return to them) and have period 2. The chain on the right has one more edge, making the whole chain recurrent and aperiodic.</i> <br><br>  For the return state, we can calculate the average return time, which is the <strong>expected return time</strong> when leaving the state.  Note that even the probability of return is 1, this does not mean that the expected return time is of course.  Therefore, among all return states, we can distinguish between <strong>positive return states</strong> (with a finite expected return time) and <strong>zero return states</strong> (with an infinite expected return time). <br><br><h4>  Stationary distribution, limiting behavior and ergodicity </h4><br>  In this subsection, we consider the properties that characterize some aspects of the (random) dynamics described by the Markov chain. <br><br>  The probability distribution œÄ over the state space E is called a <strong>stationary distribution</strong> if it satisfies the expression <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/736/d70/e5d/736d70e5ddfc5aeb788d29ebfa79f9ec.png"></div><br>  Since we have <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b98/c4e/e6c/b98c4ee6c3b8032b074c7543db816c7e.png"></div><br>  Then the stationary distribution satisfies the expression <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2b8/df4/0a8/2b8df40a8a9a2ae90eb49a7c60dafb55.png"></div><br>  By definition, the stationary probability distribution does not change with time.  That is, if the initial distribution q is stationary, then it will be the same at all subsequent stages of time.  If the state space is finite, then p can be represented as a matrix, and œÄ - as a row vector, and then we get <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee0/565/9fc/ee05659fc4ed268ee3189342a76d9311.png"></div><br>  This again expresses the fact that the stationary probability distribution does not change with time (as we can see, multiplying the probability distribution by p on the right allows us to calculate the probability distribution at the next time step).  Note that an indecomposable Markov chain has a stationary probability distribution if and only if one of its states is positive recurrent. <br><br>  Another interesting property related to the stationary probability distribution is as follows.  If the chain is positive returnable (that is, there is a stationary distribution in it) and aperiodic, then whatever the initial probabilities are, the probability distribution of the chain converges as the time intervals tend to infinity: they say that the chain has a <strong>limit distribution</strong> , which is nothing else as a stationary distribution.  In general, it can be written as: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/95b/fb9/1c6/95bfb91c67d024e2df40b0e6dcdaf747.png"></div><br>  Once again, we emphasize the fact that we make no assumptions about the initial probability distribution: the probability distribution of a chain is reduced to a stationary distribution (an equilibrium distribution of a chain), regardless of the initial parameters. <br><br>  Finally, <strong>ergodicity</strong> is another interesting property related to the behavior of the Markov chain.  If a Markov chain is indecomposable, then it is also said that it is ‚Äúergodic‚Äù because it satisfies the following ergodic theorem.  Suppose we have a function f (.), Going from the state space E to the axis (this can be, for example, the price of being in each state).  We can determine the average value that moves this function along a given trajectory (temporary average).  For nth first terms, this is denoted as <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/af1/8c5/4a3/af18c54a3d7dc1e1ad4a4015ab7ad64c.png"></div><br>  We can also calculate the average value of the function f on the set E, weighted by the stationary distribution (spatial average), which is denoted <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/04a/352/c77/04a352c77b0687ef3cc89f3b7e0edf38.png"></div><br>  Then the ergodic theorem tells us that when the trajectory becomes infinitely long, the temporal average is equal to the spatial average (weighted by the stationary distribution).  The ergodicity property can be written as: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6cb/37d/c4d/6cb37dc4dcf0a3e53cc8e6baec8f4b1a.png"></div><br>  In other words, it means that in the previous limit, the behavior of the trajectory becomes insignificant and only the long-term stationary behavior is important in the calculation of the time average. <br><br><h4>  Let's return to the example with the reader of the site. </h4><br>  Consider again an example with a site reader.  In this simple example, it is obvious that a chain is indecomposable, aperiodic, and all its states are positively reflexive. <br><br>  To show what interesting results can be calculated using Markov chains, we consider the average return time to the state R (the state ‚Äúvisits the site and reads the article‚Äù).  In other words, we want to answer the following question: if our reader comes to the site one day and reads an article, how many days will we have to wait on average for what he will come back and read the article?  Let's try to get an intuitive understanding of how this value is calculated. <br><br>  First we denote <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2a2/57c/c74/2a257cc74db27e5ac89ffc1e06bd9ed9.png"></div><br>  So we want to calculate m (R, R).  Arguing about the first interval reached after exiting from R, we get <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4f9/e1a/a6e/4f9e1aa6e04e736fde182693398a4dca.png"></div><br>  However, this expression requires that we know m (N, R) and m (V, R) to calculate m (R, R).  These two values ‚Äã‚Äãcan be expressed in a similar way: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e23/7cb/f2d/e237cbf2d81597544f800d38b5a59e91.png"></div><br>  So, we got 3 equations with 3 unknowns and after solving them we get m (N, R) = 2.67, m (V, R) = 2.00 and m (R, R) = 2.54.  The value of the average return time to the state R is then 2.54.  That is, using linear algebra, we were able to calculate the average return time to the state R (as well as the average transition time from N to R and the average transition time from V to R). <br><br>  To finish with this example, let's see what the stationary distribution of the Markov chain will be.  To determine the stationary distribution, we need to solve the following equation of linear algebra: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bb3/73d/068/bb373d068a04d681c0501d8276731c0a.png"></div><br>  That is, we need to find the left eigenvector p associated with the eigenvector 1. Solving this problem, we obtain the following stationary distribution: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0c6/abe/19e/0c6abe19e37b67af5f380eb3e5c0beb9.jpg"></div><br>  <i>Stationary distribution in the example of the reader site.</i> <br><br>  It can also be noted that œÄ (R) = 1 / m (R, R), and if we reflect a bit, then this identity is quite logical (but we will not discuss this in detail). <br><br>  Since the chain is indecomposable and aperiodic, this means that in the long run the probability distribution will converge to a stationary distribution (for any initial parameters).  In other words, whatever the initial state of the site reader, if we wait a long time and randomly choose a day, we get the probability œÄ (N) that the reader will not enter the site that day, the probability œÄ (V) that the reader will enter, but will not read the article, and the probability œÄ¬Æ that the reader will enter and read the article.  To better understand the convergence property, let's take a look at the following graph showing the evolution of probability distributions starting from different starting points and (rapidly) converging to a stationary distribution: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/128/58e/a88/12858ea88a0e3bd05950b9d30096b776.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Visualization of the convergence of 3 probability distributions with different initial parameters (blue, orange and green) to the stationary distribution (red).</font></font></i> <br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Classic example: PageRank algorithm </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It's time to go back to PageRank! </font><font style="vertical-align: inherit;">But before proceeding further, it is worth mentioning that the interpretation of PageRank given in this article is not the only possible one, and the authors of the original article did not necessarily count on the use of Markov chains when developing the methodology. </font><font style="vertical-align: inherit;">However, our interpretation is good because it is very understandable.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Arbitrary web user </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PageRank tries to solve the following problem: how do we rank the existing set (we can assume that this set has already been filtered, for example, by some query) with the help of the links that already exist between the pages? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To solve this problem and be able to rank the pages, PageRank roughly performs the following process. We believe that an arbitrary Internet user is located on one of the pages at the initial moment of time. Then this user starts randomly moving by clicking on each page one of the links that lead to another page of the set in question (it is assumed that all links leading outside of these pages are prohibited). On any page, all valid links have the same probability of being clicked.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So we set the Markov chain: pages are possible states, transition probabilities are set by links from page to page (weighted in such a way that on each page all related pages have the same probability of choice), and the lack of memory properties are clearly defined by the user's behavior. If we also assume that the given chain is positively recurrent and aperiodic (small tricks are applied to meet these requirements), then in the long run the probability distribution of the ‚Äúcurrent page‚Äù converges to a stationary distribution. That is, whatever the initial page, after a long time each page has a probability (almost fixed) to become current if we choose a random moment of time.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PageRank is based on the following hypothesis: the most likely pages in the stationary distribution should also be the most important (we visit these pages often because they receive links from pages that are also frequently visited during the transition). </font><font style="vertical-align: inherit;">Then the stationary probability distribution determines the PageRank value for each state.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Artificial example </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To make this much clearer, let's consider an artificial example. </font><font style="vertical-align: inherit;">Suppose we have a tiny website with 7 pages, labeled from 1 to 7, and the links between these pages correspond to the next column.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84a/01a/072/84a01a0729fb1a772e89f2fa6c257a7d.gif"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For the sake of clarity, the probabilities of each transition are not shown in the animation shown above. </font><font style="vertical-align: inherit;">However, since it is understood that ‚Äúnavigation‚Äù should be exclusively random (this is called ‚Äúrandom walk‚Äù), the values ‚Äã‚Äãcan be easily reproduced from the following simple rule: for a node with K outgoing links (a page with K links to other pages), the probability of each outgoing link equals 1 / k. </font><font style="vertical-align: inherit;">That is, the transition probability matrix has the form:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b30/3ec/a66/b303eca66763a4187d027842214ff529.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">where 0.0 values ‚Äã‚Äãare replaced by ‚Äú.‚Äù for convenience. </font><font style="vertical-align: inherit;">Before performing further calculations, we can notice that this Markov chain is indecomposable and aperiodic, that is, in the long run the system converges to a stationary distribution. </font><font style="vertical-align: inherit;">As we saw, we can calculate this stationary distribution by solving the following left eigenvector problem</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e2f/8da/687/e2f8da6879f6f19fdc921803c8c7e371.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> By doing this, we get the following PageRank values ‚Äã‚Äã(stationary distribution values) for each page. </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0a8/b69/a5b/0a8b69a5b2916bca1f5fa45955af1b4b.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The PageRank values ‚Äã‚Äãcalculated for our artificial example of 7 pages. </font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Then the PageRank ranking of this tiny website looks like 1&gt; 7&gt; 4&gt; 2&gt; 5 = 6&gt; 3.</font></font><br><br><hr><br><h3>  findings </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The main conclusions of this article are: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> random processes are sets of random variables, often indexed by time (indices often denote discrete or continuous time) </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> for a random process, a Markov property means that for a given current, the probability of the future does not depend on the past (this property is also called ‚Äúno memory‚Äù) </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> discrete-time Markov chain are random processes with discrete-time indices that satisfy the Markov property </font></font></li><li>                 (  ,  ‚Ä¶) </li><li>     PageRank ( )    -,       ;          ( ,             ,  ,      ) </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In conclusion, we emphasize once again how powerful the Markov chains are in modeling problems associated with random dynamics. Due to their good properties, they are used in various fields, for example, in queuing theory (optimization of the performance of telecommunication networks, in which messages often have to compete for limited resources and are queued when all resources are already occupied), in statistics (well-known Monte Carlo methods). Carlo is based on the Markov chain for generating random variables based on Markov chains), in biology (modeling the evolution of biological populations), in computer science (hidden Markov models are important tools umentami in information theory, and speech recognition), as well as in other areas.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course, the enormous possibilities offered by Markov chains from the point of view of modeling and computation are much wider than those considered in this modest review. </font><font style="vertical-align: inherit;">Therefore, we hope that we have been able to arouse the reader‚Äôs interest in the further study of these tools, which occupy an important place in the arsenal of the scientist and data expert.</font></font></div><p>Source: <a href="https://habr.com/ru/post/455762/">https://habr.com/ru/post/455762/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455754/index.html">Tests of the drifting stratostat. Launch of Rogozin and LoRa into the stratosphere</a></li>
<li><a href="../455756/index.html">Is [favor] nd</a></li>
<li><a href="../455758/index.html">Growth Hacking in Retail Rocket: from hypothesis searching to testing methodology</a></li>
<li><a href="../45576/index.html">3D interface for information archive</a></li>
<li><a href="../455760/index.html">Magic SwiftUI or Function builders</a></li>
<li><a href="../455770/index.html">AERODISK: waiting vs. reality</a></li>
<li><a href="../455774/index.html">Aviation gas turbine engines</a></li>
<li><a href="../45578/index.html">Interview with the creator of ASP.NET</a></li>
<li><a href="../455784/index.html">Because of which dark gray is lighter than gray in CSS</a></li>
<li><a href="../455788/index.html">Walks on Peter's data centers and telecoms</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>