<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Trillion Little Shingles</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Image source: www.nikonsmallworld.com 


 Anti-plagiarism is a specialized search engine, which was already mentioned earlier . Any search engine, any...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Trillion Little Shingles</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/hc/qv/ma/hcqvmaxyzdevsbs7cs8lw_fpile.jpeg"></p><br><p>  <sub><em>Image source: <a href="https://www.nikonsmallworld.com/galleries/2016-photomicrography-competition/butterfly-proboscis">www.nikonsmallworld.com</a></em></sub> </p><br><p>  Anti-plagiarism is a specialized search engine, which was already mentioned <a href="https://habr.com/ru/company/antiplagiat/blog/413361/">earlier</a> .  Any search engine, anyway, to work quickly, you need your own index, which takes into account all the features of the search area.  In my first article on <a href="https://habr.com/ru/company/antiplagiat/">Habr√©,</a> I will talk about the current implementation of our search index, the history of its development and the reasons for choosing a particular solution.  Efficient algorithms on .NET are not a myth, but a hard and productive reality.  We will plunge into the world of hashing, bit-by-bit compression and multi-level priority caches.  What to do if you need a search faster than <b>O (1)</b> ? </p><br><p>  If someone else does not know where the shingles are in this picture, welcome ... </p><br><p><a name="habracut"></a></p><br><h1>  Shingles, index and why to look for them </h1><br><p>  Shingle - this is a piece of text, the size of a few words.  Shingles go overlapping one after another, hence the name (English, shingles - scales, tiles).  Their specific size is an open secret - 4 words.  Or 5?  Well, it depends.  However, even this value does little and depends on the composition of stop words, the algorithm for normalizing words and other details that are not significant in this article.  In the end, we calculate on the basis of this shingle 64-bit hash, which we will call shingle later on. </p><br><p>  The text of the document can form a set of shingles, the number of which is comparable to the number of words in the document: </p><br><p>  <em>text: string ‚Üí shingles: uint64 []</em> </p><br><p>  If several documents coincide with two documents, we will assume that the documents overlap.  The more shingles match, the more identical text in this pair of documents.  The index searches for documents with the most intersections with the document being checked. </p><br><p><img src="https://habrastorage.org/webt/ud/th/z_/udthz_wa_avl6zbaij-cydicgx8.jpeg"></p><br><p>  <sub><em>Image source: <a href="https://ru.wikipedia.org/wiki/%25D0%25A7%25D0%25B5%25D1%2588%25D1%2583%25D0%25B5%25D0%25BA%25D1%2580%25D1%258B%25D0%25BB%25D1%258B%25D0%25B5">Wikipedia</a></em></sub> </p><br><p>  The index of shingle allows you to perform two basic operations: </p><br><ol><li><p>  Index the shingles of documents with their identifiers: </p><br><p>  <i>index.Add (docId, shingles)</i> </p></li><li><p>  Search in yourself and produce a ranked list of identifiers of overlapping documents: </p><br><p>  <i>index.Search (shingles) ‚Üí (docId, score) []</i> </p></li></ol><br><p>  The ranking algorithm, I think, is worthy of a separate article, so we will not write about it here. </p><br><p>  The index of shingles is very different from well-known full-text counterparts, such as Sphinx, Elastic or larger ones: Google, Yandex, etc ... On the one hand, it does not require any NLP and other pleasures of life.  All text processing is taken out and does not affect the process, as well as the order of shingles in the text.  On the other hand, a search query is not a word or a phrase made of several words, but up to several hundred thousand <a href="https://ru.wikipedia.org/wiki/%25D0%25A5%25D0%25B5%25D1%2588%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5">hashes</a> , which have a meaning all in the aggregate, and not separately. </p><br><p>  Hypothetically, one can use the full-text index as a substitute for the index of shingles, but the differences are too great.  The easiest way to use any known key-value-storage, this will be mentioned below.  We saw our <s>bicycle</s> implementation, which is called ShingleIndex. </p><br><p>  Why do we bother?  But why? </p><br><ul><li>  <u>Volumes</u> : <br><ol><li>  A lot of documents.  Now we have about 650 million of them, and this year there will obviously be more of them; </li><li>  The number of unique shingles is growing by leaps and bounds and already reaches hundreds of billions.  We are waiting for a trillion. </li></ol></li><li>  <u>Speed</u> : <br><ol><li>  During the day, during the summer session, more than 300 thousand documents are checked by the <a href="https://www.antiplagiat.ru/">Antiplagiat system</a> .  This is a bit by the standards of popular search engines, but keeps in good shape; </li><li>  For a successful check of documents for uniqueness, the number of indexed documents must be orders of magnitude larger than those being checked.  The current version of our index on average can be filled at a speed of more than 4000 average documents per second. </li></ol></li></ul><br><p>  And this is all on the same machine!  Yes, we are able to <a href="https://en.wikipedia.org/wiki/Replication_(computing)">replicate</a> , we gradually approach dynamic <a href="https://en.wikipedia.org/wiki/Shard_(database_architecture)">sharding</a> on a cluster, but since 2005 and to this day, the index on one machine with careful care has been able to cope with all the difficulties listed above. </p><br><h1>  Strange experience </h1><br><p>  However, it is now we are so experienced.  Like it or not, but we also grew up and in the course of growth we tried different things, which are now fun to remember. </p><br><p><img src="https://habrastorage.org/webt/nx/l4/jx/nxl4jxkzhzumxh91qyds84byk70.jpeg"></p><br><p>  <sub><em>Image source: <a href="https://ru.wikipedia.org/wiki/%25D0%25A7%25D0%25B5%25D1%2588%25D1%2583%25D0%25B5%25D0%25BA%25D1%2580%25D1%258B%25D0%25BB%25D1%258B%25D0%25B5">Wikipedia</a></em></sub> </p><br><p>  First of all, an inexperienced reader would want to use a SQL database.  You are not the only ones who think so; the SQL implementation has served us for several years to implement very small collections.  Nevertheless, the focus was immediately on millions of documents, so I had to go further. </p><br><p>  Nobody likes bicycles, and LevelDB was not yet in the public domain, so in 2010 our eyes fell on BerkeleyDB.  Everything is cool - a persistent embedded key-value-base with the appropriate btree and hash access methods and a long history.  Everything about her was wonderful, but: </p><br><ul><li>  In the case of a hash-implementation, when it reaches 2GB, it just fell.  Yes, we were still working in 32-bit mode; </li><li>  The B + tree implementation worked stably, but with volumes over several gigabytes, the search speed started to drop significantly. </li></ul><br><p>  We'll have to admit that we have not found a way to adjust it for our task.  Maybe the problem is in .net-binding, which still had to finish.  The BDB implementation was eventually used as a replacement for SQL as an intermediate index before pouring into the main one. </p><br><p>  As time went.  In 2014, tried LMDB and LevelDB, but did not implement.  The guys from our Research Department in <a href="https://www.antiplagiat.ru/">Antiplagiat</a> as an index used RocksDB for their needs.  At first glance, it was a godsend.  But the slow replenishment and mediocre search speed even on small volumes reduced everything to nothing. </p><br><p>  All of the above we did, in parallel, developing our own custom index.  As a result, he became so good at solving our problems that we abandoned the previous ‚Äúgags‚Äù and focused on improving it, which we now use in production everywhere. </p><br><h1>  Index layers </h1><br><p>  In the end, what do we have now?  In fact, the index of shingles consists of several layers (arrays) with elements of constant length - from 0 to 128 bits - which depends not only on the layer and is not necessarily a multiple of eight. </p><br><p>  Each layer plays a certain role.  Some makes the search faster, some saves space, and some never used, but very necessary.  We will try to describe them in order of increasing their total search efficiency. </p><br><p><img src="https://habrastorage.org/webt/sd/y9/ze/sdy9zefei-lyrhgpafxq9viz9pc.jpeg"></p><br><p>  <sub><em>Image source: <a href="https://ru.wikipedia.org/wiki/%25D0%25A7%25D0%25B5%25D1%2588%25D1%2583%25D0%25B5%25D0%25BA%25D1%2580%25D1%258B%25D0%25BB%25D1%258B%25D0%25B5">Wikipedia</a></em></sub> </p><br><h4>  1. Index array </h4><br><p>  Without loss of generality, we will now assume that the document is mapped to a single shingle, </p><br><p>  <i>(docId ‚Üí shingle)</i> </p><br><p>  Swap the elements of the pair in places (inverted, because the index is actually ‚Äúinverted‚Äù!), </p><br><p>  <i>(shingle ‚Üí docId)</i> </p><br><p>  Sort by shingle values ‚Äã‚Äãand form a layer.  Because  Since the size of the shingle and the document identifier are constant, now anyone who understands <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25B2%25D0%25BE%25D0%25B8%25D1%2587%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BF%25D0%25BE%25D0%25B8%25D1%2581%25D0%25BA">binary search</a> can find a pair of <b>O (logn)</b> file reads.  What a lot, damn a lot.  But this is better than just <b>O (n)</b> . </p><br><p>  If the document has several shingles, then there will be several such pairs from the document.  If there are several documents with the same shingle, then this will also change little - there will be several pairs in succession with the same shingle.  In these both cases, the search will go for comparable time. </p><br><h4>  2. Group array </h4><br><p>  Carefully divide the index elements from the previous step into groups in any convenient way.  For example, so that they fit into the <s>sector cluster</s> allocation unit unit (read, 4096 bytes), taking into account the number of bits and other tricks and form an effective dictionary.  We will have a simple array of positions of such groups: </p><br><p>  <i>group_map (hash (shingle)) ‚Üí group_position.</i> </p><br><p>  When searching for a shingle, we will now first look for the position of the group in this dictionary, and then unload the group and search directly in the memory.  The whole operation requires two readings. </p><br><p>  The vocabulary of group positions takes several orders of magnitude less space than the index itself, it can often be simply unloaded into memory.  Thus, the readings will not be two, but one.  Total, <b>O (1)</b> . </p><br><h4>  3. Bloom filter </h4><br><p>  At interviews, candidates often solve puzzles, producing unique solutions with <b>O (n ^ 2)</b> or even <b>O (2 ^ n)</b> .  But we are not engaged in nonsense.  Is there an <b>O (0)</b> in the world, that is the question?  Let's try without much hope for the result ... </p><br><p>  Referring to the subject area.  If the student did a good job and wrote the work himself, or simply there is no text, but rubbish, then a significant part of his shingles will be unique and will not appear in the index.  Such a data structure as <a href="https://ru.wikipedia.org/wiki/%25D0%25A4%25D0%25B8%25D0%25BB%25D1%258C%25D1%2582%25D1%2580_%25D0%2591%25D0%25BB%25D1%2583%25D0%25BC%25D0%25B0">a Bloom filter is</a> well known in the world.  Before searching, check the shingle for it.  If there is no shingle in the index, then you can not search further, otherwise we go further. </p><br><p>  The Bloom filter itself is fairly simple, but using the vector of hash functions with our volumes is meaningless.  Just use one: <b>+1</b> reading from Bloom‚Äôs filter.  This gives <b>-1</b> or <b>-2</b> readings from subsequent stages, in case the shingle is unique, and there is no false positive response in the filter.  Watch your hands! </p><br><p>  The probability of Bloom‚Äôs filter error is given when building; the probability of an unknown shingle is determined by the student‚Äôs integrity.  Simple calculations can come to the following dependency: </p><br><ul><li>  If we trust the honesty of people (ie, in fact, the document is original), then the search speed will decrease; </li><li>  If the document is clearly styrenny, then the search speed will increase, but we will need a lot of memory. </li></ul><br><p>  With confidence in the students, we have the principle of ‚Äútrust, but verify,‚Äù and practice shows that there is a profit from Bloom‚Äôs filter. </p><br><p>  Given that this data structure is also smaller than the index itself and can be cached, at best it allows you to discard a shingle without any disk access at all. </p><br><h4>  4. Heavy tails </h4><br><p>  There are shingles that are found almost everywhere.  Their share of the total is scanty, but when building the index in the first step, groups of tens and hundreds of MB can be obtained in the second step.  Remember them separately and we will immediately throw away from the search query. </p><br><p>  When for the first time in 2011 this trivial step was used, the size of the index was halved, and the search itself was accelerated. </p><br><h4>  5. Other tails </h4><br><p> Even so, there may be a lot of documents on a shingle.  And that's fine.  Dozens, hundreds, thousands ... It becomes unprofitable to keep them inside the main index;  We will carry them out in a separate sequence with more efficient storage.  Statistics show that such a decision is more than justified.  Moreover, various bit-wrap packages allow reducing the number of disk accesses and decreasing the index volume. </p><br><p>  As a result, for ease of maintenance, we will print all these layers into one big file - chunk.  There are ten such layers in it.  But the part is not used when searching, the part is quite small and is always stored in memory, the part is actively cached as needed / possible. </p><br><p>  In combat, most often the search for a shingle comes down to one or two random file reads.  In the worst case, you have to do three.  All layers are effectively (sometimes broken) packed arrays of elements of constant length.  Such is the normalization.  The time for unpacking is insignificant in comparison with the price of the final volume during storage and the possibility of better caching. </p><br><p>  When building, the sizes of layers are mainly calculated in advance, are written sequentially, so this procedure is quite fast. </p><br><h1>  How did you go there, did not know where </h1><br><p></p><blockquote><code>     2010         ,                .    ,          .  ,      .</code> </blockquote> <br><p><img src="https://habrastorage.org/webt/2x/f7/-f/2xf7-fs8nt4rmfx7cvmeyyb_ftq.jpeg"></p><br><p>  <sub><em>Image source: <a href="https://ru.wikipedia.org/wiki/%25D0%25A7%25D0%25B5%25D1%2588%25D1%2583%25D0%25B5%25D0%25BA%25D1%2580%25D1%258B%25D0%25BB%25D1%258B%25D0%25B5">Wikipedia</a></em></sub> </p><br><p>  Initially, our index consisted of two parts - the constant described above and the temporary one, which was either SQL, or BDB, or its own update log.  Occasionally, for example, once a month (and sometimes a year), the temporary one was sorted, filtered and merged with the main one.  The result was a combined, and two old ones were deleted.  If the temporary could not fit into the RAM, then the procedure went through external sorting. </p><br><p>  This procedure was rather troublesome, started in a semi-manual mode and required the actual rewriting of the entire index file from scratch.  Rewriting hundreds of gigabytes for the sake of a couple of millions of documents - well, so-so pleasure, I tell you ... </p><br><p></p><div class="spoiler">  <b class="spoiler_title">Memories from the past ...</b> <div class="spoiler_text"><blockquote> <code>       SSD.        ,  31    SSD          wcf-       .  ,          . ,  .</code> </blockquote> </div></div><br><p>  So that the SSD is not particularly strained, and the index is updated more often, in 2012, involved a chain of several pieces, chunk'ov according to the following scheme: </p><br><p><img src="https://habrastorage.org/webt/v4/5s/xo/v45sxoctvil0bhwkf2pfp7vamrs.png"></p><br><p>  Here the index consists of a chain of similar chunks, except the very first one.  The first, addon, was an append-only magazine with an in-memory index.  Subsequent chunk'i increased in size (and age) until the very last (zero, main, root, ...). </p><br><p></p><div class="spoiler">  <b class="spoiler_title">Note cyclist ...</b> <div class="spoiler_text">  Sometimes you should not catch at writing code and not even think, but just google more carefully.  Up to the notation, the diagram is similar to this from the 1996 article <a href="https://www.cs.umb.edu/~poneil/lsmtree.pdf">‚ÄúThe log-structured merge-tree‚Äù</a> : <img src="https://habrastorage.org/webt/1z/r2/yh/1zr2yhxxboh0syuyfozcujnm5hm.png"></div></div><br><p>  When you add a document first formed in the addon.  At its overflow or by other criteria, a constant chunk was built on it.  The next few chunks, if necessary, merged into a new one, and the original ones were deleted.  Updating a document or deleting it was done in the same way. </p><br><p>  Criteria for merging, chain length, traversal algorithm, accounting for deleted items and updates, other parameters tyunilis.  The approach itself was used in several similar tasks and took shape as a separate internal <a href="https://ru.wikipedia.org/wiki/LSM-%25D0%25B4%25D0%25B5%25D1%2580%25D0%25B5%25D0%25B2%25D0%25BE">LSM</a> framework on pure .net.  Around the same time, LevelDB came out and became popular. </p><br><p></p><div class="spoiler">  <b class="spoiler_title">Little remark about LSM-tree</b> <div class="spoiler_text">  <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree">LSM-Tree is a</a> rather interesting algorithm with a good rationale.  But, IMHO, there was some blurring of the meaning of the term Tree.  The original <a href="https://www.cs.umb.edu/~poneil/lsmtree.pdf">article</a> dealt specifically with a chain of trees with the possibility of carrying branches.  In modern implementations, this is not always the case.  So, our framework was eventually named LsmChain, that is, the lsm chain of chunks. </div></div><br><p>  The <a href="https://ru.wikipedia.org/wiki/LSM-%25D0%25B4%25D0%25B5%25D1%2580%25D0%25B5%25D0%25B2%25D0%25BE">LSM</a> algorithm in our case has very suitable features: </p><br><ol><li>  instant insert / delete / update, </li><li>  reduced load on the SSD during the update, </li><li>  simplified chunks format, </li><li>  selective search only for old / new chunks, </li><li>  trivial backup, </li><li>  what else do you want. </li><li>  ... </li></ol><br><p>  In general, bicycles are sometimes useful for self-development. </p><br><h1>  Macro, micro, nano optimization </h1><br><p>  And finally, we will share technical advice, as we in <a href="https://www.antiplagiat.ru/">Antiplagiat</a> do such things on .Net (and not only on it). </p><br><p>  Note in advance that often everything depends very much on your particular hardware, data, or mode of use.  Having tightened up in one place, we will take off from the CPU cache, in the other we will rest on the bandwidth of the SATA interface, in the third we will start to hang in the GC.  And somewhere and at all in the inefficiency of the implementation of a specific system call. </p><br><p><img src="https://habrastorage.org/webt/gl/pq/sp/glpqspyystghvhhemtthxysivp0.jpeg"></p><br><p>  <sub><em>Image source: <a href="https://ru.wikipedia.org/wiki/%25D0%25A7%25D0%25B5%25D1%2588%25D1%2583%25D0%25B5%25D0%25BA%25D1%2580%25D1%258B%25D0%25BB%25D1%258B%25D0%25B5">Wikipedia</a></em></sub> </p><br><h1>  Work with file </h1><br><p>  The problem with access to the file, we are not unique.  There is a <s>terabyte exabyte</s> large file, the size of which is many times greater than the amount of RAM.  The task is to read a million scattered over it some small random values.  And do it quickly, efficiently and inexpensively.  We have to squeeze, benchmark and think a lot. </p><br><p>  Let's start with the simple.  To read the coveted byte you need: </p><br><ol><li>  Open file (new FileStream); </li><li>  Move to the desired position (Position or Seek, no difference); </li><li>  Read the desired byte array (Read); </li><li>  Close file (Dispose). </li></ol><br><p>  And this is bad, because long and dreary.  By trial, error and repeated attack on a rake, we identified the following sequence of actions: </p><br><ul><li><p>  <b>Single open, multiple read</b> </p><br><p>  If we make this sequence head-on, for each request to the disk, we will be bent very quickly.  Each of these points goes into a query to the OS kernel, which is expensive. </p><br><p>  Obviously, you should open the file once and read all the millions of our values ‚Äã‚Äãfrom it consistently, which we do </p></li><li><p>  <b>Nothing Extra</b> </p><br><p>  Getting the size of the file, the current position in it - also quite heavy operations.  Even if the file has not changed. </p><br><p>  You should avoid any queries like getting the size of the file or the current position in it. </p></li><li><p>  <b>FileStreamPool</b> </p><br><p>  Further.  Alas, FileStream is essentially single threaded.  If you want to read the file in parallel, you will have to create / close new file streams. </p><br><p>  Until they create something like aiosync, they will have to reinvent their own bikes. </p><br><p>  My advice is to create a pool of file streams per file.  This will avoid wasting time opening / closing the file.  And if you combine it with ThreadPool and take into account that SSD gives its mega-ops with strong multithreading ... Well, you understood me. </p></li><li><p>  <b>Allocation unit</b> </p><br><p>  Further.  Data storage devices (HDD, SSD, Optane) and file system operate on block-level files (cluster, sector, allocation unit).  They may not coincide, but now it is almost always 4096 bytes.  Reading one or two bytes on the border of two such blocks in the SSD is about one and a half times slower than inside the block itself. </p><br><p>  You should organize your data so that the elements to be read are within the boundaries of the <s>cluster sector</s> cluster. </p></li><li><p>  <b>No buffer.</b> </p><br><p>  Further.  FileStream uses a default buffer of 4096 bytes.  And there is bad news - it can not be turned off.  However, if you read data larger than the buffer size, the latter will be ignored. </p><br><p>  With random reading, you should set the buffer to 1 byte (less will not work) and then assume that it is not used. </p></li><li><p>  <b>Use buffer.</b> </p><br><p>  In addition to random readings, there are also sequential ones.  Here the buffer may already be useful if you do not want to read everything at once.  I advise you to start by referring to this <a href="https://www.microsoft.com/en-us/research/publication/sequential-file-programming-patterns-and-performance-with-net/">article</a> .  What size of the buffer to set, depends on whether the file is on the HDD or on the SSD.  In the first case 1MB will be optimal, in the second there will be enough standard 4kB.  If the size of the readable data area is comparable with these values, then it is better to read it at once, skipping the buffer, as in the case of random reading.  Large buffers will not bring profit in speed, but will start to beat on GC. </p><br><p>  When sequentially reading large pieces of a file, set the buffer to 1MB for HDD and 4kB for SSD.  Well, it depends. </p></li></ul><br><h1>  MMF vs FileStream </h1><br><p>  In 2011, the targeting to MemoryMappedFile came, since this mechanism was implemented starting from .Net Framework v4.0.  At first it was used when caching Bloom's filter, which already caused inconvenience in 32-bit mode due to the 4GB limit.  But when moving to the world of 64 bits, I wanted more.  The first tests were impressive.  Free caching, Freaky speed, user-friendly interface for reading structures.  But there were problems: </p><br><ul><li>  First, oddly enough, speed.  If the data has already been cached, then everything is ok.  But if not, reading one byte from a file was accompanied by ‚Äúlifting up‚Äù a lot more data than it would have been with regular reading. </li><li>  Secondly, oddly enough, memory.  When warming up, shared memory grows, workingset does not, which is logical.  But then the neighboring processes begin to behave not very well.  They can go into a swap, or involuntarily fall from OoM.  The volume occupied by the MMF in RAM, alas, is impossible to control.  And the profit from the cache in the case when the readable file itself is a couple of orders of magnitude more memory becomes meaningless. </li></ul><br><p>  The second problem could still be fought.  It disappears if the index is working in the docker or on a dedicated virtual machine.  But the problem of speed was fatal. </p><br><p>  As a result, the MMF was abandoned a little more than completely.  Caching in Antiplagiat began to be made explicitly, if possible keeping in memory the most frequently used layers at the given priorities and limits. </p><br><p><img src="https://habrastorage.org/webt/qr/em/sd/qremsdrzpkqcxqrbam_finb4dyw.jpeg"></p><br><p>  <sub><em>Image source: <a href="https://ru.wikipedia.org/wiki/%25D0%25A7%25D0%25B5%25D1%2588%25D1%2583%25D0%25B5%25D0%25BA%25D1%2580%25D1%258B%25D0%25BB%25D1%258B%25D0%25B5">Wikipedia</a></em></sub> </p><br><h1>  Bits / Bytes </h1><br><p>  Not bytes the world is one.  Sometimes you need to descend to the level of a bit. </p><br><p>  For example: imagine that you have a trillion partially ordered numbers, eager to save and frequent reading.  How to work with all this? </p><br><ul><li>  Simple BinaryWriter.Write?  - quickly, but slowly.  Size does matter.  Cold reading primarily depends on the file size. </li><li>  Another variation VarInt?  - quickly, but slowly.  Constancy matters.  The volume begins to depend on the data, which requires additional memory for positioning. </li><li>  Bitwise packaging?  - quickly, but slowly.  You have to control your hands even more carefully. </li></ul><br><p>  There is no perfect solution, but in a particular case, simple compression of the range from 32 bits to the required storage of tails saved 12% more (tens of GB!) Than VarInt (while preserving only the difference of neighboring ones, of course), and that one - at times from base case. </p><br><p>  Another example.  You have a link in the file to an array of numbers.  Link 64-bit file per terabyte.  Everything seems ok.  Sometimes there are many numbers in the array, sometimes few.  Often a little.  Often.  Then just take and store the entire array in the link itself.  Profit.  Pack gently just do not forget. </p><br><h1>  Struct, unsafe, batching, micro-opts </h1><br><p>  Well, and other micro-optimizations.  I will not write here about the banal "is it worth it to keep the Length of the array in a loop" or "which is faster, for or foreach". </p><br><p>  There are two simple rules, and we will follow them: 1. "benchmark everything", 2. "more benchmark". </p><br><ul><li><p>  <b>Struct</b> .  Used everywhere.  Do not ship GC.  And, as it is fashionable today, we also have our megastry ValueList. </p></li><li><p>  <b>Unsafe</b> .  Allows you to map (and spread) structures into an array of bytes when used.  Thus, we do not need separate means of serialization.  There are, however, questions about pinning and heap defragmentation, but it has not yet manifested itself.  Well, it depends. </p></li><li><p>  <b>Batching</b>  Work with many elements should be through packs / groups / blocks.  Read / write file, transfer between functions.  A separate question is the size of these packs.  Usually there is an optimum, and its size is often in the range from 1kB to 8MB (CPU cache size, cluster size, page size, size of something else).  Try to pump through the function IEnumerable &lt;byte&gt; or IEnumerable &lt;byte [1024]&gt; and feel the difference. </p></li><li><p>  <b>Pooling</b> .  Every time you write "new", a kitten dies somewhere.  Once a new byte [ <a href="">85000</a> ] - and the tractor drove a ton of geese.  If it is not possible to use stackalloc, then create a pool of any objects and reuse it again. </p></li><li><p>  <b>Inlining</b> .  How can creating two functions instead of one accelerate everything tenfold?  Simply.  The smaller the size of the body of the function (method), the greater the chance that it will be inline.  Unfortunately, in the dotnet world, it is not yet possible to do a partial inlining, so if you have a hot function that, in 99% of cases, goes after processing the first few lines, and the remaining hundred lines go to processing the remaining 1%, then safely break it into two (or three), carrying a heavy tail into a separate function. </p></li></ul><br><h1>  What else? </h1><br><ul><li><p>  <b>Span &lt;T&gt;</b> , <b>Memory &lt;T&gt;</b> - promising.  The code will be simpler and maybe a little faster.  We are waiting for the release. Net Core v3.0 and Std v2.1, to go to them, because  our kernel on .Net Std v2.0, which doesn‚Äôt normally support span. </p></li><li><p>  <b>Async / await</b> is still ambiguous.  The simplest benchmarks of random reading showed that CPU consumption is indeed falling, but the reading speed is also decreasing.  Must watch.  Inside the index is not yet used. </p></li></ul><br><h1>  Conclusion </h1><br><p>  I hope I have the remoteness to give you the pleasure of understanding the beauty of some decisions.  We really like our index.  It is effective, beautiful code, works fine.  A highly specialized solution in the core of the system, the critical place of its work is better than the general one.  Our version control system remembers assembler inserts in C ++ code.  Now four pluses - only pure C #, only .Net.  On it we write even the most complex search algorithms and do not regret it at all.  With the advent of .Net Core, the transition to Docker has made the path to a bright DevOps future easier and clearer.  Ahead - solving the problem of dynamic shardization and replication without reducing the efficiency and beauty of the solution. </p><br><p>  Thanks to everyone who read to the end.  About all ochepyatkah and other inconsistencies, please write comments.  I will be glad to any reasonable advice and refutations in the comments. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/445952/">https://habr.com/ru/post/445952/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445932/index.html">Client application security: practical tips for Front-end developer</a></li>
<li><a href="../445936/index.html">Electronics development. About microcontrollers on fingers</a></li>
<li><a href="../445940/index.html">AMA with Habr, v 7.0. Lemon, Donates and News</a></li>
<li><a href="../445946/index.html">MWC: instructions for use</a></li>
<li><a href="../445948/index.html">C ++ Inheritance: beginner, intermediate, advanced</a></li>
<li><a href="../445954/index.html">AI accelerator from HSE, MTS and Rostelecom</a></li>
<li><a href="../445958/index.html">SPDS GraphiCS - facade and roofing system</a></li>
<li><a href="../445962/index.html">Internship in IT: manager's point of view</a></li>
<li><a href="../445964/index.html">A student information security Olympiad will take place at MEPI: how to participate and what does it give</a></li>
<li><a href="../445966/index.html">Note frontend-architect # 1. You just can not take and use Redux</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>