<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Random Forest model for classification, implementation on c #</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Good day, reader. Random Forest today is one of the most popular and extremely effective methods for solving machine learning problems, such as classi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Random Forest model for classification, implementation on c #</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/e7a/aad/06b/e7aaad06bdbca9cc09f6400df3e63fd3.jpg" align="right" width="320">  Good day, reader.  <a href="http://en.wikipedia.org/wiki/Random_forest">Random Forest</a> today is one of the most popular and extremely effective methods for solving machine learning problems, such as classification and regression.  In terms of efficiency, it competes with support vector machines, neural networks and boosting, although of course it is not without its drawbacks.  In appearance, the learning algorithm is extremely simple (in comparison, let's say with the support vector machine learning algorithm, to whom there is little thrill in life, I strongly advise you to do this at your leisure).  We will try to understand the basic ideas of Random Forest (binary decision tree, bootstrap aggregation or bagging, the method of random subspaces and decorrelation) in an accessible form and understand why all this works together.  The model with respect to its competitors is still quite young: it all started with a <a href="http://www.cis.jhu.edu/publications/papers_in_database/GEMAN/shape.pdf">1997 article</a> in which the authors proposed a method for constructing a single decision tree using the method of random subspaces of signs when creating new tree nodes;  Then there was a series of articles, which ended with the <a href="http://stat-www.berkeley.edu/users/breiman/RandomForests/cc_home.htm">publication of a canonical version of the algorithm in 2001</a> , in which an ensemble of decision trees was built based on bootstrap aggregation, or bugging.  At the end, a simple, not at all smart, but extremely visual way of implementing this model in c # will be given, as well as a series of tests.  By the way, in the <a href="http://tainy.net/3598-zagadka-tancuyushhego-lesa.html">photo on the right</a> you can see a real random forest that grows here in the Kaliningrad region on the <a href="http://ru.wikipedia.org/wiki/%25D0%259A%25D1%2583%25D1%2580%25D1%2588%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D0%25BA%25D0%25BE%25D1%2581%25D0%25B0">Curonian Spit</a> . <br><br><a name="habracut"></a><br><br><h4>  Binary decision tree </h4><br>  It should start with a tree, as with the main structural element of the forest, but in the context of the model under study.  The presentation will be based on the assumption that the reader understands what a <a href="http://ru.wikipedia.org/wiki/%25D0%2594%25D0%25B5%25D1%2580%25D0%25B5%25D0%25B2%25D0%25BE_(%25D1%2581%25D1%2582%25D1%2580%25D1%2583%25D0%25BA%25D1%2582%25D1%2583%25D1%2580%25D0%25B0_%25D0%25B4%25D0%25B0%25D0%25BD%25D0%25BD%25D1%258B%25D1%2585)">tree is, as a data structure</a> .  The tree will be built <i>approximately</i> according to <a href="http://ru.wikipedia.org/wiki/CART_(%25D0%25B0%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC)">the CART</a> (Classification and Regression Tree) <a href="http://ru.wikipedia.org/wiki/CART_(%25D0%25B0%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC)">algorithm</a> , which builds binary decision trees.  By the way, here on Habr√© there is a good article on building such <a href="http://habrahabr.ru/post/171759/">trees based on entropy minimization</a> , in our version this will be a special case.  So imagine the feature space, let's say two-dimensional, that would be easier to visualize, in which a set of objects of two classes is given. <br><img src="https://habrastorage.org/getpro/habr/post_images/261/592/978/261592978406aff5388d5da09af69782.png"><br>  We introduce a series of notation.  Denote the set of features as follows: <br><img src="https://habrastorage.org/getpro/habr/post_images/29f/817/af4/29f817af4c1da4e21c903de9d54e4505.gif"><br>  For each feature, you can select a set of its values, based either on the training set, or using other a priori information about the task, we denote as follows a finite set of characteristic values: <br><img src="https://habrastorage.org/getpro/habr/post_images/3fa/6e6/0e1/3fa6e60e1dda5cf78b28397ed661a202.gif"><br>  It is also necessary to introduce the so-called measure of <i>heterogeneity of a</i> set relative to its labels.  Imagine that a subset of the training set consists of <b>5</b> red and <b>10</b> blue objects, then we can say that in this subset the probability of pulling out the red object will be <b>1/3</b> , and the blue <b>2/3</b> .  Let us denote the probability of the k-th class as follows in some subset of the training set: <br><img src="https://habrastorage.org/getpro/habr/post_images/b25/19d/1af/b2519d1af8448b6317200e7d0ebd94c7.gif"><br>  Thus, we have defined an empirical discrete probability distribution of labels in a subset of observations.  A measure of the <i>heterogeneity of</i> this subset is called a function of the following form, where <b>K (A)</b> is the total number of labels of the subset <b>A</b> : <br><img src="https://habrastorage.org/getpro/habr/post_images/4e8/eda/b08/4e8edab08834bedbd845060e99490365.gif"><br>  The measure of heterogeneity is set in such a way that the value of the function increases as far as possible with increasing set <i>variance</i> , reaching its maximum when the set consists of the same number of various labels, and the minimum if the set consists only of the labels of one class (once again I advise you to look <a href="http://habrahabr.ru/post/171759/">at entropy example with pictures</a> ). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Let's take a look at some examples of measures of heterogeneity (the vector <b>p</b> consists of <b>m</b> probabilities of the marks found in a certain subset <b>A of the</b> training set): <br><ul><li>  Most commonly occurring class: <img src="https://habrastorage.org/getpro/habr/post_images/011/706/9b3/0117069b35a016710a84b14e5801c036.gif"></li><li>  <a href="http://en.wikipedia.org/wiki/Gini_coefficient">Gini index</a> : <img src="https://habrastorage.org/getpro/habr/post_images/7f1/52f/c30/7f152fc308c9005d8f6c099874db0bd6.gif"></li><li>  <a href="http://en.wikipedia.org/wiki/Cross_entropy">Cross-entropy</a> : <img src="https://habrastorage.org/getpro/habr/post_images/f46/b81/0b3/f46b810b322faeecdae706e86ab61b0b.gif"></li></ul><br><br>  The algorithm for constructing a binary decision tree works according to the <a href="http://ru.wikipedia.org/wiki/%25D0%2596%25D0%25B0%25D0%25B4%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25B0%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC">greedy algorithm</a> scheme: at each iteration for the input subset of the training set, such a partition of space is constructed by a hyperplane (orthogonal to one of their coordinate axes) that would minimize the average measure of heterogeneity of the two subsets obtained.  This procedure is performed recursively for each resulting subset until the stopping criteria are reached.  Let us write this more formally; for the input set <b>A,</b> we find a pair of &lt; <b>attribute</b> , <b>characteristic value</b> &gt;, that the measure of heterogeneity will be minimal: <br><img src="https://habrastorage.org/getpro/habr/post_images/2eb/738/8c4/2eb7388c494153fb283a15a1f377a98b.gif"><br>  Where <img src="https://habrastorage.org/getpro/habr/post_images/b09/8cc/300/b098cc300aacc16f6a70bc234fed4ec5.gif">  - the probability vector obtained by the above procedure from a subset of the set <b>A</b> , consisting of those elements for which the condition <b>f &lt;x</b> is satisfied.  Also, do not forget that the average cost of a partition should not exceed the cost of the original set.  Let's now go back to the original picture and look at what is really happening, divide the original set of data as described above: <br><img src="https://habrastorage.org/getpro/habr/post_images/64f/ec3/120/64fec3120cdc76553df31a518c945243.png"><br>  As you can see, the set above the line <b>y = 2.840789</b> consists entirely of blue marks, so it has washed away to break further only the second set. <br><img src="https://habrastorage.org/getpro/habr/post_images/648/bb4/554/648bb4554ef1f3f54332165a2172717e.png"><br>  This time the line is <b>x = 2.976719</b> .  In general, it is interesting to indulge with this picture, here is the code for R: <br><div class="spoiler">  <b class="spoiler_title">Visualization code</b> <div class="spoiler_text"><pre><code class="diff hljs">rm(list=ls()) library(mvtnorm) labCount &lt;- 100 lab1 &lt;- rmvnorm(n=labCount, mean=c(1,1), sigma=diag(c(1, 1))) lab0 &lt;- rmvnorm(n=labCount, mean=c(2,2), sigma=diag(c(0.5, 2))) df &lt;- data.frame(x=append(lab1[, 1], lab0[, 1]), y=append(lab1[, 2], lab0[, 2]), lab=append(rep(1, labCount), rep(0, labCount))) plot(df$x, df$y, col=append(rep("red", labCount), rep("blue", labCount)), pch=19, xlab="Feature 1", ylab="Feature 2") giniIdx &lt;- function(data) { p1 &lt;- sum(data$lab == 1)/length(data$lab) p0 &lt;- sum(data$lab == 0)/length(data$lab) return(p0*(1 - p0) + p1*(1 - p1)) } p.norm &lt;- giniIdx getSeparator &lt;- function(data) { idx &lt;- NA idx.val &lt;- NA cost &lt;- p.norm(data) for(i in 1:(dim(data)[2] - 1)) { for(i.val in unique(data[, i])) { #print(paste("i = ", i, "; v = ", i.val, sep="")) cost.tmp &lt;- 0.5*(p.norm(data[data[, i] &lt; i.val, ]) + p.norm(data[data[, i] &gt;= i.val, ])) if(is.nan(cost.tmp)) { next } if(cost.tmp &lt; cost) { cost &lt;- cost.tmp idx &lt;- i idx.val &lt;- i.val } } } return(c(idx, idx.val)) } s1 &lt;- getSeparator(df) lines(c(-100, 100), c(s1[2], s1[2]), lty=2, lwd=2, type="l")</code> </pre> <br></div></div><br>  We list the possible stopping criteria: the maximum depth of the node has been reached;  the probability of the dominant class in the partition exceeds some threshold (I use 0.95);  the number of elements in the subset is less than a certain threshold.  As a result, we get a partition of the whole set into (hyper) rectangles, and each such subset of the learning set will be associated with one leaf of the tree, and all internal nodes are one of the partitioning conditions;  or in other words some predicate.  At the current node, the left descendant is associated with those elements of the set for which the predicate is true, and the right one, respectively, with the topic for which the predicate returns false.  It looks something like this: <br><img src="//habrastorage.org/files/e74/68c/875/e7468c875acd4abba1b557e456ed52ed.png"><br>  So we got a tree, how to make a decision on it?  It will not be difficult for us to determine to which of the subsets of the training set any input image belongs, in the opinion of a particular decision tree.  Then we just have to choose the dominant class in this subset and return it to the client, or return the probability distribution of labels in this subset. <br><br>  By the way at the expense of the <b>regression</b> task.  The described tree construction method easily changes from the classification problem to the regression problem.  For this, it is necessary to replace the measure of heterogeneity with a certain measure of forecasting error, for example, the standard deviation.  And when deciding, instead of the dominant class, the average value of the target variable is used. <br><br>  It seems with the trees all decided.  We will not dwell on the pros and cons of this method, <a href="http://ru.wikipedia.org/wiki/%25D0%2594%25D0%25B5%25D1%2580%25D0%25B5%25D0%25B2%25D0%25BE_%25D0%25BF%25D1%2580%25D0%25B8%25D0%25BD%25D1%258F%25D1%2582%25D0%25B8%25D1%258F_%25D1%2580%25D0%25B5%25D1%2588%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B9">there is a good list</a> in <a href="http://ru.wikipedia.org/wiki/%25D0%2594%25D0%25B5%25D1%2580%25D0%25B5%25D0%25B2%25D0%25BE_%25D0%25BF%25D1%2580%25D0%25B8%25D0%25BD%25D1%258F%25D1%2582%25D0%25B8%25D1%258F_%25D1%2580%25D0%25B5%25D1%2588%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B9">Wikipedia</a> .  But in the end I want to add an illustration from the book <a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a> about the difference of linear models and trees. <br><img src="//habrastorage.org/files/c6f/f70/de5/c6ff70de55794bbfb4855dea661d5d09.png"><br>  This illustration shows the difference between a linear model and a binary decision tree, as you can see in the case of linear separability, in general, the tree will show a less accurate result than a simple linear classifier. <br><br><h4>  Bootstrap aggregating or bagging </h4><br>  Let's move on to the next ideological component of random forest.  So the name <b>BAG</b> ging, formed from <b>B</b> ootstrap <b>AG</b> gregating.  In statistics, bootstrap is understood as a method for estimating the standard error of the statistics of a sample probability distribution, and a method for <a href="http://en.wikipedia.org/wiki/Sampling_(statistics)">sampling</a> samples from a data set based on the <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%259C%25D0%25BE%25D0%25BD%25D1%2582%25D0%25B5-%25D0%259A%25D0%25B0%25D1%2580%25D0%25BB%25D0%25BE">Monte Carlo method</a> . <br><br>  Bootstrap sampling is quite simple in its idea, and is used when we are unable to obtain a large number of samples from the actual distribution, and this is almost always the case.  Suppose we want to get <b>m</b> sets of observations of size <b>n</b> , but we have only one set of <b>n</b> observations at our disposal.  Then we generate <b>m</b> sets by <a href="http://en.wikipedia.org/wiki/Uniform_distribution_(discrete)">equiprobable</a> selection of <b>n</b> elements from the original set with the return of the selected element ( <a href="http://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D0%25B7%25D0%25BC%25D0%25B5%25D1%2589%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">sampling with repetition or return</a> ).  For large values ‚Äã‚Äãof <b>n</b> , the number of unique elements obtained by bootstrap sampling sets will be <b>(1 - 1 / e) ‚âà 63.2%</b> of the total number of unique observations of the original set.  Denote by <b>D <sup>i</sup></b> - the <b>i</b> -th set obtained by bootstrapping by sampling, we estimate on it some parameter <b>a <sup>i</sup></b> , and repeat this procedure <b>m</b> times.  The standard error of the parameter estimation bootstrap is written as follows: <br><img src="//habrastorage.org/files/f02/be4/dcd/f02be4dcda024047846c8b511aa3e802.gif"><br>  So, the <i>statistical bootstrap</i> allows us to estimate the error in estimating a certain distribution parameter.  But this is a distraction from the topic, we are also interested in the bootstrap sampling method. <br><br>  Now consider a set of <b>m</b> independent randomly selected elements <b>x</b> from one probability distribution, with some expectation and variance <b>œÉ <sup>2</sup></b> .  Then the sample average will be equal to: <br><img src="//habrastorage.org/files/c89/903/d2b/c89903d2b1d6474bb81e474d65f0dcf6.gif"><br>  The sample mean is not a distribution parameter, unlike the expectation and variance, but a function of random variables, i.e.  is also a random variable, from a certain probability distribution of sample means.  And it, in turn, has the variance parameter, which is expressed as follows: <br><img src="//habrastorage.org/files/36c/f82/92a/36cf8292a30c4733995d4f4d5fdf1b0b.gif"><br>  It turns out that <i>averaging the set of values ‚Äã‚Äãof a random variable reduces variability</i> .  This is the basis for the idea of ‚Äã‚Äãaggregating bootstrap samples.  Generate <b>m</b> bootstrap samples of size <b>n</b> from the training set <b>D</b> (also of size <b>n</b> ): <br><img src="//habrastorage.org/files/15b/61a/005/15b61a0057bd49c482dd6510d1c7e7c8.gif"><br>  At each bootstrap sample, we train model <b>f</b> and introduce the following function, this approach is called bootstrap aggregating or bagging: <br><img src="//habrastorage.org/files/159/839/ccb/159839ccb63c46eda5a1b3f88b499491.gif"><br>  Bagging can be illustrated by the <a href="http://en.wikipedia.org/wiki/Bootstrap_aggregating">following wikipedia chart</a> , where the bag model is represented by a red line and is the averaging of many other models. <br><img src="//habrastorage.org/files/c7b/fe6/1c1/c7bfe61c116e430bb60c92a6cbd3027b.png"><br><br><h4>  Decorrelation </h4><br>  I think it is already clear how to get a simple forest: we will generate some amount of bootstrap samples and train a decision tree on each of them.  But there is a small problem, almost all trees will be more or less the same structure.  Let's do an experiment, take a <a href="https://archive.ics.uci.edu/ml/datasets/Breast%2BCancer%2BWisconsin%2B(Diagnostic)">set with two classes and 32 features</a> , build 1000 decision trees on bootstrap samples, and look at the variability of the predicate of the root node. <br><img src="//habrastorage.org/files/341/f33/4c3/341f334c31284c5ebf00a85f0e3b2104.png"><br>  We see that of 1000 trees, the 22nd feature (obviously, the value of the feature is the same) occurs in 526 trees, and almost all the child nodes are the same.  In other words, the trees are <i>correlated</i> relative to each other.  It turns out that it makes no sense to build 1000 trees, if only a few, but most often one or two is enough.  And now let's try to use in the construction of a tree, when dividing each node, only some small random set of features from the set of all features, say 7 random ones from 32. <br><img src="//habrastorage.org/files/fbc/f49/5c9/fbcf495c9b614ea3ad8ed30b2ab6c3ba.png"><br>  As you can see, the distribution has significantly changed in the direction of a greater variety of trees (by the way, not only in the root node, but also in the subsidiaries), which was the goal of such a trick.  Now 22 signs are found only in 158 cases.  The choice of " <i>7 random of 32</i> signs" is justified by empirical observation (I never found the author of this observation), and in classification problems this is usually the square root of the total number of signs.  In other words, trees have become less <i>correlated</i> , and the process is called <i>decorrelation</i> . <br><br>  Such a method is generally called the <a href="">Random subspace method</a> and is used not only for decision trees, but also for other models, such as neural networks. <br><br>  In general, something like this would look like a regular flat forest, and decorrelated. <br><img src="//habrastorage.org/files/157/18b/522/15718b52265a44009c6a1c268fa648aa.jpg"><br><br><h4>  Code </h4><br>  Let's turn to implementation.  Once again I want to remind you that the example I have cited is not a quick implementation of random forest, but is of an educational nature only, designed to help understand the basic ideas of the model.  For example, <a href="http://semanticsearchart.com/downloads/RF.txt">here you will find an example of a suitable and quick implementation</a> , but unfortunately less understandable. <br><br>  Comments I will insert where necessary right in the code, so as not to break the classes into pieces. <br><div class="spoiler">  <b class="spoiler_title">Plain tree</b> <div class="spoiler_text"><pre> <code class="cs hljs"><span class="hljs-comment"><span class="hljs-comment">//   ,            public class TreeNode&lt;T&gt; { public TreeNode() { Childs = new LinkedList&lt;TreeNode&lt;T&gt;&gt;(); } public TreeNode(T data) { Data = data; Childs = new LinkedList&lt;TreeNode&lt;T&gt;&gt;(); } public TreeNode&lt;T&gt; Parent { get; set; } public LinkedList&lt;TreeNode&lt;T&gt;&gt; Childs { get; set; } public T Data { get; set; } public virtual bool AddChild(T data) { TreeNode&lt;T&gt; node = new TreeNode&lt;T&gt;() {Data = data}; node.Parent = this; Childs.AddLast(node); return true; } public virtual bool AddChild(TreeNode&lt;T&gt; node) { node.Parent = this; Childs.AddLast(node); return true; } public bool IsLeaf { get { return Childs.Count == 0; } } public int Depth { get { int d = 0; TreeNode&lt;T&gt; node = this; while (node.Parent != null) { d++; node = node.Parent; } return d; } } }</span></span></code> </pre><br></div></div><br><br>  The unit of observation in my case is represented by the following class. <br><div class="spoiler">  <b class="spoiler_title">Observation</b> <div class="spoiler_text"><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">DataItem</span></span>&lt;<span class="hljs-title"><span class="hljs-title">T</span></span>&gt; { <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> T[] _input = <span class="hljs-literal"><span class="hljs-literal">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> T[] _output = <span class="hljs-literal"><span class="hljs-literal">null</span></span>; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">DataItem</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>)</span></span> { } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">DataItem</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">T[] input, T[] output</span></span></span><span class="hljs-function">)</span></span> { _input = input; _output = output; } <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> T[] Input { <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> _input; } <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> { _input = <span class="hljs-keyword"><span class="hljs-keyword">value</span></span>; } } <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> T[] Output { <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> _output; } <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> { _output = <span class="hljs-keyword"><span class="hljs-keyword">value</span></span>; } } }</code> </pre><br></div></div><br><br>  For each node of the tree, it is necessary to store information about the predicate, which is used to make a classification decision. <br><div class="spoiler">  <b class="spoiler_title">Tree node data</b> <div class="spoiler_text"><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">ClassificationTreeNodeData</span></span> { <span class="hljs-comment"><span class="hljs-comment">//   &lt;id ,  &gt;,         //    p      internal IDictionary&lt;double, double&gt; Probabilities { get; set; } //   ,   ,         //         internal double Cost { get; set; } //       ,   , //    ,            -) internal Predicate&lt;double[]&gt; Predicate { get; set; } //     ,       internal IList&lt;DataItem&lt;double&gt;&gt; DataSet { get; set; } //  ,     internal string Name { get; set; } //        internal int FeatureIndex { get; set; } //       internal double FeatureValue { get; set; } //            ,    [OnSerializing] private void OnSerializing(StreamingContext context) { Predicate = null; } [OnDeserialized] [OnSerialized] private void OnDeserialized(StreamingContext context) { Predicate = v =&gt; v[FeatureIndex] &lt; FeatureValue; } }</span></span></code> </pre><br></div></div><br><br>  Consider a binary decision tree class. <br><div class="spoiler">  <b class="spoiler_title">Binary deciding tree</b> <div class="spoiler_text"><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">ClassificationBinaryTree</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> TreeNode&lt;ClassificationTreeNodeData&gt; _rootNode = <span class="hljs-literal"><span class="hljs-literal">null</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> INorm&lt;<span class="hljs-keyword"><span class="hljs-keyword">double</span></span>&gt; _norm = <span class="hljs-literal"><span class="hljs-literal">null</span></span>; <span class="hljs-comment"><span class="hljs-comment">//    private int _minLeafDataCount = 1; //         private int[] _trainingFeaturesSubset = null; //         private int _randomSubsetSize = 0; //   ,      private Random _random = null; private double _maxProbability = 0; //   ,        private int _maxDepth = Int32.MaxValue; //    private bool _showLog = false; private int _featuresCount = 0; public ClassificationBinaryTree(INorm&lt;double&gt; norm, int minLeafDataCount, int[] trainingFeaturesSubset = null, int randomSubsetSize = 0, double maxProbability = 0.95, int maxDepth = Int32.MaxValue, bool showLog = false) { _norm = norm; _minLeafDataCount = minLeafDataCount; _trainingFeaturesSubset = trainingFeaturesSubset; _randomSubsetSize = randomSubsetSize; _maxProbability = maxProbability; _maxDepth = maxDepth; _showLog = showLog; } public TreeNode&lt;ClassificationTreeNodeData&gt; RootNode { get { return _rootNode; } } //   public void Train(IList&lt;DataItem&lt;double&gt;&gt; data) { _featuresCount = data.First().Input.Length; if (_randomSubsetSize &gt; 0) { _random = new Random(Helper.GetSeed()); } IDictionary&lt;double, double&gt; rootProbs = ComputeLabelsProbabilities(data); _rootNode = new TreeNode&lt;ClassificationTreeNodeData&gt;(new ClassificationTreeNodeData() { DataSet = data, Probabilities = rootProbs, Cost = _norm.Calculate(rootProbs.Select(x =&gt; x.Value).ToArray()) }); //    Queue&lt;TreeNode&lt;ClassificationTreeNodeData&gt;&gt; queue = new Queue&lt;TreeNode&lt;ClassificationTreeNodeData&gt;&gt;(); queue.Enqueue(_rootNode); while (queue.Count &gt; 0) { if (_showLog) { Logger.Instance.Log("Tree training: queue size is " + queue.Count); } TreeNode&lt;ClassificationTreeNodeData&gt; node = queue.Dequeue(); int sourceCount = node.Data.DataSet.Count; //   TrainNode(node, node.Data.DataSet, _trainingFeaturesSubset, _randomSubsetSize); if (_showLog &amp;&amp; node.Childs.Count() &gt; 0) { Logger.Instance.Log("Tree training: source " + sourceCount + " is splitted into " + node.Childs.First().Data.DataSet.Count + " and " + node.Childs.Last().Data.DataSet.Count); } //       foreach (TreeNode&lt;ClassificationTreeNodeData&gt; child in node.Childs) { if (child.Data.Probabilities.Count == 1 || child.Data.DataSet.Count &lt;= _minLeafDataCount || child.Data.Probabilities.First().Value &gt; _maxProbability || child.Depth &gt;= _maxDepth) { child.Data.DataSet = null; continue; } queue.Enqueue(child); } } } //   private void TrainNode(TreeNode&lt;ClassificationTreeNodeData&gt; node, IList&lt;DataItem&lt;double&gt;&gt; data, int[] featuresSubset, int randomSubsetSize) { // argmin  double minCost = node.Data.Cost; int idx = -1; double threshold = 0; IDictionary&lt;double, double&gt; minLeftProbs = null; IDictionary&lt;double, double&gt; minRightProbs = null; IList&lt;DataItem&lt;double&gt;&gt; minLeft = null; IList&lt;DataItem&lt;double&gt;&gt; minRight = null; double minLeftCost = 0; double minRightCost = 0; //     ,   if (randomSubsetSize &gt; 0) { featuresSubset = new int[randomSubsetSize]; IList&lt;int&gt; candidates = new List&lt;int&gt;(); for (int i = 0; i &lt; _featuresCount; i++) { candidates.Add(i); } for (int i = 0; i &lt; randomSubsetSize; i++) { int idxRandom = _random.Next(0, candidates.Count); featuresSubset[i] = candidates[idxRandom]; candidates.RemoveAt(idxRandom); } } else if (featuresSubset == null) { featuresSubset = new int[data.First().Input.Length]; for (int i = 0; i &lt; data.First().Input.Length; i++) { featuresSubset[i] = i; } } //     foreach (int i in featuresSubset) { IList&lt;double&gt; domain = data.Select(x =&gt; x.Input[i]).Distinct().ToList(); //        foreach (double t in domain) { IList&lt;DataItem&lt;double&gt;&gt; left = new List&lt;DataItem&lt;double&gt;&gt;(); //      IList&lt;DataItem&lt;double&gt;&gt; right = new List&lt;DataItem&lt;double&gt;&gt;(); //    IDictionary&lt;double, double&gt; leftProbs = new Dictionary&lt;double, double&gt;(); //      IDictionary&lt;double, double&gt; rightProbs = new Dictionary&lt;double, double&gt;(); foreach (DataItem&lt;double&gt; di in data) { if (di.Input[i] &lt; t) { left.Add(di); if (!leftProbs.ContainsKey(di.Output[0])) { leftProbs.Add(di.Output[0], 0); } leftProbs[di.Output[0]]++; } else { right.Add(di); if (!rightProbs.ContainsKey(di.Output[0])) { rightProbs.Add(di.Output[0], 0); } rightProbs[di.Output[0]]++; } } if (right.Count == 0 || left.Count == 0) { continue; } //   leftProbs = leftProbs.ToDictionary(x =&gt; x.Key, x =&gt; x.Value/left.Count); rightProbs = rightProbs.ToDictionary(x =&gt; x.Key, x =&gt; x.Value/right.Count); double leftCost = _norm.Calculate(leftProbs.Select(x =&gt; x.Value).ToArray()); //    double rightCost = _norm.Calculate(rightProbs.Select(x =&gt; x.Value).ToArray()); double avgCost = (leftCost + rightCost)/2; //    if (avgCost &lt; minCost) { minCost = avgCost; idx = i; threshold = t; minLeftProbs = leftProbs; minRightProbs = rightProbs; minLeft = left; minRight = right; minLeftCost = leftCost; minRightCost = rightCost; } } } //         node.Data.DataSet = null; if (idx != -1) { //node should be splitted node.Data.Predicate = v =&gt; v[idx] &lt; threshold; //        node.Data.Name = "x[" + idx + "] &lt; " + threshold; node.Data.Probabilities = null; node.Data.FeatureIndex = idx; node.Data.FeatureValue = threshold; node.AddChild(new ClassificationTreeNodeData() { Probabilities = minLeftProbs.OrderByDescending(x =&gt; x.Value).ToDictionary(x =&gt; x.Key, x =&gt; x.Value), DataSet = minLeft, Cost = minLeftCost }); node.AddChild(new ClassificationTreeNodeData() { Probabilities = minRightProbs.OrderByDescending(x =&gt; x.Value).ToDictionary(x =&gt; x.Key, x =&gt; x.Value), DataSet = minRight, Cost = minRightCost }); } } //     ,     private IDictionary&lt;double, double&gt; ComputeLabelsProbabilities(IList&lt;DataItem&lt;double&gt;&gt; data) { IDictionary&lt;double, double&gt; p = new Dictionary&lt;double, double&gt;(); double denominator = data.Count; foreach (double label in data.Select(x =&gt; x.Output[0]).Distinct()) { p.Add(label, data.Where(x =&gt; x.Output[0] == label).Count() / denominator); } return p; } //    public IDictionary&lt;double, double&gt; Classify(double[] v) { TreeNode&lt;ClassificationTreeNodeData&gt; node = _rootNode; while (!node.IsLeaf) { node = node.Data.Predicate(v) ? node.Childs.First() : node.Childs.Last(); } return node.Data.Probabilities; } //     GraphVis http://www.graphviz.org/ public void WriteDotFile(StreamWriter sw, bool separateTerminalNode = false) { sw.WriteLine("digraph G{"); sw.WriteLine("graph [ordering=\"out\"];"); Queue&lt;TreeNode&lt;ClassificationTreeNodeData&gt;&gt; q = new Queue&lt;TreeNode&lt;ClassificationTreeNodeData&gt;&gt;(); q.Enqueue(_rootNode); int terminalCount = 0; ISet&lt;string&gt; styles = new HashSet&lt;string&gt;(); while (q.Count &gt; 0) { TreeNode&lt;ClassificationTreeNodeData&gt; node = q.Dequeue(); foreach (TreeNode&lt;ClassificationTreeNodeData&gt; child in node.Childs) { string childName = child.Data.Name; if (String.IsNullOrEmpty(childName)) { if (separateTerminalNode) { childName = "TNode #" + terminalCount + "; Class: " + child.Data.Probabilities.First().Key; } else { childName = "Class: " + child.Data.Probabilities.First().Key; } styles.Add("\"" + childName + "\" [" + "color=red, style=filled" + "];"); terminalCount++; } sw.WriteLine("\"" + node.Data.Name + "\" -&gt; " + "\"" + childName + "\";"); q.Enqueue(child); } } foreach (string style in styles) { sw.WriteLine(style); } sw.WriteLine("}"); } }</span></span></code> </pre><br></div></div><br><br>  Let us dwell on the <a href="http://en.wikipedia.org/wiki/Norm_(mathematics)">norm</a> used in the class of the decision tree. <br><div class="spoiler">  <b class="spoiler_title">Interface norm</b> <div class="spoiler_text"><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">interface</span></span> <span class="hljs-title"><span class="hljs-title">INorm</span></span>&lt;<span class="hljs-title"><span class="hljs-title">T</span></span>&gt; { <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">double</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Calculate</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">T[] v</span></span></span><span class="hljs-function">)</span></span>; }</code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Gini Index</b> <div class="spoiler_text"><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">internal</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">GiniIndex</span></span> : <span class="hljs-title"><span class="hljs-title">INorm</span></span>&lt;<span class="hljs-title"><span class="hljs-title">double</span></span>&gt; { <span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">region</span></span></span><span class="hljs-meta"> INorm&lt;double&gt; Members public double Calculate(double[] v) { return v.Sum(p =&gt; p*(1 - p)); } #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">endregion</span></span></span><span class="hljs-meta"> }</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Cross entropy</b> <div class="spoiler_text"><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">internal</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">MetricsBasedNorm</span></span>&lt;<span class="hljs-title"><span class="hljs-title">T</span></span>&gt; : <span class="hljs-title"><span class="hljs-title">INorm</span></span>&lt;<span class="hljs-title"><span class="hljs-title">T</span></span>&gt; { <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> IMetrics&lt;T&gt; _m = <span class="hljs-literal"><span class="hljs-literal">null</span></span>; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">internal</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">MetricsBasedNorm</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">IMetrics&lt;T&gt; m</span></span></span><span class="hljs-function">)</span></span> { _m = m; } <span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">region</span></span></span><span class="hljs-meta"> INorm&lt;T&gt; Members public double Calculate(T[] v) { return _m.Calculate(v, v); } #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">endregion</span></span></span><span class="hljs-meta"> } public interface IMetrics&lt;T&gt; { /// &lt;summary&gt; /// Calculate value of metrics /// &lt;/summary&gt; double Calculate(T[] v1, T[] v2); /// &lt;summary&gt; /// Get centroid/clusteroid of data /// &lt;/summary&gt; T[] GetCentroid(IList&lt;T[]&gt; data); /// &lt;summary&gt; /// Calculate value of partial derivative by v2[v2Index] /// &lt;/summary&gt; T CalculatePartialDerivaitveByV2Index(T[] v1, T[] v2, int v2Index); } internal class CrossEntropy : MetricsBase&lt;double&gt; { internal CrossEntropy() { } /// &lt;summary&gt; /// \sum_i v1_i * ln(v2_i) /// &lt;/summary&gt; public override double Calculate(double[] v1, double[] v2) { </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">if</span></span></span><span class="hljs-meta"> (v1.Length != v2.Length) { throw new ArgumentException("Length of v1 and v2 should be equal"); } </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">if</span></span></span><span class="hljs-meta"> (v1.Length == 0 || v2.Length == 0) { throw new ArgumentException("Vector dimension can't be 0"); } double d = 0; for (int i = 0; i &lt; v1.Length; i++) { d += v1[i]*Math.Log(v2[i] + Double.Epsilon); } return -d; } public override double CalculatePartialDerivaitveByV2Index(double[] v1, double[] v2, int v2Index) { return v2[v2Index] - v1[v2Index]; } }</span></span></code> </pre><br></div></div><br><br>  Well, it remains to consider only the class random forest. <br><div class="spoiler">  <b class="spoiler_title">Random forest</b> <div class="spoiler_text"><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">ClassificationRandomForest</span></span> { <span class="hljs-comment"><span class="hljs-comment">//     ,   private INorm&lt;double&gt; _norm = null; private int _minLeafDataCount = 1; private int[] _trainingFeaturesSubset = null; private int _randomSubsetSize = 0; //zero if all features needed private double _maxProbability = 0; private int _maxDepth = Int32.MaxValue; private bool _showLog = false; private int _forestSize = 0; //   private ConcurrentBag&lt;ClassificationBinaryTree&gt; _trees = null; public ClassificationRandomForest(INorm&lt;double&gt; norm, int forestSize, int minLeafDataCount, int[] trainingFeaturesSubset = null, int randomSubsetSize = 0, double maxProbability = 0.95, int maxDepth = Int32.MaxValue, bool showLog = false) { _norm = norm; _minLeafDataCount = minLeafDataCount; _trainingFeaturesSubset = trainingFeaturesSubset; _randomSubsetSize = randomSubsetSize; _maxProbability = maxProbability; _maxDepth = maxDepth; _forestSize = forestSize; _showLog = showLog; } public void Train(IList&lt;DataItem&lt;double&gt;&gt; data) { if (_showLog) { Logger.Instance.Log("Training is started"); } //     ,       _trees = new ConcurrentBag&lt;ClassificationBinaryTree&gt;(); Parallel.For(0, _forestSize, i =&gt; { ClassificationBinaryTree ct = new ClassificationBinaryTree( _norm, _minLeafDataCount, _trainingFeaturesSubset, _randomSubsetSize, _maxProbability, _maxDepth, false ); ct.Train(BasicStatFunctions.Sample(data, data.Count, true)); _trees.Add(ct); if (_showLog) { Logger.Instance.Log("Training of tree #" + _trees.Count + " is completed!"); } }); } // ,      bagging public IDictionary&lt;double, double&gt; Classify(double[] v) { IDictionary&lt;double, double&gt; p = new Dictionary&lt;double, double&gt;(); foreach (ClassificationBinaryTree ct in _trees) { IDictionary&lt;double, double&gt; tr = ct.Classify(v); double winClass = tr.First().Key; if (!p.ContainsKey(winClass)) { p.Add(winClass, 0); } p[winClass]++; } double denominator = p.Sum(x =&gt; x.Value); return p.ToDictionary(x =&gt; x.Key, x =&gt; x.Value/denominator) .OrderByDescending(x =&gt; x.Value) .ToDictionary(x =&gt; x.Key, x =&gt; x.Value); } public IList&lt;ClassificationBinaryTree&gt; Forest { get { return _trees.ToList(); } } }</span></span></code> </pre><br></div></div><br><br><h4>  Conclusion and links </h4><br>  If you looked at the code, you might notice that there is a function in the tree for writing the structure and conditions in the <a href="http://www.graphviz.org/">dot format, which is rendered by the GraphVis program</a> .  If you run a random forest with the following parameters <a href="https://archive.ics.uci.edu/ml/datasets/Breast%2BCancer%2BWisconsin%2B(Diagnostic)">on the above set</a> : <br><br><pre> <code class="cs hljs">ClassificationRandomForest crf = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ClassificationRandomForest( NormCreator.CreateByMetrics(MetricsCreator.CrossEntropy()), <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-literal"><span class="hljs-literal">null</span></span>, Convert.ToInt32(Math.Round(Math.Sqrt(ds.TrainSet.First().Input.Length))), <span class="hljs-number"><span class="hljs-number">0.95</span></span>, <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-literal"><span class="hljs-literal">true</span></span> ); crf.Train(ds.TrainSet);</code> </pre><br><br>  Then the following code will help us visualize this forest: <br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">foreach</span></span> (ClassificationBinaryTree tree <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> crf.Forest) { <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> (StreamWriter sw = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> StreamWriter(<span class="hljs-string"><span class="hljs-string">@"e:\Neuroximator\NetworkTrainingOCR\TreeTestData\Forest\"</span></span> + (<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> DirectoryInfo(<span class="hljs-string"><span class="hljs-string">@"e:\Neuroximator\NetworkTrainingOCR\TreeTestData\Forest\"</span></span>)).GetFiles().Count() + <span class="hljs-string"><span class="hljs-string">".dot"</span></span>)) { tree.WriteDotFile(sw); sw.Close(); } }</code> </pre><br><br><pre> <code class="bash hljs">dot.exe -Tpng <span class="hljs-string"><span class="hljs-string">"tree.dot"</span></span> -o <span class="hljs-string"><span class="hljs-string">"tree.png"</span></span></code> </pre><br><br>  Let's look at some of them, they turn out to be completely different due to the decorrelation. <br><div class="spoiler">  <b class="spoiler_title">Time</b> <div class="spoiler_text"><img src="//habrastorage.org/files/b4e/c85/364/b4ec853641a047c183e3ba5113f60755.png"><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Two</b> <div class="spoiler_text"><img src="//habrastorage.org/files/7ed/02e/f75/7ed02ef7507642f2858406758e585d4c.png"><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Three</b> <div class="spoiler_text"><img src="//habrastorage.org/files/629/7df/2b0/6297df2b02d04f44a10810740d4c94c5.png"><br></div></div><br><br>  And finally, some useful links: <br><ul><li>  <a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a> </li><li>  <a href="http://semanticsearchart.com/downloads/RF.txt">C # Elementary DEMO of Random Forest</a> </li><li>  <a href="https://www.dropbox.com/sh/ejohmuzkwi71j8d/lSlnSrMjK6">The code from the previous link is refactored by the</a> user <a href="http://habrahabr.ru/users/eocron/" class="user_link">eocron</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/215453/">https://habr.com/ru/post/215453/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../215443/index.html">DesignSpark Mechanical: Modeling a useful box for free (that is, for nothing)</a></li>
<li><a href="../215445/index.html">Crowdsourcing to search for a missing plane</a></li>
<li><a href="../215447/index.html">Multitasking in iOS 7</a></li>
<li><a href="../215449/index.html">Another archive storage format: dar</a></li>
<li><a href="../215451/index.html">Do startups have a life after death?</a></li>
<li><a href="../215455/index.html">The first Samsung U28 monitor U28D590 is already on sale</a></li>
<li><a href="../215457/index.html">The most popular passwords in 2013</a></li>
<li><a href="../215459/index.html">Google launched Add-ons for Google Docs and Sheets</a></li>
<li><a href="../215461/index.html">Wordpress for serious</a></li>
<li><a href="../215467/index.html">Kabam absorbs, India attracts, and Zynga puts everything on mobile apps - the main mobile news for the week</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>