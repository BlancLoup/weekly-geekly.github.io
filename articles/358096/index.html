<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Dive into pyTorch</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello. My name is Arthur Kadurin, and I lead research in the field of in-depth training for the development of new drugs at Insilico Medicine . At Ins...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Dive into pyTorch</h1><div class="post__text post__text-html js-mediator-article"><p>  Hello.  My name is Arthur Kadurin, and I lead research in the field of in-depth training for the development of new drugs at <a href="http://insilico.com/">Insilico Medicine</a> .  At Insilico, we use state-of-the-art machine learning techniques, and we also develop and publish many articles ourselves in order to cure diseases such as cancer or Alzheimer's disease, and possibly aging as such. </p><br><p>  In preparation for <a href="https://otus.ru/lessons/deep-learning-engineer/">my in</a> -depth training <a href="https://otus.ru/lessons/deep-learning-engineer/">course</a> , I am going to publish a series of articles on Adversarial networks with an analysis of what it is and how to use it.  This series of articles will not be another review of GANs ( <a href="">Generative Adversarial Networks</a> ), but will allow you to take a deeper look under the hood of neural networks and cover a wider range of architectures.  Although we will also analyze the GANs. </p><a name="habracut"></a><br><p>  In order to continue discussing the adversarial networks without any further obstacles, I decided to first make a small introduction to <a href="https://pytorch.org/">pyTorch</a> .  I want to immediately note that this is not an introduction to neural networks, so I assume that you already know such words as ‚Äúlayer‚Äù, ‚Äúbatch‚Äù, ‚Äúbackprop‚Äù, etc.  In addition to basic knowledge of neural networks, of course, you will need an understanding of the python language. </p><br><p>  In order to be comfortable using pyTorch, I prepared a <a href="https://www.docker.com/">docker</a> container with <a href="http://jupyter.org/">jupyter</a> and code in laptops.  If you want to run training on a video card, then for NVIDIA video cards you need nvidia-docker, I think most of you will have no problems with this part, so I leave the rest to you. </p><br><p>  Everything you need for this post is available in my <b>spoilt333 / adversarial repository</b> with the <b>intro</b> tag on the <a href="https://hub.docker.com/">Docker Hub</a> , or in my <a href="https://github.com/spoilt333/adversarial">GitHub repository</a> . </p><br><p>  After installing the docker, you can start the container, for example, with the following command: </p><br><pre><code class="bash hljs">docker run -id --name intro -p 8765:8765 spoilt333/adversarial:intro</code> </pre> <br><p>  The jupyter server, which will be available at <a href="http://127.0.0.1:8765/">http://127.0.0.1:8765</a> with the password "password" (without quotes), will automatically start in the container.  If you do not want to run someone else's container on your machine (correctly!), Then you can assemble your own one, after checking that everything is OK there, you can do it from the file that is in the GitHub repository. </p><br><p>  If everything started and you were able to connect to jupyter, then let's move on to what pyTorch is.  pyTorch is a large framework that allows you to create dynamic computation graphs and automatically calculate gradients from these graphs.  For machine learning, this is exactly what you need.  But, besides the very possibility of teaching models, pyTorch is also a huge library including datasets, ready-made models, modern layers and communities around it all. </p><br><p>  In Deep Learning, for quite a long time, it was practically the standard to test all new models on the task of recognizing handwritten numbers.  Dataset <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> consists of 70,000 handwritten numbers marked up roughly equally distributed between classes.  It is immediately divided into training and test sets in order to ensure the same conditions to all who are tested on this dataset.  In pyTorch, of course, there are simple interfaces for it.  Despite the fact that comparing the state-of-the-art model with each other on this dataset does not make much sense anymore, for demonstration purposes it will suit us perfectly. </p><br><img src="https://habrastorage.org/webt/eb/2c/s8/eb2cs8_u037yozqw6xueejyjnjk.png" alt="Examples of numbers from MNIST"><br><p>  Each example in MNIST is a 28x28 pixel image in shades of gray.  And, as it is easy to see, not all figures can easily be recognized even by a person.  In the mnist.ipynb laptop, you can look at an example of loading and displaying datasets, and several useful functions are included in the utils.py file.  But let's move on to the main "dish". </p><br><p>  The mnist-basic.ipynb laptop has a dual-layer fully connected neural network that solves the classification problem.  One way to make a neural network using pyTorch is to inherit from the <i>nn.Module</i> class and implement its initialization and <i>forward</i> functions. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> super(Net, self).__init__() self.fc1 = nn.Linear(<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) self.fc2 = nn.Linear(<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)</code> </pre> <br><p>  Inside the __init__ function, we declare the layers of the future neural network.  In our case, these are linear layers <em>nn.Linear</em> which have the form <strong>W'x + b</strong> , where W is the weights matrix of size <em>(input, output)</em> and b is the displacement vector of <em>output</em> size.  These very weights will be ‚Äútrained‚Äù in the process of neural network training. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">forward</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, x)</span></span></span><span class="hljs-function">:</span></span> x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>*<span class="hljs-number"><span class="hljs-number">28</span></span>) x = F.relu(self.fc1(x)) x = self.fc2(x) x = F.softmax(x, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x</code> </pre> <br><p>  The <em>forward</em> method is used directly to convert input data using a given neural network into its outputs.  For simplicity of the example, we will work with examples from MNIST not as with images, but as with vectors each dimension of which corresponds to one of the pixels.  The <em>view ()</em> function is an analogue of <em>numpy.reshape ()</em> , it re- <em>indexes the</em> tensor with the given data.  " <em>-1</em> " as the first argument of the function means that the number of elements in the first dimension will be calculated automatically.  If the initial tensor <strong>x</strong> has the dimension <em>(N, 28, 28)</em> , then after </p><br><pre> <code class="python hljs">x = x.view(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>*<span class="hljs-number"><span class="hljs-number">28</span></span>)</code> </pre> <br><p>  its dimension will be equal to <em>(N, 784)</em> . </p><br><pre> <code class="hljs objectivec">x = F.relu(<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.fc1(x))</code> </pre> <br><p>  Applying layers to data in pyTorch is implemented as simply as possible; you can "call" a layer by passing the data batch as an argument to it and get the result of the conversion.  The activation functions are similarly arranged.  In this case, I use <em>relu</em> , since this is the most popular activation function in computer vision tasks, however you can easily experiment with other functions implemented in pyTorch, since there are enough of them. </p><br><pre> <code class="python hljs">x = self.fc2(x) x = F.softmax(x, dim=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre> <br><p>  Since we solve the classification problem into 10 classes, the output of our network has a dimension of 10. We use <em>softmax</em> as the activation function at the output of the network.  Now the values ‚Äã‚Äãreturned by the function <em>forward</em> can be interpreted as the probabilities that the input example belongs to the corresponding classes. </p><br><pre> <code class="python hljs">model = Net() optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number"><span class="hljs-number">0.01</span></span>)</code> </pre> <br><p>  Now we can create an instance of our network and select the optimization function.  In order to get a nice training schedule, I chose an ordinary stochastic gradient descent, but in pyTorch, of course, more advanced methods are implemented.  You can try for example RMSProp or Adam. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(epoch)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> data, target <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> train_loader: optimizer.zero_grad() output = model(data) loss = F.cross_entropy(output, target) loss.backward() optimizer.step()</code> </pre> <br><p>  The <em>train</em> function contains the main training cycle in which we iterate over the batches from the training set.  <em>data</em> are examples, and <em>target</em> are appropriate labels.  At the beginning of each iteration, we reset the current value of the gradients: </p><br><pre> <code class="python hljs">optimizer.zero_grad()</code> </pre> <br><p>  Data processing by the entire network in pyTorch is no different from the use of a separate layer.  The call to the function <em>forward</em> is hidden behind the call to <em>model (data)</em> , so the network outputs fall into the output.  Now it only remains to calculate the value of the error function and take a backward propagation step: </p><br><pre> <code class="python hljs">loss = F.cross_entropy(output, target) loss.backward()</code> </pre> <br><p>  In fact, when calling <em>loss.backward (), the</em> weights of the network are not yet updated, but for all weights used in the error calculation, pyTorch counts the gradients using the constructed graph of calculations.  In order to update weights, we call <em>optimizer.step ()</em> , which, based on its parameters (we have this learning rate), updates weights. </p><br><img src="https://habrastorage.org/webt/vb/pc/bu/vbpcbuq6qryhbkxtyxbgarpbvg8.png"><br><p>  After 20 epochs of learning, our network guesses numbers with an accuracy of 91%, which, of course, is far from SOTA results, but not bad for 5 minutes of programming.  Here is an example from a test set with predicted answers. </p><br><img src="https://habrastorage.org/webt/t7/h0/jr/t7h0jrj908zjetopuf6k8bzeceg.png"><br><pre> <code class="hljs json">[[<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">6</span></span> <span class="hljs-number"><span class="hljs-number">9</span></span> <span class="hljs-number"><span class="hljs-number">9</span></span> <span class="hljs-number"><span class="hljs-number">9</span></span> <span class="hljs-number"><span class="hljs-number">9</span></span>] [<span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">7</span></span>] [<span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">9</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span>] [<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-number"><span class="hljs-number">7</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-number"><span class="hljs-number">6</span></span>] [<span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">6</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-number"><span class="hljs-number">9</span></span>]]</code> </pre> <br><p>  In the next posts I will try to talk about adversarial networks in the same style with code examples and prepared docker containers, in particular, I plan to touch on topics such as domain adaptation, style transfer, generative adversarial networks and analyze some of the most important articles in this area. </p><br><p>  Upd.1: As correctly indicated in the comments, it is no longer necessary to wrap the tensors in the <em>Variable</em> , so I deleted the corresponding line.  It will naturally remain in the docker-container, but everything works without it too. <br>  Upd.2: Pictures with numbers were mixed up in places, so I changed them </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/358096/">https://habr.com/ru/post/358096/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../358086/index.html">Not a single code: what affects the operation of blockchain systems</a></li>
<li><a href="../358088/index.html">Go assembler guide</a></li>
<li><a href="../358090/index.html">Understanding the life cycle of the React component</a></li>
<li><a href="../358092/index.html">Why think about web security when it's too late?</a></li>
<li><a href="../358094/index.html">Friday JS: random mixing</a></li>
<li><a href="../358100/index.html">How to write on Habrahabr and Geektimes</a></li>
<li><a href="../358102/index.html">The book "Swift. Basics of developing applications for iOS and macOS. 4th ed. supplemented and revised "</a></li>
<li><a href="../358104/index.html">Your A / B tests are broken.</a></li>
<li><a href="../358106/index.html">3CX IP phone auto configuration options</a></li>
<li><a href="../358108/index.html">Unity, ECS and all-all-all</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>