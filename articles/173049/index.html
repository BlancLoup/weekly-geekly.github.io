<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Data preprocessing and model analysis</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello. In the last post I talked about some basic classification methods. Today, due to the specifics of the last house, the post will be not so much ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Data preprocessing and model analysis</h1><div class="post__text post__text-html js-mediator-article">  Hello.  In the last post I talked about some basic classification methods.  Today, due to the specifics of the last house, the post will be not so much about the methods themselves, as about data processing and analysis of the models obtained. <br><br><h5>  Task </h5><br><br>  The data was provided by the Department of Statistics of the University of Munich.  Here you can take a dataset itself, as well as the <a href="http://www.stat.uni-muenchen.de/service/datenarchiv/kredit/kreditvar_e.html">description of the data</a> itself (the names of the fields are given in German).  The data collected loan applications, where each application is described by 20 variables.  In addition, each application corresponds to whether the applicant has been given a loan or not.  Here you can see in detail what of the variables means. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Our task was to build a model that would predict a decision to be made on one or another applicant. <br><br><img src="https://habrastorage.org/storage2/438/ae7/b2c/438ae7b2c22acbbc6cce3b517161388f.jpg"><br><a name="habracut"></a><br><br>  Alas, there were no test data and systems for testing our models, as, for example, it was done in <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> .  In this regard, we had some room for imagination in terms of the validation of our models. <br><br><h5>  Pre-processing of data </h5><br><br>  Let's first take a look at the data itself.  The following graph shows the histograms of the distributions of all available variables.  The order of occurrence of variables is specifically modified for clarity. <br><br><img src="https://habrastorage.org/storage2/1a4/052/716/1a40527160282ac9d18bef865d140614.jpg"><br><img src="https://habrastorage.org/storage2/4bc/7b0/cb9/4bc7b0cb9dd10afa5070157420409028.jpg"><br><img src="https://habrastorage.org/storage2/7ca/5d0/aaf/7ca5d0aafaa3ceac3b1a7723f7aa609f.jpg"><br><br>  Looking at these graphs, we can draw several conclusions.  First, most of the variables are categorical in fact, that is, they take only a couple (pair) of values.  Secondly, there are only two (well, maybe three) conditionally continuous variables, namely, <b>hoehe</b> and <b>alter</b> .  Third, there are no emissions. <br><br>  When working with continuous variables, generally speaking, you can pretty much forgive the mind of their distribution.  For example, multimodality, when the density has a strange hilly shape, with several peaks.  Something similar can be seen on the density graph of the <b>laufzeit</b> variable.  But the extended tails of the distributions are the main headache in the construction of models, since they very strongly influence their properties and appearance.  Emissions also greatly affect the quality of the models built, but since we are lucky and we don‚Äôt have them, I‚Äôll tell you about the emissions some time next. <br><br>  Returning to our tails, the variables <b>hoehe</b> and <b>alter</b> have one feature: they are not normal.  Ie, they are very similar to lognormal, due to the strong right tail.  Considering all the above, we have some reason to recologize these variables in order to tighten these tails. <br><br><h5>  What is the strength in, brother?  Or who are all these <s>people</s> variable? </h5><br><br>  Often, when analyzing, some variables are unnecessary.  That is, well, absolutely.  This means that if we throw them out of the analysis, then in solving our problem, we will lose almost nothing even in the worst case.  In our case of credit scoring, by losses we mean a slightly decreased classification accuracy. <br><br>  This is what will happen in the worst case.  However, practice <a href="https://www.kaggle.com/c/benchmark-bond-trade-price-challenge/forums/t/1833/congratulations">shows</a> that with careful selection of variables, popularly known as <b>feature selection</b> , you can even win exactly.  Insignificant variables introduce noise exclusively into the model, with almost no effect on the result.  And when they are collected quite a lot, we have to separate the wheat from the chaff. <br><br>  In practice, this task arises from the fact that by the time of data collection, experts do not yet know which variables will be most significant in the analysis.  At the same time, during the experiment itself and data collection, no one bothers the experimenters to collect all the variables that can be collected at all.  They say that we will collect all that is, and analysts will somehow sort it out themselves. <br><br>  Cutting variables must be wisely.  If we simply cut the data on the frequency of occurrence of a trait, for example, in the case of the gastarb variable, then we cannot guarantee that we will not discard a very significant trait.  In some textual or biological data, this problem is even more pronounced, since there are very rarely any variables that take on non-zero values. <br><br>  The problem with the selection of signs is that for each model, tied to the data, the criterion for the selection of signs will be its own, specially built for this model.  For example, for linear models, <a href="http://www.weibull.com/DOEWeb/hypothesis_tests_in_multiple_linear_regression.htm">t-statistics on the significance of the coefficients are used</a> , and for Random Forest - the <a href="http://www.ehu.es/ccwintco/uploads/8/8b/ElsaVariableSelectionGenuer.pdf">relative importance of variables in the cascade of trees</a> .  And sometimes feature selection can generally be <a href="http://www.csie.ntu.edu.tw/~cjlin/papers/l1_glmnet/long-glmnet.pdf">explicitly built into the model</a> . <br><br>  For simplicity, consider only the significance of variables in a linear model.  We simply construct a generalized linear model, GLM.  Since our target variable is a class label, it therefore has (conditionally) a binomial distribution.  Using the function glm in R, we construct this model and look under the hood for it, calling a summary for it.  As a result, we get the following label: <br><br><img src="https://habrastorage.org/storage2/969/15b/25b/96915b25be3724479c674d53cf9958bc.jpg"><br><br>  We are interested in the very last column.  This column indicates the probability that our coefficient is zero, that is, it does not play a role in the final model.  The asterisks here indicate the relative significance of the coefficients.  From the table we see that, generally speaking, we can ruthlessly cut out almost all variables, except <b>laufkont</b> , <b>laufzeit</b> , <b>moral</b> and <b>sparkont</b> ( <b>intercept</b> is the shift parameter, we also need it).  We chose them on the basis of the statistics obtained, that is, those variables for which the statistics for ‚Äúdeparture‚Äù is less than or equal to 0.01. <br><br>  If you close your eyes to the validation of the model, assuming that the linear model does not overtake our data, you can check the validity of our hypothesis.  Namely, we will test the accuracy of two models on all data: a model with 4 variables and a model with 20.  For 20 variables, the classification accuracy will be 77.1%, while for a model with 4 variables, 76.1%.  As you can see, not very sorry. <br><br>  It is interesting that the variables we predicted do not affect the model in any way.  Being never prologized and prologicized twice, they did not reach even 0.1. <br><br><h5>  Analysis </h5><br><br>  We decided to build the classifiers in Python using Scikit.  In the analysis, we decided to use all the main classifiers that scikit provides, having played with their hyperparameters.  Here is a list of what was launched: <br><br><ol><li>  <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%259B%25D0%25BE%25D0%25B3%25D0%25B8%25D1%2581%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D1%2580%25D0%25B5%25D0%25B3%25D1%2580%25D0%25B5%25D1%2581%25D1%2581%25D0%25B8%25D1%258F">GLM</a> </li><li>  <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%259C%25D0%25B0%25D1%2588%25D0%25B8%25D0%25BD%25D0%25B0_%25D0%25BE%25D0%25BF%25D0%25BE%25D1%2580%25D0%25BD%25D1%258B%25D1%2585_%25D0%25B2%25D0%25B5%25D0%25BA%25D1%2582%25D0%25BE%25D1%2580%25D0%25BE%25D0%25B2">SVM</a> </li><li>  <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25B1%25D0%25BB%25D0%25B8%25D0%25B6%25D0%25B0%25D0%25B9%25D1%2588%25D0%25B8%25D1%2585_%25D1%2581%25D0%25BE%25D1%2581%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25B9">kNN</a> </li><li>  <a href="http://ru.wikipedia.org/wiki/Random_forest">Random forest</a> </li><li>  <a href="http://en.wikipedia.org/wiki/Gradient_boosting">en.wikipedia.org/wiki/Gradient_boosting</a> </li></ol><br><br>  At the end of the article - links to the documentation for classes that implement these algorithms. <br><br>  Since we did not have the opportunity to test the output explicitly, we used the cross-validation method.  We took 10 as the number of folds. As a result, we derive the average value of the classification accuracy from all 10 folds. <br>  The implementation is very transparent. <br><br><div class="spoiler">  <b class="spoiler_title">View code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.externals <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> joblib <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cross_validation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> svm <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> neighbors <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GradientBoostingClassifier <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RandomForestClassifier <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LogisticRegression <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">avg</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> s = <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> x: s += t <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (s/len(x))*<span class="hljs-number"><span class="hljs-number">100.0</span></span> dataset = joblib.load(<span class="hljs-string"><span class="hljs-string">'kredit.pkl'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   ,    target = [x[0] for x in dataset] target = np.array(target) train = [x[1:] for x in dataset] numcv = 10 #  glm = LogisticRegression(penalty='l1', tol=1) scores = cross_validation.cross_val_score(glm, train, target, cv = numcv) print("Logistic Regression with L1 metric - " + ' avg = ' + ('%2.1f'%avg(scores))) linSVM = svm.SVC(kernel='linear', C=1) scores = cross_validation.cross_val_score(linSVM, train, target, cv = numcv) print("SVM with linear kernel - " + ' avg = ' + ('%2.1f'%avg(scores))) poly2SVM = svm.SVC(kernel='poly', degree=2, C=1) scores = cross_validation.cross_val_score(poly2SVM, train, target, cv = numcv) print("SVM with polynomial kernel degree 2 - " + ' avg = ' + ('%2.1f' % avg(scores))) rbfSVM = svm.SVC(kernel='rbf', C=1) scores = cross_validation.cross_val_score(rbfSVM, train, target, cv = numcv) print("SVM with rbf kernel - " + ' avg = ' + ('%2.1f'%avg(scores))) knn = neighbors.KNeighborsClassifier(n_neighbors = 1, weights='uniform') scores = cross_validation.cross_val_score(knn, train, target, cv = numcv) print("kNN 1 neighbour - " + ' avg = ' + ('%2.1f'%avg(scores))) knn = neighbors.KNeighborsClassifier(n_neighbors = 5, weights='uniform') scores = cross_validation.cross_val_score(knn, train, target, cv = numcv) print("kNN 5 neighbours - " + ' avg = ' + ('%2.1f'%avg(scores))) knn = neighbors.KNeighborsClassifier(n_neighbors = 11, weights='uniform') scores = cross_validation.cross_val_score(knn, train, target, cv = numcv) print("kNN 11 neighbours - " + ' avg = ' + ('%2.1f'%avg(scores))) gbm = GradientBoostingClassifier(learning_rate = 0.001, n_estimators = 5000) scores = cross_validation.cross_val_score(gbm, train, target, cv = numcv) print("Gradient Boosting 5000 trees, shrinkage 0.001 - " + ' avg = ' + ('%2.1f'%avg(scores))) gbm = GradientBoostingClassifier(learning_rate = 0.001, n_estimators = 10000) scores = cross_validation.cross_val_score(gbm, train, target, cv = numcv) print("Gradient Boosting 10000 trees, shrinkage 0.001 - " + ' avg = ' + ('%2.1f'%avg(scores))) gbm = GradientBoostingClassifier(learning_rate = 0.001, n_estimators = 15000) scores = cross_validation.cross_val_score(gbm, train, target, cv = numcv) print("Gradient Boosting 15000 trees, shrinkage 0.001 - " + ' avg = ' + ('%2.1f'%avg(scores))) #     -  forest = RandomForestClassifier(n_estimators = 10, n_jobs = 1) scores = cross_validation.cross_val_score(forest, train, target, cv=numcv) print("Random Forest 10 - " +' avg = ' + ('%2.1f'%avg(scores))) forest = RandomForestClassifier(n_estimators = 50, n_jobs = 1) scores = cross_validation.cross_val_score(forest, train, target, cv=numcv) print("Random Forest 50 - " +' avg = ' + ('%2.1f'%avg(scores))) forest = RandomForestClassifier(n_estimators = 100, n_jobs = 1) scores = cross_validation.cross_val_score(forest, train, target, cv=numcv) print("Random Forest 100 - " +' avg = '+ ('%2.1f'%avg(scores))) forest = RandomForestClassifier(n_estimators = 200, n_jobs = 1) scores = cross_validation.cross_val_score(forest, train, target, cv=numcv) print("Random Forest 200 - " +' avg = ' + ('%2.1f'%avg(scores))) forest = RandomForestClassifier(n_estimators = 300, n_jobs = 1) scores = cross_validation.cross_val_score(forest, train, target, cv=numcv) print("Random Forest 300 - " +' avg = '+ ('%2.1f'%avg(scores))) forest = RandomForestClassifier(n_estimators = 400, n_jobs = 1) scores = cross_validation.cross_val_score(forest, train, target, cv=numcv) print("Random Forest 400 - " +' avg = '+ ('%2.1f'%avg(scores))) forest = RandomForestClassifier(n_estimators = 500, n_jobs = 1) scores = cross_validation.cross_val_score(forest, train, target, cv=numcv) print("Random Forest 500 - " +' avg = '+ ('%2.1f'%avg(scores)))</span></span></code> </pre> <br></div></div><br><br>  After we launched our script, we got the following results: <br><br><table><tbody><tr><th>  Method with parameters </th><th>  Average accuracy on 4 variables </th><th>  Average accuracy on 20 variables </th></tr><tr><td>  Logistic Regression, L1 metric </td><td>  75.5 </td><td>  75.2 </td></tr><tr><td>  SVM with linear kernel </td><td>  73.9 </td><td>  74.4 </td></tr><tr><td>  SVM with polynomial kernel </td><td>  72.6 </td><td>  74.9 </td></tr><tr><td>  SVM with rbf kernel </td><td>  74.3 </td><td>  74.7 </td></tr><tr><td>  kNN 1 neighbor </td><td>  68.8 </td><td>  61.4 </td></tr><tr><td>  kNN 5 neighbors </td><td>  72.1 </td><td>  65.1 </td></tr><tr><td>  kNN 11 neighbors </td><td>  72.3 </td><td>  68.7 </td></tr><tr><td>  Gradient Boosting 5000 trees shrinkage 0.001 </td><td>  75.0 </td><td>  77.6 </td></tr><tr><td>  Gradient Boosting 10000 trees shrinkage 0.001 </td><td>  73.8 </td><td>  77.2 </td></tr><tr><td>  Gradient Boosting 15000 trees shrinkage 0.001 </td><td>  73.7 </td><td>  76.5 </td></tr><tr><td>  Random Forest 10 </td><td>  72.0 </td><td>  71.2 </td></tr><tr><td>  Random Forest 50 </td><td>  72.1 </td><td>  75.5 </td></tr><tr><td>  Random Forest 100 </td><td>  71.6 </td><td>  75.9 </td></tr><tr><td>  Random Forest 200 </td><td>  71.8 </td><td>  76.1 </td></tr><tr><td>  Radom Forest 300 </td><td>  72.4 </td><td>  75.9 </td></tr><tr><td>  Random Forest 400 </td><td>  71.9 </td><td>  76.7 </td></tr><tr><td>  Random Forest 500 </td><td>  72.6 </td><td>  76.2 </td></tr></tbody></table><br><br>  More clearly, they can be visualized as follows: <br><br><img src="https://habrastorage.org/storage2/a5f/309/764/a5f309764ae752cfb0c3d8a2b2dc0028.png"><br><img src="https://habrastorage.org/storage2/438/3f3/a3a/4383f3a3aa5c7befdbe2e24876936afe.png"><br><br>  The average accuracy for all models on 4 variables is 72.7 <br>  The average accuracy for all models on all-all variables is 73.7 <br>  The discrepancy with the ones predicted at the beginning of the article is explained by the fact that those tests were performed on a different framework. <br><br><h5>  findings </h5><br><br>  Looking at the results of the accuracy of the models we obtained, we can draw a couple of interesting conclusions.  We built a stack of different models, linear and nonlinear.  As a result, all these models show approximately the same accuracy on the data.  That is, such models as RF and SVM did not give significant advantages in accuracy in comparison with the linear model.  This is most likely a consequence of the fact that the original data is almost certainly some kind of linear relationship and was generated. <br><br>  The consequence of this is that it is pointless to chase these data for accuracy using complex massive methods, such as Random Forest, SVM or Gradient Boosting.  That is, everything that could be caught in this data was already caught by the linear model.  Otherwise, if there are explicit non-linear dependencies in the data, this difference in accuracy would be more significant. <br><br>  This tells us that sometimes the data is not as complex as it seems, and very quickly you can come to the actual maximum of what can be squeezed out of them. <br><br>  Moreover, out of the highly abbreviated data through the use of feature selection, our accuracy did not actually suffer.  Ie our solution for this data was not only simple (cheaply), but also compact. <br><br><h5>  Documentation </h5><br><br>  <a href="http://scikit-learn.org/0.11/modules/generated/sklearn.linear_model.LogisticRegression.html">Logistic Regression (our GLM case)</a> <br>  <a href="http://scikit-learn.org/dev/modules/generated/sklearn.svm.SVC.html">SVM</a> <br>  <a href="http://scikit-learn.org/dev/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">kNN</a> <br>  <a href="http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.RandomForestClassifier.html">Random forest</a> <br>  <a href="http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html">Gradient boosting</a> <br><br>  In addition, here is <a href="http://scikit-learn.org/dev/modules/cross_validation.html">an example of</a> working with cross-validation. <br><br>  For help in writing an article thanks to the <a href="http://gamechangers.ru/tracks/data/">Data Mining</a> track <a href="http://gamechangers.ru/tracks/data/">from GameChangers</a> , as well as Alexey Natyokin. </div><p>Source: <a href="https://habr.com/ru/post/173049/">https://habr.com/ru/post/173049/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../173029/index.html">Other side 22nm: unknown heroes of Silicon Valley</a></li>
<li><a href="../173031/index.html">Enhance SSH Access Security on Juniper SRX Routers</a></li>
<li><a href="../173037/index.html">Interesting possibilities of storage systems HP 3PAR. Part 3 - Peer Motion</a></li>
<li><a href="../173041/index.html">Sieve: server-side mail filtering</a></li>
<li><a href="../173045/index.html">So what is this ‚Äúterrible‚Äù point at the end of the domain name?</a></li>
<li><a href="../173059/index.html">Oracle ADF. Business Components</a></li>
<li><a href="../173063/index.html">Moving and renaming files in GitHub</a></li>
<li><a href="../173065/index.html">The digest of news from the world of mobile development for the last week ‚Ññ6 (March 11 - 17, 2013)</a></li>
<li><a href="../173067/index.html">Application security: it's almost simple</a></li>
<li><a href="../173071/index.html">Why study Clojure?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>