<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>CUDA: How does the GPU work</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The internal model of nVidia GPU is a key moment in understanding GPGPU using CUDA. This time I will try to tell you in more detail about the software...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>CUDA: How does the GPU work</h1><div class="post__text post__text-html js-mediator-article">  The internal model of nVidia GPU is a key moment in understanding GPGPU using CUDA.  This time I will try to tell you in more detail about the software device GPUs. <a name="habracut"></a>  I will talk about the key points of the CUDA compiler, the CUDA runtime API, well, and finally, I will give an example of using CUDA for simple mathematical calculations. <br><br>  Let's get started <br><br><h2>  Computational GPU model: </h2><br>  Consider the computational model of the GPU in more detail. <br><ol><li>  The top level of the GPU core consists of blocks that are grouped into a grid or grid (dimension) of dimension N1 * N2 * N3.  This can be represented as follows: <br><img title="Fig. 1. Computing device GPU." src="http://www.picamatic.com/show/2009/03/17/10/12/2857369_300x286.png"><br>  Fig.  1. Computing device GPU. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The dimension of the grid of blocks can be found using the cudaGetDeviceProperties function, in the resulting structure, the maxGridSize field is responsible for this.  For example, on my GeForce 9600M GS, the dimension of the grid of blocks is 65535 * 65535 * 1, that is, the grid of blocks is two-dimensional (the obtained data satisfy Compute Capability v.1.1). <br></li><li>  Any block in turn consists of threads (threads), which are the direct performers of the calculations.  The threads in the block are formed as a three-dimensional array (Fig. 2), the dimension of which can also be found using the cudaGetDeviceProperties function, the maxThreadsDim field is responsible for this. </li></ol><br><img title="Fig. 2. GPU block device" src="http://www.picamatic.com/show/2009/03/17/10/12/2857371_487x359.png"><br>  Fig.  2. Device GPU unit. <br><br>  When using a GPU, you can use the grid of the required size and configure the blocks for the needs of your task. <br><br><h2>  CUDA and C language: </h2><br>  The technology itself CUDA (compiler nvcc.exe) introduces a number of additional extensions for the C language, which are necessary for writing code for the GPU: <br><ol><li>  Function specifiers that show how and from where the functions will be performed. </li><li>  Variable specifiers that serve to indicate the type of memory used by the GPU. </li><li>  GPU kernel launch qualifiers. </li><li>  Built-in variables for identifying threads, blocks, and other parameters when executing code in the GPU core. </li><li>  Additional variable types. </li></ol><br>  As mentioned, function specifiers determine how and from where functions will be called.  There are 3 such specifiers in CUDA: <br><ul><li>  <b>__host__</b> - will be executed on the CPU, called from the CPU (in principle, you can not specify it). </li><li>  <b>__global__</b> - executed on the GPU, called from the CPU. </li><li>  <b>__device__</b> - executed on the GPU, called from the GPU. </li></ul><br>  Kernel launch qualifiers are used to describe the number of blocks, threads, and memory that you want to allocate when calculating on a GPU.  The kernel startup syntax is as follows: <br><br>  myKernelFunc &lt;&lt;&lt; gridSize, blockSize, sharedMemSize, cudaStream &gt;&gt;&gt; (float * param1, float * param2), where <br><ul><li>  <b>gridSize</b> is the dimension of the grid of blocks (dim3) allocated for calculations, </li><li>  <b>blockSize</b> is the block size (dim3) allocated for calculations, </li><li>  <b>sharedMemSize</b> is the size of the additional memory allocated when the kernel starts, </li><li>  <b>cudaStream</b> is a cudaStream_t variable that specifies the stream in which the call will be made. </li></ul><br>  And of course the myKernelFunc itself is a kernel function (specifier __global__).  Some variables when calling the kernel can be omitted, for example sharedMemSize and cudaStream. <br><br>  It is also worth mentioning the built-in variables: <br><ul><li>  <b>gridDim</b> is the grid dimension, has type dim3.  Allows you to find out the size of the grid allocated during the current kernel call. </li><li>  <b>blockDim</b> - the dimension of the block, also has the type dim3.  Allows you to find out the size of the block allocated during the current kernel call. </li><li>  <b>blockIdx</b> - the index of the current block in the calculation on the GPU, is of type uint3. </li><li>  <b>threadIdx</b> - the index of the current thread in the calculation on the GPU, is of type uint3. </li><li>  <b>warpSize</b> is the size of warp, it is of type int (I have not tried it myself yet). </li></ul><br>  By the way, gridDim and blockDim are the very variables that we pass when the GPU kernel starts up, although in the kernel they can be read only. <br><br>  Additional variable types and their specifiers will be discussed directly in the examples of working with memory. <br><br><h2>  CUDA host API: </h2><br>  Before you start using CUDA directly for computing, you need to familiarize yourself with the so-called CUDA host API, which is the link between the CPU and the GPU.  The CUDA host API, in turn, can be divided into a low-level API called the CUDA driver API, which provides access to the CUDA user-mode driver, and the high-level API, the CUDA runtime API.  In my examples, I will use the CUDA runtime API. <br><br>  The CUDA runtime API includes the following function groups: <br><ul><li>  <b>Device Management</b> - includes functions for general management of the GPU (obtaining information about the capabilities of the GPU, switching between the GPU during the SLI mode, etc.). </li><li>  <b>Thread Management</b> - thread management. </li><li>  <b>Stream Management</b> - <b>stream</b> management. </li><li>  <b>Event Management</b> is the function of creating and managing events. </li><li>  <b>Execution Control</b> - functions for starting and executing the CUDA core. </li><li>  <b>Memory Management</b> - GPU memory management functions. </li><li>  <b>Texture Reference Manager</b> - working with texture objects via CUDA. </li><li>  <b>OpenGL Interoperability</b> - functions for interacting with the OpenGL API. </li><li>  <b>Direct3D 9 Interoperability</b> - functions to interact with the Direct3D 9 API. </li><li>  <b>Direct3D 10 Interoperability</b> - functions to interact with the Direct3D 10 API. </li><li>  <b>Error Handling</b> - error handling functions. </li></ul><br><h2>  We understand the work of the GPU: </h2><br>  As it was said, the thread is the direct performer of the calculations.  What, then, is the parallelization of calculations between threads?  Consider the work of a single block. <br><br>  <b>Task.</b>  It is required to calculate the sum of two vectors of dimension N elements. <br><br>  We know the maximum size of our block: 512 * 512 * 64 threads.  Since we have a one-dimensional vector, for now we will limit ourselves to using the x-dimensions of our block, that is, we will use only one strip of threads from the block (Fig. 3). <br><img title="Fig. 3. Our strip of threads from the used block." src="http://www.picamatic.com/show/2009/03/17/10/12/2857373_382x78.png"><br>  Fig.  3. Our strip of threads from the used block. <br><br>  Note that the x-dimension of the block is 512, that is, we can add at one time vectors whose length is N &lt;= 512 elements.  In other matters, with more massive calculations, you can use a larger number of blocks and multidimensional arrays.  I also noticed one interesting feature, perhaps some of you thought that you can use 512 * 512 * 64 = 16777216 threads in one block, naturally this is not so, in general, this work cannot exceed 512 (at least, my video card). <br><br>  The program itself must perform the following steps: <br><ol><li>  Get data for calculations. </li><li>  Copy this data to GPU memory. </li><li>  Perform calculation in the GPU through the kernel function. </li><li>  Copy the calculated data from the GPU memory to the RAM. </li><li>  View results. </li><li>  Free up used resources. </li></ol><br>  Moving directly to writing code: <br><br>  First of all, we write the kernel function, which will add vectors: <br><blockquote><code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black"><font color="#008000">//    </font> <br> <font color="#0000ff">__global__ void</font> addVector( <font color="#0000ff">float</font> * left, <font color="#0000ff">float</font> * right, <font color="#0000ff">float</font> * result) <br> { <br> <font color="#008000">// id  .</font> <br> <font color="#0000ff">int</font> idx = threadIdx.x; <br> <br> <font color="#008000">// .</font> <br> result[idx] = left[idx] + right[idx]; <br> } <br></font> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br><br>  Thus, parallelization will be performed automatically when the kernel starts.  This function also uses the built-in variable threadIdx and its field x, which allows you to set the correspondence between the calculation of the vector element and the thread in the block.  We calculate each element of the vector in a separate thread. <br><br>  We write the code, which is responsible for 1 and 2 points in the program: <br><br><blockquote> <code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black"><font color="#0000ff">#define</font> SIZE 512 <br> <font color="#0000ff">__host__ int</font> main() <br> { <br> <font color="#008000">//   </font> <br> <font color="#0000ff">float</font> * vec1 = <font color="#0000ff">new</font> <font color="#0000ff">float</font> [SIZE]; <br> <font color="#0000ff">float</font> * vec2 = <font color="#0000ff">new</font> <font color="#0000ff">float</font> [SIZE]; <br> <font color="#0000ff">float</font> * vec3 = <font color="#0000ff">new</font> <font color="#0000ff">float</font> [SIZE]; <br> <br> <font color="#008000">//  </font> <br> <font color="#0000ff">for</font> ( <font color="#0000ff">int</font> i = 0; i &lt; SIZE; i++) <br> { <br> vec1[i] = i; <br> vec2[i] = i; <br> } <br> <br> <font color="#008000">//   </font> <br> <font color="#0000ff">float</font> * devVec1; <br> <font color="#0000ff">float</font> * devVec2; <br> <font color="#0000ff">float</font> * devVec3; <br> <br> <font color="#008000">//     </font> <br> cudaMalloc(( <font color="#0000ff">void</font> **)&amp;devVec1, <font color="#0000ff">sizeof</font> ( <font color="#0000ff">float</font> ) * SIZE); <br> cudaMalloc(( <font color="#0000ff">void</font> **)&amp;devVec2, <font color="#0000ff">sizeof</font> ( <font color="#0000ff">float</font> ) * SIZE); <br> cudaMalloc(( <font color="#0000ff">void</font> **)&amp;devVec3, <font color="#0000ff">sizeof</font> ( <font color="#0000ff">float</font> ) * SIZE); <br> <br> <font color="#008000">//    </font> <br> cudaMemcpy(devVec1, vec1, <font color="#0000ff">sizeof</font> ( <font color="#0000ff">float</font> ) * SIZE, cudaMemcpyHostToDevice); <br> cudaMemcpy(devVec2, vec2, <font color="#0000ff">sizeof</font> ( <font color="#0000ff">float</font> ) * SIZE, cudaMemcpyHostToDevice); <br> ‚Ä¶ <br> } <br></font> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br><br>  To allocate memory on a video card, the <b>cudaMalloc</b> function is <b>used</b> , which has the following prototype: <br>  cudaError_t cudaMalloc (void ** devPtr, size_t count), where <br><ol><li>  <b>devPtr</b> - the pointer to which the address of the allocated memory is written, </li><li>  <b>count</b> - the size of the allocated memory in bytes. </li></ol><br>  Returns: <br><ol><li>  <b>cudaSuccess</b> - with successful memory allocation </li><li>  <b>cudaErrorMemoryAllocation</b> - on memory allocation error </li></ol><br>  To copy data into the video card's memory, use cudaMemcpy, which has the following prototype: <br>  cudaError_t cudaMemcpy (void * dst, const void * src, size_t count, enum cudaMemcpyKind kind), where <br><ol><li>  <b>dst</b> is a pointer containing the address of the copy destination; </li><li>  <b>src</b> - pointer containing the address of the copy source, </li><li>  <b>count</b> - the size of the copied resource in bytes, </li><li>  <b>cudaMemcpyKind</b> is an enumeration that indicates the direction of the copy (maybe cudaMemcpyHostToDevice, cudaMemcpyDeviceToHost, cudaMemcpyHostToHost, cudaMemcpyDeviceToDevice). </li></ol><br>  Returns: <br><ol><li>  cudaSuccess - with successful copying </li><li>  cudaErrorInvalidValue - invalid argument parameters (for example, copy size is negative) </li><li>  cudaErrorInvalidDevicePointer - invalid memory pointer in video card </li><li>  cudaErrorInvalidMemcpyDirection is the wrong direction (for example, the source and copy destination are confused) </li></ol><br>  Now we come to the direct call of the kernel for computing on the GPU. <br><blockquote> <code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black">‚Ä¶ <br> dim3 gridSize = dim3(1, 1, 1); <font color="#008000">//  </font> <br> dim3 blockSize = dim3(SIZE, 1, 1); <font color="#008000">//  </font> <br> <br> <font color="#008000">//   </font> <br> addVector&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(devVec1, devVec2, devVec3); <br> ‚Ä¶ <br></font> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br>  In our case, it is not necessary to determine the size of the grid and the block, since we use only one block and one dimension in the block, so the code above can be written: <br><blockquote> <code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black">addVector&lt;&lt;&lt;1, SIZE&gt;&gt;&gt;(devVec1, devVec2, devVec3);</font> <br> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br>  Now we need to copy the result of the calculation from the video memory to the host memory.  But the core functions have a special feature - asynchronous execution, that is, if after the kernel call, the next piece of code began to work, then this does not mean that the GPU performed the calculations.  To complete the work of a given kernel function, you must use synchronization tools, such as eventa.  Therefore, before copying the results to the host, we synchronize the threads of the GPU through the event. <br><br>  Code after calling the kernel: <br><blockquote> <code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black"><font color="#008000">//   </font> <br> addVector&lt;&lt;&lt;blocks, threads&gt;&gt;&gt;(devVec1, devVec2, devVec3); <br> <br> <font color="#008000">// event'</font> <br> cudaEvent_t syncEvent; <br> <br> cudaEventCreate(&amp;syncEvent); <font color="#008000">// event</font> <br> cudaEventRecord(syncEvent, 0); <font color="#008000">// event</font> <br> cudaEventSynchronize(syncEvent); <font color="#008000">// event</font> <br> <br> <font color="#008000">//    </font> <br> cudaMemcpy(vec3, devVec3, <font color="#0000ff">sizeof</font> ( <font color="#0000ff">float</font> ) * SIZE, cudaMemcpyDeviceToHost); <br></font> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br>  Let's take a closer look at the functions from the Event Managment API. <br><br>  An event is created using the <b>cudaEventCreate</b> function, the prototype of which is: <br>  cudaError_t cudaEventCreate (cudaEvent_t * event), where <br><ol><li>  <b>* event</b> - a pointer to record the event handle. </li></ol><br>  Returns: <br><ol><li>  cudaSuccess - if successful </li><li>  cudaErrorInitializationError - initialization error </li><li>  cudaErrorPriorLaunchFailure - error during previous asynchronous function launch </li><li>  cudaErrorInvalidValue - incorrect value </li><li>  cudaErrorMemoryAllocation - memory allocation error </li></ol><br>  Event'a is recorded using the <b>cudaEventRecord</b> function, the prototype of which is: <br>  cudaError_t cudaEventRecord (cudaEvent_t event, CUstream stream), where <br><ol><li>  <b>event</b> - handle of the event to write, </li><li>  <b>stream</b> is the number of the stream in which we are recording (in our case this is the main zero stream). </li></ol><br>  Returns: <br><ol><li>  cudaSuccess - if successful </li><li>  cudaErrorInvalidValue - incorrect value </li><li>  cudaErrorInitializationError - initialization error </li><li>  cudaErrorPriorLaunchFailure - error during previous asynchronous function launch </li><li>  cudaErrorInvalidResourceHandle - invalid event handle </li></ol><br>  The event is synchronized by the cudaEventSynchronize function.  This function waits for all the threads of the GPU to end and pass a given event and only then returns control to the caller.  The function prototype is: <br>  cudaError_t cudaEventSynchronize (cudaEvent_t event), where <br><ol><li>  <b>event</b> - the event handle that is expected to pass. </li></ol><br>  Returns: <br><ol><li>  cudaSuccess - if successful </li><li>  cudaErrorInitializationError - initialization error </li><li>  cudaErrorPriorLaunchFailure - error during previous asynchronous function launch </li><li>  cudaErrorInvalidValue - incorrect value </li><li>  cudaErrorInvalidResourceHandle - invalid event handle </li></ol><br>  You can understand how cudaEventSynchronize works from the following scheme: <br><br><img title="Fig. 4. Synchronization of the basic and GPU programs." src="http://www.picamatic.com/show/2009/03/17/10/12/2857374_628x350.png"><br>  Fig.  4. Synchronization of the basic and GPU programs. <br><br>  In Figure 4, the ‚ÄúWaiting for Event'A Passage‚Äù block is the call to the cudaEventSynchronize function. <br><br>  Well, in conclusion, we display the result on the screen and clean the allocated resources. <br><blockquote> <code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black"><font color="#008000">// </font> <br> <font color="#0000ff">for</font> ( <font color="#0000ff">int</font> i = 0; i &lt; SIZE; i++) <br> { <br> printf( <font color="#A31515">"Element #%i: %.1f\n"</font> , i , vec3[i]); <br> } <br> <br> <font color="#008000">//</font> <br> <font color="#008000">//  </font> <br> <font color="#008000">//</font> <br> <br> cudaEventDestroy(syncEvent); <br> <br> cudaFree(devVec1); <br> cudaFree(devVec2); <br> cudaFree(devVec3); <br> <br> delete[] vec1; vec1 = 0; <br> delete[] vec2; vec2 = 0; <br> delete[] vec3; vec3 = 0; <br></font> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br>  I think that there is no need to describe the resource release functions.  Unless, it is possible to remind that they also return cudaError_t values, if there is a need to check their work. <br><br><h2>  Conclusion </h2><br>  I hope that this material will help you understand how the GPU functions.  I described the most important points that you need to know to work with CUDA.  Try it yourself to write the addition of two matrices, but do not forget about the hardware limitations of the video card. <br><br>  PS: It turned out not very briefly.  Hope not tired.  If you need all the source code, I can send it by mail. <br>  PSS: Ask questions. </div><p>Source: <a href="https://habr.com/ru/post/54707/">https://habr.com/ru/post/54707/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../54693/index.html">We work with MSSQL in linux</a></li>
<li><a href="../54696/index.html">50 monochrome designs of web sites</a></li>
<li><a href="../54700/index.html">Change Linux partitions, grub recovery</a></li>
<li><a href="../54703/index.html">Managing presentation - the right thing for any conference</a></li>
<li><a href="../54704/index.html">Kirgudu - Mac Keyboard Layout Switch</a></li>
<li><a href="../54708/index.html">first thoughts</a></li>
<li><a href="../54710/index.html">Five conditions of those awesome. support</a></li>
<li><a href="../54711/index.html">If you do not know where to go</a></li>
<li><a href="../54712/index.html">Cisco servers: official announcement</a></li>
<li><a href="../54714/index.html">Monetizing binary traffic?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>