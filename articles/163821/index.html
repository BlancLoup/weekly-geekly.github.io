<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>EBay data center and adiabatic wetting</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content=""The data center equipment should be operated in a temperature range of up to + 25 ¬∞ C, and cooling is preferably carried out using chillers or precis...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>EBay data center and adiabatic wetting</h1><div class="post__text post__text-html js-mediator-article">  "The data center equipment should be operated in a temperature range of up to + 25 ¬∞ C, and cooling is preferably carried out using chillers or precision air conditioners."  Until recently, thanks to the recommendations of the ASHRAE (American Society of Heating, Refrigerating and Air Conditioning Engineers, one of the most respected organizations in the field of refrigeration and air conditioning), this was an axiom.  But the cost of electricity for cooling the data center grew along with tariffs and equipment capacity, and in the end, cooling systems began to consume 35-40% of all the energy required for the operation of the data center. <br><a name="habracut"></a><br><br><h4>  Intro </h4><br>  There is a traditional approach to reducing the energy consumption of a refrigeration unit, which consists in finding more efficient refrigerants and selecting system parameters.  But this is an evolutionary development, in essence, a battle for a few percent of the increase in energy efficiency.  In this context, the complete abandonment of condensing units and the transition to the use of outdoor air can be considered a revolutionary way.  In Murmansk or Norilsk, such an approach would be fully justified.  But a data center with freecooling in a hot desert is, in the opinion of a non-expert, already from the field of unobvious marketing solutions invented for the sake of "green itch" and other phenomena that are not yet understood to us. <br><br><img src="https://habrastorage.org/storage2/a01/adb/96b/a01adb96b7740ca52991ddb3504dfc94.jpg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      It is all the more surprising that such a solution has much less to do with marketing, and such an unconventional approach is primarily due to economic and technical reasons.  The data center "Mercury" in Phoenix, Arizona, belongs to the well-known to all of us eBay.  This company chooses the location for its sites in such a way as to minimize delays for users around the world, because eBay makes deals for $ 2,000 every second, which means that it is vital for the company's services to remain available 24/7/365.  Thus, the data center in Phoenix is ‚Äã‚Äãgeographically well located.  But the climate ... Tropical desert climate, very hot summer with maxima around +50 .  It‚Äôs time to think that not every refrigerating machine can withstand work in such conditions, not to mention free-cooling.  But the initial conditions ‚Äî maximum equipment density and maximum computing power per watt of energy consumed by the data center ‚Äî left no choice: the use of traditional cooling systems would have shattered all dreams of high energy efficiency.  After a thorough analysis, Global Foundation Services specialists (eBay division) came to the conclusion that it was ‚Äúfree-cooling‚Äù that would best provide the necessary efficiency, and announced a competition to design a data center in the desert, but with year-round cooling by outside air. <br><br><h4>  Operating principle </h4><br>  Of course, you can not take a project of a conventional data center with traditional server racks and redo its cooling system under the outside air.  Equipment for which the IT standard +25 .. + 27 C is stable, simply will not withstand this transition, because ‚Äúfree cooling‚Äù, especially in a hot region, in principle cannot provide the necessary temperature.  Requires equipment capable of operating normally at higher temperatures.  And such equipment was found in the Dell line: modular data centers with Dell PowerEdge servers and rack density up to 30 kW. <br>  But what about air temperatures up to +50 degrees Celsius?  Of course, the temperature range of PowerEdge equipment is somewhat wider (up to 45 degrees for short peaks), but not to the same degree!  And it was here that a solution that was very wild for most IT specialists was applied: adiabatic humidification, or cooling using the heat of vaporization of water to extract heat from the air.  The essence is very simple: in the dry hot air (which is the air in Phoenix) droplets with a diameter of, as a rule, 0.06-0.08 mm, ordinary water is sprayed, purified from impurities.  The specific heat of water vaporization is 2260 KJ / kg, the specific heat capacity of air is 1.006 KJ / kg * ¬∞ C.  Thus, due to the evaporation of a kilogram of water, the temperature of 2,200 kilograms of air can be reduced by one degree.  In practice, the air flow temperature decreases significantly (on average by 7 degrees, it depends on a combination of factors).  The downside of this approach is the increase in humidity.  Thanks to numerous stereotypes, everyone knows that high humidity is a death for equipment, it is a failure and a premature server outage. <br>  Numerous studies of industry giants have shown that this is not the case.  Most of the equipment is able to withstand a temperature rise and an increase in air humidity without any harm to it. <br><br><img src="https://habrastorage.org/storage2/23c/798/bd4/23c798bd4e72ae0371e5701505894617.jpg"><br><br>  Specially designed for such conditions, racks and servers, of course, also tolerate high humidity.  Operational experience of the Mercury data center showed that in short hot periods the evaporation of water is able to maintain the air temperature at a level that is acceptable for the data center, while adiabatic cooling is not required at all for most of the year - there are very cold months in Phoenix.  There are no ‚Äúpeak‚Äù and duplicate systems in the data center; thus, the data center equipment is cooled using an inexpensive and highly reliable system, due to the absence of complex units. <br><br><h4>  Nuances </h4><br>  Of course, the implementation of such a system, and even in such an unconventional version, is associated with a lot of practical difficulties.  So, it is extremely important to combine the correct droplet diameter and airflow rate: if it does not fit into the ‚Äúbeauty standards‚Äù or the airflow rate is too high, it will be taken out of the space where the heat exchange takes place, and accordingly ‚Äúthe focus will not succeed‚Äù.  Water must meet fairly serious requirements for CaCO3 content and hardness (8‚Äì12 degrees of hardness, 1 degree of hardness corresponds to CaCO3 in an amount of 1 mEq / l of impurity), the pH should also not be higher than 7, otherwise the cooling system elements will be exposed corrosion.  There are less obvious difficulties: for example, what to do with water that does not evaporate, how and where to collect it? <br><br><h4>  Profit </h4><br>  However, after overcoming these difficulties, the use of such an unconventional cooling system for data centers has led to an enviable energy efficiency.  The coefficient of PUE (Power usage effectiveness, calculated as the total power of the equipment divided by the power of the IT equipment) in August day was 1.043, i.e.  auxiliary equipment, including the cooling system, consumes only about four percent of the data center energy even in summer, and in winter even less, PUE in the region of 1.018.  The efficiency of condensing systems based on chillers or DX air conditioners is significantly lower, for them PUE in the region of 1.3 is an achievement.  Even on the hottest days, the ‚Äúfree‚Äù data center cooling system allows servers to function reliably and reliably.  Remember, because the site owns eBay.  If there were any doubts about the effectiveness and stability of such a solution, the company, whose life depends on the availability of its sites, would never have done it.  But the Mercury data center with an area of ‚Äã‚Äã12,600 square meters and a capacity of 4 MW has been operating for over a year. <br>  Interestingly, such a cooling system and the placement of data processing modules on the roof of the data center make it possible not only to cool them effectively, but also to quickly increase the computing power, if necessary.  So, with the help of special cranes, one and a half thousand servers can be raised to the roof in twenty minutes.  Then they are quickly connected to electricity and water, and after an hour they are in the ranks.  The data center has the ability to rapidly expand capacity to 6 MW, as well as the necessary infrastructure to increase it to 12 MW.  12,600 square meters - quite a bit by the standards of modern data centers, but such power and density - this is serious. <br>  The use of free-cooling in conjunction with adiabatic evaporative cooling in a ‚Äúhot‚Äù data center is a bold, unconventional, non-obvious, but already proven solution. <br><br> <a href=""><img src="https://habrastorage.org/storage2/85f/631/d5d/85f631d5d8f9aaeb80a6ce558bf5c584.png"></a> <br><br>  Of course, the precision and chillers will not disappear anywhere, and the increase in the average air temperature in an air-cooled data center must be approached carefully.  But even if ASHRAE, in its recommendations of 2011, recognizes the existence of equipment classes A3 (up to 40 ¬∞ C) and A4 (up to 45 ¬∞ C), and eBay with such equipment is already in full swing, it means that neither humidity nor elevated temperature should be feared just because that they are and there is a rumor about their poor compatibility with servers.  Competently selected equipment, an efficient cooling system and well-established monitoring are all the secrets of super-efficient data centers, whose share will surely grow in the coming years, including in our country. <br><br><h4>  Native penates </h4><br>  Where are such conclusions from?  The reason is simple: FZ-261 establishes a fairly rigid framework for all serious consumers of resources, as well as serious indicators of increasing energy efficiency ‚Äî 40% by 2020.  The transition to natural refrigerants and the use of new thermostatic valves do not achieve these indicators.  In addition, it is - with a sufficiently large investment - not always any tangible savings.  But the transition to a fundamentally different paradigm of data center cooling with the help of outside air is almost the required tens of percent, and, taking into account the constantly growing energy tariffs, significant savings in the operation of the data center.  Money, the growing power of server hardware, as well as regulatory documents that, after the advent of the FZ-261, breed like mushrooms after the rain - this is what will very soon lead free-cooling to domestic data centers. </div><p>Source: <a href="https://habr.com/ru/post/163821/">https://habr.com/ru/post/163821/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../163811/index.html">How to become a puppeteer or Puppet for beginners</a></li>
<li><a href="../163813/index.html">Introduction to the development of WinRT applications in HTML / JavaScript. Styling applications</a></li>
<li><a href="../163815/index.html">New IBM programs for students and IT professionals</a></li>
<li><a href="../163817/index.html">Gold and silicon fever - what is common?</a></li>
<li><a href="../163819/index.html">Pre-training of the neural network using a limited Boltzmann machine</a></li>
<li><a href="../163825/index.html">DevCon 2013: last chance for a 25% discount</a></li>
<li><a href="../163827/index.html">Live Suggest.io search: +7 to relevance algorithms</a></li>
<li><a href="../163829/index.html">Network Rendering on Hybrid Cluster</a></li>
<li><a href="../163831/index.html">Amazing Mars</a></li>
<li><a href="../163835/index.html">CentOS 5.x and Motion: View video stream after authorization</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>