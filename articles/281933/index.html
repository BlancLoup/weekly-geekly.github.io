<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Flume - manage data streams. Part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! We continue the series of articles on Apache Flume . In the previous part, we examined this tool superficially and figured out how to config...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Flume - manage data streams. Part 2</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  We continue the series of articles on <a href="https://flume.apache.org/">Apache Flume</a> .  In the <a href="https://habrahabr.ru/company/dca/blog/280386/">previous part,</a> we examined this tool superficially and figured out how to configure and launch it.  This time the article will be devoted to the key components of Flume, with the help of which it is not terrible to manipulate already real data. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/5d2/3e9/fbc/5d23e9fbc8d54dd1a9ad28de7f3f409a.jpg"></div><a name="habracut"></a><br><h2>  File channel </h2><br>  In the last article we looked at the Memory Channel.  Obviously, the channel that uses memory to store data is not reliable.  Restarting the node will result in all the data stored in the channel being lost.  This does not make the Memory Channel useless, there are some cases where its use is very justified by virtue of speed.  However, a truly fault tolerant solution is needed for a truly reliable transport system. <br><br>  Such a solution is the file channel - File Channel.  It is easy to guess that this channel stores data in files.  At the same time, the channel uses Random Access to work with the file, allowing you to add and pick up events in this way, preserving their sequence.  For fast navigation, the channel uses a labeling system (checkpoints), with the help of which the WAL mechanism is implemented.  All this, in general, is hidden ‚Äúunder the hood‚Äù of the channel, and the following parameters are used to configure it (the bold font is the required parameters). <br><table><tbody><tr><th width="100">  Parameter </th><th>  Description </th><th width="120">  Default </th></tr><tr><td><pre>  type </pre></td><td>  Channel implementation, must be specified <i>file</i> </td><td>  - </td></tr><tr><td><pre>  checkpointDir </pre></td><td>  Folder for storing files with tags.  If not specified, the channel will use the Flume home folder. </td><td><pre>  $ HOME / ... </pre></td></tr><tr><td><pre>  useDualCheckpoints </pre></td><td>  Do backup folders with tags. </td><td><pre>  false </pre></td></tr><tr><td><pre>  backupCheckpointDir </pre></td><td>  The folder for backups of files with labels, you must specify if <i>useDualCheckpoints = true</i> (of course, it is better to keep this backup away from the original - for example, on another disk). </td><td><pre>  - </pre></td></tr><tr><td><pre>  dataDirs </pre></td><td>  Comma-separated list of folders in which data files will be placed.  It is better to specify several folders on different disks to improve performance.  If no folders are specified, the channel will also use the Flume home folder. </td><td><pre>  $ HOME / ... </pre></td></tr><tr><td><pre>  capacity </pre></td><td>  The capacity of the channel indicates the number of events. </td><td><pre>  1,000,000 </pre></td></tr><tr><td><pre>  transactionCapacity </pre></td><td>  The maximum number of events in a single transaction.  A very important parameter that may affect the performance of the entire transport system.  More on this will be written below. </td><td><pre>  10,000 </pre></td></tr><tr><td><pre>  checkpointInterval </pre></td><td>  The interval between the creation of new labels, in milliseconds.  Tags play an important role during a restart, allowing you to ‚Äújump over‚Äù portions of the data files when the channel status is restored.  As a result, the channel does not re-read the data files entirely, which significantly speeds up the launch when the channel is ‚Äúclogged‚Äù. </td><td><pre>  30,000 </pre></td></tr><tr><td><pre>  checkpointOnClose </pre></td><td>  Whether to record the label when closing the channel.  The trailing label will allow the channel to recover as soon as possible after restarting - but creating it will take some time when the channel is closed (in fact, very insignificant). </td><td><pre>  true </pre></td></tr><tr><td><pre>  keep-alive </pre></td><td>  Timeout (in seconds) for adding to the channel.  That is, if the channel is clogged, the transaction ‚Äúwill give it a chance‚Äù after waiting some time.  And if there is no free space in the channel, the transaction will be rolled back. </td><td>  3 </td></tr><tr><td><pre>  maxFileSize </pre></td><td>  Maximum file size of the channel, in bytes.  The value of this parameter does not determine how much space your channel can ‚Äúbite off‚Äù - it sets the size of a single data file, and a channel can create several of these files. </td><td>  2146435071 (2GB) </td></tr><tr><td><pre>  minimumRequiredSpace </pre></td><td>  If your disk has less free space than specified in this parameter, the channel will not accept new events.  In case data folders are located on multiple disks, Flume will use </td><td>  524288000 (500MB) <br></td></tr></tbody></table>  <a href="https://flume.apache.org/FlumeUserGuide.html">The remaining settings</a> relate to encrypting data in the channel files and the process of recovery (replay).  Now a few words about what needs to be considered when working with the file channel. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  <b>Make sure Flume has the right to write data to folders</b> . <br>  Or, to be more precise, the user, on whose behalf Flume is running, has write rights in the folders for <i>checkpoints</i> and <i>data</i> . <br></li></ul><br><ul><li>  <b>SSD significantly speeds up the channel</b> . <br>  The graph below shows the sending time of a pack of 500 events to Flume nodes using file channels.  One of the nodes uses an SSD to store channel data, the other is SATA.  The difference is significant. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d0d/32a/7e4/d0d32a7e4990413fbe85ac69af675445.png"></div><br>  If you perform a simple division, you will find that the Flume node with the file channel on the SSD can digest up to 500 / 0.025 = 20,000 events per second (for reference, the size of the messages in this example is about 1 KB, and the channel uses only one disk to store). <br></li></ul><br><ul><li>  <b>Channel capacity is very sensitive to change</b> . <br>  If you suddenly decide to change the capacity of your channel, then an unpleasant surprise may be waiting for you - the channel will launch a replay for data recovery.  This means that instead of using checkpoints files for further navigation / work with data, the channel will run through all the data files.  If there is a lot of data in the channel, the process can take a fair amount of time. <br></li></ul><br><ul><li>  <b>An abnormal stopping of the channel may result in data loss</b> . <br>  This can happen if you kill the Flume process (or hard reset).  And it may not happen.  In my memory, this happened to us only once - the data file was ‚Äúcorrupted‚Äù and we had to manually delete all the data files of the channel (fortunately, the channels were not clogged and we managed to avoid losses).  Thus, the channel still does not give 100% reliability - there is always the possibility that someone will ‚Äúpull the switch‚Äù and the irreparable will happen.  Well, if this happens and the channel refuses to start, your actions may be as follows: <br><br><ol><li>  Try deleting checkpoints files - in this case the channel will try to recover only by data files. <br><br></li><li>  If the previous item did not help and the channel writes something in the style of ‚ÄúUnable to read data from channel, channel will be closed‚Äù, then the data file is corrupted.  Only complete cleaning of all data folders will help here.  Alas. </li></ol></li></ul><br>  Alternatively, File-Channel Flume offers several more channels - in particular, the <a href="https://flume.apache.org/FlumeUserGuide.html">JDBC-channel</a> , which uses a database as its buffer, and the <a href="https://flume.apache.org/FlumeUserGuide.html">Kafka-channel</a> .  Of course, to use such channels, you need to separately deploy the database and Kafka. <br><br><h2>  Avro Source and Avro Sink </h2><br>  Avro is one of the <a href="https://avro.apache.org/docs/current/">data serialization tools</a> , thanks to which the source and the drain got their names.  Networking of these components is implemented using Netty.  Compared to Netcat Source, discussed in the previous article, Avro Source has the following advantages: <br><br><ul><li>  May use headers in events (i.e., transmit supporting information with data). <br><br></li><li>  It can receive information from several clients at the same time.  Netcat runs on a regular socket and accepts incoming connections sequentially, which means it can serve only one client at a time. </li></ul><br>  So, consider the settings that offers us Avro Source. <br><table><tbody><tr><td width="100">  <b>Parameter</b> </td><td>  <b>Description</b> </td><td width="120">  <b>Default</b> </td></tr><tr><td><pre>  type </pre></td><td>  Source implementation, <i>avro</i> must be specified. </td><td>  - </td></tr><tr><td><pre>  channels </pre></td><td>  Channels to which the source will send events (separated by spaces). </td><td>  - </td></tr><tr><td><pre>  bind </pre></td><td>  Host / IP to which we fix the source. </td><td>  - </td></tr><tr><td><pre>  port </pre></td><td>  The port on which the source will accept connections from clients. </td><td>  - </td></tr><tr><td><pre>  threads </pre></td><td>  The number of threads that handle incoming events (I / O workers).  When choosing a value, you should be guided by the number of potential customers who will send events to this source.  You need to set at least 2 streams, otherwise your source may simply ‚Äúhang‚Äù, even if the client has only one.  If you are not sure how many threads are needed - do not specify this parameter in the configuration. </td><td>  not limited </td></tr><tr><td><pre>  compression-type </pre></td><td>  Data compression, there are few options - either <i>none</i> , or <i>deflate</i> .  It is necessary to specify only if the client transmits data in a compressed form.  Compression will help you significantly save traffic, and the more events you transmit at a time, the more significant this savings will be. </td><td><pre>  none </pre></td></tr></tbody></table>  As with any other source, for Avro Source you can specify: <br><br><ol><li> <b><i>selector.type</i></b> is a channel selector, I mentioned them in the previous article.  Allow to divide or duplicate events in several channels by some rules.  More selectors will be discussed below. <br><br></li><li>  <b><i>interceptors</i></b> - list of interceptors, separated by spaces.  Interceptors are triggered BEFORE events get into the channel.  They are used to somehow modify the events (for example, add headers or change the contents of the event).  They will also be discussed below. </li></ol><br>  Also for this source you can configure Netty filters and <a href="https://flume.apache.org/FlumeUserGuide.html">data encryption settings</a> .  You can use this code to send events to this source. <br><br><div class="spoiler">  <b class="spoiler_title">Primitive Java-client for Avro Source</b> <div class="spoiler_text"><pre><code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.util.HashMap; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.util.Map; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.flume.Event; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.flume.EventDeliveryException; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.flume.api.RpcClient; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.flume.api.RpcClientFactory; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.flume.event.EventBuilder; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.flume.event.SimpleEvent; <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">FlumeSender</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(String[] args)</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">throws</span></span></span><span class="hljs-function"> EventDeliveryException </span></span>{ RpcClient avroClient = RpcClientFactory.getDefaultInstance(<span class="hljs-string"><span class="hljs-string">"127.0.0.1"</span></span>, <span class="hljs-number"><span class="hljs-number">50001</span></span>); Map&lt;String, String&gt; headers = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> HashMap&lt;&gt;(); headers.put(<span class="hljs-string"><span class="hljs-string">"type"</span></span>, <span class="hljs-string"><span class="hljs-string">"common"</span></span>); Event event = EventBuilder.withBody(<span class="hljs-string"><span class="hljs-string">" "</span></span>.getBytes(), headers); avroClient.append(event); avroClient.close(); } }</code> </pre> </div></div><br>  Now consider the configuration of Avro-drain. <br><table><tbody><tr><th width="100">  Parameter </th><th>  Description </th><th width="120">  Default </th></tr><tr><td><pre>  type </pre></td><td>  Implementation of the flow, should be specified <i>avro</i> . </td><td>  - </td></tr><tr><td><pre>  channel </pre></td><td>  The channel from which the stock will pull the event. </td><td>  - </td></tr><tr><td><pre>  hostname </pre></td><td>  The host / IP to which the stock will send events. </td><td>  - </td></tr><tr><td><pre>  port </pre></td><td>  The port on which the specified host machine is <i>listening</i> for clients. </td><td>  - </td></tr><tr><td><pre>  batch-size </pre></td><td>  <b><font color="red">A very important</font></b> parameter: the size of the "packs" of events sent to the client in one request.  At the same time, the same value is used when the channel is empty.  That is, it is also the number of events read from the channel in one transaction. </td><td><pre>  100 </pre></td></tr><tr><td><pre>  connect-timeout </pre></td><td>  Connection timeout (handshake), in milliseconds. </td><td><pre>  20,000 </pre></td></tr><tr><td><pre>  request-timeout </pre></td><td>  Request timeout (sending event bundles), in milliseconds. </td><td><pre>  20,000 </pre></td></tr><tr><td><pre>  reset-connection-interval </pre></td><td>  Interval "host change".  It is understood that several machines serviced by the balancer may be hidden behind the specified <i>hostname</i> .  This parameter forces the stock to switch between machines at a specified time interval.  Convenience, according to the creators of the runoff, is that if a new machine is added to the balancer‚Äôs area of ‚Äã‚Äãresponsibility, there is no need to restart the Flume node - the runoff will figure out that another ‚Äúdestination‚Äù has appeared.  By default, the stock does not change hosts. </td><td><pre>  -one </pre></td></tr><tr><td><pre>  maxIoWorkers </pre></td><td>  Analog <i>threads</i> for Avro Source. </td><td><pre>  2 * PROC_CORES </pre></td></tr><tr><td><pre>  compression-type </pre></td><td>  The same as for Avro Source.  The difference is that the stock compresses the data, and the source, in contrast, unpacks.  Accordingly, if Avro Sink sends events to Avro Source, the type of compression on both should be the same. </td><td><pre>  none </pre></td></tr><tr><td><pre>  compression-level </pre></td><td>  Compression level, only if <i>compression-type = deflate</i> (0 - do not compress, 9 - maximum compression). </td><td><pre>  6 </pre></td></tr></tbody></table>  Now let's talk about what is important to consider when setting up these components. <br><br><ul><li>  <font color="red"><b>Carefully choose Batch Size</b></font> . <br><br>  As I have already said, this is a very important parameter, whose ill-considered choice can significantly impair your life.  First of all, batch-size must be less than or equal to the transaction capacity of the channel (transactionCapacity).  This is clearly Avro Sink and implicitly Avro Source.  Consider an example: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/4ed/817/8e6/4ed8178e6c7d417db1b18227ae9a3ad5.png"></div><br>  Here TC is transactionCapacity, and BS is batch-size.  The condition for normal operation is that: BS &lt;= TC1 and BS &lt;= TC2.  That is, it is necessary to take into account not only the transaction capacity of the channel with which the drain works, but the transaction capacity of the channel (s) with which the receiving Avro Source works.  Otherwise, the stock will not be able to empty its channel, and the source will not add events to its own.  In such cases, Flume begins to intensively log error messages. <br><blockquote>  <font color="#999999"><i><b>A case from practice</b> .</i></font>  <font color="#999999"><i>In one of the stocks we somehow set batch-size = 10,000, while TC = 5000 was set for the channel on the receiving node. And everything worked fine.</i></font>  <font color="#999999"><i>As long as the amount of data was small, the stock simply didn‚Äôt pull 10,000 events at a time out of the channel ‚Äî the channel did not have time to accumulate so many events.</i></font>  <font color="#999999"><i>But after a while, the amount of data increased and we started having problems.</i></font>  <font color="#999999"><i>The receiving node began to reject large bursts of data.</i></font>  <font color="#999999"><i>The error was noticed in time, the parameters were changed, and the data accumulated in the channel with a mischievous stream began to reach their destination.</i></font> </blockquote></li></ul><br><ul><li>  <b>Send events in large batches</b> . <br>  Transaction - the operation is quite expensive in terms of resources.  Less transactions - more performance.  Again, compression when transmitting a large number of events works much more efficiently.  Accordingly, in addition to batch-size, you will have to increase the transactionCapacity of your channels. <br></li></ul><br><ul><li>  <b>Override the netty dependency for your nodes</b> . <br>  We use the <b>netty</b> version <b>3.10.5 Final</b> , while Flume pulls up the older <b>netty 3.6.2 Final</b> .  The problem with the old version is a small bug, due to which Avro Sink / Avro Source cannot periodically connect to each other.  This leads to the fact that in the transmission of data there are periodic idle times for a few minutes (then everything returns to normal).  In case the data should arrive as fast as possible, such traffic jams can become a problem. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a7a/b1c/9ce/a7ab1c9cec3c4a91a231fa356f4f0b45.png"></div><br>  In case you start Flume with Java tools, you can override the dependency using Maven tools.  If you configure Flume using Cloudera or as <a href="https://dpaynedudhe.wordpress.com/2015/06/16/installing-flume-on-ubuntu/">a service</a> , you will have to manually change the Netty dependency.  You can find them in the following folders: <br><br><ul><li>  Cloudera - <i>/ opt / cloudera / parcels / CDH - $ {VERSION} / lib / flume-ng / lib</i> ; </li><li>  Service (stand-aloone) - <i>$ FLUME_HOME / lib</i> . </li></ul></li></ul><br><h2>  File Roll Sink </h2><br>  So, we figured out how to configure transport nodes based on Avro Source / Sink and file channel.  It remains now to deal with the components that close (i.e., output data from the Flume area of ‚Äã‚Äãresponsibility) our transport network. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/671/365/221/6713652217eb4ee78c584099e2a6ed9f.png"></div><br>  The first closing stock worth considering is File-Roll Sink.  I would say that this is a stock for the lazy.  It supports a minimum of settings and can do only one thing - write events to files. <br><table><tbody><tr><th width="100">  Parameter </th><th>  Description </th><th width="120">  Default </th></tr><tr><td><pre>  type </pre></td><td>  The implementation of the flow must be specified <i>file_roll</i> . </td><td>  - </td></tr><tr><td><pre>  channel </pre></td><td>  The channel from which the stock will pull the event. </td><td>  - </td></tr><tr><td><pre>  directory </pre></td><td>  The folder in which the files will be stored. </td><td>  - </td></tr><tr><td><pre>  rollInterval </pre></td><td>  The interval between the creation of new files (0 - write everything in one file), in seconds. </td><td><pre>  thirty </pre></td></tr><tr><td><pre>  serializer </pre></td><td>  Serialization of events.  You can specify: TEXT, HEADER_AND_TEXT, AVRO_EVENT or your class that implements the <i>EventSerializer.Builder</i> interface. </td><td><pre>  TEXT </pre></td></tr><tr><td><pre>  batch-size </pre></td><td>  Similar to Avro Sink, the size of a bundle of events taken from a channel transaction. </td><td><pre>  100 </pre></td></tr></tbody></table><br>  Why do I consider it a drain for the lazy?  Because absolutely nothing can be configured in it.  No compression, no file name (the timestamp of the creation will be used as the name), no grouping by subfolders ‚Äî nothing.  Even the file size can not be limited.  This stock is suitable, perhaps, only for cases when ‚Äúthere is no time to explain - we need to urgently begin to receive data!‚Äù. <br><blockquote>  <font color="#999999"><i><b>Note</b></i></font>  <font color="#999999"><i>Since there is still a need to write data to files, we came to the conclusion that it is more expedient to implement our own file drain than to use this.</i></font>  <font color="#999999"><i>Given that all the Flume sources are open, it was not difficult to make it, we did it in a day.</i></font>  <font color="#999999"><i>On the second day, minor bugs were corrected - and the stock has been working for more than a year already, putting the data into folders in neat archives.</i></font>  <font color="#999999"><i>I‚Äôll post this stock on GitHub after the third part of the cycle.</i></font> </blockquote><br><h2>  HDFS Sink </h2><br>  This stock is already more serious - it supports a lot of settings.  It is a little surprising that File-Roll Sink is not made in the same way. <br><table><tbody><tr><th width="100">  Parameter </th><th>  Description </th><th width="120">  Default </th></tr><tr><td><pre>  type </pre></td><td>  Implementation of the flow, should be specified <i>hdfs</i> . </td><td>  - </td></tr><tr><td><pre>  channel </pre></td><td>  The channel from which the stock will pull the event. </td><td>  - </td></tr><tr><td><pre>  hdfs.path </pre></td><td>  The folder in which the files will be written.  Make sure that this folder has the necessary permissions.  If you configure the stock using Cloudera, the data will be written on behalf of the user <i>flume</i> . </td><td>  - </td></tr><tr><td><pre>  hdfs.filePrefix </pre></td><td>  File name prefix.  The base file name, as for File-Roll, is the timestamp of its creation.  Accordingly, if you specify <i>my-data</i> , the resulting file name will be <i>my-data1476318264182</i> . </td><td><pre>  Flumedata </pre></td></tr><tr><td><pre>  hdfs.fileSuffix </pre></td><td>  Postfix file name.  It is added to the end of the file name.  Can be used to specify an extension, for example <i>.gz</i> . </td><td>  - </td></tr><tr><td><pre>  hdfs.inUsePrefix </pre></td><td>  Similar to filePrefix, but for a temporary file to which data is still being recorded. </td><td>  - </td></tr><tr><td><pre>  hdfs.inUseSuffix </pre></td><td>  Same as fileSuffix, but for a temporary file.  In essence, a temporary extension. </td><td><pre>  .tmp </pre></td></tr><tr><td><pre>  hdfs.rollInterval </pre></td><td>  The period of creation of new files, in seconds.  If the files do not need to be closed by this criterion, set 0. </td><td><pre>  thirty </pre></td></tr><tr><td><pre>  hdfs.rollSize </pre></td><td>  Trigger to close files by volume, indicated in bytes.  We also set 0 if this criterion does not suit us. </td><td><pre>  1024 </pre></td></tr><tr><td><pre>  hdfs.rollCount </pre></td><td>  Trigger to close files by the number of events.  You can also put 0. </td><td><pre>  ten </pre></td></tr><tr><td><pre>  hdfs.idleTimeout </pre></td><td>  Trigger to close files due to inactivity, in seconds.  That is, if nothing is written to the file for a while, it closes.  This trigger is disabled by default. </td><td><pre>  0 </pre></td></tr><tr><td><pre>  hdfs.batchSize </pre></td><td>  Same as for other stocks.  Although the documentation for the drain says that this is the number of events recorded in the file before they are reset to HDFS.  When choosing, we also focus on the channel transaction volume. </td><td><pre>  100 </pre></td></tr><tr><td><pre>  hdfs.fileType </pre></td><td>  File type - <i>SequenceFile</i> (Hadoop file with key-value pairs, as a rule, the timestamp from the ‚Äútimestamp‚Äù hider is used as the key), <i>DataStream</i> (text data is, in fact, a line recording with the specified serialization, as in File- Roll Sink) or <i>CompressedStream</i> (analogue DataStream, but with compression). </td><td><pre>  Sequencefile </pre></td></tr><tr><td><pre>  hdfs.writeFormat </pre></td><td>  The recording format is <i>Text</i> or <i>Writable</i> .  Only for <i>SequenceFile</i> .  The difference is that the value will be written either as text ( <i>TextWritable</i> ) or bytes ( <i>BytesWritable</i> ). </td><td><pre>  5000 </pre></td></tr><tr><td><pre>  serializer </pre></td><td>  Configurable for <i>DataStream</i> and <i>CompressedStream</i> , similar to File-Roll Sink. </td><td><pre>  TEXT </pre></td></tr><tr><td><pre>  hdfs.codeC </pre></td><td>  This parameter must be specified if you use the file type <i>CompressedStream</i> .  The following compression options are offered: <i>gzip, bzip2, lzo, lzop, snappy</i> . </td><td>  - </td></tr><tr><td><pre>  hdfs.maxOpenFiles </pre></td><td>  The maximum number of simultaneously open files.  If this threshold is exceeded, the oldest file will be closed. </td><td><pre>  5000 </pre></td></tr><tr><td><pre>  hdfs.minBlockReplicas </pre></td><td>  <b><font color="red">An important parameter</font></b> .  Minimum number of replicas per HDFS block.  If not specified, it is taken from the Hadoop configuration specified in the classpath at startup (i.e. your cluster settings).  Honestly, I can't explain the reason for the Flume behavior associated with this parameter.  The bottom line is that if the value of this parameter is different from 1, then the drain will start closing files without regard to other triggers and in a record time will produce a lot of small files. </td><td>  - </td></tr><tr><td><pre>  hdfs.maxOpenFiles </pre></td><td>  The maximum number of simultaneously open files.  If this threshold is exceeded, the oldest file will be closed. </td><td><pre>  5000 </pre></td></tr><tr><td><pre>  hdfs.callTimeout </pre></td><td>  HDFS access timeout (open / close file, reset data), in milliseconds. </td><td><pre>  10,000 </pre></td></tr><tr><td><pre>  hdfs.closeTries </pre></td><td>  The number of attempts to close the file (if the first time did not work).  0 - try to the bitter end. </td><td><pre>  0 </pre></td></tr><tr><td><pre>  hdfs.retryInterval </pre></td><td>  How often to try to close a file in case of failure, in seconds. </td><td><pre>  180 </pre></td></tr><tr><td><pre>  hdfs.threadsPoolSize </pre></td><td>  The number of threads performing IO operations with HDFS.  If you have a ‚Äúhodgepodge‚Äù of events that are packaged in many files, then it is better to put this number more. </td><td><pre>  ten </pre></td></tr><tr><td><pre>  hdfs.rollTimerPoolSize </pre></td><td>  Unlike the previous pool, this thread pool performs tasks on a schedule (closes files).  Moreover, it works on the basis of two parameters - <i>rollInterval</i> and <i>retryInterval</i> .  Those.  this pool performs both a scheduled shutdown by trigger and periodic repeated attempts to close the file.  One thread should be enough. </td><td><pre>  one </pre></td></tr><tr><td><pre>  hdfs.useLocalTimeStamp </pre></td><td>  HDFS stock implies the use of date elements in the name of the generated files (for example, <i>hdfs.path = / logs /% Y-% m-% d</i> will allow you to group files by day).  The use of a date implies that it must be obtained from somewhere.  This parameter offers two options: use the time at the time the event is processed ( <i>true</i> ) or use the time specified in the event ‚Äî namely, in the ‚Äútimestamp‚Äù header (false).  If you use timestamp events, make sure that your events have this header.  Otherwise they will not be recorded in HDFS. </td><td><pre>  false </pre></td></tr><tr><td><pre>  hdfs.round </pre></td><td>  Round up the timestamp to some value. </td><td><pre>  false </pre></td></tr><tr><td><pre>  hdfs.roundValue </pre></td><td>  How much to round off the timestamp. </td><td><pre>  one </pre></td></tr><tr><td><pre>  hdfs.roundUnit </pre></td><td>  In what units to round ( <i>second</i> , <i>minute</i> or <i>hour</i> ). </td><td><pre>  second </pre></td></tr></tbody></table>  Here is a huge list of settings for HDFS-flow.  This drain allows you to cut the data into files almost as you like - especially nice that you can use the date elements.  The official documentation on this stock <a href="https://flume.apache.org/FlumeUserGuide.html">is on the same page</a> . <br><br>  Perhaps you noticed an interesting feature of the HDFS configuration - there is no parameter indicating the HDFS address.  The creators of the drain suggest that this drain should be used on the same machines as HDFS. <br><br>  So, what should be considered when setting up this flow. <br><br><ul><li>  <b>Use large batch-size and transactionCapacity</b> . <br>  In general, everything here is similar with other drains - the transaction is quite expensive in terms of resources, so it is better to pour large portions. <br></li></ul><br><ul><li>  <b>Do not abuse macros in file naming</b> . <br>  Using date elements in file / folder names or placeholders for headings is certainly a handy tool.  But not very fast.  It seems to me that the date creators could have made the most optimal substitution - if you look at the sources, then you will be surprised at the number of operations performed to format these lines.  Suppose we decided to set up the following folder structure: <br><blockquote><pre>  hdfs.path = / logs /% {dir}
 hdfs.filePrefix =% {src} /% Y-% m-% d /% Y-% m-% d-% H-% M-% S.% {host}.% {src} </pre></blockquote>  Here dir and src - the values ‚Äã‚Äãof the event headers with acc.  keys.  The resulting file will look like <i>/logs/web/my-source/2016-04-15/2016-04-15-12-00-00.my-host.my-source.gz</i> .  On my computer, generating this name for 1 million events takes almost <b>20 seconds</b> !  Those.  for 10,000 events, this will take about 200ms.  We conclude: if you claim to write 10,000 events per second, be prepared to give 20% of the time to generate the file name.  It's horrible.  This can be cured by taking responsibility for generating the file name on the client side.  Yes, for this you will have to write some code, but you can change the flow settings to these: <br><blockquote><pre>  hdfs.path = / logs
 hdfs.filePrefix =% {file-name} </pre></blockquote>  By transferring the generated file name in the <i>file-name</i> header, you will save resources and time.  Forming a file path using such a header no longer takes 20 seconds, but 500-600 milliseconds for 1 million events.  Ie, almost 40 times faster. <br></li></ul><br><ul><li>  <b>Combine events</b> . <br>  Another small hack that allows you to significantly improve the performance of the flow.  If you write events to the file line by line, then you can combine them on the client side.  For example, your service generates logs that should go to the same file.  So why not combine several lines into one, using \ n as a delimiter?  By itself, writing data to HDFS or a file system takes much less time than all this ‚Äúdigital bureaucracy‚Äù around data. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/7df/e41/f31/7dfe41f31e434f0e99ab3aa17b6e118b.png"></div><br>  Combining events in a ratio of at least 5 to 1, you already get a significant performance boost.  Naturally, you need to be careful here - if the events on the client are generated one at a time, then filling the buffer to combine the events may take some time.  All this time, events will be stored in memory, waiting for the formation of a group to combine.  This means that the chances of losing data increase.  Summary: <br><br><ol><li>  For small amounts of data, it is better for the client to send events to Flume one at a time - less likely to lose them. <br><br></li><li>  For large amounts of data it is preferable to use event combining  If events are generated intensively, the buffer for 5-10 events will be typed quickly enough.  At the same time you will significantly increase the productivity of wastewater. </li></ol></li></ul><br><ul><li>  <b>Deploy sewers across multiple HDFS cluster machines</b> . <br>  When configuring Flume via Cloudera, it is possible to start a separate Flume node on each node of the cluster.  And it is better to use this opportunity - since in this way the load is distributed among all the cluster machines.  However, if you are using a common configuration (i.e., the same configuration file on all machines), make sure that you do not have file name conflicts.  This can be done by using the event interceptor, which adds the host name to the headers.  Correspondingly, you will only need to specify this title in the template of the file name (see below). <br><blockquote>  <font color="#999999"><i><b>Note</b></i></font>  <font color="#999999"><i>In fact, when making such a decision, it is worth considering - after all, every stock will write uniform data to its file.</i></font>  <font color="#999999"><i>As a result, you can get a bunch of small files on HDFS.</i></font>  <font color="#999999"><i>The solution must be weighted - if the amount of data is small, then you can limit yourself to one Flume node for writing to HDFS.</i></font>  <font color="#999999"><i>This is the so-called <a href="https://flume.apache.org/FlumeUserGuide.html">data consolidation</a> - when data from multiple sources end up in a single sink.</i></font>  <font color="#999999"><i>However, if the data is "flowing river", then one node may not be enough.</i></font>  <font color="#999999"><i>In more detail about design of all transport network we will talk in the following article of this cycle.</i></font> </blockquote></li></ul><br><h2>  Event Interceptors (Flume Interceptors) </h2><br>  I mentioned these mysterious interceptors many times, now is probably the time to talk about what it is.  Interceptors are event handlers that work at the stage between receiving events at the source and sending them to the channel.  Interceptors can transform events, modify them, or filter them. <br><br>  Flume <a href="https://flume.apache.org/FlumeUserGuide.html">provides many interceptors by default</a> , allowing you to: <br><br><ul><li>  Add static headers (constants, timestamp, hostname). </li><li>  Generate random UUID in headers. </li><li>  Extract values ‚Äã‚Äãfrom the event body (by regular expressions) and use them as headers. </li><li>  Modify the contents of events (again regular expressions). </li><li>  Filter events based on content. </li></ul><br><div class="spoiler">  <b class="spoiler_title">An example configuration of various interceptors</b> <div class="spoiler_text"><pre> <code class="apache hljs"><span class="hljs-comment"><span class="hljs-comment"># ============================ Avro-   ============================ # #    Vvro- my-agent.sources.avro-source.type = avro my-agent.sources.avro-source.bind = 0.0.0.0 my-agent.sources.avro-source.port = 50001 my-agent.sources.avro-source.channels = my-agent-channel #    ,    (   ) my-agent.sources.avro-source.interceptors = ts directory host replace group-replace filter extractor # ------------------------------------------------------------------------------ # #        . #    "dir",   ‚Äî "test-folder". my-agent.sources.avro-source.interceptors.directory.type = static my-agent.sources.avro-source.interceptors.directory.key = dir my-agent.sources.avro-source.interceptors.directory.value = test-folder #      ‚Äî   (  ‚Äî false) my-agent.sources.avro-source.interceptors.directory.preserveExisting = true # ------------------------------------------------------------------------------ # #     "timestamp"       ,   my-agent.sources.avro-source.interceptors.ts.type = timestamp my-agent.sources.avro-source.interceptors.ts.preserveExisting = true # ------------------------------------------------------------------------------ # #      /IP   my-agent.sources.avro-source.interceptors.host.type = host my-agent.sources.avro-source.interceptors.host.useIP = true #   ( directory.key) my-agent.sources.avro-source.interceptors.host.hostHeader = host my-agent.sources.avro-source.interceptors.host.preserveExisting = true # ------------------------------------------------------------------------------ # #        ;    my-agent.sources.avro-source.interceptors.replace.type = search_replace my-agent.sources.avro-source.interceptors.replace.searchPattern = \t my-agent.sources.avro-source.interceptors.replace.replaceString = ; #    byte[],     (  ‚Äî UTF-8) my-agent.sources.avro-source.interceptors.replace.charset = UTF-8 # ------------------------------------------------------------------------------ # #  ""   my-agent.sources.avro-source.interceptors.group-replace.type = search_replace # ,      2014-01-20        20/01/2014 #     .  ""   4  ()    , #        my-agent.sources.avro-source.interceptors.group-replace.searchPattern = (\\d{4})-(\\d{2})-(\\d{2})(.*) my-agent.sources.avro-source.interceptors.group-replace.replaceString = $3/$2/$1$4 # ------------------------------------------------------------------------------ # # -,      my-agent.sources.avro-source.interceptors.filter.type = regex_filter my-agent.sources.avro-source.interceptors.filter.regex = error$ #  true ‚Äî   ,      , #    ‚Äî  ,      my-agent.sources.avro-source.interceptors.filter.excludeEvents = true # ------------------------------------------------------------------------------ # # ,          my-agent.sources.avro-source.interceptors.extractor.type = regex_extractor # ,    : "2016-04-15;WARINING;- " my-agent.sources.avro-source.interceptors.extractor.regex = (\\d{4}-\\d{2}-\\d{2});(.*); #   ‚Äî        , #   .     # (\\d{4}-\\d{2}-\\d{2}) -&gt; $1 -&gt; ts # (.*) -&gt; $2 -&gt; loglevel my-agent.sources.avro-source.interceptors.extractor.serializers = ts loglevel #      ,     TS my-agent.sources.avro-source.interceptors.extractor.serializers.ts.type = org.apache.flume.interceptor.RegexExtractorInterceptorMillisSerializer my-agent.sources.avro-source.interceptors.extractor.serializers.ts.name = timestamp my-agent.sources.avro-source.interceptors.extractor.serializers.ts.pattern = yyyy-MM-dd #     as is my-agent.sources.avro-source.interceptors.extractor.serializers.loglevel.name = level</span></span></code> </pre> </div></div><br>   ,  ,     . ,       . ,     Flume,        Flume ‚Äî . <br><br><h2>   (Flume Channel Selectors) </h2><br>     ,  , <a href="https://flume.apache.org/FlumeUserGuide.html">     </a> .   2  : <br><br><ol><li> <b>replicating</b> ‚Äî ,         .    Flume  .  ,     ¬´¬ª .    ,         . <br><br></li><li> <b>multiplexing</b> ‚Äî ,       .   multiplexing-         . </li></ol><br><div class="spoiler"> <b class="spoiler_title">  multiplexing-</b> <div class="spoiler_text"><pre> <code class="apache hljs"><span class="hljs-comment"><span class="hljs-comment"># ============================ Avro-   ============================ # my-source.sources.avro-source.type = avro my-source.sources.avro-source.port = 50002 my-source.sources.avro-source.bind = 127.0.0.1 my-source.sources.avro-source.channels = hdfs-channel file-roll-channel null-channel #   ‚Äî multiplexing,    # ,       ""  ""  , #         HDFS,   ‚Äî    my-source.sources.avro-source.selector.type = multiplexing #   ,      my-source.sources.avro-source.selector.header = type #  type = important,      HDFS,     my-source.sources.avro-source.selector.mapping.important = hdfs-channel file-roll-channel #  type = common,      my-source.sources.avro-source.selector.mapping.common = file-roll-channel #   type     - ,     # ( ,     memchannel  null-sink) my-source.sources.avro-source.selector.mapping.default = hdfs-null-channel</span></span></code> </pre> <br></div></div><br> <b><font color="red">    .</font></b>  ,          (,   )        . <br><br><h2>  Conclusion </h2><br>    ,             .         Flume  HDFS.           ‚Äî   2000      .        roll (¬´15m¬ª  ¬´60m¬ª), dir  sr ‚Äî       . <br><br><div class="spoiler"> <b class="spoiler_title"> Flume  HDFS</b> <div class="spoiler_text"><pre> <code class="apache hljs"><span class="hljs-attribute"><span class="hljs-attribute">flume</span></span>-hdfs.sources = hdfs-source flume-hdfs.channels = hdfs-15m-channel hdfs-60m-channel hdfs-null-channel flume-hdfs.sinks = hdfs-15m-sink hdfs-60m-sink # =========== Avro-,      host ============ # flume-hdfs.sources.hdfs-source.type = avro flume-hdfs.sources.hdfs-source.port = 50002 flume-hdfs.sources.hdfs-source.bind = 0.0.0.0 flume-hdfs.sources.hdfs-source.interceptors = hostname flume-hdfs.sources.hdfs-source.interceptors.hostname.type = host flume-hdfs.sources.hdfs-source.interceptors.hostname.hostHeader = host flume-hdfs.sources.hdfs-source.channels = hdfs-null-channel hdfs-15m-channel flume-hdfs.sources.hdfs-source.selector.type = multiplexing flume-hdfs.sources.hdfs-source.selector.header = roll flume-hdfs.sources.hdfs-source.selector.mapping.15m = hdfs-15m-channel flume-hdfs.sources.hdfs-source.selector.mapping.60m = hdfs-60m-channel flume-hdfs.sources.hdfs-source.selector.mapping.default = hdfs-null-channel # ============================  , 15  ============================ # flume-hdfs.channels.hdfs-15m-channel.type = file flume-hdfs.channels.hdfs-15m-channel.maxFileSize = 1073741824 flume-hdfs.channels.hdfs-15m-channel.capacity = 10000000 flume-hdfs.channels.hdfs-15m-channel.transactionCapacity = 10000 flume-hdfs.channels.hdfs-15m-channel.dataDirs = /flume/flume-hdfs/hdfs-60m-channel/data1,/flume/flume-hdfs/hdfs-60m-channel/data2 flume-hdfs.channels.hdfs-15m-channel.checkpointDir = /flume/flume-hdfs/hdfs-15m-channel/checkpoint # ============================  , 60  ============================ # flume-hdfs.channels.hdfs-60m-channel.type = file flume-hdfs.channels.hdfs-60m-channel.maxFileSize = 1073741824 flume-hdfs.channels.hdfs-60m-channel.capacity = 10000000 flume-hdfs.channels.hdfs-60m-channel.transactionCapacity = 10000 flume-hdfs.channels.hdfs-60m-channel.dataDirs =/flume/flume-hdfs/hdfs-60m-channel/data1,/flume/flume-hdfs/hdfs-60m-channel/data2 flume-hdfs.channels.hdfs-60m-channel.checkpointDir = /flume/flume-hdfs/hdfs-60m-channel/checkpoint # ===========   ,   15  (5 . ) =========== # flume-hdfs.sinks.hdfs-15m-sink.type = hdfs flume-hdfs.sinks.hdfs-15m-sink.channel = hdfs-15m-channel flume-hdfs.sinks.hdfs-15m-sink.hdfs.filePrefix = <span class="hljs-variable"><span class="hljs-variable">%{src}</span></span>/%Y-%m-%d/%Y-%m-%d-%H-%M-%S.<span class="hljs-variable"><span class="hljs-variable">%{src}</span></span>.<span class="hljs-variable"><span class="hljs-variable">%{host}</span></span>.log flume-hdfs.sinks.hdfs-15m-sink.hdfs.path = /logs/<span class="hljs-variable"><span class="hljs-variable">%{dir}</span></span> flume-hdfs.sinks.hdfs-15m-sink.hdfs.fileSuffix = .gz flume-hdfs.sinks.hdfs-15m-sink.hdfs.writeFormat = Text flume-hdfs.sinks.hdfs-15m-sink.hdfs.codeC = gzip flume-hdfs.sinks.hdfs-15m-sink.hdfs.fileType = CompressedStream flume-hdfs.sinks.hdfs-15m-sink.hdfs.minBlockReplicas = 1 flume-hdfs.sinks.hdfs-15m-sink.hdfs.rollInterval = 0 flume-hdfs.sinks.hdfs-15m-sink.hdfs.rollSize = 0 flume-hdfs.sinks.hdfs-15m-sink.hdfs.rollCount = 0 flume-hdfs.sinks.hdfs-15m-sink.hdfs.idleTimeout = 300 flume-hdfs.sinks.hdfs-15m-sink.hdfs.round = true flume-hdfs.sinks.hdfs-15m-sink.hdfs.roundValue = 15 flume-hdfs.sinks.hdfs-15m-sink.hdfs.roundUnit = minute flume-hdfs.sinks.hdfs-15m-sink.hdfs.threadsPoolSize = 8 flume-hdfs.sinks.hdfs-15m-sink.hdfs.batchSize = 10000 # ===========   ,   60  (20 . ) =========== # flume-hdfs.sinks.hdfs-60m-sink.type = hdfs flume-hdfs.sinks.hdfs-60m-sink.channel = hdfs-60m-channel flume-hdfs.sinks.hdfs-60m-sink.hdfs.filePrefix = <span class="hljs-variable"><span class="hljs-variable">%{src}</span></span>/%Y-%m-%d/%Y-%m-%d-%H-%M-%S.<span class="hljs-variable"><span class="hljs-variable">%{src}</span></span>.<span class="hljs-variable"><span class="hljs-variable">%{host}</span></span>.log flume-hdfs.sinks.hdfs-60m-sink.hdfs.path = /logs/<span class="hljs-variable"><span class="hljs-variable">%{dir}</span></span> flume-hdfs.sinks.hdfs-60m-sink.hdfs.fileSuffix = .gz flume-hdfs.sinks.hdfs-60m-sink.hdfs.writeFormat = Text flume-hdfs.sinks.hdfs-60m-sink.hdfs.codeC = gzip flume-hdfs.sinks.hdfs-60m-sink.hdfs.fileType = CompressedStream flume-hdfs.sinks.hdfs-60m-sink.hdfs.minBlockReplicas = 1 flume-hdfs.sinks.hdfs-60m-sink.hdfs.rollInterval = 0 flume-hdfs.sinks.hdfs-60m-sink.hdfs.rollSize = 0 flume-hdfs.sinks.hdfs-60m-sink.hdfs.rollCount = 0 flume-hdfs.sinks.hdfs-60m-sink.hdfs.idleTimeout = 1200 flume-hdfs.sinks.hdfs-60m-sink.hdfs.round = true flume-hdfs.sinks.hdfs-60m-sink.hdfs.roundValue = 60 flume-hdfs.sinks.hdfs-60m-sink.hdfs.roundUnit = minute flume-hdfs.sinks.hdfs-60m-sink.hdfs.threadsPoolSize = 8 flume-hdfs.sinks.hdfs-60m-sink.hdfs.batchSize = 10000 # ================ NULL- +     =============== # flume-hdfs.channels.hdfs-null-channel.type = memory flume-hdfs.channels.hdfs-null-channel.capacity = 30000 flume-hdfs.channels.hdfs-null-channel.transactionCapacity = 10000 flume-hdfs.channels.hdfs-null-channel.byteCapacityBufferPercentage = 20 flume-hdfs.sinks.hdfs-null-sink.channel = hdfs-null-channel flume-hdfs.sinks.hdfs-null-sink.type = null</code> </pre></div></div><br>  ,   ,  : <br><br><ul><li>        Flume. </li><li>    . </li><li>   ,      . </li></ul></div><p>Source: <a href="https://habr.com/ru/post/281933/">https://habr.com/ru/post/281933/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../281917/index.html">The story of my participation in the Ubuntu Scope Showdown 2016</a></li>
<li><a href="../281919/index.html">Osliki.Net - social delivery</a></li>
<li><a href="../281925/index.html">Bitrix gives knowledge - all #FailOverConf materials</a></li>
<li><a href="../281929/index.html">The speed of dynamic RAM and the ridiculous idea how to increase it</a></li>
<li><a href="../281931/index.html">Electron, SASS, TypeScript, Pug (Jade), Polymer and a bit of flight cycling to create an application for Windows</a></li>
<li><a href="../281935/index.html">April 20 from 10 to 18 hours (MCK) Q & A on Windows Server, Windows 10 and Microsoft MVP Cloud and Microsoft employees</a></li>
<li><a href="../281937/index.html">Standard for managing access rights to corporate file information resources</a></li>
<li><a href="../281941/index.html">35 useful virtualization tools</a></li>
<li><a href="../281943/index.html">7 online scanners to search for open ports on the server</a></li>
<li><a href="../281945/index.html">Experience in ensuring the reliability of computing equipment during long-term operation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>