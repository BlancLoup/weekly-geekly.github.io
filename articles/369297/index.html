<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>SkyNet will not work. Google DeepMind is developing a "big red button" to turn off artificial intelligence.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="If something goes wrong, artificial intelligence can be quickly neutralized. 


 What will it be, AI? Depends on us people 

 Computer systems are bec...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>SkyNet will not work. Google DeepMind is developing a "big red button" to turn off artificial intelligence.</h1><div class="post__text post__text-html js-mediator-article">  <b>If something goes wrong, artificial intelligence can be quickly neutralized.</b> <br><br><img src="https://habrastorage.org/files/e2c/223/58a/e2c22358a90246328968d92e2d38ee73.jpg"><br>  <i>What will it be, AI?</i>  <i>Depends on us people</i> <br><br>  Computer systems are becoming more productive and "smart" every year.  The progress in the development of artificial intelligence (or its elements, to be more precise) involved both individual scientists and major corporations such as Google, Facebook, Microsoft and others.  Computers do show significant results.  Even in a game that is so difficult for machine intelligence as GO, the computer has already begun to defeat famous champions ( <a href="https://geektimes.ru/post/272782/">Lee Sedol's series of games with DeepMind is</a> direct evidence of this). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Many bright minds of our time support the idea of ‚Äã‚Äãcreating AI, but fears are expressed in terms of "birth."  artificial intelligence, believing that it can harm a person in a certain way.  Last year, businessmen, scientists, experts in the field of robotics and other fields signed <a href="http://futureoflife.org/AI/open_letter_autonomous_weapons">an open letter</a> , which warns weapons manufacturers from developing combat systems with 100% autonomy.  Among the signatories are Ilon Musk, Stephen Hawking, Noam Chomsky, Steve Wozniak. <br><a name="habracut"></a><br>  Currently, AI exists only in its weak form, so to speak.  This means that computer systems can solve complex tasks like translating a voice into text or <a href="https://geektimes.ru/post/274428/">converting images</a> .  A strong AI is already a computer system capable of understanding the information with which it works.  The second category of AI does not exist yet, but it is she who is the subject of fierce discussions of scientists and entrepreneurs.  How to stop a strong AI, if he suddenly starts to harm a person?  The answer to this question is trying to find a team of researchers from the University of Oxford and the laboratory DeepMind. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/a35/c62/1fd/a35c621fdbf44db9173e25e5383fad5a.jpg"><br><br>  Representatives of the project believe that it is unlikely that a strong AI will behave all the time as intended.  ‚ÄúIf such an agent (AI, - Ed.) Will work in real time under the control of a person, from time to time a human operator will need a big red button to stop agent‚Äôs actions dangerous for the agent or his environment,‚Äù says project description. <br><br>  At the same time, experts believe that a self-learning computer system may eventually detect a certain mechanism for bypassing or even ignoring the stop operator command.  To do this, scientists say, you need to create a framework that simply does not allow the AI ‚Äã‚Äãagent to block the commands of the human operator. <br><br>  Such a ‚Äúbig red button‚Äù should be a guarantee of the possibility of interrupting the current work of a computer system, as well as to protect him and others in a dangerous situation and prevent actions that could lead to irreversible consequences. <br><br>  The collaboration between the Future of Humanity Institute and DeepMind is a remarkable fact.  The fact is that DeepMind solves the problem of creating AI, and the Institute of the Future of Humanity is trying to identify problems that threaten the existence of our nation, and look for solutions to such problems.  Director of the Institute Nick Bostrom considers uncontrolled AI to be quite dangerous: ‚ÄúMan is the most unreliable system.  Today, hackers often turn to the principles of social engineering in order to gain access to someone else's computer.  And if the hacker-manipulator turns out to be a super-intellect, then we can assume that he will easily find an accomplice for himself, or he will simply use us against our will as his arms and legs. ‚Äù <br><br>  Nick Bostrom also believes that it is very important to create not only a self-learning system, but also a system that can improve on its own: ‚ÄúIt is important for us to create artificial intelligence that is smart enough to learn from our mistakes.  He will be able to perfect himself infinitely.  The first version will be able to create the second, which will be better, and the second, being smarter than the original, will create an even more advanced third, and so on.  Under certain conditions, such a process of self-improvement can be repeated until an intellectual explosion is reached - the moment when the intellectual level of the system jumps in a short time from a relatively modest level to the level of super-intelligence. ‚Äù <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/586/01c/2a0/58601c2a00ccd21300e32bbb16225807.jpg"><br><br>  The team in question believes that some self-learning systems can be stopped without problems.  This is, for example, a <a href="http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html">Q-learning algorithm</a> .  But <a href="http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html">Sarsa-</a> type algorithms cannot be simply ‚Äúslowed down‚Äù in the form in which they exist now.  But if we supplement Q-learning with a number of improvements, then the ‚Äúbig red button‚Äù can be used here without any problems.  Another important issue is the regular interruption of the AI.  For example, AI can be turned off every night at 2 am for 1 hour.  Here, scientists also consider it important not only to prevent AI resistance, but also to plan everything so that such an interruption of the agent‚Äôs work does not affect the performance of certain tasks.  The results of their research and thought, scientists have <a href="http://intelligence.org/files/Interruptibility.pdf">published on the web</a> . <br><br>  The essence of the work of specialists can be described with another quotation from Nick Bostrom: ‚ÄúIf an intellectual explosion threatens us with extinction, then we need to understand whether we can control the detonation process.  Today, it would be more reasonable to speed up work on solving the problem of control, rather than suspend the conduct of research in the field of artificial intelligence.  But so far, six people are involved in solving the problem of control, while dozens, if not hundreds of thousands, are working on creating artificial intelligence. ‚Äù <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/c88/a4d/323/c88a4d323886bbc8c22d3f5286d7b1ab.jpg"><br><br>  Well, the danger of AI for a man was perfectly described by science fiction writer Frederick Brown in his story ‚ÄúThe Answer‚Äù.  Here we are talking about the unification of computer systems of millions of planets of the Galaxy, and the subsequent birth of artificial intelligence.  The first answer of the AI ‚Äã‚Äãto the question of a man put all the dots above "and": <br><br>  <i>- Is there a god?</i> <i><br></i>  <i>A mighty voice rang out immediately.</i> <i><br></i>  <i>- YES.</i>  <i>NOW GOD IS!</i> <i><br></i>  <i>Dwar Ev did not understand right away, but then fear distorted his face - he rushed to the switch ...</i> <i><br></i>  <i>Lightning fell from the cloudless sky and incinerated him on the spot, tightly soldering the connection.</i> <br><br>  The story was published by the author in 1954. </div><p>Source: <a href="https://habr.com/ru/post/369297/">https://habr.com/ru/post/369297/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../369287/index.html">Andy Weir, author of The Martian, writes a book about a woman on the moon</a></li>
<li><a href="../369289/index.html">Ask Ethan # 31: Why are we made of matter?</a></li>
<li><a href="../369291/index.html">Where are the autopilots going?</a></li>
<li><a href="../369293/index.html">DEXP Ixion ML145 Snatch SE: Junior Bro with Giant Battery</a></li>
<li><a href="../369295/index.html">State Duma: all drones heavier than 250 grams need to register</a></li>
<li><a href="../369299/index.html">Vkontakte: there was no hacking of the resource, the old database is sold on the network</a></li>
<li><a href="../369301/index.html">Xiaomi Router R2D with a disk per terabyte</a></li>
<li><a href="../369303/index.html">Home weather station on esp8266</a></li>
<li><a href="../369305/index.html">Intel¬Æ Atom ‚Ñ¢ X3 Features: The Irbis TZ94 Example</a></li>
<li><a href="../369307/index.html">Unique children's prostheses with mount for air battles</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>