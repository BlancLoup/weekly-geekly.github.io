<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>A simple introduction to the ALU for neural networks: an explanation, physical meaning and implementation</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recently, researchers from Google DeepMind, including a renowned scientist in the field of artificial intelligence, the author of ‚Äú Understanding Deep...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>A simple introduction to the ALU for neural networks: an explanation, physical meaning and implementation</h1><div class="post__text post__text-html js-mediator-article">  Recently, researchers from Google DeepMind, including a renowned scientist in the field of artificial intelligence, the author of ‚Äú <a href="https://www.manning.com/books/grokking-deep-learning/">Understanding Deep Learning,</a> ‚Äù Andrew Trask, published an impressive article that describes a neural network model for extrapolating the values ‚Äã‚Äãof simple and complex numerical functions with a high degree of accuracy. <br><br>  In this post I will explain the architecture of <abbr title="Neural arithmetic logic unit">NALU</abbr> (neural <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D1%2580%25D0%25B8%25D1%2584%25D0%25BC%25D0%25B5%25D1%2582%25D0%25B8%25D0%25BA%25D0%25BE-%25D0%25BB%25D0%25BE%25D0%25B3%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B5_%25D1%2583%25D1%2581%25D1%2582%25D1%2580%25D0%25BE%25D0%25B9%25D1%2581%25D1%2582%25D0%25B2%25D0%25BE">arithmetic logic devices</a> , NALU), their components and significant differences from traditional neural networks.  The main goal of this article is to simply and intuitively explain <abbr title="Neural arithmetic logic unit">NALU</abbr> (and the implementation, and idea) for scientists, programmers and students who are not familiar with neural networks and deep learning. <br><br>  <b>Note from the author</b> : I also highly recommend reading the <a href="https://arxiv.org/pdf/1808.00508/">original article</a> for a more detailed study of the topic. <br><a name="habracut"></a><br><h2>  When are neural networks wrong? </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/6ba/260/98c/6ba26098c22a26f200ac6c7c9abce91d.jpg" alt="Classic neural network"><br>  <i>Image taken from <a href="https://towardsdatascience.com/machine-learning-fundamentals-ii-neural-networks-f1e7b2cb3eef">this article.</a></i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In theory, neural networks should approximate functions well.  They are almost always able to identify significant matches between input data (factors or features) and output (labels or targets).  That is why neural networks are used in many areas, from object recognition and classification to speech-to-text translation and the implementation of game algorithms that can beat world champions.  Many different models have already been created: convolutional and recurrent neural networks, auto-encoders, etc. Successes in creating new models of neural networks and in-depth learning are themselves a big topic to learn. <br><br>  However, according to the authors of the article, neural networks do not always cope with tasks that seem obvious to people and even to <abbr title="Reference [7] from the original article: C. Randy Gallistel. Finding numbers in the brain. Philosophical Transactions of the Royal Society B, 373, 2017.">bees</abbr> !  For example, it is a verbal account or operations with numbers, as well as the ability to detect dependence from relationships.  The article showed that the standard models of neural networks do not even cope with the <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25BE%25D0%25B6%25D0%25B4%25D0%25B5%25D1%2581%25D1%2582%25D0%25B2%25D0%25B5%25D0%25BD%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BE%25D1%2582%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D0%25B6%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">identical mapping</a> (a function that translates the argument into itself, <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mi>x</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.846ex" height="2.66ex" viewBox="0 -832 3808.6 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-29" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-3D" x="2179" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-78" x="3236" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> f (x) = x </script>  ) - the most obvious numerical value.  The figure below shows the <abbr title="mean square error">MSE of</abbr> various models of neural networks when learning the values ‚Äã‚Äãof this function. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f1e/e09/8e7/f1ee098e705e22745970e40e13782ef0.png" alt="Mean square error for standard neural networks"><br>  <i>The figure shows the root mean square error for standard neural networks using the same architecture and different (nonlinear) activation functions in the inner layers</i> <br><br><h2>  Why are neural networks wrong? </h2><br>  As can be seen from the figure, the main cause of misses is the <b>nonlinearity of the activation functions</b> on the inner layers of the neural network.  This approach works great for defining non-linear relationships between input data and responses, but it terribly makes mistakes when going beyond the data on which the network has learned.  Thus, neural networks do an excellent job of <b>memorizing</b> numerical dependencies from the training data, but they do not know how to extrapolate it. <br><br>  This is similar to cramming an answer or topic before an exam without understanding the essence of the subject being studied.  It is easy to pass the test, if the questions are similar to homework, but if it is the understanding of the subject that is checked, and not the ability to memorize, we will fail. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/888/a65/e3b/888a65e3b7a49c678be14c5e2b16d6d4.jpg" alt="Harry Potter"><br>  <i>This was not in the course program!</i> <br><br>  The degree of error is directly related to the level of nonlinearity of the selected activation function.  The previous diagram clearly shows that nonlinear functions with hard constraints, such as sigmoid ( <b>Sigmoid</b> ) or hyperbolic tangent ( <b>Tanh</b> ), cope with the task of generalizing dependencies much worse than soft constraints, such as a truncated linear transformation ( <b>ELU</b> , <b>PReLU</b> ). <br><br><h2>  Solution: Neural Battery (NAC) </h2><br>  The Neural Battery ( <abbr title="Neuron Battery">NAC</abbr> ) is at the core of the <abbr title="Neural arithmetic logic unit">NALU</abbr> model.  This is a simple but effective part of the neural network that handles <b>addition and subtraction</b> , which is necessary for efficiently calculating linear connections. <br><br>  <abbr title="Neuron Battery">NAC</abbr> is a special linear layer of the neural network, on whose weights a simple condition is imposed: they can take only 3 values ‚Äã‚Äã- <b>1, 0 or -1</b> .  Such restrictions do not allow the battery to change the range of input data values, and it remains constant across all layers of the network, regardless of their number and connections.  Thus, the output is a <b>linear combination of the</b> values ‚Äã‚Äãof the input vector, which can easily represent addition and subtraction operations. <br><br>  <b>Thoughts out loud</b> : for a better understanding of this statement, let us consider an example of constructing layers of a neural network that perform linear arithmetic operations on input data. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/851/d6a/ac8/851d6aac8016eae4df4c594cfa2fb517.jpg" alt="Linear extrapolation in the neural network"><br>  <i>The figure explains how the layers of the neural network without adding a constant and with possible weights of -1, 0 or 1, can perform a linear extrapolation</i> <br><br>  As shown above in the image of the layers, the neural network can learn to extrapolate the values ‚Äã‚Äãof such simple arithmetic functions as addition and subtraction ( <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.862ex" height="2.178ex" viewBox="0 -676.4 5107.3 937.7" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-79" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-3D" x="775" y="0"></use><g transform="translate(1831,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-31" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-2B" x="3080" y="0"></use><g transform="translate(4080,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-32" x="809" y="-213"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>x</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-2"> y = x_1 + x_2 </script>  and <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>&amp;#x2212;</mo><msub><mi>x</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.862ex" height="2.178ex" viewBox="0 -676.4 5107.3 937.7" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-79" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-3D" x="775" y="0"></use><g transform="translate(1831,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-31" x="809" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-2212" x="3080" y="0"></use><g transform="translate(4080,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-32" x="809" y="-213"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>‚àí</mo><msub><mi>x</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-3"> y = x_1 - x_2 </script>  ), with the help of weight limits possible values ‚Äã‚Äãof 1, 0 and -1. <br><br>  <b>Note: The NAC layer in this case does not contain a free term (constant) and does not apply non-linear transformations to the data.</b> <br><br>  Since standard neural networks do not cope with the solution of the problem with such restrictions, the authors of the article offer a very useful formula for calculating such parameters through classical (unlimited) parameters <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ hat {W} </script>  and <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-5"> \ hat {M} </script>  .  These weights, like all parameters of neural networks, can be initialized randomly and selected in the process of network training.  Formula to calculate the vector <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.435ex" height="2.057ex" viewBox="0 -780.1 1048.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-57" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-6"> W </script>  through <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-7"> \ hat {W} </script>  and <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-8"> \ hat {M} </script>  looks like that: <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>W</mi><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy=&quot;false&quot;>(</mo><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow><mo stretchy=&quot;false&quot;>)</mo><mtext>&amp;#xA0;</mtext><mi>o</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi><mo stretchy=&quot;false&quot;>(</mo><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="38.562ex" height="2.66ex" viewBox="0 -832 16603.1 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-57" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-3D" x="1326" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="2382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-61" x="2744" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6E" x="3273" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-68" x="3874" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-28" x="4450" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-68" x="5090" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-61" x="5666" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="6196" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-57" x="6557" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-29" x="7606" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6F" x="8245" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-64" x="8731" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6F" x="9254" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="9740" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-73" x="10351" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-69" x="10821" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-67" x="11166" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6D" x="11647" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-61" x="12525" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-28" x="13055" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-68" x="13694" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-61" x="14271" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="14800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-4D" x="15162" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-29" x="16213" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>W</mi><mo>=</mo><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo stretchy="false">(</mo><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow><mo stretchy="false">)</mo><mtext>&nbsp;</mtext><mi>o</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi><mo stretchy="false">(</mo><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-9"> W = tanh (\ hat {W}) \ odot \ sigma (\ hat {M}) </script></p>  <i>The <a href="">formula</a> uses the elementwise product of matrices.</i> <br><br>  The use of this formula <b>guarantees the</b> boundedness of the range of values ‚Äã‚Äãof W by the segment [-1, 1], which is closer to the set of -1, 0, 1. Also, the functions from this equation are <b>differentiable</b> by weighting parameters.  Thus, our <abbr title="Neuron Battery">NAC</abbr> layer will be easier to learn the values <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.435ex" height="2.057ex" viewBox="0 -780.1 1048.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-57" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-10"> W </script>  using <b>gradient descent and back propagation of error</b> .  Below is a diagram with the architecture of the <abbr title="Neuron Battery">NAC</abbr> layer. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eea/018/509/eea018509b6580a9364a7e7e91bff537.png" alt="Neural Battery Architecture"><br>  <i>Architecture of neural accumulator for learning on elementary (linear) arithmetic functions</i> <br><br><h2>  Implementing NAC in Python using Tensorflow </h2><br>  As we have already understood, <abbr title="Neuron Battery">NAC</abbr> is a fairly simple neural network (network layer) with small features.  Below is the implementation of a single <abbr title="Neuron Battery">NAC</abbr> layer in Python using the Tensoflow and NumPy libraries. <br><br><div class="spoiler">  <b class="spoiler_title">Python code</b> <div class="spoiler_text"><pre><code class="hljs haskell"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf #    (<span class="hljs-type"><span class="hljs-type">NAC</span></span>)  / # -&gt;     / def nac_simple_single_layer(<span class="hljs-title"><span class="hljs-title">x_in</span></span>, <span class="hljs-title"><span class="hljs-title">out_units</span></span>): '''  : x_in -&gt;   X out_units -&gt;     : y_out -&gt;     W -&gt;     ''' #       in_features = x_in.shape[1] #  W_hat  M_hat W_hat = tf.get_variable(<span class="hljs-title"><span class="hljs-title">shape</span></span>=[<span class="hljs-title"><span class="hljs-title">in_shape</span></span>, <span class="hljs-title"><span class="hljs-title">out_units</span></span>], <span class="hljs-title"><span class="hljs-title">initializer</span></span>=<span class="hljs-title"><span class="hljs-title">tf</span></span>.<span class="hljs-title"><span class="hljs-title">initializers</span></span>.<span class="hljs-title"><span class="hljs-title">random_uniform</span></span>(<span class="hljs-title"><span class="hljs-title">minval</span></span>=-2, <span class="hljs-title"><span class="hljs-title">maxval</span></span>=2), trainable=True, name='W_hat') M_hat = tf.get_variable(<span class="hljs-title"><span class="hljs-title">shape</span></span>=[<span class="hljs-title"><span class="hljs-title">in_shape</span></span>, <span class="hljs-title"><span class="hljs-title">out_units</span></span>], <span class="hljs-title"><span class="hljs-title">initializer</span></span>=<span class="hljs-title"><span class="hljs-title">tf</span></span>.<span class="hljs-title"><span class="hljs-title">initializers</span></span>.<span class="hljs-title"><span class="hljs-title">random_uniform</span></span>(<span class="hljs-title"><span class="hljs-title">minval</span></span>=-2, <span class="hljs-title"><span class="hljs-title">maxval</span></span>=2), trainable=True, name='M_hat') #  W   W = tf.nn.tanh(<span class="hljs-type"><span class="hljs-type">W_hat</span></span>) * tf.nn.sigmoid(<span class="hljs-type"><span class="hljs-type">M_hat</span></span>) y_out = tf.matmul(<span class="hljs-title"><span class="hljs-title">x_in</span></span>, <span class="hljs-type"><span class="hljs-type">W</span></span>) return y_out, W</code> </pre> </div></div><br>  In the given code <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-11"> \ hat {W} </script>  and <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-12"> \ hat {M} </script>  are initialized using a uniform distribution, but you can use <b>any</b> recommended method of generating an initial approximation for these parameters.  You can see the full version of the code in my <a href="https://github.com/faizan2786/nalu_implementation">GitHub repository</a> (the link is duplicated at the end of the post). <br><br><h2>  Moving on: from addition and subtraction to NAC for complex arithmetic expressions </h2><br>  Although the simple model of a neural network described above copes with simple operations like addition and subtraction, we need to be able to learn on the set of values ‚Äã‚Äãof more complex functions, such as multiplication, division and exponentiation. <br><br>  The following is a modified <abbr title="Neuron Battery">NAC</abbr> architecture that is adapted for selecting more <b>complex arithmetic operations</b> through <b>logarithmization and taking exponents</b> within the model.  Note the differences in this <abbr title="Neuron Battery">NAC</abbr> implementation from those already discussed above. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d75/2aa/629/d752aa629eae69110dc33e828cd98430.png" alt="image"><br>  <i><abbr title="Neuron Battery">NAC</abbr> architecture for more complex arithmetic operations</i> <br><br>  As can be seen from the figure, we logarithm the input data before multiplying by the weights matrix, and then we calculate the exponent of the result obtained.  The formula for calculations is as follows: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>Y</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mtext>&amp;#xA0;</mtext><mi>b</mi><mi>u</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>t</mi><mo stretchy=&quot;false&quot;>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo stretchy=&quot;false&quot;>|</mo></mrow><mo>+</mo><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="38.878ex" height="2.66ex" viewBox="0 -832 16739 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-59" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-3D" x="1041" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-65" x="2097" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-78" x="2564" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-70" x="3136" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-28" x="3640" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-57" x="4029" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-62" x="5328" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-75" x="5757" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6C" x="6330" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6C" x="6628" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-65" x="6927" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="7393" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-28" x="7755" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6C" x="8144" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6F" x="8443" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-67" x="8928" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-28" x="9409" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-7C" x="9798" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-78" x="10077" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-7C" x="10649" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-2B" x="11150" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-65" x="12401" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-70" x="12867" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-73" x="13371" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-69" x="13840" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6C" x="14186" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6F" x="14484" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6E" x="14970" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-29" x="15570" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-29" x="15960" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-29" x="16349" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>Y</mi><mo>=</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>W</mi><mtext>&nbsp;</mtext><mi>b</mi><mi>u</mi><mi>l</mi><mi>l</mi><mi>e</mi><mi>t</mi><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mo stretchy="false">|</mo></mrow><mo>+</mo><mtext>&nbsp;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-13"> Y = exp (W \ bullet (log (| x | + \ epsilon))) </script></p>  <i><a href="">The output formula</a> for the second version of <abbr title="Neuron Battery">NAC</abbr> .</i> <math> </math><i><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="7.942ex" height="2.419ex" viewBox="0 -780.1 3419.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-65" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-70" x="716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-73" x="1220" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-69" x="1689" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6C" x="2035" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6F" x="2333" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-6E" x="2819" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-14"> \ epsilon </script></i>   <i>here is a very small number to prevent log (0) situations during training</i> <br><br>  Thus, for both <abbr title="Neuron Battery">NAC</abbr> models, the principle of operation, including the calculation of the weights matrix with constraints <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>W</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.435ex" height="2.057ex" viewBox="0 -780.1 1048.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-57" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-15"> W </script>  through <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>W</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.424ex" height="2.057ex" viewBox="0 -780.1 2766 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-57" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>W</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-16"> \ hat {W} </script>  and <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>M</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.431ex" height="2.057ex" viewBox="0 -780.1 2769 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-68" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-61" x="826" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-74" x="1356" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-4D" x="1717" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>h</mi><mi>a</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mi>M</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-17"> \ hat {M} </script>  , does not change.  The only difference is the use of logarithmic operations on the input and output in the second case. <br><br><h2>  Second version of NAC in Python using Tensorflow </h2><br>  The code, like the architecture, is almost unchanged, with the exception of these improvements in the calculations of the tensor of output values. <br><br><div class="spoiler">  <b class="spoiler_title">Python code</b> <div class="spoiler_text"><pre> <code class="hljs pgsql">#    (NAC)     # -&gt;      ,   ,      def nac_complex_single_layer(x_in, out_units, epsilon=<span class="hljs-number"><span class="hljs-number">0.000001</span></span>): <span class="hljs-string"><span class="hljs-string">''' :param x_in:   X :param out_units:    :param epsilon:    (,    log(0)   ) :return m:     :return W:     '''</span></span> in_features = x_in.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] W_hat = tf.get_variable(shape=[in_shape, out_units], initializer=tf.initializers.random_uniform(minval=<span class="hljs-number"><span class="hljs-number">-2</span></span>, maxval=<span class="hljs-number"><span class="hljs-number">2</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, <span class="hljs-type"><span class="hljs-type">name</span></span>="W_hat") M_hat = tf.get_variable(shape=[in_shape, out_units], initializer=tf.initializers.random_uniform(minval=<span class="hljs-number"><span class="hljs-number">-2</span></span>, maxval=<span class="hljs-number"><span class="hljs-number">2</span></span>), trainable=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, <span class="hljs-type"><span class="hljs-type">name</span></span>="M_hat") #  W   W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) #          x_modified = tf.log(tf.abs(x_in) + epsilon) m = tf.exp(tf.matmul(x_modified, W)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> m, W</code> </pre></div></div><br><br>  Again I remind you that you can find the full version of the code in my <a href="https://github.com/faizan2786/nalu_implementation">GitHub repository</a> (the link is duplicated at the end of the post). <br><br><h2>  Putting it all together: neural arithmetic logic unit (NALU) </h2><br>  As many have already guessed, we can learn almost any kind of arithmetic operations, combining the two models discussed above.  This is the <b>main idea of</b> <abbr title="Neural arithmetic logic unit">NALU</abbr> , which includes a <b>weighted combination of</b> elementary and complex <abbr title="Neuron Battery">NAC</abbr> , controlled through a training signal.  Thus, <abbr title="Neuron Battery">NAC</abbr> are the building blocks for <abbr title="Neural arithmetic logic unit">NALU</abbr> assembly, and if you understand their device, <abbr title="Neural arithmetic logic unit">it</abbr> will be easy to build <abbr title="Neural arithmetic logic unit">NALU</abbr> .  If you still have questions, try again to read the explanations for both <abbr title="Neuron Battery">NAC</abbr> models.  Below is a diagram with the <abbr title="Neural arithmetic logic unit">NALU</abbr> architecture. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ad9/8e3/2ae/ad98e32aea75b4bb4f020338e9b87bf8.png" alt="image"><br>  <i><abbr title="Neural arithmetic logic unit">NALU</abbr> architecture <abbr title="Neural arithmetic logic unit">diagram</abbr> with explanations</i> <br><br>  As can be seen from the figure above, both <abbr title="Neuron Battery">NAC</abbr> blocks (purple blocks) inside <abbr title="Neural arithmetic logic unit">NALU are</abbr> interpolated (combined) through a sigmoid of the training signal (orange block).  This allows (de) to activate the output of any of them, depending on the arithmetic function, the values ‚Äã‚Äãof which we are trying to find. <br><br>  As mentioned above, the elementary <abbr title="Neuron Battery">NAC</abbr> unit is an accumulating function, which allows <abbr title="Neural arithmetic logic unit">NALU</abbr> to perform elementary linear operations (addition and subtraction), while the complex NAC unit is responsible for multiplication, division and exponentiation.  <b>The output</b> in <abbr title="Neural arithmetic logic unit">NALU</abbr> can be represented as a formula: <br><br><div class="spoiler">  <b class="spoiler_title">Pseudocode</b> <div class="spoiler_text"><pre> <code class="hljs perl">Simple NAC : a = WX Complex NAC: <span class="hljs-keyword"><span class="hljs-keyword">m</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">exp</span></span>(W <span class="hljs-keyword"><span class="hljs-keyword">log</span></span>(|X| + e))  W = tanh(W_hat) * sigmoid(M_hat) <span class="hljs-comment"><span class="hljs-comment">#  G -       : g = sigmoid(GX) # , ,   NALU #  *      NALU: y = g * a + (1 - g) * m</span></span></code> </pre></div></div><br>  From the formula <abbr title="Neural arithmetic logic unit">NALU</abbr> above, we can conclude that when <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>g</mi><mo>=</mo><mn>0</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.377ex" height="2.298ex" viewBox="0 -728.2 2315.1 989.6" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-67" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-3D" x="758" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-30" x="1814" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mo>=</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-18"> g = 0 </script>  the neural network will only select values ‚Äã‚Äãfor complex arithmetic operations, but not for elementary ones;  and vice versa - in the case of <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>g</mi><mo>=</mo><mn>1</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.377ex" height="2.298ex" viewBox="0 -728.2 2315.1 989.6" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-67" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-3D" x="758" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMAIN-31" x="1814" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-19"> g = 1 </script>  .  Thus, in general, <abbr title="Neural arithmetic logic unit">NALU is</abbr> able to study any arithmetic operation consisting of addition, subtraction, multiplication, division, and exponentiation and successfully extrapolate the result beyond the limits of the initial data values. <br><br><h2>  Implementing NALU in Python using Tensorflow </h2><br>  In the implementation of <abbr title="Neural arithmetic logic unit">NALU,</abbr> we will use elementary and complex <abbr title="Neuron Battery">NAC</abbr> , which we have previously defined. <br><br><div class="spoiler">  <b class="spoiler_title">Python code</b> <div class="spoiler_text"><pre> <code class="hljs python"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">nalu</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x_in, out_units, epsilon=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.000001</span></span></span></span><span class="hljs-function"><span class="hljs-params">, get_weights=False)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">''' :param x_in:   X :param out_units:    :param epsilon:    (,    log(0)   ) :param get_weights:   True      :return y_out:     :return G: o   :return W_simple:    NAC1 ( NAC) :return W_complex:    NAC2 ( NAC) '''</span></span> in_features = x_in.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-comment"><span class="hljs-comment">#     NAC a, W_simple = nac_simple_single_layer(x_in, out_units) #     NAC m, W_complex = nac_complex_single_layer(x_in, out_units, epsilon=epsilon) #    G = tf.get_variable(shape=[in_shape, out_units], initializer=tf.random_normal_initializer(stddev=1.0), trainable=True, name="Gate_weights") g = tf.nn.sigmoid(tf.matmul(x_in, G)) y_out = g * a + (1 - g) * m if(get_weights): return y_out, G, W_simple, W_complex else: return y_out</span></span></code> </pre></div></div><br>  Once again, in the code above, I again initialized the parameter matrix <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>G</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.827ex" height="2.057ex" viewBox="0 -780.1 786.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/422777/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPZ1kkOJQJ0rGlw9iM5CqrTtoo5A#MJMATHI-47" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>G</mi></math></span></span><script type="math/tex" id="MathJax-Element-20"> G </script>  using a uniform distribution, but you can use <b>any</b> recommended method of generating an initial approximation. <br><hr><br><h2>  Results </h2><br>  For me personally, the idea of <abbr title="Neural arithmetic logic unit">NALU</abbr> is a serious breakthrough in the field of AI, especially in neural networks, and it looks promising.  This approach can open doors to those areas of application where standard neural networks could not cope. <br><br>  The authors of the article talk about various experiments using <abbr title="Neural arithmetic logic unit">NALU</abbr> : from selecting the values ‚Äã‚Äãof elementary arithmetic functions to counting the number of handwritten numbers in a given series of <a href="https://ru.wikipedia.org/wiki/MNIST_(%25D0%25B1%25D0%25B0%25D0%25B7%25D0%25B0_%25D0%25B4%25D0%25B0%25D0%25BD%25D0%25BD%25D1%258B%25D1%2585)">MNIST</a> images, which allows neural networks to check computer programs! <br><br>  The results make a tremendous impression and prove that <abbr title="Neural arithmetic logic unit">NALU</abbr> copes with <b>almost any tasks</b> related to the numerical representation, better than the standard models of neural networks.  I recommend that readers familiarize themselves with the results of the experiments in order to better understand how and where the <abbr title="Neural arithmetic logic unit">NALU</abbr> model can be useful. <br><br>  However, it must be remembered that neither <abbr title="Neuron Battery">NAC</abbr> nor <abbr title="Neural arithmetic logic unit">NALU</abbr> is the <a href="https://en.wikipedia.org/wiki/No_Silver_Bullet">ideal solution</a> for any task.  They rather represent a general idea of ‚Äã‚Äãhow to create models for a particular class of arithmetic operations. <br><hr><br>  Below is a link to my GitHub repository, which contains the full implementation of the code from the article. <br>  <a href="https://github.com/faizan2786/nalu_implementation">github.com/faizan2786/nalu_implementation</a> <br><br>  You can independently check the work of my model on various functions, selecting hyper parameters for the neural network.  Please ask questions and share your thoughts in the comments under this post, and I will do my best to answer you. <br><br>  <b>PS (from the author): this is my first ever written post, so if you have any tips, suggestions and recommendations for the future (both technical and general plan), please write to me.</b> <br><br>  PPS (from translator): if you have comments to the translation or to the text, please write me a personal message.  I am particularly interested in the formulation for the learned gate signal - I am not sure that I was able to accurately translate this term. </div><p>Source: <a href="https://habr.com/ru/post/422777/">https://habr.com/ru/post/422777/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../422767/index.html">Conference DEFCON 16. Fedor, hacker InSecure.org. NMAP Internet Scan</a></li>
<li><a href="../422769/index.html">Winners Startup Battlefield TechCrunch Disrupt San Francisco 2018</a></li>
<li><a href="../422771/index.html">Design rules, a new level and design thinking</a></li>
<li><a href="../422773/index.html">Nvidia. Revealing the secrets of the next generation GPU Turing architecture: double Ray Tracing, GDDR6, and more</a></li>
<li><a href="../422775/index.html">Conference DEFCON 22. Andrew "Zoz" Brooks. Don't screw it up! Part 1</a></li>
<li><a href="../422781/index.html">Fintech-digest: SWIFT will continue to work in the Russian Federation, VISA will allow you to transfer funds by phone number, expensive biometrics</a></li>
<li><a href="../422783/index.html">Better, faster, more powerful: styled-components v4</a></li>
<li><a href="../422785/index.html">Digitization of the plant: a look from the front</a></li>
<li><a href="../422787/index.html">Major changes in leading chip architectures</a></li>
<li><a href="../422789/index.html">@Pythonetc compilation, august 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>