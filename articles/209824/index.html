<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Test boost :: lockfree on the speed and delay of message transmission</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Not so long ago in boost-1.53 ‚Äã‚Äãa whole new section appeared - lockfree implementing non-blocking queues and a stack. 
 For the past few years I have ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Test boost :: lockfree on the speed and delay of message transmission</h1><div class="post__text post__text-html js-mediator-article">  Not so long ago in boost-1.53 ‚Äã‚Äãa whole new section appeared - <a href="http://www.boost.org/doc/libs/1_53_0/doc/html/lockfree.html">lockfree</a> implementing non-blocking queues and a stack. <br>  For the past few years I have been working with so-called non-blocking algorithms (lock-free data structures), we wrote them ourselves, tested them ourselves, used them <s>and were secretly proud of them</s> .  Naturally, we immediately had a question, whether to switch from self-made libraries to boost, and if to switch, when? <br>  That's when I came up with the idea for the first time to apply to boost :: lockfree some of the techniques we used to try our own code.  Fortunately, we will not have to test the algorithm itself, and we can focus on measuring performance. <br>  I will try to make the article interesting for everyone.  Those who have not yet encountered such problems will be useful to look at the fact that such algorithms are capable, and most importantly, where and how they should or should not be used.  For those who have experience in developing non-blocking queues, it may be interesting to compare quantitative measurement data.  At least I have not yet seen such publications myself. <br><a name="habracut"></a><br><h5>  Introduction: What are non-blocking data structures and algorithms? </h5><br>  The concept of multithreading has become firmly established in modern programming, but working with threads is impossible without synchronization tools, this is how mutex, semaphore, condition variable and their subsequent descendants appeared.  However, the first standard functions were rather heavy and slow, moreover, they were implemented inside the kernel, that is, they required context switching for each call.  The switching time is weakly dependent on the CPU, so the faster the processors became, the more relative time was required to synchronize the threads.  Then the idea emerged that, with minimal hardware support, it would be possible to create data structures invariant when working simultaneously with several threads.  For those who would like to know more about this, I recommend <a href="http://habrahabr.ru/company/ifree/blog/195770/">this series of publications</a> . <br>  The main algorithms have been developed and put on the back burner, their time has not come yet.  They got a second life when the concept of message processing time (latency) became almost more important than the usual CPU speed.  What is it all about? <div class="spoiler">  <b class="spoiler_title">Here is a simple example:</b> <div class="spoiler_text">  Suppose I have a server that receives messages, processes and sends a response.  Suppose that I received 1 million messages and the server handled them in 2 seconds, that is, 2 Œºs per transaction and it suits me.  <b>This is what is called bandwidth and is not a correct measure when processing messages</b> .  Later, I was surprised to learn that every client sent me a message, the client received an answer no earlier than in 1 second, that they are not satisfied with it at all.  What's the matter?  One possible scenario: the server quickly receives all messages and puts them into a buffer;  then processes them in parallel, spending every 1 second, but manages to process everything together in just 2 seconds;  and quickly sends back.  This is an example of a good speed system as a whole and at the same time unacceptably high latency. <br></div></div><br>  You can read more in the <a href="http://habrahabr.ru/post/43905/">interview with</a> <a href="http://www.gotw.ca/">Herb Sutter</a> , he is in a slightly different context, but he is very temperamental in discussing this problem.  Intuitively, it seems that the concepts of speed and latency are identical - the larger the first, the smaller the second.  On closer examination, it turns out, however, that they are independent and even anti-correlated. <br>  What does this have to do with non-blocking structures?  The most direct, the fact is that for latency any attempt to slow down or stop the flow is destructive.  It is easy to put the stream to sleep, but it is impossible to wake it up.  It can be awakened with a <s>tender kiss</s> only by the core of the operating system, and it does this strictly on schedule <s>and with lunch breaks</s> .  Try to explain to someone that your program, which pledged on those.  the task to respond within 200 <i>nanoseconds</i> , currently falls asleep for 10 <i>milliseconds</i> (typical time for * nix systems) and it is better not to disturb her.  Lock-free data structures that do not require stopping the stream for synchronization with other threads come to the rescue. <br>  Here we will talk about one such structure. <br><br><h5>  The first approach to the platform </h5><br>  I will work only with one of the structures - <a href="">boost :: lockfree :: queue that</a> implements a unidirectional queue with an arbitrary number of writing and reading threads.  This structure exists in two versions - allocating memory as needed and having an infinite capacity, and a variant with a fixed buffer.  Strictly speaking, both of them are not non-blocking, the first because the system memory allocation is not lock-free, the second because sooner or later the buffer will overflow and the writing streams will have to wait indefinitely until the recording space appears.  Let's start with the first option, and closer to the end I will compare with the results for a fixed buffer. <br>  I will add that I have a 4-core Linux Mint-15. <br>  Take the code right <a href="http://www.boost.org/doc/libs/1_53_0/doc/html/lockfree/examples.html">from here</a> and try to run it, here‚Äôs what happens: <br><pre> boost :: lockfree :: queue is lockfree
 produced 40,000,000 objects.
 consumed 40,000,000 objects.

 real 0m15.332s
 user 1m0.376s
 sys 0m0.064s
</pre><br><br>  That is, if you approach the case on a simple, about 400 ns per message, it is quite satisfactory.  This implementation transmits an int and starts 4 reading and writing streams. <br>  Let's modify the code a bit, I want to run an arbitrary number of threads, and I also like to see statistics.  What will be the distribution if you run the test 100 times in a row? 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/2bb/401/31a/2bb40131ad8ecf69eed5989f9f9783ac.jpg"><br>  Here you are, it looks quite reasonable.  On the X axis, the total execution time in nanoseconds divided by the number of transmitted messages, on the Y axis the number of such events. <br><br>  But the result for a different number of writers / readers: <br><img src="https://habrastorage.org/getpro/habr/post_images/b2b/456/480/b2b456480c8ac3671a1e31dcc2dc48e7.jpg"><br><img src="https://habrastorage.org/getpro/habr/post_images/3ca/971/2b7/3ca9712b7ab0dd534ec064511538a7fe.jpg"><br>  Here everything is not so rosy, any broadening of the distribution speaks from the fact that something is not optimally working.  In this case, it is approximately clear that the reading threads in this test never give up control, and when their number approaches the number of cores, the system simply has to suspend them. <br><br><h5>  The second approach to the platform </h5><br>  Let's make one more improvement in the test, instead of transmitting a useless <i>int</i> , let the writing thread send the current time to the nearest nanosecond.  Then the recipient will be able to calculate the latency for each message.  We do, run: <br><br><pre> threads: 1 write, 1 read
 failed: 0 pushes, 3267 pops
 bandwidth: 177.864 ns
 latency: 1.03614e + 08 ns
</pre><br>  We are now also counting the number of unsuccessful attempts to read a message from the queue and write to the queue (the first one here will of course always be zero, this is the allotment option). <br>  However, what else is this?  The delay, which we intuitively assumed the same order - 200 ns, suddenly exceeds 100 milliseconds, half a million times more!  It just can't be. <br>  But after all, we now know the delay of each message, so go ahead and see how it looks in real time, here are the results of several identical launches so that you can see how random the process is: <br><img src="https://habrastorage.org/getpro/habr/post_images/27e/8c9/146/27e8c9146cfb633bcf9f013ff577c96c.jpg"><br><br>  that is, if we write and read one stream at a time, and if we have four then: <br><img src="https://habrastorage.org/getpro/habr/post_images/050/98d/934/05098d934c8fda81d52720e1bc2e4b26.jpg"><br><br>  What is going on?  At some arbitrary moment, some of the reading threads are sent by the system to rest.  The queue begins to grow rapidly, messages sit in it and wait for processing.  After some time, the situation changes, the number of writing streams becomes less than the reading and the queue slowly resolves.  Such fluctuations occur with a period from milliseconds to seconds and the queue works in batch mode ‚Äî one million messages were recorded, one million were read.  The speed remains very high, but each individual message can spend several milliseconds in the queue. <br>  What do we do?  First of all, let's think about it, the test in this form is clearly inadequate.  In our country, half of the active threads are occupied only by inserting messages into the queue, this simply cannot happen on a real system, in other words, the test is designed so that traffic generates a deliberately rising power of the machine. <br>  We'll have to limit the input traffic, just insert <i>usleep (0)</i> after each entry into the queue.  On my machine, this gives a delay of 50 ¬µs with good accuracy.  We'll see: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cb3/4b6/fd5/cb34b6fd5683eac2f3e8d880ba2f8992.jpg"><br>  The red line - the original test without delay, green - with a delay. <br>  It is quite another thing, now you can count the statistics. <br><br>  Here is the result for several combinations of the number of writing and reading streams, to maintain an acceptable X scale, 1% of the largest counts dropped: <br><img src="https://habrastorage.org/getpro/habr/post_images/df0/628/95f/df062895f8540e9ec0240026c84d5382.jpg"><br>  Note that latency remains confidently within 300 ns and only the distribution tail is pulled further and further. <br><br>  But the results for one and four write threads, respectively. <br><img src="https://habrastorage.org/getpro/habr/post_images/e2c/1be/93d/e2c1be93d44ae26413be6adb69808a87.jpg"><br><img src="https://habrastorage.org/getpro/habr/post_images/ecf/c7b/347/ecfc7b3475e2dfe98ee319748a7e9a59.jpg"><br>  Here, a significant increase in the delay is evident, mainly due to the sharp growth of the tail.  Again, we see that four (== CPU) streams that continuously thrash idly by generating their own quantum of time, which causes a large number of uncontrolled slowdowns.  Although the average delay surely remains within 600 ns, for some tasks it is already on the verge of permissible, for example, if you have a TK clearly stipulating that 99.9% of messages must be delivered within a certain time (this happened to me). <br>  Note also how much the total execution time has grown, once in 150. This is a demonstration of the statement I made at the very beginning - minimum latency and maximum speed are not achieved at the same time.  A sort of peculiar principle of uncertainty. <br><br>  That's actually all that we could get out of the tests, not so little.  We measured the delay with good accuracy, showed that in a number of modes the average latency grows by many orders or, more precisely, the concept of average for it loses its meaning. <br>  Let's finally consider the last question. <br><br><h5>  What about fixed capacity queue? </h5><br>  fixed capacity is another option of boost :: lockfree :: queue built on a fixed-size internal buffer.  On the one hand, this allows avoiding a call to the system allocator, on the other hand, if the buffer is full, the writing thread will also have to wait.  For some types of tasks this is completely excluded. <br>  We will work on the same method here.  First, by learning from experience, we will look at the dynamics of delays: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/116/c02/17a/116c0217a241e37d4ac9fa4572a35ce4.jpg"><br>  The red graph corresponds to the 128 bytes used in the example from the boost, the green to the maximum possible 65534 bytes. <br><div class="spoiler">  <b class="spoiler_title">By the way</b> <div class="spoiler_text">  The documentation says that the maximum size is 65535 bytes - do not believe, get a core dump </div></div><br>  We did not insert an artificial delay, therefore it is natural that the queue works in batch mode, it is filled and released in large chunks.  However, the fixed buffer capacity introduces a certain order and it is clearly seen that the average for the delay at least exists.  Another unexpected conclusion <s>for fans of huge buffers is</s> that the buffer size does not affect the overall execution speed.  That is, if you are satisfied with latency of 32 Œºs (which is quite enough for many applications, by the way), you can use fixed_capacity lockfree :: queue with tiny memory usage and get extra speed. <br>  But nevertheless, let's evaluate how this option behaves in a multi-threaded program: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/022/c7c/2b8/022c7c2b859cc4afef09c71b8c0b5f6a.jpg"><br>  it was a little unexpected to see such a clear division into two groups, where the speed of the readers exceeds the speed of the writers, we get our desired hundreds of nanoseconds, where, on the contrary, it rises abruptly to 30-40 microseconds, it looks like this is a context switch on my machine.  This is the result for a 128-byte buffer, and 64K looks very similar, only the right-hand group crawls far, for tens of milliseconds. <br>  Is it good or bad?  Depends on the task, on the one hand, we can confidently guarantee that the delay will never exceed 40 Œºs under any conditions, and this is good;  on the other hand, if we need to <i>guarantee</i> some maximum delay less than this value, then we will have a hard time.  Any change in the balance of readers / writers, for example, due to a slight change in the processing of messages, can lead to an abrupt change in the delay. <br><br>  Recall, however, that we are generating messages that are known to be faster than our system can handle (see above, in the section about dynamically queues) and try to insert a plausible delay: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bbb/812/f15/bbb812f15e885dc7283c7be6bd66293c.jpg"><br>  This is already quite good, the two groups did not merge completely, but the right one approached so that the maximum latency does not exceed 600 ns.  Take my word for it, the statistics for a large buffer is 64K, it looks exactly the same, not the slightest difference. <br><br><h5>  It's time to go to the conclusion </h5><br>  I hope that those who have experience will be able to extract something useful from the test results themselves.  This is what I think myself: <br><ul><li>  If you are only interested in speed, all options are roughly equivalent and give an average time of the order of hundreds of nanoseconds per message.  In this fixed_capacity queue is more lightweight, since it takes a fixed amount of memory.  However, there are applications, for example, where it is critically important to ‚Äúrelease‚Äù the reading thread as quickly as possible, in which case the allotted queue is better suited, on the other hand, it can consume memory indefinitely. </li><li>  If minimization of latency is required, the processing time of each message separately, the situation is complicated.  For applications where it is necessary not to block the writing stream (loggers), it is certainly worth choosing the allotment option.  For the case of limited memory, fixed_capacity is definitely suitable, the buffer sizes will have to be selected based on the signal statistics. </li><li>  In any case, the algorithm is unstable relative to the intensity of the data stream.  When a certain critical threshold is exceeded, the delay abruptly increases by several orders of magnitude and then actually (but not formally) becomes blocking.  As a rule, fine tuning is required to make the system work without falling into blocking mode. </li><li>  Full isolation of the input and output streams is possible only in the allocating version; this is achieved, however, due to uncontrolled memory consumption and an uncontrollably large delay. </li><li>  Fixed_capacity allows you to achieve fast data transfer while simultaneously limiting the maximum latency to some reasonable limit.  The fixed_capacity queue itself is essentially a very lightweight structure.  The main disadvantage is that writing streams are blocked if the readers do not cope or hang for any reason.  Large buffer sizes, in my opinion, are rarely needed, they allow to achieve transient dynamics, something approaching the allotting queue. </li><li>  A very unpleasant surprise for me was the great negative impact of continuous idle reading threads on the dynamics.  Even in the case when the total number of threads &lt;= CPU, the addition of another stream that consumes 100% does not improve, but worsens the dynamics.  It seems that the strategy of ‚Äúlarge servers‚Äù, when a separate core is assigned to each important thread, does not always work. </li><li>  In this regard, one unresolved and still unsolved problem is how to efficiently use a thread waiting for an event.  If you put him to sleep - the <s>karma of</s> latency is fatally spoiled; if you use it for other tasks - the problem arises of quickly switching from task to task.  I think a good approximation to the ideal would be to enable the reading stream to be added to boost :: io_service, so as to effectively serve at least rare events.  Would love to hear if anyone has any ideas. </li></ul><br><br><div class="spoiler">  <b class="spoiler_title">For those who need to - code</b> <div class="spoiler_text"><pre><code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;boost/thread/thread.hpp&gt; #include &lt;boost/lockfree/queue.hpp&gt; #include &lt;time.h&gt; #include &lt;atomic&gt; #include &lt;iostream&gt; std::atomic&lt;int&gt; producer_count(0); std::atomic&lt;int&gt; consumer_count(0); std::atomic&lt;unsigned long&gt; push_fail_count(0); std::atomic&lt;unsigned long&gt; pop_fail_count(0); #if 1 boost::lockfree::queue&lt;timespec, boost::lockfree::fixed_sized&lt;true&gt;&gt; queue(65534); #else boost::lockfree::queue&lt;timespec, boost::lockfree::fixed_sized&lt;false&gt;&gt; queue(128); #endif unsigned stat_size=0, delay=0; std::atomic&lt;unsigned long&gt;* stat=0; std::atomic&lt;int&gt; idx(0); void producer(unsigned iterations) { timespec t; for (int i=0; i != iterations; ++i) { ++producer_count; clock_gettime(CLOCK_MONOTONIC, &amp;t); while (!queue.push(t)) ++push_fail_count; if(delay) usleep(0); } } boost::atomic&lt;bool&gt; done (false); void consumer(unsigned iterations) { timespec t, v; while (!done) { while (queue.pop(t)) { ++consumer_count; clock_gettime(CLOCK_MONOTONIC, &amp;v); unsigned i=idx++; v.tv_sec-=t.tv_sec; v.tv_nsec-=t.tv_nsec; stat[i]=v.tv_sec*1000000000+v.tv_nsec; } ++pop_fail_count; } while (queue.pop(t)) { ++consumer_count; clock_gettime(CLOCK_MONOTONIC, &amp;v); unsigned i=idx++; v.tv_sec-=t.tv_sec; v.tv_nsec-=t.tv_nsec; stat[i]=v.tv_sec*1000000000+v.tv_nsec; } } int main(int argc, char* argv[]) { boost::thread_group producer_threads, consumer_threads; int indexed=0, quiet=0; int producer_thread=1, consumer_thread=1; int opt; while((opt=getopt(argc,argv,"idqr:w:")) !=-1) switch(opt) { case 'r': consumer_thread=atol(optarg); break; case 'w': producer_thread=atol(optarg); break; case 'd': delay=1; break; case 'i': indexed=1; break; case 'q': quiet=1; break; default : return 1; } int iterations=6000000/producer_thread/consumer_thread; unsigned stat_size=iterations*producer_thread*consumer_thread; stat=new std::atomic&lt;unsigned long&gt;[stat_size]; timespec st, fn; clock_gettime(CLOCK_MONOTONIC, &amp;st); for (int i=0; i != producer_thread; ++i) producer_threads.create_thread([=](){ producer(stat_size/producer_thread); }); for (int i=0; i != consumer_thread; ++i) consumer_threads.create_thread([=]() { consumer(stat_size/consumer_thread); }); producer_threads.join_all(); done=true; consumer_threads.join_all(); clock_gettime(CLOCK_MONOTONIC, &amp;fn); std::cerr &lt;&lt; "threads : " &lt;&lt; producer_thread &lt;&lt;" write, " &lt;&lt; consumer_thread &lt;&lt; " read" &lt;&lt; std::endl; std::cerr &lt;&lt; "failed : " &lt;&lt; push_fail_count &lt;&lt; " pushes, " &lt;&lt; pop_fail_count &lt;&lt; " pops" &lt;&lt; std::endl; fn.tv_sec-=st.tv_sec; fn.tv_nsec-=st.tv_nsec; std::cerr &lt;&lt; "bandwidth: " &lt;&lt; (fn.tv_sec*1e9+fn.tv_nsec)/stat_size &lt;&lt; " ns"&lt;&lt; std::endl; double ct=0; for(auto i=0; i &lt; stat_size; ++i) ct+=stat[i]; std::cerr &lt;&lt; "latency : "&lt;&lt; ct/stat_size &lt;&lt; " ns"&lt;&lt; std::endl; if(!quiet) { if(indexed) for(auto i=0; i &lt; stat_size; ++i) std::cout&lt;&lt;i&lt;&lt;" "&lt;&lt;stat[i]&lt;&lt;std::endl; else for(auto i=0; i &lt; stat_size; ++i) std::cout&lt;&lt;stat[i]&lt;&lt;std::endl; } return 0; }</span></span></span></span></code> </pre> <br></div></div></div><p>Source: <a href="https://habr.com/ru/post/209824/">https://habr.com/ru/post/209824/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../209814/index.html">New payment method: Bitcoin and Litecoin</a></li>
<li><a href="../209816/index.html">Day-to-Day Service Plan - Part 2: Automatic Update of Statistics</a></li>
<li><a href="../209818/index.html">FORTH: nano-servers and nanoclients. Part 1</a></li>
<li><a href="../209820/index.html">Validating Forms in a Declarative Style (C #)</a></li>
<li><a href="../209822/index.html">Testers - a supporting role?</a></li>
<li><a href="../209826/index.html">The implementation of the shingle algorithm on Node.JS. Finding Fuzzy Duplicates for English Texts</a></li>
<li><a href="../209828/index.html">Hekslet: Erlang, logic, operating systems, Java 2</a></li>
<li><a href="../209830/index.html">Recognition of document images using the roulette algorithm</a></li>
<li><a href="../209832/index.html">Persuasion architecture, 7 user manipulation mechanisms</a></li>
<li><a href="../209834/index.html">Transition to ADFS authorization and authentication mechanisms as part of a marketing strategy</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>