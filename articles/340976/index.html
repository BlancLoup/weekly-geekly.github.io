<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Photo storage and upload architecture in Badoo</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Artem Denisov ( bo0rsh201 , Badoo ) 
 Badoo is the world's largest dating site. At the moment, we have about 330 million users worldwide. But what is ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Photo storage and upload architecture in Badoo</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/069/34a/be1/06934abe175d9b8c2305f9627cf976f4.jpg"><br><br><h2>  Artem Denisov ( <a href="https://habrahabr.ru/users/bo0rsh201/" class="user_link">bo0rsh201</a> , <a href="https://habrahabr.ru/company/badoo/">Badoo</a> ) </h2><br>  Badoo is the world's largest dating site.  At the moment, we have about 330 million users worldwide.  But what is much more important in the context of our conversation today is that we store about 3 petabytes of user photos.  Every day, our users upload about 3.5 million new photos, and the reading load is about <b>80 thousand requests per second</b> .  This is quite a lot for our backend, and sometimes there are difficulties with this. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/49a/f57/2bc/49af572bcedc3a580c8d7fcc985a0d5e.png" alt="image">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I will talk about the design of this system, which stores and gives away pictures as a whole, and will give a look at it from the point of view of the developer.  There will be a brief retrospective about how it developed, where I will outline the main milestones, but I will only speak in more detail about the solutions that we are currently using. <br><a name="habracut"></a><br>  Now let's get started. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/SU9ETg39FEg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  As I said, this will be a retrospective, and in order to start somewhere with it, let's take the most banal example. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/508/62e/e30/50862ee30b4caca24b44a655e0eb9871.png"><br><br>  We have a common task, we need to take, store and give photos of users.  In this form, the overall task, we can use anything: <br><br><ul><li>  modern cloud storage, </li><li>  a boxed solution, which is also a lot now; </li><li>  we can populate several machines in our data center and put big hard disks on them and store photos there. </li></ul><br>  Badoo historically - and now, and then (at the time when it was in its infancy) - lives on our own servers, inside our own DC.  Therefore, for us this option was optimal. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ec7/8e2/2a9/ec78e22a9ed58b8043423ba51e10c24f.png"><br><br>  We just took a few cars, called them "photos", we got such a cluster that stores photos.  But it seems that something is missing.  In order for all this to work, you need to somehow determine which machine we will store which photos.  And here, too, do not need to discover America. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cf1/b50/fbe/cf1b50fbe395e534a8fd2f6844b8581b.png"><br><br>  We add some field to our repository with user info.  This will be the sharding key.  In our case, we called it place_id, and this id of the place indicates the place where the photos of the users are stored.  We make maps. <br><br>  At the first stage, you can even do it with your hands - we say that a photo of this user with such a play will land on such a server.  Thanks to this map, we always know when a user uploads a photo, where to save it, and we know where to get it from. <br><br>  This is an absolutely trivial scheme, but it has quite significant advantages.  The first is that it is simple, as I said, and the second is that with this approach we can easily scale horizontally by simply delivering new cars and adding them to the map.  Nothing more to do. <br><br>  So it was for some time with us. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/554/fd2/812/554fd281284a90d46b49f180cb2156f0.png"><br><br>  It was somewhere in 2009.  Delivered cars, delivered ... <br><br>  And at some point we began to notice that this scheme has certain disadvantages.  What are the disadvantages? <br><br>  First of all, it is limited capacity.  We can not push as many hard drives as we would like on a single physical server.  And this over time and with the growth of dataset has become a definite problem. <br><br>  And the second.  This is an atypical configuration of machines, since such machines are difficult to reuse in some other clusters, they are quite specific, i.e.  they should be weak in performance, but at the same time with a large hard drive. <br><br>  It was all for 2009, but, in principle, these requirements are still relevant today.  We have a retrospective, so in 2009 everything was bad with this at all. <br><br>  And the last point is the price. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fe8/076/3a2/fe80763a231457c31cc40fb550bd8d32.png"><br><br>  The price then was very biting, and we needed to look for some alternatives.  Those.  we needed to somehow better dispose of both the place in the data centers, and directly the physical servers on which all this is located.  And our system engineers began a large study, which reviewed a bunch of different options.  They also looked at cluster file systems, such as PolyCeph and Luster.  There were performance problems and quite heavy maintenance.  Refused.  We tried to mount the entire dataset on NFS for each wheelbarrow, so that somehow scaled.  Reading too badly went, tried different solutions from different vendors. <br><br>  And in the end we stopped at the fact that we began to use the so-called Storage Area Network. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b42/544/d78/b42544d787a0eda27424520222f98ece.png"><br><br>  These are such large SHD, which are just focused on storing large amounts of data.  They are shelves with discs that are mounted on the final optics returning machines.  So  we have some kind of a small pool of machines, and these SHDs that are transparent to our rendering logic, i.e.  for our nginx or someone else, service requests for these photos. <br><br>  This solution had obvious advantages.  This is SHD.  It is focused on storing pictures.  It turns out cheaper than we just setup machines with hard disks. <br><br>  The second plus. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cdd/3ab/e1b/cdd3abe1b0b19296ac912142018959cb.png"><br><br>  This is what capacity has become much more, i.e.  we can accommodate much more storage in a much smaller volume. <br><br>  But there were also minuses, which showed up quickly enough.  With the increase in the number of users and the load on this system, performance problems began to arise.  And the problem here is quite obvious - any SHD designed to store a lot of photos in a small volume, as a rule, suffers from intensive reading.  This is actually true for any cloud storage, and whatever it is.  Now we don‚Äôt have an ideal storage, which would be infinitely scalable, we could push anything into it, and it would endure reading very well.  Especially random reads. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0af/bf3/259/0afbf32592838e83a4981600179ecbbb.png"><br><br>  As is the case with our photos, because photos are requested inconsistently, and this greatly affects their performance. <br><br>  Even according to today's figures, if we have somewhere more than 500 RPS for the photos on the machine to which storage is connected, problems already begin.  And it was bad enough for us, because the number of users is growing, everything should only get worse.  It is necessary to optimize it somehow. <br><br>  In order to optimize, we decided at that time, obviously, to look at the load profile - what, in general, is happening, what needs to be optimized. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0fe/a64/355/0fea64355c8a4edcca2d0f0a407cb837.png"><br><br>  And here everything plays into our hands. <br><br>  I have already said in the first slide: we have 80 thousand requests per second for reading with only 3.5 million apploads per day.  That is a difference of three orders.  Obviously, it is necessary to optimize the reading and it is almost clear how. <br><br>  There is another little moment.  The specificity of the service is such that a person registers, fills in a photo, then begins to actively watch other people, like them, and is actively shown to other people.  Then he finds a pair or does not find a pair, that's how it will turn out, and for some time he stops using the service.  At this moment, when he enjoys, his pictures are very hot - they are in demand, they are watched by a lot of people.  As soon as he stops doing this, he quickly falls out of such intense shows to other people as they were before, and his pictures are practically not requested. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aac/ef5/004/aacef5004c3ea6cbf9fd50ca604bb3e0.png"><br><br>  Those.  we have a very small hot dataset.  But at the same time there are a lot of requests for it.  And an obvious solution here is to add a cache. <br><br>  Cache with LRU will solve all our problems.  What are we doing? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/742/d2a/f18/742d2af18107168349b6d79ccb1378d2.png"><br><br>  We add in front of our large cluster with storage one more relatively small one, called photoscache.  This is, in essence, just a caching proxy. <br><br>  How does it work from the inside?  Here is our user, here is storage.  Everything, as before.  What do we add between them? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eb6/56d/52b/eb656d52b64bccc9b4a20908098c89ce.png"><br><br>  It is just a machine with a physical local disk, which is fast.  This is with the SSD, let's say.  And here on this disk any local cache is stored. <br><br>  What does this look like?  The user sends a request for a photo.  NGINX searches for it first in the local cache.  If not, it just does proxy_pass to our storage, downloads a photo from there and gives it to the user. <br><br>  But this is very trite and incomprehensible what is happening inside.  It works like this. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/95f/cd6/0db/95fcd60db7b595a6f393cc10dee8396e.png"><br><br>  The cache is logically divided into three layers.  When I say "three layers", this does not mean that there is some kind of complex system.  No, this is conventionally just three directories in the file system: <br><br><ol><li>  This is the buffer where the photos just downloaded from the proxy are placed. </li><li>  This is a hot cache that stores the photos that are currently actively requested. </li><li>  And a cold cache where photos are gradually pushed out of the hot when less request'ov comes to them. </li></ol><br>  For this to work, we need to somehow manage this cache, we need to rearrange the photos in it, etc.  This is also a very primitive process. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e1a/49c/c9e/e1a49cc9e63f9d29b0d302e7c0a9a69c.png"><br><br>  Nginx simply writes to each request on RAMDisk access.log, which indicates the path to the photo that he has served (relative path, of course), and how it was served by the partition.  Those.  ‚Äúphoto 1‚Äù can be written there and then either buffer, or hot cache, or cold cache, or proxy. <br><br>  Depending on this, we need to somehow decide what to do with the photo. <br><br>  We have a small daemon running on each machine, which constantly reads this log and keeps a memory of the use of certain photos in its memory. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5ce/75b/10b/5ce75b10ba3ce887f59cee88b0585e0a.png"><br><br>  He simply collects there, keeps counters, and periodically does the following.  Actively requested photos, for which there are many requests, he moves to the hot cache, wherever they lie. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ff4/1c2/a3e/ff41c2a3e86dced49bd54c5b9e504ffc.png"><br><br>  Photos that are rarely requested and are less frequently requested, it gradually pushes from the hot cache to the cold. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b2c/e6c/f41/b2ce6cf417f899f002bc1df063a4fa15.png"><br><br>  And when our cache runs out of space, we just start to delete everything from the cold cache without parsing.  And this, by the way, works well. <br><br>  In order for the photo to be saved immediately when proxying to the buffer, we use the proxy_store directive and the buffer is also a RAMDisk, i.e.  for the user it works very fast.  This is what concerns the internals of the caching server itself. <br><br>  There was a question with how to distribute requests'y on these servers. <br><br>  Suppose there is a cluster of twenty storage-machines and three caching servers (as it happens). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c3a/28e/20a/c3a28e20a3bf43ddbebcb6a76de8f11c.png"><br><br>  We need to somehow determine which requests for which photos and where to land. <br><br>  The most banal option is Round Robin.  Or do it by chance? <br><br>  This obviously has a number of drawbacks, because we will be very inefficient to use the cache in this situation.  Requests will land on some random machines: here it is cached, on the next it is gone.  And if all this will work, it will be very bad.  Even with a small number of machines in the cluster. <br><br>  We need to somehow unambiguously determine which server to land on which request. <br><br>  There is a banal way.  We take the hash from the URL or the hash from our sharding key, which is in the URL, and divide it entirely by the number of servers.  Will work?  Will be. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/622/5f1/a8b/6225f1a8bb36c6b43d03e83b7fc0292b.png"><br><br>  Those.  we have one hundred percent request, for example, for some ‚Äúexample_url‚Äù will always land on the server with the index ‚Äú2‚Äù, and the cache will be constantly utilized as best as possible. <br><br>  But there is a problem with rewarding in such a scheme.  Resharding - I mean the change in the number of servers. <br><br>  Suppose that our caching cluster stopped coping, and we decided to add another machine. <br><br>  We add. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ffd/d55/71f/ffdd5571f63d351b06f9432a82631745.png"><br><br>  Now everything is divided completely into three instead of three.  Thus, almost all the keys that we previously had, almost all URLs now live on other servers.  The entire cache was invalidated just by the moment.  All requests fell on our cluster-storage, he became ill, the service was denied and dissatisfied users.  So do not want to do. <br><br>  This option does not suit us either. <br><br>  So  what should we do?  We must somehow effectively use the cache, constantly landing one request on the same server, but at the same time be resistant to resarding.  And there is such a solution, it is not that difficult.  Called consistent hashing. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d6d/73a/9f4/d6d73a9f4bc826126902ca6c42d6bd98.png"><br><br>  What does this look like? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/16a/326/92e/16a32692e54bc86a49f896e1e27b7fe1.png"><br><br>  We take some function from the sharding key and smear all its values ‚Äã‚Äãon the circle.  Those.  at point 0 we have its minimum and maximum values.  Next, we place all of our servers on the same circle in a similar way: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8af/ccd/d40/8afccdd40fd87237a04a2d423156fe62.png"><br><br>  Each server is defined by one point, and the sector that goes to it in a clockwise direction, respectively, is served by this host.  When requests come to us, we immediately see that, for example, request A - it has such a hash there - and it is serviced by server 2. Request B - by server 3. And so on. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0b0/1a2/e6e/0b01a2e6ed01049eb4b877332a031990.png"><br><br>  What happens in this situation when resarding? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b14/317/5ea/b143175eaebfc766cfd44abaaf93ceb7.png"><br><br>  We do not invalidate the entire cache, as before, and do not shift all the keys, but we shift each sector a short distance so that, to put it that way, our sixth server that we want to add got into the free space and add it there. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/990/928/57a/99092857a71f0bf87ea8c7158fc0f541.png"><br><br>  Of course, in such a situation, the keys also move out.  But they move much weaker than before.  And we see that our first two keys remain on their servers, and the caching server has changed only for the last key.  This works quite efficiently, and if you add new hosts incrementally, there is no big problem.  You add a little bit ‚Äî add, wait ‚Äî until the cache is full again, and everything works well. <br><br>  The only question remains with failures.  Suppose that we have some kind of car out of order. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/90c/1c7/670/90c1c7670927a6d9a54df06e633e6540.png"><br><br>  And we would not really like to regenerate this card at this moment, to invalidate a part of the cache, and so on, if, for example, the machine reboot, but we need to somehow serve the requests.  We simply keep one backup photocache on each site, which serves as a replacement for any machine that has now crashed.  And if suddenly we have some server became unavailable, the traffic goes there.  At the same time, naturally, we have no cache there, i.e.  it's cold, but at least user requests are processed.  If this is a short interval, then we calmly experience it.  Just more storage load goes.  If this interval is long, then we can already decide whether to remove this server from the map or not, or maybe replace it with another one. <br><br>  This is about the caching system.  Let's look at the results. <br><br>  It would seem that nothing complicated here.  But this way of managing the cache gave us about 98% of the crate.  Those.  of these 80 thousand requests per second, only 1600 comes to storage, and this is a perfectly normal load, they calmly experience this, we always have a reserve. <br><br>  We placed these servers in our three DCs, and got three points of presence - Prague, Miami and Hong Kong. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cf9/5e7/4db/cf95e74db3d74eedb4a6a10aeeabfbb9.png"><br><br>  So  they are more or less locally located to each of our target markets. <br><br>  And as a nice bonus, we received this caching proxy, on which the CPU is actually idle, because it is not so much needed to upload content.  And there with the help of NGINX + Lua, we implemented a lot of utilitarian logic. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8f6/cb9/a5a/8f6cb9a5a2a5be9e5970734877081ed9.png"><br><br>  For example, we can experiment with webp or progressive jpeg (these are effective modern formats), look at how this affects traffic, make some decisions, include for certain countries, etc .;  do dynamic resize or crop photos on the fly. <br><br>  This is a good usecase when, for example, you have a mobile application that shows pictures, and the mobile application does not want to spend the client's CPU to request a large photo and then resize it to a certain size in order to stuff it into a view.  We can simply dynamically specify in the URL some conditional in the UPort conditional, and the photocache itself otresayzit photo.  As a rule, he will select the size that we physically have on the disk, as close as possible to the requested one, and zadunskellit it in specific coordinates. <br><br><blockquote>  <font color="gray">By the way, we posted in open access the video of the last five years of the conference of developers of high-loaded systems <a href="http://www.highload.ru/">HighLoad ++</a> .</font>  <font color="gray">Watch, learn, share and subscribe to <a href="https://www.youtube.com/user/profyclub">the YouTube channel</a> .</font> </blockquote><br>  We can also add a lot of grocery logic there.  For example, we can add different watermarks by URL parameters, we can blur photos, blur or pixelate.  This is when we want to show a photo of a person, but we don‚Äôt want to show his face, it works well, it‚Äôs all implemented here. <br><br>  What did we get?  We got three points of presence, a good hit rate, and at the same time, the CPU on these machines is not idle.  He now became, of course, more important than before.  We need to put the car ourselves stronger, but it's worth it. <br><br>  This is with regard to the return of photos.  Everything is clear and obvious here.  I think that I did not discover America; almost any DN works this way. <br><br>  And, most likely, the sophisticated listener might have a question: why not just not take and not change everything for DN?  It would be about the same, all modern DN can do it.  And here are a number of reasons. <br><br>  The first is the pictures. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a7a/450/ae6/a7a450ae6a5b580edee4b9d12173d64a.png"><br><br>  This is one of the key points of our infrastructure, and we need as much control over them as possible.  If this is some kind of solution from a third-party vendor, and you do not have any power over it, it will be hard enough for you to live with it when you have a large dataset and when you have a very large stream of user requests. <br><br>  I will give an example.  Now, on our infrastructure, we can, for example, in case of any problems or underground knocks, go to the car, take it up, conditionally speaking.  We can add a collection of some metrics that only we need, we can somehow experiment, see how it affects graphics, and so on.  Now a lot of statistics are collected on this caching cluster.  And we periodically look at it and for a long time we investigate some anomalies.  If this were on the side of the DN, it would be much harder to control.  Or, for example, if some kind of accident happens, we know what happened, we know how to live with it and how to overcome it.  This is the first conclusion. <br><br>  The second conclusion is also rather historical, because the system has been developing for a long time, and there were many different business requirements at different stages, and they do not always fit into the DN concept. <br><br>  And the item arising from the previous - <br><br><img src="https://habrastorage.org/getpro/habr/post_images/468/9a3/825/4689a3825f58604dd84ffc063c072de7.png"><br><br>  This is something that on photo caches we have a lot of specific logic, which is not always possible to add upon request.  It is unlikely that any CDN will add any custom items to your request.  For example, URL encryption if you don‚Äôt want the client to change anything.  You want to change the URL on the server and encrypt it, and then give some dynamic parameters here. <br><br>  What conclusion suggests itself?  In our case, a CDN is not a good alternative. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/198/e9c/de9/198e9cde91c52dfc36d3295fc7235024.png"><br><br>  And in your case, if you have any specific business requirements, then you can absolutely calmly realize what I showed you.  And this with a similar load profile will work fine. <br><br>  But if you have some kind of general solution, and the task is not very private, you can safely take the CDN.  Or if time and resources are much more important for you than control. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d82/e83/085/d82e830854bbac87f0077bb6ea79391c.png"><br><br>  And modern DN have practically everything that I told you about now.  Except for the plus or minus some features. <br><br>  This is about the return of photos. <br><br>  Let us now move forward a little bit in our retrospective and talk about storage. <br><br>  2013 was coming. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1aa/204/9b9/1aa2049b9f90fbe1df7c4308eab071b8.png"><br><br>  Caching servers were added, performance issues were gone.  All is well.  Dataset is growing.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In 2013, we had about 80 servers that are connected to storage, and about 40 caching in each DC. This is 560 terabytes of data on each data center, i.e. about a petabyte in total. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f6/b70/e5a/4f6b70e5a23d0206285e71f8b30ff6b0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And with the growth of dataset, operational costs began to grow. What was it expressed in? </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/482/96c/f5e/48296cf5e4888da3c3c7c871ca7b1f85.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this scheme, which is drawn - with SAN, with machines connected to it and caches - a lot of points of failure. If we had already coped with the failure of caching servers, everything is more or less predictable and understandable, then on the storage side, everything was much worse. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First, the Storage Area Network (SAN) itself, which can fail. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Secondly, it is connected via optics to the end machines. There may be problems with optical cards, candles.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a39/12f/f18/a3912ff1836ba138e9cb300048808f6a.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course, there are not so many of them as with SAN itself, but, nevertheless, these are also points of failure. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Next, the machine itself, which is connected to the storage. She, too, can fail. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/daa/3fb/497/daa3fb497cf94446a3ef10a5c8db25f0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Total we have three points of failure. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Further, in addition to points of failure, this is a difficult maintenance of the storage itself. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is a complex multicomponent system, and it can be difficult for system engineers. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And the last, most important point. If a failure occurs at any of these three points, we have a non-zero probability of losing user data, as the file system may be beaten.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/32e/97f/776/32e97f77680b62fa7dcf7b48f3bfc2e8.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Suppose we have a beating file system. Its restoration is, firstly, long - it can take a week with a large amount of data. And secondly, as a result, we will most likely get a bunch of incomprehensible files that will need to be somehow mated to users' photos. And we risk losing data. The risk is quite high. And the more often such situations happen, and the more problems arise in this whole chain, the higher the risk. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We had to do something about it. And we decided that we just need to back up the data. This is actually an obvious solution and a good one.</font></font> What have we done? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f48/9df/7b6/f489df7b62bae39d35dd871cc57807d0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is how our server looked like, which was connected to storage earlier. This is one main section, it‚Äôs just a block device, which actually represents a mount for remote storage over optics. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We just added the second section. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/74d/3b7/7f3/74d3b77f326f73b3bcce860b4e51c4a4.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We set up a second storage (good, for the money it is not so expensive), and called it a backup partition. It is also connected via optics, is located on the same machine. But we need to somehow synchronize the data between them. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here we just make an asynchronous queue nearby.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a6f/2c8/a43/a6f2c8a43f40395f80e9226c1f5e1b0f.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">She is not very loaded. We know that we have few records. The queue is just a sign in MySQL, into which lines like ‚Äúyou need to save this photo‚Äù are written. With any change or upload, we copy from the main partition to the backup asynchronous or just some kind of background worker. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And so we always have two consistent sections. Even if one part of this system fails, we can always change the main partition with the backup, and everything will continue to work.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But because of this, the reading load greatly increases, because in addition to clients who read from the main section, because they first look at the photo there (it‚Äôs more recent there), and then they search on the backup if they don‚Äôt find it (but NGINX just does it), another plus is our system backup'a now reads from the main section. Not that it was a bottleneck, but did not want to increase the load, in fact, just like that. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And we added a third disk, which is a small SSD, and called it a buffer. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c07/346/746/c073467461127a7748e879d30cb85650.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How it works now.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">User upload'it photo on the buffer, then throws the event in the queue that it must be excavated into two sections. </font><font style="vertical-align: inherit;">It is copied, and the photo for some time (say, a day) lives on the buffer, and only then it blurs from there. </font><font style="vertical-align: inherit;">This greatly improves the user experience, because the user uploads a photo, as a rule, after it‚Äôs immediately begin to go request 's, or he himself refreshes the page, registered it. </font><font style="vertical-align: inherit;">But it all depends on the application that makes the upload.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Or, for example, other people to whom he began to show, immediately for this photo send request 's. </font><font style="vertical-align: inherit;">It is not in the cache yet, the first request is very fast. </font><font style="vertical-align: inherit;">In fact, the same as with the photo cache. </font><font style="vertical-align: inherit;">Slow storage does not participate at all in this. </font><font style="vertical-align: inherit;">And when in a day it will be spoiled, it is either cached on our caching layer, or it is likely that no one needs it.</font></font> Those.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">user experience here is very cool grown due to such simple manipulations. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Well, and most importantly: we stopped losing data. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/b0c/efc/0dc/b0cefc0dcf32b30aad590499edd1347f.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We, so to say, stopped </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">potentially</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> losing data, because we didn‚Äôt lose them. But the danger was. We see that such a solution is, of course, a good one, but it is a bit like smoothing the symptoms of the problem, rather than resolving it to the end. And some problems remain. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First, this is the point of failure in the form of the physical host itself, on which all this machinery works, it has not gone anywhere. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/b9c/dec/3b4/b9cdec3b487fb238326e502d36b4a64e.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Secondly, there were problems with the SANs, their heavy maintenance remained, etc. This is not something that was a critical factor, but I wanted to try to live somehow without it.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And we made the third version (in fact, the second one actually) - the reservation version.</font></font> What did it look like? <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is what was - </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a99/cbe/966/a99cbe9668545f818fb639bd483ac502.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The main problems we have with the fact that this is a physical host. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First, we remove SANs, because we want to experiment, we just want to try local hard drives. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/dfc/4d9/d82/dfc4d9d824bb4bd4b05ccbd037d381b7.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is already the year 2014-2015, and at that time the situation with disks and their capacity in one host has become much better. We decided, why not try. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And then we just take our backup-section and transfer it physically to a separate machine. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/47c/e9a/90b/47ce9a90b36f79dcccd94c040a476287.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Thus, we get just such a scheme. We have two cars that store the same datasets. They reserve each other completely and synchronize data over the network through an asynchronous queue in the same MySQL. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/30b/dbb/a73/30bdbba73fcbae2f8d01f31498539ddd.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Why it works well - because we have few records.</font></font> Those.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">if the recording were commensurate with the reading, perhaps we would get some kind of network overhead and problems. There is little writing, a lot of reading - this method works well, i.e. we rarely copy photos between these two servers. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How does this work, if a little more detailed look. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/041/6a6/c99/0416a6c9976955ab2ea181d6b4bcfc00.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Upload. The balancer simply selects random hosts with a pair and uploads to it. At the same time, he naturally does health checks, looks to prevent the car from falling out.</font></font> Those.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">He uploads pictures only to a live server, and then through an asynchronous queue, this is all copied to his neighbor. With upload, everything is extremely simple. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With the task a little more difficult. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/56f/2c6/d19/56f2c6d1942c700d0ad3c6e21e32ca4a.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lua helped us here, because on vanilla NGINX this logic is hard to make. We first make a request to the first server, see if there is a picture there, because potentially it can be uploaded, for example, to a neighbor, but has not arrived here yet. If the photo is there, that's good. We immediately give it to the client and, perhaps, we cache it. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/02b/416/9e0/02b4169e084e4c35ce3e51201b66b3d5.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If it is not there, we simply make a request for a neighbor and from there we get it guaranteed.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/9dc/d04/a62/9dcd04a62481ad996106ba69f5706663.png"><br><br>  So<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">again we can say: there may be problems with performance, because constant round trips are filled up with a photo, there is no photo, we make two requests instead of one, it should work slowly. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In our situation, it does not work slowly. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/8ed/721/86f/8ed72186fe463059eed025bdf27bcd85.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We are going to have a lot of metrics on this system, and the conditional cunning rate of such a mechanism is about 95%.</font></font> Those.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The lag of this backup is small, and due to this we are almost guaranteed, after the photo has been uploaded, we take it away the first time and do not go anywhere twice. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So what else have we got, and what's very cool? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Previously, we had a basic backup section, and we read them sequentially.</font></font> Those.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">we always looked first on the main, and then on the backup. It was one move. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now we recycle reading from two machines at once. Distribute requests Round Robin'om. In a small percentage of cases we make two requests. But on the whole, we now have twice as much reading margin as before. And the load is really great and decreased on the returning cars, and directly on the storage, which we also had at that time. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As for fault tolerance. Actually, we fought for this basically. With fault tolerance, everything is gorgeous here. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/208/030/5fc/2080305fccda42944ee03ba3d3a59c7e.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">One car fails. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/576/b40/9ca/576b409ca6dda668e4269ecdfd3f4d95.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No problem! The system engineer may not even wake up at night, wait until the morning, nothing terrible will happen.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If, even with the refusal of this machine, the queue is out of order, there are also no problems, just the log will be accumulated first on the live machine, and then get in the queue, and then on the car that will go into service after some time. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/05c/337/856/05c337856f116d6bbfd1521fc1e0fe49.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Same with maintenance. We just turn off one of the machines, pull it out of all pools with our hands, it stops traffic, we do some maintenance, we rule something, then we return it to the system, and this backup catches up pretty quickly.</font></font> Those.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">per day downtime of one car catching up within a couple of minutes. This is directly very small. With fault tolerance, I say it again, everything is cool here. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What can be summed up from this scheme with reservation? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Received fault tolerance. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Easy operation. Since the machines have local hard drives, this is much more convenient from the point of view of the exploitation of the engineers who work with it. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Received a double margin on reading. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is a very good bonus in addition to resiliency.</font></font><br><br>  But there are problems.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now we have a much more complex development of some features related to this, because the system has become 100% eventually consistent. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/48e/24e/9c8/48e24e9c84a687241bf82b86d775427e.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have to, say, in some background job, constantly think: ‚ÄúAnd on which server are we running now?‚Äù, ‚ÄúIs there really an actual photo here?‚Äù, Etc. This, of course, is all wrapped up in wrappers, and for a programmer who writes business logic, this is transparent. But, nevertheless, this large complex layer appeared. But we are ready to put up with it in exchange for those buns that we got from it. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And here again there is some conflict. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I said at first that storing everything on local hard drives is bad. And now I say we liked it.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yes, indeed, over time, the situation has changed dramatically, and now this approach has many advantages. First of all, we get much simpler operation. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Secondly, it is more productive, because we do not have these automatic controllers, connection to disk shelves. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is a huge machine, and these are just a few disks that are collected here on the machine in a raid.</font></font><br><br>  But there are also disadvantages. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/49c/22e/283/49c22e283dccc7208fe277bf8e2d97a3.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is about 1.5 times more expensive than using SANs even at today's prices. Therefore, we decided so boldly not to convert our entire large cluster into cars with local hard drives and decided to leave the hybrid solution. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Half of the machines we work with hard drives (well, not half - 30 percent, probably). And the rest is old cars, which used to be the first reservation scheme. We simply remounted them, since we didn‚Äôt need any new data or anything else, we just moved the mount from one physical host to two. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And we have a large supply of reading, and we have enlarged. If earlier we mounted one storage on one machine, now we mount four for one pair, for example. And it works fine.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's summarize what we did, what we fought for, and whether it worked.</font></font><br><br><h3>  Results </h3><br>  We have users - as many as 33 million. <br><br>  We have three points of presence - Prague, Miami, Hong Kong. <br><br>  They contain a caching layer, which is a wheelbarrow with fast local disks (SSD), which is powered by unpretentious machinery from NGINX, its access.log and daemons in Python, which process all this and manage the cache. <br><br>  If you wish, you are in your project, if the pictures for you are not as critical as for us, or if the trade-off control is against development speed and resource costs for you in the other direction, then you can safely replace it with CDN, modern CDN is doing well. <br><br>  Next comes the storage layer, on which we have a cluster of pairs of machines that reserve each other, and asynchronously copy files from one to another for any change. <br><br>  At the same time, some of these machines work with local hard drives. <br><br>  Some of these machines are connected to SANs. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6d6/e7c/889/6d6e7c889a7d21e92772e1561b014f83.png"><br><br>  And, on the one hand, it is more convenient to use and a little more productive, on the other hand, it is convenient in terms of density and price per gigabyte. <br><br>  This is such a brief overview of the architecture of what we got and how it all developed. <br><br>  A few more tips from the cap, quite simple. <br><br>  Firstly, if you suddenly decide that you urgently need to improve everything in your infrastructure of photos, first try it on, because perhaps you don‚Äôt need to improve anything. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2a4/4a9/052/2a44a9052747fb8cb1e5bfc139107112.png"><br><br>  I will give an example.  We have a cluster of machines, which gives photos from attachment'a in chat rooms, and that is where the scheme is still working since 2009, and no one suffers from it.  Everyone is good, everyone likes everything. <br><br>  In order to measure, first hang up a bunch of metrics, look at them, and then decide what you are unhappy with, what needs to be improved.  In order to measure this, we have a cool tool called Pinba. <br><br>  It allows you to collect a very detailed NGINX list for each request and response codes, and the distribution of times is everything.  He has a binding in all sorts of different systems for building analytics, and then you can watch all this beautifully. <br><br>  At first they measured and then improved. <br><br>  Further.  We optimize reading with cache, writing with sharding, but this is an obvious point. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/82f/96b/c81/82f96bc817442b10ad80500b0862bd84.png"><br><br>  Further.  If you are just starting to build your system, it is much better to take photos as immutable files.  Because you lose immediately a whole class of problems with cache invalidation, with how the logic has to find the right version of the photo and so on. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4f4/f33/425/4f4f334254766f8e1d4c9610d956561e.png"><br><br>  Suppose, weave poured, then turned it, make it so that it was physically another file.  Those.  no need to think: now I will save a little bit of space, write it down in the same file, change the version.  It always works bad, with this sweat a lot of headaches. <br><br>  The next item.  About resize on the fly. <br><br>  We used to, when upload users or a photo, cut a whole bunch of sizes at once for all occasions, under different clients, and they all lay on the disk.  Now we have abandoned it. <br><br>  We left only three main sizes: small, medium and large.  Everything else we just downscale from the size that is behind the one we were asked in Uport, just do downscale and give it to the user. <br><br>  The caching layer's CPU here is much cheaper than if we constantly re-generated these sizes on each storage area.  Suppose we want to add a new one, this business for a month - to get rid of the script everywhere, which would have done all this neatly, without putting down the cluster.  Those.  if there is an opportunity to choose now, it is better to do as little as possible of physical dimensions, but for at least some distribution to be, say, three.  And everything else is easy to resize on the fly using ready-made modules.  It is now all very easy and affordable. <br><br>  And incremental asynchronous backup is good. <br><br>  As our practice has shown, this scheme works great with deferred copying of modified files. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fcc/383/672/fcc383672aa42e9894a99838c8b4d816.png"><br><br>  The last point is also obvious.  If there are no such problems in your infrastructure now, but there is something that can break, it will surely break when it becomes a bit more.  Therefore, it is better to think about it in advance and not to experience problems.  That's all I wanted to say. <br><br><h3>  Contacts </h3><br>  ¬ª <a href="https://habrahabr.ru/users/bo0rsh201/" class="user_link">Bo0rsh201</a> <br>  ¬ª <a href="https://habrahabr.ru/company/badoo/">Badoo company blog</a> <br><br><blockquote>  <font color="gray">This report is a transcript of one of the best speeches at the conference of developers of high-loaded <a href="http://highload.ru/%3Futm_source%3Dhabr%26utm_medium%3Dmedia%26utm_campaign%3Dpast.articles%26utm_content%3Dcommon">HighLoad ++</a> systems.</font>  <font color="gray">Less than a month is left before the HighLoad ++ 2017 conference.</font> <font color="gray"><br><br></font>  <font color="gray">We have already prepared <a href="http://www.highload.ru/2017/abstracts">the conference program</a> , now the schedule is being actively formed.</font> <font color="gray"><br><br></font>  <font color="gray">This year we continue to explore the topic of architecture and scaling:</font> <font color="gray"><br><br></font> <ul><li>  <a href="http://www.highload.ru/2017/abstracts/2946.html%3Futm_source%3Dhabr%26utm_medium%3Dmedia%26utm_campaign%3Dpast.articles%26utm_content%3Dcommon">How to serve a billion users and give the terabit of traffic</a> / Igor Vasilyev </li><li>  <a href="http://www.highload.ru/2017/abstracts/3088.html%3Futm_source%3Dhabr%26utm_medium%3Dmedia%26utm_campaign%3Dpast.articles%26utm_content%3Dcommon">How to rewrite VKontakte personal message database from scratch and migrate to it without downtime</a> / Dmitry Egorov </li><li>  <a href="http://www.highload.ru/2017/abstracts/2843.html%3Futm_source%3Dhabr%26utm_medium%3Dmedia%26utm_campaign%3Dpast.articles%26utm_content%3Dcommon">Death from the sale: how Yandex.Money tried to accelerate to Black Friday and survive</a> / Anatoly Plaskovsky </li><li>  <a href="http://www.highload.ru/2017/abstracts/3003.html%3Futm_source%3Dhabr%26utm_medium%3Dmedia%26utm_campaign%3Dpast.articles%26utm_content%3Dcommon">Fail-safe architecture of the frontal system of the bank</a> / Roman Shekhovtsov, Alexey Gromatchikov </li><li>  <a href="http://www.highload.ru/2017/abstracts/2948.html%3Futm_source%3Dhabr%26utm_medium%3Dmedia%26utm_campaign%3Dpast.articles%26utm_content%3Dcommon">Payment system architecture: almost enterprise</a> / Philippe Delgyado </li></ul><br>  Also, some of these materials are used by us in an online training course on the development of high-load systems <a href="http://highload.guide/%3Futm_source%3Dhabr%26utm_medium%3Dmedia%26utm_campaign%3Dpast.articles%26utm_content%3Dcommon">HighLoad.Guide</a> is a chain of specially selected letters, articles, materials, videos.  Already, in our textbook more than 30 unique materials.  Get connected! <br></blockquote></div><p>Source: <a href="https://habr.com/ru/post/340976/">https://habr.com/ru/post/340976/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../340964/index.html">Perfect shuffle</a></li>
<li><a href="../340966/index.html">High-quality image reduction in constant time</a></li>
<li><a href="../340968/index.html">Window shop in Vivaldi 1.13.997.3</a></li>
<li><a href="../340972/index.html">10 most simple and frequent mistakes on sites</a></li>
<li><a href="../340974/index.html">Student supercomputer competitions: instructions for use</a></li>
<li><a href="../340978/index.html">PostgreSQL Indexes - 7</a></li>
<li><a href="../340982/index.html">Mathematical model of a vibrating level gauge with a resonator in the form of a cantilever elliptical tube</a></li>
<li><a href="../340984/index.html">Ethereum smart contract to calculate bonuses using fractional degrees</a></li>
<li><a href="../340986/index.html">Six questions for FrontFest speakers</a></li>
<li><a href="../340992/index.html">Blockchain: how it works, and why this technology will change the world</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>