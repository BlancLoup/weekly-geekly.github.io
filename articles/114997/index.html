<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Fuzzy search in text and dictionary</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 Fuzzy search algorithms (also known as similarity or fuzzy string search ) are the basis of spell check systems and full-fledged search...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Fuzzy search in text and dictionary</h1><div class="post__text post__text-html js-mediator-article"><h4>  Introduction </h4><br>  Fuzzy search algorithms (also known as <i>similarity</i> or <i>fuzzy string search</i> ) are the basis of spell check systems and full-fledged search engines like Google or Yandex.  For example, such algorithms are used for functions like ‚ÄúPerhaps you meant ...‚Äù in the same search engines. <br><br>  In this review article I will consider the following concepts, methods and algorithms: <br><ul><li>  Levenshtein distance </li><li>  Distance Damerau-Levenshteyn </li><li>  Bitap algorithm with modifications from Wu and Manber </li><li>  Sample extension algorithm </li><li>  N-gram method </li><li>  Signature hashing </li><li>  BK-trees </li></ul>  And also I will conduct a comparative testing of the quality and performance of the algorithms. <br><a name="habracut"></a><br><h4>  So... </h4><br>  Fuzzy search is an extremely useful feature of any search engine.  At the same time, its effective implementation is much more complicated than the implementation of a simple search by exact coincidence. <br><br>  The task of a fuzzy search can be formulated as follows: <br>  "For a given word, find in the text or a dictionary of size <b>n</b> all words that match this word (or begin with this word), taking into account <b>k</b> possible differences." 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      For example, when requesting "Machine" with two possible errors, find the words "Machine", "Machina", "Malina", "Kalina" and so on. <br><br>  Fuzzy search algorithms are characterized by a <i>metric</i> - a function of the distance between two words, which allows to evaluate the degree of their similarity in a given context.  A rigorous mathematical definition of a <i>metric</i> includes the need to meet the triangle inequality condition (X is a set of words, p is a metric): <br><br><img src="https://habrastorage.org/storage/habraeffect/f3/e4/f3e40b4ae60965d3ec0ee17e21a1435a.png" alt="Triangle inequality"><br><br>  Meanwhile, in most cases, the metric implies a more general concept that does not require the fulfillment of such a condition; this concept can also be called <i>distance</i> . <br><br>  Among the most well-known metrics are <i>Hamming</i> , <i>Levenshtein</i> and <i>Damerau-Levenshtein distances</i> .  Moreover, the Hamming distance is a metric only on a set of words of the same length, which severely limits its scope. <br><br>  However, in practice, the Hamming distance turns out to be practically useless, yielding to the more natural from the point of view of man metrics, which will be discussed below. <br><br><h4>  Levenshtein distance </h4><br>  The most commonly used metric is the Levenshtein distance, or the editing distance, whose calculation algorithms can be found at every step. <br>  Nevertheless, it is worth making a few remarks regarding the most popular calculation algorithm - <a href="http://algolist.manual.ru/search/lcs/vagner.php">the Wagner-Fisher method</a> . <br>  The original version of this algorithm has the time complexity <b>O (mn)</b> and consumes <b>O (mn)</b> memory, where <b>m</b> and <b>n</b> are the lengths of the compared strings.  The whole process can be represented by the following matrix: <br><br><img src="https://habrastorage.org/storage/habraeffect/00/50/0050eb93113049be1fa37d1656bb921a.png" alt="Levenshtein distance matrix"><br><br>  If you look at the process of the algorithm, it is easy to notice that at each step only the last two rows of the matrix are used, therefore, memory consumption can be reduced to <b>O (min (m, n))</b> . <br><br><img src="https://habrastorage.org/storage/habraeffect/93/e6/93e6af9530371facbf3f63bd6a33016d.png" alt="The process of the Levenshtein algorithm"><br><br>  But that's not all - you can further optimize the algorithm if the task is to find no more than k differences.  In this case, it is necessary to calculate in the matrix only a diagonal strip of width <b>2k + 1</b> (Ukkonen cut-off), which reduces the time complexity to <b>O (k min (m, n))</b> . <br><br><h5>  Prefix distance </h5><br>  It is also necessary to calculate the distance between the sample prefix and the string ‚Äî that is, to find the distance between the given prefix and the nearest prefix of the string.  In this case, you need to take the smallest of the distances from the prefix of the sample to all the prefixes of the string.  Obviously, the prefix distance cannot be considered a metric in a strict mathematical sense, which limits its use. <br><br>  Often with a fuzzy search, it is not so much the value of the distance itself that is important as the fact whether it exceeds a certain value. <br><br><h4>  Distance Damerau-Levenshteyn </h4><br>  This variation adds another rule to the definition of the Levenshtein distance: the <i>transposition</i> (permutation) of two adjacent letters is also taken into account as one operation, along with inserts, deletions and replacements. <br>  Just a couple of years ago, Frederic Damerau could guarantee that most typing errors are transpositions.  Therefore, it is this metric that gives the best results in practice. <br><br><img src="https://habrastorage.org/storage/habraeffect/31/61/31610d09a5734180a75b2274c6bc9901.png" alt="The process of the algorithm Damerau-Levenshteyn"><br><br>  To calculate such a distance, it is enough to slightly modify the algorithm for finding the usual Levenshtein distance as follows: store not the two, but the last three rows of the matrix, and add an appropriate additional condition ‚Äî if a transposition is detected, when calculating the distance, also take its cost into account. <br><br>  In addition to those discussed above, there are many other distances that are sometimes used in practice, such as <a href="http://en.wikipedia.org/wiki/Jaro%25E2%2580%2593Winkler_distance">the Jaro-Winkler metric</a> , many of which are available in the <a href="http://staffwww.dcs.shef.ac.uk/people/S.Chapman/simmetrics.html">SimMetrics</a> and <a href="http://sourceforge.net/projects/secondstring/">SecondString libraries</a> . <br><br><h4>  Fuzzy search algorithms without indexing (Online) </h4><br>  These algorithms are designed to search in a previously unknown text, and can be used, for example, in text editors, programs for viewing documents or web browsers to search the page.  They do not require preprocessing of the text and can work with a continuous stream of data. <br><br><h4>  Linear search </h4><br>  Simple sequential application of a given metric (for example, Levenshtein metric) to words from the input text.  When using a metric with a constraint, this method allows you to achieve optimal performance.  But, at the same time, the larger the <b>k</b> , the stronger the running time.  The asymptotic estimate of time is <b>O (kn)</b> . <br><br><h4>  Bitap (also known as Shift-Or or Baeza-Yates-Gonnet, and its modification from Wu-Manber) </h4><br>  The <i>Bitap</i> algorithm and its various modifications are most often used for fuzzy search without indexing.  Its variation is used, for example, in agix unix-utility, which performs functions similar to standard <a href="http://ru.wikipedia.org/wiki/Grep">grep</a> , but with the support of errors in the search query and even providing limited opportunities for the use of regular expressions. <br><br>  For the first time the idea of ‚Äã‚Äãthis algorithm was proposed by the citizens of <b>Ricardo Baeza-Yates</b> and <b>Gaston Gonnet</b> , having published the corresponding article in 1992. <br>  The original version of the algorithm deals only with symbol substitutions, and, in fact, calculates the <i>Hamming</i> distance.  But a little later, <b>Sun Wu</b> and <b>Udi Manber</b> proposed a modification of this algorithm to calculate the <i>Levenshtein distance</i> , i.e.  introduced support for inserts and deletions, and developed on its basis the first version of the agrep utility. <br><br>  <b>Bitshift operation</b> <br><br><img src="https://habrastorage.org/storage/habraeffect/53/a6/53a6cb76ace5d9ef13b85fcaf88d7259.png" alt="Bitshift operation"><br><br>  <b>Inserts</b> <br><img src="https://habrastorage.org/storage/habraeffect/19/1e/191e0882697342b80763901cb81d04af.png" alt="Inserts"><br>  <b>Deletions</b> <br><img src="https://habrastorage.org/storage/habraeffect/83/51/835183de0bef3933a9ba1d2690087c04.png" alt="Deletions"><br>  <b>Replacements</b> <br><img src="https://habrastorage.org/storage/habraeffect/ea/f5/eaf541b9354c2dbfe4c19fe7443322c9.png" alt="Replacements"><br>  <b>Resulting value</b> <br><img src="https://habrastorage.org/storage/habraeffect/2e/eb/2eeb9d2ca106a9a4303d838a1df23c99.png" alt="Result"><br><br>  Where <b>k</b> is the number of errors, <b>j</b> is the character index, <b>s</b> <sub>x</sub> is the character mask (in the mask, the unit bits are located at the positions corresponding to the positions of the given character in the request). <br>  The match or mismatch of the query is determined by the most recent bit of the resulting vector R. <br><br>  The high speed of this algorithm is provided by the bit parallelism of computations - in one operation it is possible to perform computations on 32 or more bits simultaneously. <br>  At the same time, the trivial implementation supports searching for words no longer than 32. This restriction is determined by the width of the standard <i>int</i> type (on 32-bit architectures).  You can use the types of large dimensions, but this may to some extent slow down the operation of the algorithm. <br><br>  Despite the fact that the asymptotic time of operation of this algorithm <b>O (kn)</b> coincides with that of the linear method, it is much faster with long queries and the number of errors <b>k is</b> more than 2. <br><br><h4>  Testing </h4><br>  Testing was carried out on the text of 3.2 million words, the average word length - 10. <br><br><h6>  Exact search </h6>  Search time: 3562 ms <br><br><h6>  Search using the Levenshtein metric </h6>  Search time at <i>k = 2</i> : 5728 ms <br>  Search time at <i>k = 5</i> : 8385 ms <br><br><h6>  Search using Bitap algorithm with Wu-Manber modifications </h6>  Search time at <i>k = 2</i> : 5499 ms <br>  Search time at <i>k = 5</i> : 5928 ms <br><br>  Obviously, a simple brute force using the metric, unlike the Bitap algorithm, strongly depends on the number of errors <b>k</b> . <br><br>  However, if it comes to searching in large unchanged texts, the search time can be significantly reduced by performing preprocessing of such text, also called <i>indexing</i> . <br><br><h4>  Algorithms of fuzzy search with indexing (Offline) </h4><br>  A feature of all fuzzy search algorithms with indexing is that the index is based on a dictionary compiled from the source text or a list of records in a database. <br><br>  These algorithms use different approaches to solving the problem - some of them use reduction to an exact search, others use metric properties to build various spatial structures, and so on. <br><br>  First of all, the first step in the source text is to build a dictionary containing words and their positions in the text.  Also, you can count the frequency of words and phrases to improve the quality of search results. <br><br>  It is assumed that the index, as well as the dictionary, is entirely loaded into memory. <br><br>  Technical characteristics of the dictionary: <ul><li>  The source text is 8.2 gigabytes of Moshkov library materials ( <a href="http://lib.ru/">lib.ru</a> ), 680 million words; </li><li>  Dictionary size is 65 megabytes; </li><li>  The number of words - 3.2 million; </li><li>  The average word length is 9.5 characters; </li><li>  The mean square word length (may be useful in evaluating some algorithms) is 10.0 characters; </li><li>  Alphabet - capital letters AZ, without E (to simplify some operations).  Words containing non-alphabetical characters are not included in the dictionary. </li></ul>  The dependence of the size of the dictionary on the volume of the text is not strictly linear ‚Äî up to some volume, the basic framework of words is formed, ranging from 15% for 500 thousand words to 5% for 5 million, and then the dependence approaches linear, slowly decreasing and reaching 0.5% for 680 million words.  Subsequent preservation of growth is provided in the majority due to rare words. <br><br><img src="https://habrastorage.org/storage/habraeffect/47/26/4726c1e62497ddcee6dcb649c9b84448.png" alt="Dictionary size growth"><br><br><h4>  Sample extension algorithm </h4><br>  This algorithm is often used in spell-checking systems (i.e., in spell-checkers), where the size of the dictionary is small, or where the speed of work is not the main criterion. <br>  It is based on the reduction of the fuzzy search problem to the exact search problem. <br><br>  A set of ‚Äúerroneous‚Äù words is constructed from the initial request, for each of which an exact search is then performed in the dictionary. <br><br><img src="https://habrastorage.org/storage/habraeffect/ba/45/ba45205a3a3d2b6d18e36dad0116ee14.png" alt="Sample extension"><br><br>  The time of its operation depends strongly on the number k of errors and on the size of the alphabet A, and in the case of using a binary dictionary search, it is: <br><img src="https://habrastorage.org/storage/habraeffect/c8/93/c8931f2456845c659ea5b4b57e407809.png" alt="image"><br><br>  For example, if <i>k = 1</i> and a word of length 7 (for example, ‚ÄúCrocodile‚Äù) in the Russian alphabet, the set of erroneous words will be about 450 in size, that is, 450 requests to the dictionary will have to be made, which is quite acceptable. <br>  But already for <i>k = 2, the</i> size of such a set will be more than 115 thousand variants, which corresponds to a complete enumeration of a small dictionary, or 1/27 in our case, and, therefore, the work time will be long enough.  At the same time, one should not forget that for each of these words it is necessary to conduct a search for an exact match in the dictionary. <br><br><h6>  Features: </h6>  The algorithm can be easily modified to generate ‚Äúerroneous‚Äù variants according to arbitrary rules, and, moreover, it does not require any preprocessing of the dictionary, and, accordingly, additional memory. <br><br><h6>  Possible improvements: </h6>  It is possible to generate not all the set of ‚Äúerroneous‚Äù words, but only those that are most likely to occur in a real situation, for example, words, taking into account common spelling errors or typing errors. <br><br><h4>  N-gram method </h4><br>  This method was invented quite a long time ago, and is the most widely used, since its implementation is extremely simple, and it provides fairly good performance.  The algorithm is based on the principle: <br>  "If the word A coincides with the word B, taking into account several errors, then with a high degree of probability they will have at least one common substring of length N". <br>  These substrings of length N are called N-grams. <br>  During indexing, the word is broken down into such N-grams, and then the word falls into the lists for each of these N-grams.  During the search, the query is also divided into N-grams, and each of them is sequentially searched through the list of words containing such a substring. <br><br><img src="https://habrastorage.org/storage/habraeffect/57/98/579859076e907968952471e047fd8827.png" alt="N-gram method"><br><br>  The most frequently used in practice are trigrams - substrings of length 3. The choice of a larger N value leads to a restriction on the minimum word length, at which error detection is already possible. <br><br><h6>  Features: </h6>  The N-gram algorithm does not find all possible misspelled words.  If you take, for example, the word VOTKA, and decompose it into trigrams: VO KA ‚Üí VO <b>T</b> O TK <b>T</b> KA, then you will notice that they all contain an error T. Thus, the word "VODKA" will not be found, because it does not contain any of these trigrams, and does not fall into the corresponding lists.  Thus, the shorter the word length and the more errors it contains, the higher the chance that it will not fit into the lists corresponding to the N-grams of the query, and will not be present in the result. <br><br>  Meanwhile, the N-gram method leaves full scope for using own metrics with arbitrary properties and complexity, but you have to pay for it - if you use it, there is a need for sequential enumeration of about 15% of the dictionary, which is quite a lot for large dictionaries. <br><br><h6>  Possible improvements: </h6>  You can break the N-gram hash tables by word length and by the position of the N-gram in the word (modification 1).  As the length of the searched word and query cannot differ by more than <b>k</b> , so the positions of the N-gram in the word can differ by no more than k.  Thus, it will be necessary to check only the table corresponding to the position of this N-gram in the word, as well as the k tables on the left and k tables on the right, i.e.  total <b>2k + 1</b> adjacent tables. <br><br><img src="https://habrastorage.org/storage/habraeffect/31/aa/31aabd5501fe3712b37aa5c5d388f738.png" alt="Modification 1 of the N-gram method"><br><br>  It is possible to slightly reduce the size of the set required for viewing by splitting the tables along the length of the word, and similarly looking only at the adjacent <b>2k + 1</b> tables (modification 2). <br><br><h4>  Signature hashing </h4><br>  This algorithm is described in the article LM Boitsova.  "Signature hashing".  It is based on a fairly obvious representation of the word ‚Äústructure‚Äù in the form of bit bits used as a hash (signature) in the hash table. <br><br>  When indexing such hashes are calculated for each of the words, and the table corresponds to the list of dictionary words to this hash.  Then, during the search, the hash is calculated for the request and all neighboring hashes that differ from the original in no more than k bits are searched.  For each of these hashes, the list of words corresponding to it is searched. <br><br>  The process of calculating the hash - each bit of the hash is associated with a group of characters from the alphabet.  Bit 1 at position <i>i</i> in the hash means that the source word contains a symbol from the <i>i-th</i> alphabet group.  The order of the letters in the word is absolutely irrelevant. <br><br><img src="https://habrastorage.org/storage/habraeffect/b6/19/b619ac1d4f823ab6f05ef845282b4f44.png" alt="Signature hashing"><br><br>  Deleting a single character either does not change the hash value (if there are still characters from the same alphabet group in the word), or the bit corresponding to this group changes to 0. When inserted, in the same way, either one bit will be 1 or no changes will be made.  When replacing characters, everything is a bit more complicated - the hash can either remain completely unchanged or change in 1 or 2 positions.  When permutations, no changes occur at all, because the order of the characters in constructing the hash, as noted earlier, is not taken into account.  Thus, to fully cover the k errors, you need to change at least <b>2k</b> bits in the hash. <br><br><img src="https://habrastorage.org/storage/habraeffect/42/9a/429a79a33e8a322f317e7a9522877b9e.png" alt="List of hash bugs"><br><br>  Operating time, on average, with k "incomplete" (insertion, deletion and transposition, as well as a small part of the substitutions) errors: <br><img src="https://habrastorage.org/storage/habraeffect/b6/a0/b6a09b40a753e6cfa3638c3b3ac6dce4.png" alt="Asymptotic signature hashing time"><br><br><h6>  Features: </h6>  From the fact that replacing one character can change two bits at once, an algorithm that implements, for example, distortion of no more than 2 bits at the same time will not actually produce the full amount of results due to the absence of a significant part (depending on the ratio of the size of the hash to the alphabet) of the words with two substitutions (and the larger the size of the hash, the more often the replacement of the symbol will lead to distortion of two bits at once, and the less complete the result).  In addition, this algorithm does not allow prefix searches. <br><br><h4>  BK-trees </h4><br>  The <i>Burkhard-Keller</i> trees are metric trees, the algorithms for constructing such trees are based on the metric property corresponding to the triangle inequality: <br><br><img src="https://habrastorage.org/storage/habraeffect/f3/e4/f3e40b4ae60965d3ec0ee17e21a1435a.png" alt="Triangle inequality"><br><br>  This property allows metrics to form metric spaces of arbitrary dimension.  Such metric spaces are not necessarily <i>Euclidean</i> , for example, the <i>Levenshtein</i> and <i>Damerau-Levenshtein</i> metrics form <i>non</i> - <i>Euclidean</i> spaces.  Based on these properties, you can build a data structure that searches in such a metric space, which is the Barkhard-Keller trees. <br><br><img src="https://habrastorage.org/storage/habraeffect/c4/fa/c4fa48469d65ab0af2e6212ec0970c91.png" alt="Bk-tree"><br><br><h6>  Improvements: </h6>  You can use the ability of some metrics to calculate the distance with a constraint, setting an upper limit equal to the sum of the maximum distance to the children of the vertex and the resulting distance, which will speed up the process a bit: <br><br><img src="https://habrastorage.org/storage/habraeffect/26/3f/263fec663f341a02274764d381aae1e3.png" alt="The restriction of metrics in the algorithm of BK-trees"><br><br><h4>  Testing </h4><br>  Testing was carried out on a laptop with Intel Core Duo T2500 (2GHz / 667MHz FSB / 2MB), 2Gb of RAM, OS - Ubuntu 10.10 Desktop i686, JRE - OpenJDK 6 Update 20. <br><br><img src="https://habrastorage.org/storage/habraeffect/e4/59/e4598435e0286cccc8decffbace314b0.png" alt="Work time comparison"><br><br>  Testing was carried out using the Damerau-Levenshtein distance and the number of errors <i>k = 2</i> .  The size of the index is specified along with the dictionary (65 MB). <br><br><h6>  Sample extension </h6>  Index size: 65 MB <br>  Search time: 320 ms / 330 ms <br>  Completeness of results: 100% <br><br><h6>  N-gram (original) </h6>  Index size: 170 MB <br>  Index creation time: 32 s <br>  Search time: 71 ms / 110 ms <br>  Completeness of results: 65% <br><br><h6>  N-gram (modification 1) </h6>  Index size: 170 MB <br>  Index creation time: 32 s <br>  Search time: 39 ms / 46 ms <br>  Completeness of results: 63% <br><br><h6>  N-gram (modification 2) </h6>  Index size: 170 MB <br>  Index creation time: 32 s <br>  Search time: 37 ms / 45 ms <br>  Completeness of results: 62% <br><br><h6>  Signature hashing </h6>  Index size: 85 MB <br>  Index creation time: 0.6 s <br>  Search time: 55 ms <br>  Completeness of results: 56.5% <br><br><h6>  BK-trees </h6>  Index size: 150 MB <br>  Index creation time: 120 s <br>  Search time: 540 ms <br>  Completeness of results: 63% <br><br><h4>  Total </h4><br>  Most fuzzy search algorithms with indexing are not truly sublinear (that is, having an asymptotic operating time of <b>O (log n)</b> or lower), and their speed of operation usually directly depends on <b>N.</b>  Nevertheless, multiple improvements and improvements make it possible to achieve a sufficiently small working time, even with very large volumes of dictionaries. <br><br>  There are also many more diverse and inefficient methods, based, among other things, on the adaptation of various, already-used techniques and techniques to this subject area.  Among such methods is the <a href="http://www.cs.mcgill.ca/~tim/tries/tries.html">adaptation of prefix trees (Trie) to fuzzy search problems</a> , which I left unattended in view of its low efficiency.  But there are algorithms based on original approaches, for example, <a href="http://yury.name/internet/09ia-seminar.ppt">the <i>Maass-Novak</i> algorithm</a> , which, although it has sublinear asymptotic operation time, is extremely inefficient due to the huge constants behind such a time estimate, which manifest themselves in the form of a huge index size. <br><br>  The practical use of fuzzy search algorithms in real search engines is closely related to <a href="http://habrahabr.ru/blogs/algorithm/114947/">phonetic algorithms</a> , lexical stemming algorithms - extracting the base part from different word forms of the same word (for example, <a href="http://snowball.tartarus.org/">Snowball</a> and <a href="http://company.yandex.ru/technology/mystem/">Yandex mystem</a> provide this functionality), as well as ranking based on statistical information or using sophisticated sophisticated metrics. <br><br>  You can find my Java implementations on the <a href="http://code.google.com/p/fuzzy-search-tools">http://code.google.com/p/fuzzy-search-tools</a> link: <ul><li>  Levenshtein distance (with clipping and prefix option); </li><li>  Damerau-Levenshtein distance (with clipping and prefix option); </li><li>  Bitap algorithm (Shift-OR / Shift-AND with modifications of Wu-Manber); </li><li>  Sample expansion algorithm; </li><li>  N-gram method (original and with modifications); </li><li>  Signature hashing method; </li><li>  BK-trees. </li></ul>  I wanted to make the code easy to understand, and at the same time quite effective for practical use.  Squeezing the last juice from the JVM was not part of my task.  Enjoy <br><br>  It is worth noting that in the process of studying this topic, I had some own developments that allow us to reduce the search time by an order of magnitude due to a moderate increase in the size of the index and some restriction in the freedom to choose metrics.  But that's another story. <br><br><h4>  References: </h4><ol><li>  Source code for the article in Java.  <a href="http://code.google.com/p/fuzzy-search-tools">http://code.google.com/p/fuzzy-search-tools</a> </li><li>  Levenshtein distance.  <a href="http://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D1%2581%25D1%2582%25D0%25BE%25D1%258F%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%259B%25D0%25B5%25D0%25B2%25D0%25B5%25D0%25BD%25D1%2588%25D1%2582%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25B0">http://ru.wikipedia.org/wiki/Levenshteyn_Distance</a> </li><li>  Distance Damerau-Levenshteyn.  <a href="http://en.wikipedia.org/wiki/Damerau%25E2%2580%2593Levenshtein_distance">http://en.wikipedia.org/wiki/Damerau‚ÄìLevenshtein_distance</a> </li><li>  A good description of the Shift-Or with modifications Wu-Manber, however, in German.  <a href="http://de.wikipedia.org/wiki/Baeza-Yates-Gonnet-Algorithmus">http://de.wikipedia.org/wiki/Baeza-Yates-Gonnet-Algorithmus</a> </li><li>  N-gram method.  <a href="http://www.cs.helsinki.fi/u/ukkonen/TCS92.pdf">http://www.cs.helsinki.fi/u/ukkonen/TCS92.pdf</a> </li><li>  Signature hashing.  <a href="">http://itman.narod.ru/articles/rtf/confart.zip</a> </li><li>  The site of Leonid Moiseevich Boytsov, entirely devoted to fuzzy search.  <a href="http://itman.narod.ru/">http://itman.narod.ru/</a> </li><li>  Implementation of Shift-Or and some other algorithms.  <a href="http://johannburkard.de/software/stringsearch/">http://johannburkard.de/software/stringsearch/</a> </li><li>  Fast Text Searching with Agrep (Wu &amp; Manber).  <a href="">http://www.at.php.net/utils/admin-tools/agrep/agrep.ps.1</a> </li><li>  Damn Cool Algorithms - Levenshtein automaton, BK-trees, and some other algorithms.  <a href="http://blog.notdot.net/2007/4/Damn-Cool-Algorithms-Part-1-BK-Trees">http://blog.notdot.net/2007/4/Damn-Cool-Algorithms-Part-1-BK-Trees</a> </li><li>  Java BK trees  <a href="http://code.google.com/p/java-bk-tree/">http://code.google.com/p/java-bk-tree/</a> </li><li>  The Maassa-Novak algorithm.  <a href="http://yury.name/internet/09ia-seminar.ppt">http://yury.name/internet/09ia-seminar.ppt</a> </li><li>  SimMetrics Metrics Library.  <a href="http://staffwww.dcs.shef.ac.uk/people/S.Chapman/simmetrics.html">http://staffwww.dcs.shef.ac.uk/people/S.Chapman/simmetrics.html</a> </li><li>  SecondString metrics library.  <a href="http://sourceforge.net/projects/secondstring/">http://sourceforge.net/projects/secondstring/</a> </li></ol><br>   : <a href="http://ntz-develop.blogspot.com/2011/03/fuzzy-string-search.html">Fuzzy string search</a> </div><p>Source: <a href="https://habr.com/ru/post/114997/">https://habr.com/ru/post/114997/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../114991/index.html">About the benefits and dangers of specialization for developers</a></li>
<li><a href="../114992/index.html">[DIY] Usability Desk</a></li>
<li><a href="../114993/index.html">Free Apache and Eclipse License Transfers</a></li>
<li><a href="../114994/index.html">Traveling on board</a></li>
<li><a href="../114995/index.html">Math on the Internet - harmless fun or universal evil</a></li>
<li><a href="../114998/index.html">Hardware accelerated graphics in Flash on Windows and Linux</a></li>
<li><a href="../114999/index.html">DevCon'11 - Software Development Conference</a></li>
<li><a href="../115000/index.html">Dynamic pricing concept</a></li>
<li><a href="../115001/index.html">Acer Aspire One Happy or a netbook with android</a></li>
<li><a href="../115002/index.html">Control start tasks using Twitter</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>