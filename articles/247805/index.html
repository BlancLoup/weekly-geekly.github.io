<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Concurrent programming with CUDA. Part 3: Fundamental GPU algorithms: convolution (reduce), scan (scan) and histogram (histogram)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Content 
 Part 1: Introduction. 
 Part 2: GPU hardware and parallel communication patterns. 
 Part 3: GPU fundamental algorithms: convolution (reduce)...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Concurrent programming with CUDA. Part 3: Fundamental GPU algorithms: convolution (reduce), scan (scan) and histogram (histogram)</h1><div class="post__text post__text-html js-mediator-article"><h4>  Content </h4><br>  <a href="http://habrahabr.ru/company/epam_systems/blog/245503/">Part 1: Introduction.</a> <br>  <a href="http://habrahabr.ru/company/epam_systems/blog/245523/">Part 2: GPU hardware and parallel communication patterns.</a> <br>  <b>Part 3: GPU fundamental algorithms: convolution (reduce), scan (scan) and histogram (histogram).</b> <br>  Part 4: Fundamental GPU algorithms: compaction (compact), segmented scan (segmented scan), sorting.  Practical application of some algorithms. <br>  Part 5: Optimization of GPU programs. <br>  Part 6: Examples of parallelization of sequential algorithms. <br>  Part 7: Additional Parallel Programming Topics, Dynamic Concurrency. <br><br><div class="spoiler">  <b class="spoiler_title">Disclaimer</b> <div class="spoiler_text">  This part is mainly theoretical, and most likely you will not need to practice - all of these algorithms have long been implemented in many libraries. <br></div></div><br><a name="habracut"></a><br><h4>  The number of steps (steps) vs the number of operations (work) </h4><br>  Many readers of Habr are probably familiar with the notation of a <a href="https://ru.wikipedia.org/wiki/%25C2%25ABO%25C2%25BB_%25D0%25B1%25D0%25BE%25D0%25BB%25D1%258C%25D1%2588%25D0%25BE%25D0%25B5_%25D0%25B8_%25C2%25ABo%25C2%25BB_%25D0%25BC%25D0%25B0%25D0%25BB%25D0%25BE%25D0%25B5">large O</a> , used to estimate the running time of the algorithms.  For example, they say that the running time of the <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25BE%25D1%2580%25D1%2582%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25BA%25D0%25B0_%25D1%2581%25D0%25BB%25D0%25B8%25D1%258F%25D0%25BD%25D0%25B8%25D0%25B5%25D0%25BC">merge sorting</a> algorithm can be estimated as <i>O (n * log (n))</i> .  Actually, this is not entirely correct: it would be more correct to say that the running time of this algorithm using a <b>single</b> processor or the <b>total number of operations</b> can be estimated as <i>O (n * log (n))</i> .  For clarification, consider the algorithm execution tree: <br><img src="https://habrastorage.org/files/3ca/967/d24/3ca967d2435249998c7a6e4c71783831.png"><br>  So, we start with an unsorted array of <i>n</i> elements.  Next, 2 recursive calls are made to sort the left and right half of the array, and then merge the sorted halves, which is done in <i>O (n)</i> operations.  Recursive calls will be executed until the size of the sorted part becomes 1 - the array of one element is always sorted.  This means that the tree height is <i>O (log (n))</i> and at each level <i>O (n)</i> operations are performed (at the 2nd level - 2 merges in <i>O (n / 2)</i> , on the 3rd - 4 merges in <i>O ( n / 4)</i> , etc.).  Total - <i>O (n * log (n))</i> operations.  If we have only one processor (a processor is some abstract handler, not a CPU), then this is also an estimate of the execution time of the algorithm.  However, suppose that we could somehow perform a merge <b>in parallel with several handlers</b> - at best, we would divide <i>O (n)</i> operations at each level so that each handler performs a constant number of operations - then each level will be executed in <i>O ( 1)</i> time, and the estimate of the execution time of the entire algorithm becomes <i>O (log (n))</i> ! <br>  In simple terms, the number of steps will be equal to the height of the algorithm execution tree (provided that operations of the same level can be performed independently of each other - therefore, in standard form, the number of steps of the merge sort algorithm is not equal to <i>O (log (n))</i> - operations of one level are not independent), well, the number of operations - the number of operations :) Thus, when programming on the GPU, it makes sense to use algorithms with fewer steps, sometimes even by increasing the total number of operations. <br>  Next, we consider different implementations of 3 fundamental algorithms for parallel programming and analyze them in terms of the number of steps and operations. <br><br><h4>  Convolution (reduce) </h4><br>  The convolution operation is performed on a certain array of elements and is determined by the convolution operator.  The convolution operator must be <b>binary</b> and <b>associative</b> ‚Äî that is, take 2 elements as input and satisfy the equality <i>a <b>*</b> (b <b>*</b> c) = (a <b>*</b> b) <b>*</b> c</i> , where <i><b>*</b></i> is the operator's symbol (not to be confused with the commutative property <i>a <b>*</b> b = b <b>*</b> a</i> ).  The convolution operation on an array of elements <i>a <sub>1</sub> , ..., a <sub>n</sub></i> is defined as <i>(... ((a <sub>1</sub> <b>*</b> a <sub>2</sub> ) <b>*</b> a <sub>3</sub> ) ... * a <sub>n</sub> )</i> .  The execution tree of the sequential algorithm for the implementation of the convolution operation has the following form: <br><img src="https://habrastorage.org/files/fc2/6de/3f9/fc26de3f95e54f66919e31b2d8ef018c.png"><br>  Obviously, for this algorithm, the number of operations is equal to the number of steps and is equal to <i>n-1 = O (n)</i> . <br>  To ‚Äúparallelize‚Äù the algorithm, it suffices to take into account the property of associativity of the convolution operator, and rearrange the parentheses in places: <i>(... ((a <sub>1</sub> <b>*</b> a <sub>2</sub> ) <b>*</b> a <sub>3</sub> ) ... * a <sub>n</sub> ) = (a <sub>1</sub> <b>*</b> a <sub>2</sub> ) <b>*</b> ( a <sub>3</sub> <b>*</b> a <sub>4</sub> ) <b>*</b> ... <b>*</b> (a <sub>n-1</sub> <b>*</b> a <sub>n</sub> )</i> .  That is, we can in parallel calculate the values <i>(a <sub>1</sub> <b>*</b> a <sub>2</sub> )</i> , <i>(a <sub>3</sub> <b>*</b> a <sub>4</sub> )</i> and so on, and then perform a convolution operation on the resulting values.  Execution tree for such an algorithm: <br><img src="https://habrastorage.org/files/44a/cf3/3e4/44acf33e4d0543c49a324289e213a1a8.png"><br>  The number of steps is now equal to <i>O (log (n))</i> , the number of operations is <i>O (n)</i> , which is good news ‚Äî using a simple permutation of the parentheses, we obtained an algorithm with a significantly smaller number of steps, while the number of operations remained the same! 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Scan </h4><br>  The scan operation is also performed on the array of elements, but is determined by the scan operator <b>and the identity element</b> .  The scan operator must meet the same requirements as the convolution operator;  the unit element must have the property <i>I <b>*</b> a = a</i> , where <i>I</i> is the unit element, <i><b>*</b></i> is the scan operator, and <i>a</i> is any other element.  For example, for the addition operator, the unit element is 0, for the multiplication operator, 1. The result of applying the scan operation to the array of elements <i>a <sub>1</sub> , ..., a <sub>n</sub></i> is an array of the same dimension <i>n</i> .  There are two types of scanning operations: <br><ol><li>  Inclusive scanning - the result is calculated as: <i>[reduce ([a <sub>1</sub> ]), reduce ([a <sub>1</sub> , a <sub>2</sub> ]), ..., reduce ([a <sub>1</sub> , ..., a <sub>n</sub> ]))</i> - in place <i>i</i> th input element in the output array will be the result of applying the convolution operation of all previous elements <b>including the <i>i-</i> th element itself</b> . </li><li>  Exclusive scanning - the result is calculated as: <i>[I, reduce ([a <sub>1</sub> ]), ..., reduce ([a <sub>1</sub> , ..., a <sub>n-1</sub> ])]</i> - in place of the <i>i</i> -th input element in the output array there will be the result of applying the convolution operation of all previous elements <b>excluding the <i>i-</i> th element itself</b> - therefore the first element of the output array will be a single element. </li></ol><br>  The scanning operation is not so useful in itself, but it is one of the stages of many parallel algorithms.  If you have an implementation at hand that only includes scanning, and you need an exclusive one - just pass the array <i>[I, a <sub>1</sub> , ..., a <sub>n-1</sub> ]</i> instead of the array <i>[a <sub>1</sub> , ..., a <sub>n</sub> ]</i> ;  if on the contrary, pass the array <i>[a <sub>1</sub> , ..., a <sub>n</sub> , I]</i> and remove the first element from the resulting array.  Thus, both types of scanning are interchangeable.  The execution tree of the sequential implementation of the scan operation will look the same as the convolution operation tree ‚Äî just before each vertex of the tree we will write the current result of the convolution ( <i>a <sub>1</sub></i> before the first calculation for the inclusive scan and <i>I</i> for the exclusive) to the appropriate position of the output array. <br>  Therefore, the number of steps and operations of such an algorithm will be equal to <i>n ‚àí 1 = O (n)</i> . <br>  The easiest way to reduce the number of steps of the algorithm is fairly straightforward - since the scan operation is essentially determined by the convolution operation, what prevents us from simply starting the parallel version of the convolution operation <i>n</i> times?  The number of steps in this case will indeed decrease - since all convolutions can be calculated independently, then the total number of steps is determined by the convolution with the largest number of steps - namely, the last one, which will be calculated on the entire input array.  Total - <i>O (log (n))</i> steps.  However, the number of operations of such an algorithm is depressing - the first convolution will require 0 operations (not taking into account memory operations), the second - 1, ..., the last - <i>n-1</i> operations, total - <i>1 + 2 + ... + n-1 = (n-1) * (n) / 2 = O (n <sup>2</sup> )</i> . <br>  Consider 2 more efficient in terms of the number of operations of the algorithm for performing a scanning operation.  The authors of the first algorithm - Daniel Hillis and Guy Steele, the algorithm is named after them - Hills &amp; Steele scan.  The algorithm is very simple, and can be written in 6 lines of python-pseudo-code: <br><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">hillie_steele_scan</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(io_arr)</span></span></span><span class="hljs-function">:</span></span> N = len(io_arr) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> step <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(int(log(N, <span class="hljs-number"><span class="hljs-number">2</span></span>))+<span class="hljs-number"><span class="hljs-number">1</span></span>): dist = <span class="hljs-number"><span class="hljs-number">2</span></span>**step <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(N<span class="hljs-number"><span class="hljs-number">-1</span></span>, dist<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>): io_arr[i] = io_arr[i] + io_arr[i-dist]</code> </pre> <br>  Or in words: starting from step 0 and ending with log <sub>2</sub> (N) step (folding the fractional part), at each step <i>step</i> each element under the index <i>i</i> updates its value as <i>a [i] = a [i] + a [i-2 <sup>step</sup> ]</i> (naturally, if <i>2 <sup>step</sup> &lt;= i</i> ).  If it is in mind to trace the execution of this algorithm for an array, it becomes clear why it is correct: after step 0, each element will contain the sum of itself and one adjacent element on the left;  after step 1 - the sum of itself and 3 neighboring elements on the left ... at step <i>n</i> - the sum of itself and <i>2 <sup>n + 1</sup> -1</i> elements on the left - it is obvious that if the number of steps is equal to the integer part of log <sub>2</sub> (N), then after the last step we will get an array corresponding to the operation of <b>enabling</b> scanning.  From the description of the algorithm it is obvious that the number of steps is equal to <i>log <sub>2</sub> (N) + 1 = O (log (n))</i> .  The number of operations is <i>(N-1) + (N-2) + (N-4) ... + (N-2 <sup>log <sub>2</sub> (N)</sup> ) = O (N * log (N))</i> .  The execution tree of the algorithm on the example of an array of 8 elements and the sum operator looks like this: <br><img src="https://habrastorage.org/files/01a/239/c0d/01a239c0dad44ffa914faefe31bcffa2.png"><br>  The author of the second algorithm is Guy Blelloch, the algorithm is called (who would have thought) Blelloch scan.  This algorithm implements <b>exclusive</b> scanning and is more complicated than the previous one, however, it requires fewer operations.  The basic idea of ‚Äã‚Äãthe algorithm is that if you look closely at the implementation of the parallel convolution algorithm, you can see that as we calculate the final value, we also calculate many useful intermediate values, for example, after the 1st step - the values <nobr><i>a <sub>1</sub> <b>*</b> a <sub>2</sub> , a <sub>3</sub> <b>*</b> a <sub>4</sub> , ..., a <sub>n-1</sub> * a <sub>n</sub></i></nobr> , after the 2nd step, the values ‚Äã‚Äãare <nobr><i>a <sub>1</sub> <b>*</b> a <sub>2</sub> <b>*</b> a <sub>3</sub> <b>*</b> a <sub>4</sub> , a <sub>5</sub> <b>*</b> a <sub>6</sub> <b>*</b> a <sub>7</sub> <b>*</b> a <sub>8</sub> ...</i></nobr> and etc.  And if these values ‚Äã‚Äãare not discarded, then we can very quickly calculate, for example, the convolution of the first six elements - it is enough to take the already calculated convolution values ‚Äã‚Äãof the first 4 elements and elements <nobr><i>a <sub>5</sub> ,</i> a <sub>6</sub></nobr> and ‚Äúminimize‚Äù them.  Therefore, the algorithm consists of 2 phases - in fact, the phase of convolution, and the second phase, called down-sweep (something like ‚Äúscan down‚Äù).  Graphically, the first phase looks like this (using the example of the same array of 8 elements and the sum operator): <br><img src="https://habrastorage.org/files/e07/c45/568/e07c45568d634b04b96c66496ed4ff8c.png"><br>  That is, practically the usual convolution algorithm, only intermediate convolutions, calculated over the elements <nobr><i>a <sub>i</sub> , a <sub>i + 1</sub> , ..., a <sub>i + k,</sub></i></nobr> replace the element <nobr><i>a <sub>i + k</sub></i></nobr> in the array. <br>  The second phase is almost the mirror image of the first, but the ‚Äúspecial‚Äù operator will be used, which returns 2 values, and at the beginning of the phase the last value of the array is replaced with a single element (0 for the addition operator): <br><img src="https://habrastorage.org/files/d17/7f0/106/d177f0106c9342a7bc6f6ae0d1b777b8.png"><br>  This ‚Äúspecial‚Äù operator takes 2 values ‚Äã‚Äã‚Äî left and right; in this case, it simply returns the right input as the left output, and the result of applying the scan operator to the left and right input values ‚Äã‚Äãas the right output.  All these manipulations are needed in order to eventually minimize the calculated intermediate convolutions and get the desired result - eliminating scanning the input array.  As is clearly seen from this illustration of the algorithm, the total number of operations is <i>N-1 + N-1 + N-1 = O (N)</i> , and the number of steps is <i>2 * log <sub>2</sub> (N) = O (log (N))</i> .  As usual, all that is good (improved asymptotics in this case) has to be paid - the algorithm is not only more complicated in pseudo-code, but also more difficult to implement effectively on the GPU - in the first steps of the algorithm we have quite a lot of work that can be done in parallel;  when approaching the end of the first phase and at the beginning of the second phase, very little work will be done at each step;  and at the end of the second phase, we will again have a lot of work at each step (by the way, several interesting parallel algorithms have this execution pattern).  One of the possible solutions to this problem is to write 2 different cores - one will perform only one step, and be used at the beginning and at the end of the algorithm;  the second will be designed to perform several steps in a row and will be used in the middle of execution - at the transition between phases.  Well, on the host side it will be determined which kernel to call now, depending on the amount of work that needs to be done in the current step. <br><br><h4>  Histogram (histogram) </h4><br>  Informally, a histogram in the context of GPU programming (and not only) is the distribution of an array of elements over an array of cells, where each cell can contain only elements with certain properties.  For example, having data on basketball players, such as height, weight, and so on.  we want to know how many basketball players are &lt;180 cm tall, from 180 cm to 190 cm and&gt; 190 cm. <br>  As usual, the sequential algorithm for computing the histogram is quite simple - just walk through all the elements in the array, and for each element, increase by 1 the value in the corresponding cell.  The number of steps and operations is <i>O (N)</i> . <br>  The simplest parallel histogram calculation algorithm is to run downstream to an array element, and each stream will increase the value in the cell only for its own element.  Naturally, in this case it is necessary to use atomic operations.  The disadvantage of this method is speed.  Atomic operations force the threads to synchronize access to the cells and with an increase in the number of elements, the efficiency of this algorithm will fall. <br>  One of the ways to avoid the use of atomic operations when building a histogram is to use a separate set of cells for each stream and the subsequent convolution of these local histograms.  The disadvantage is that with a large number of threads we may simply not have enough memory to store all local histograms. <br>  Another option to improve the efficiency of the simplest algorithm takes into account the specifics of CUDA, namely that we run threads in blocks that have common memory, and we can make a common histogram for all threads of one block, and add the same atomic operations to the end of the kernel this histogram to global.  And although to build a common histogram of the block, it is still necessary to use atomic operations on the total memory of the block, they are much faster than atomic operations on global memory. <br><br><h4>  Conclusion </h4><br>  This part describes the basic primitives of many parallel algorithms.  The practical implementation of most of these primitives will be shown in the next section, where we write bitwise sorting as an example. </div><p>Source: <a href="https://habr.com/ru/post/247805/">https://habr.com/ru/post/247805/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../247787/index.html">Quantum mechanics. Theoretical minimum</a></li>
<li><a href="../247789/index.html">How I loved vim, emacs and keyboard</a></li>
<li><a href="../247791/index.html">Experiment: 10 things that I learned, using only water for a month</a></li>
<li><a href="../247793/index.html">Monitoring from Brazil - continue to conquer Latin America</a></li>
<li><a href="../247799/index.html">ATtiny13a: Controller of the STOP signal and the parking lights of the car</a></li>
<li><a href="../247807/index.html">Combinatorial algorithms: combination index, partitioning into subsets</a></li>
<li><a href="../247809/index.html">Perhaps the first game on Dart + Box2D</a></li>
<li><a href="../247811/index.html">Three-dimensional background for the site in real time on JavaScript using three.js</a></li>
<li><a href="../247813/index.html">Non-personalized recommendations: association method</a></li>
<li><a href="../247815/index.html">Micro-conference UX-Wednesday ‚Ññ21 in the framework of User Experience Russia 2014</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>