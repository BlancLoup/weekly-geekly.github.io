<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Vectorization of coordinate transformation code in space on Intel¬Æ Xeon Phi ‚Ñ¢ using low-level instructions</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 


 When solving problems of modeling the motion of objects in three-dimensional space, it is almost always necessary to use operations o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Vectorization of coordinate transformation code in space on Intel¬Æ Xeon Phi ‚Ñ¢ using low-level instructions</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/eb2/275/58a/eb227558a2c74b56977bf13da9fc27cc.png"><br><h2>  Introduction </h2><br><p>  When solving problems of modeling the motion of objects in three-dimensional space, it is almost always necessary to use operations of spatial transformation associated with the multiplication of transformation matrices and vectors.  For the N-body problem, this operation is used repeatedly to specify the rotation and displacement of the body relative to the origin.  The matrix of spatial transformation has a dimension of 4x4, and the dimension of the vector to which the transformation is applied, respectively 4x1.  Consider optimizing the performance of such an operation with a large number of matrices and vectors under the architecture of Intel¬Æ Xeon Phi ‚Ñ¢. </p><br><a name="habracut"></a><br><p> The Intel¬Æ Xeon Phi ‚Ñ¢ coprocessor has a block of 32 registers 512 bits long ( <code>zmm00-zmm31</code> ).  Accordingly, the C ++ compiler from Intel [1] supports a set of vector instructions that work with these registers, falling under the specification code-named KNC [2] (not to be confused with AVX-512).  This set of instructions is limited compared to the instructions of the AVX-512, but nonetheless includes the operations necessary to solve the problem in question.  The high-level wrappers over the machine instructions supported by the compiler are called intrinsics functions and are described in Intel‚Äôs Intel¬Æ Intrinsic Guide [3].  When registering data correctly, vector registers allow you to perform some operation directly on a data set (SIMD).  Next, we will use the names for the register variables in the code that correspond to the names of the registers ( <code>zmm00-zmm31</code> ) and assume that they are mapped to the same registers when compiled. </p><br><p>  Two operations are common in spatial transformations: matrix multiplication by a transformation matrix and matrix multiplication by a vector.  The operation of multiplication of transformation matrices in most cases can be reduced to a sequence of optimized operations of matrix multiplication by column vectors of another matrix, therefore the multiplication operation by vector is the most interesting, and matrix multiplication will not be considered separately.  To understand how vectorization works, consider the simplest example of multiplying a 4x4 matrix by a 4x1 vector. </p><br><p>  In this case, the following occurs: </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/A%5Ctimes%20%5Cvec%7Ba%7D%3D%0A%5Cbegin%7Bpmatrix%7D%0AA_%7B11%7D%5C%5C%0AA_%7B21%7D%5C%5C%0AA_%7B31%7D%5C%5C%0AA_%7B41%7D%5C%5C%0A%5Cend%7Bpmatrix%7D%5Ccirc%0A%5Cbegin%7Bpmatrix%7D%0Aa_%7B1%7D%5C%5C%0Aa_%7B1%7D%5C%5C%0Aa_%7B1%7D%5C%5C%0Aa_%7B1%7D%5C%5C%0A%5Cend%7Bpmatrix%7D%2B%0A%5Cbegin%7Bpmatrix%7D%0AA_%7B12%7D%5C%5C%0AA_%7B22%7D%5C%5C%0AA_%7B32%7D%5C%5C%0AA_%7B42%7D%5C%5C%0A%5Cend%7Bpmatrix%7D%5Ccirc%0A%5Cbegin%7Bpmatrix%7D%0Aa_%7B2%7D%5C%5C%0Aa_%7B2%7D%5C%5C%0Aa_%7B2%7D%5C%5C%0Aa_%7B2%7D%5C%5C%0A%5Cend%7Bpmatrix%7D%2B%0A%5Cbegin%7Bpmatrix%7D%0AA_%7B13%7D%5C%5C%0AA_%7B23%7D%5C%5C%0AA_%7B33%7D%5C%5C%0AA_%7B43%7D%5C%5C%0A%5Cend%7Bpmatrix%7D%5Ccirc%0A%5Cbegin%7Bpmatrix%7D%0Aa_%7B3%7D%5C%5C%0Aa_%7B3%7D%5C%5C%0Aa_%7B3%7D%5C%5C%0Aa_%7B3%7D%5C%5C%0A%5Cend%7Bpmatrix%7D%2B%0A%5Cbegin%7Bpmatrix%7D%0AA_%7B14%7D%5C%5C%0AA_%7B24%7D%5C%5C%0AA_%7B34%7D%5C%5C%0AA_%7B44%7D%5C%5C%0A%5Cend%7Bpmatrix%7D%5Ccirc%0A%5Cbegin%7Bpmatrix%7D%0Aa_%7B4%7D%5C%5C%0Aa_%7B4%7D%5C%5C%0Aa_%7B4%7D%5C%5C%0Aa_%7B4%7D%5C%5C%0A%5Cend%7Bpmatrix%7D%0A%2C" alt="A \ times \ vec {a} = \ begin {pmatrix} A_ {11} \\ A_ {21} \\ A_ {31} \\ A_ {41} \\ \ end {pmatrix} \ circ \ begin {pmatrix} a_ {1} \\ a_ {1} \\ a_ {1} \\ a_ {1} \\ \ end {pmatrix} + \ begin {pmatrix} A_ {12} \\ A_ {22} \\ A_ {32 } \\ A_ {42} \\ \ end {pmatrix} \ circ \ begin {pmatrix} a_ {2} \\ a_ {2} \\ a_ {2} \\ a_ {2} \\ \ end {pmatrix} + \ begin {pmatrix} A_ {13} \\ A_ {23} \\ A_ {33} \\ A_ {43} \\ \ end {pmatrix} \ circ \ begin {pmatrix} a_ {3} \\ a_ { 3} \\ a_ {3} \\ a_ {3} \\ \ end {pmatrix} + \ begin {pmatrix} A_ {14} \\ A_ {24} \\ A_ {34} \\ A_ {44} \ \ \ end {pmatrix} \ circ \ begin {pmatrix} a_ {4} \\ a_ {4} \\ a_ {4} \\ a_ {4} \\ \ end {pmatrix},"></div><p></p><br><p>  where for clarity, an empty circle shows the elementwise multiplication of vectors. </p><br><p>  If we place in the registers the columns of the matrix and multiply the registers of the elements of the vector, the operation can be described using intrinsic functions for multiplication and FMA (fused multiply-add - mixed multiplication with addition).  That is, to obtain the result vector in the <code>zmm00</code> vector register, you must load the matrix columns into the <code>zmm00-zmm03</code> , multiply the elements of the vector into the <code>zmm04-zmm07</code> , so that each element of the vector <code>zmm04-zmm07</code> many times and then perform 3 operations: </p><br><pre> <code class="cpp hljs">zmm00=_mm512_mul_ps(zmm00, zmm04); zmm00=_mm512_fmadd_ps(zmm01, zmm05, zmm00); zmm00=_mm512_fmadd_ps(zmm02, zmm06, zmm00); zmm00=_mm512_fmadd_ps(zmm03, zmm07, zmm00);</code> </pre> <br><p>  To arrange the columns of matrices in registers, you can use transposition, if the matrix is ‚Äã‚Äãstored in rows, but this is a time-consuming operation, so it is best to store matrices in columns.  And combine the transformations so that the matrices are obtained in columns. </p><br><p>  In this example, with real 32-bit numbers (ps suffix), registers are used only by a quarter, since the 512-bit register fits 16 elements, and in our case only 4, that is, there is a loss 4 times.  If you use double precision (equivalent code with pd suffix), then the registers will be half filled.  Even such options of partial filling of registers can give acceleration if you use 4 elements from a vector register with the result for saving, since 4 operations are performed at once, instead of one. </p><br><h1>  Features of loading and unloading data </h1><br><p>  To improve the efficiency of loading data from memory into registers and unloading from registers into memory, the memory allocated in the program should be aligned along the 64-byte boundary.  In this case, you can use the <code>_mm512_load</code> and <code>_mm512_store</code> with appropriate suffixes for data types. <br>  For dynamic allocation of aligned memory, the compiler implements a special function <code>_mm_malloc</code> , which is used in conjunction with <code>_mm_free</code> .  The difference from the usual function <code>malloc</code> that the second argument is added - the multiplicity of the alignment of the memory; for 512-bit registers, a pointer to the memory must be returned with an address multiple of 64 bytes.  For Type, the syntax for aligned memory is as follows: </p><br><pre> <code class="cpp hljs">Type* pointer = (Type*)_mm_malloc(<span class="hljs-keyword"><span class="hljs-keyword">sizeof</span></span>(Type)*N, <span class="hljs-number"><span class="hljs-number">64</span></span>);</code> </pre> <br><p>  For static data arrays, memory alignment directives are also provided, for example, the following syntax is provided for defining an aligned array in the Intel compiler: </p><br><pre> <code class="cpp hljs">__declspec(align(<span class="hljs-number"><span class="hljs-number">64</span></span>)) Type mem[N];</code> </pre> <br><p>  You can also use the supported * nix - option: </p><br><pre> <code class="cpp hljs">Type mem[N] __attribute__((aligned(<span class="hljs-number"><span class="hljs-number">64</span></span>)));</code> </pre> <br><p>  If it is impossible to level the memory, it is necessary to use intrinsiki for working with unaligned memory <code>_mm512_loadu</code> and <code>_mm512_storeu</code> .  For unknown reasons, the compiler does not implement the functions of loading into registers from unaligned memory and unloading from registers to unaligned memory for KNC, for AVX-512 there are such functions. </p><br><p>  For KNC, these functions can be implemented independently on the basis of intrinsics for partial loading to the alignment boundary: </p><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">inline</span></span> __m512d _mm512_loadu_pd(<span class="hljs-keyword"><span class="hljs-keyword">const</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>* a) { __m512d reg = _mm512_setzero_pd(); reg =_mm512_loadunpacklo_pd(reg, a); reg =_mm512_loadunpackhi_pd(reg, a+<span class="hljs-number"><span class="hljs-number">8</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> reg; }</code> </pre> <br><p>  Similarly, the upload function looks like: </p><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">inline</span></span> <span class="hljs-keyword"><span class="hljs-keyword">void</span></span> _mm512_storeu_pd(<span class="hljs-keyword"><span class="hljs-keyword">double</span></span>* a, __m512d reg) { reg =_mm512_packstorelo_pd(a, reg); reg =_mm512_packstorehi_pd(a+<span class="hljs-number"><span class="hljs-number">8</span></span>,reg); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> reg; }</code> </pre> <br><p>  Proper use of aligned memory can lead to acceleration up to 3 times, so we will continue to consider only work with aligned memory. </p><br><h1>  Optimization with special storage of matrices in memory </h1><br><p>  In the most general case for single precision numbers, it can be noted that the matrices are loaded completely into one register.  If the matrix elements are stored sequentially, then when loading we get a register with 16 elements of the same matrix.  This is good for vector multiplication operations, but bad for addition, because it is necessary to add elements within a register.  The operations of adding elements within a register are associated with slow instructions for mixing, shifting, and rearranging elements, or reducing, since there are no horizontal addition operations in the KNC instruction set.  If you write a complex algorithm that does it in intrinsiki, you get a slower code than the one generated by the compiler optimizer. </p><br><p>  To minimize the number of instructions, we need to make the calculations as follows: </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Cbegin%7Bpmatrix%7D%0AA%5Ctimes%5Cvec%7Ba%7D%5C%5C%0AB%5Ctimes%5Cvec%7Bb%7D%5C%5C%0AC%5Ctimes%5Cvec%7Bc%7D%5C%5C%0AD%5Ctimes%5Cvec%7Bd%7D%5C%5C%0A%5Cend%7Bpmatrix%7D%3D%0A%5Cbegin%7Bpmatrix%7D%0AA_%7B11%7D%5C%5C%0AA_%7B21%7D%5C%5C%0AA_%7B31%7D%5C%5C%0AA_%7B41%7D%5C%5C%0AB_%7B11%7D%5C%5C%0A%5Ccdots%5C%5C%0AD_%7B41%7D%0A%5Cend%7Bpmatrix%7D%5Ccirc%0A%5Cbegin%7Bpmatrix%7D%0Aa_%7B1%7D%5C%5C%0Aa_%7B1%7D%5C%5C%0Aa_%7B1%7D%5C%5C%0Aa_%7B1%7D%5C%5C%0Ab_%7B1%7D%5C%5C%0A%5Ccdots%5C%5C%0Ad_%7B1%7D%0A%5Cend%7Bpmatrix%7D%2B%0A%5Cbegin%7Bpmatrix%7D%0AA_%7B12%7D%5C%5C%0AA_%7B22%7D%5C%5C%0AA_%7B32%7D%5C%5C%0AA_%7B42%7D%5C%5C%0AB_%7B12%7D%5C%5C%0A%5Ccdots%5C%5C%0AD_%7B42%7D%5Cend%7Bpmatrix%7D%5Ccirc%0A%5Cbegin%7Bpmatrix%7D%0Aa_%7B2%7D%5C%5C%0Aa_%7B2%7D%5C%5C%0Aa_%7B2%7D%5C%5C%0Aa_%7B2%7D%5C%5C%0Ab_%7B2%7D%5C%5C%0A%5Ccdots%5C%5C%0Ad_%7B2%7D%0A%5Cend%7Bpmatrix%7D%2B%0A%5Ccdots%2B%0A%5Cbegin%7Bpmatrix%7D%0AA_%7B14%7D%5C%5C%0AA_%7B24%7D%5C%5C%0AA_%7B34%7D%5C%5C%0AA_%7B44%7D%5C%5C%0AB_%7B14%7D%5C%5C%0A%5Ccdots%5C%5C%0AD_%7B44%7D%5Cend%7Bpmatrix%7D%5Ccirc%0A%5Cbegin%7Bpmatrix%7D%0Aa_%7B4%7D%5C%5C%0Aa_%7B4%7D%5C%5C%0Aa_%7B4%7D%5C%5C%0Aa_%7B4%7D%5C%5C%0Ab_%7B4%7D%5C%5C%0A%5Ccdots%5C%5C%0Ad_%7B4%7D%0A%5Cend%7Bpmatrix%7D%0A%2C" alt="\ begin {pmatrix} A \ times \ vec {a} \\ B \ times \ vec {b} \\ C \ times \ vec {c} \\ D \ times \ vec {d} \\ \ end {pmatrix} = \ begin {pmatrix} A_ {11} \\ A_ {21} \\ A_ {31} \\ A_ {41} \\ B_ {11} \\ \ cdots \\ D_ {41} \ end {pmatrix} \ circ \ begin {pmatrix} a_ {1} \\ a_ {1} \\ a_ {1} \\ a_ {1} \\ b_ {1} \\ \ cdots \\ d_ {1} \ end {pmatrix} + \ begin {pmatrix} A_ {12} \\ A_ {22} \\ A_ {32} \\ A_ {42} \\ B_ {12} \\ \ cdots \\ D_ {42} \ end {pmatrix} \ circ \ begin {pmatrix} a_ {2} \\ a_ {2} \\ a_ {2} \\ a_ {2} \\ b_ {2} \\ \ cdots \\ d_ {2} \ end {pmatrix} + \ cdots + \ begin {pmatrix} A_ {14} \\ A_ {24} \\ A_ {34} \\ A_ {44} \\ B_ {14} \\ \ cdots \\ D_ {44} \ end {pmatrix} \ circ \ begin {pmatrix} a_ {4} \\ a_ {4} \\ a_ {4} \\ a_ {4} \\ b_ {4} \\ \ cdots \\ d_ {4} \ end {pmatrix},"></div><p></p><br><p>  In order to calculate 4 result vectors at once, it makes sense to arrange the data in memory so that the rows of the matrix lie alternately, and for one set of instructions, 4 matrices are multiplied by 4 vectors at once.  To do this, you need to write down new matrix elements in the following order: </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/A_%7B11%7D%2CA_%7B21%7D%2CA_%7B31%7D%2CA_%7B41%7D%2CB_%7B11%7D%2CB_%7B21%7D%2CB_%7B31%7D%2CB_%7B41%7D%2C%5Cldots%2CD_%7B14%7D%2CD_%7B24%7D%2CD_%7B34%7D%2CD_%7B44%7D" alt="A_ {11}, A_ {21}, A_ {31}, A_ {41}, B_ {11}, B_ {21}, B_ {31}, B_ {41}, \ ldots, D_ {14}, D_ { 24}, D_ {34}, D_ {44}"></div><p></p><br><p>  This shows that it is necessary to store first the first columns, then the second, then the third, and at the end the fourth immediately for 4 matrices.  In one operation, 16 elements will be placed in the register: </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/zmm00%3D(A_%7B11%7D%2CA_%7B21%7D%2CA_%7B31%7D%2CA_%7B41%7D%2CB_%7B11%7D%2CB_%7B21%7D%2CB_%7B31%7D%2CB_%7B41%7D%2C%5Cldots%2CD_%7B41%7D)" alt="zmm00 = (A_ {11}, A_ {21}, A_ {31}, A_ {41}, B_ {11}, B_ {21}, B_ {31}, B_ {41}, \ ldots, D_ {41} )"></div><p></p><br><p>  By analogy of the vector should also be arranged on the first, second, third and fourth elements at once 4 vectors.  But, as a rule, this is inconvenient, since vectors are always used as sequences of elements for addition, determining the position of bodies in space, while rotation matrices are usually calculated from trigonometric transformations, and the body orientation is given by a system of angles (for example, Euler angles) or quaternions.  That is, in the computational code vector should be stored sequentially for quick access to them.  Matrices can be stored arbitrarily, since they are calculated through parameters that describe the orientation, and we just vectorize the only operation we need with them in a special way, taking into account this special storage. </p><br><p>  Based on the sequential storage of vectors, we consider how we can obtain the necessary sequences of elements.  That is, if we had a vector in mind </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/vec%3D(a_1%2Ca_2%2Ca_3%2Ca_4%2Cb_1%2Cb_2%2Cb_3%2Cb_4%2Cc_1%2Cc_2%2Cc_3%2Cc_4%2Cd_1%2Cd_2%2Cd_3%2Cd_4%20)%2C" alt="vec = (a_1, a_2, a_3, a_4, b_1, b_2, b_3, b_4, c_1, c_2, c_3, c_4, d_1, d_2, d_3, d_4),"></div><p></p><br><p>  then you need to get the next set of registers </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Cbegin%7Barray%7Dl%0Azmm04%3D(a_1%2Ca_1%2Ca_1%2Ca_1%2Cb_1%2Cb_1%2Cb_1%2Cb_1%2Cc_1%2Cc_1%2Cc_1%2Cc_1%2Cd_1%2Cd_1%2Cd_1%2Cd_1%20)%2C%5C%5C%0Azmm05%3D(a_2%2Ca_2%2Ca_2%2Ca_2%2Cb_2%2Cb_2%2Cb_2%2Cb_2%2Cc_2%2Cc_2%2Cc_2%2Cc_2%2Cd_2%2Cd_2%2Cd_2%2Cd_2%20)%2C%5C%5C%0Azmm06%3D(a_3%2Ca_3%2Ca_3%2Ca_3%2Cb_3%2Cb_3%2Cb_3%2Cb_3%2Cc_3%2Cc_3%2Cc_3%2Cc_3%2Cd_3%2Cd_3%2Cd_3%2Cd_3%20)%2C%5C%5C%0Azmm07%3D(a_4%2Ca_4%2Ca_4%2Ca_4%2Cb_4%2Cb_4%2Cb_4%2Cb_4%2Cc_4%2Cc_4%2Cc_4%2Cc_4%2Cd_4%2Cd_4%2Cd_4%2Cd_4%20).%0A%5Cend%7Barray%7D" alt="\ begin {array} l , a_2, a_2, b_2, b_2, b_2, b_2, c_2, c_2, c_2, c_2, d_2, d_2, d_2, d_2, b_3, a_3, a_3, b_3, b_3, b_3, b_3 , C_3, C_3, C_3, C_3, d_3, d_3, d_3, d_3) , d_4, d_4). \ end {array}"></div><p></p><br><p>  Using the instruction of reproduction of elements, we can load elements <em>a</em> <sub>1</sub> , <em>b</em> <sub>1</sub> , <em>c</em> <sub>1</sub> , <em>d</em> <sub>1</sub> into 4 registers: </p><br><pre> <code class="cpp hljs">zmm08 = _mm512_extload_ps(vec + <span class="hljs-number"><span class="hljs-number">0</span></span>, _MM_UPCONV_PS_NONE, _MM_BROADCAST_1X16, _MM_HINT_NONE); zmm09 = _mm512_extload_ps(vec + <span class="hljs-number"><span class="hljs-number">4</span></span>, _MM_UPCONV_PS_NONE, _MM_BROADCAST_1X16, _MM_HINT_NONE); zmm10 = _mm512_extload_ps(vec + <span class="hljs-number"><span class="hljs-number">8</span></span>, _MM_UPCONV_PS_NONE, _MM_BROADCAST_1X16, _MM_HINT_NONE); zmm11 = _mm512_extload_ps(vec + <span class="hljs-number"><span class="hljs-number">12</span></span>, _MM_UPCONV_PS_NONE, _MM_BROADCAST_1X16, _MM_HINT_NONE);</code> </pre> <br><p>  After this, it is possible to mix them in the manner necessary for us with the help of bitmasks: </p><br><pre> <code class="cpp hljs">zmm06 = _mm512_mask_blend_ps(mask32_0, zmm08, zmm09); zmm07 = _mm512_mask_blend_ps(mask32_1, zmm10, zmm11); zmm04 = _mm512_mask_blend_ps(mask32_2, zmm06, zmm07);</code> </pre> <br><p>  For clarity, the figure shows how the loading and mixing elements in the registers. </p><br><p></p><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/files/003/bcf/ab6/003bcfab6d0c431abd78234cad8e681d.png"></div><br><p></p><br><p>  You can mix only 2 registers, so the operation is repeated 3 times with a different mask value.  After that we have the necessary content in the register <code>zmm04</code> .  Similar operations of loading-mixing allow to obtain the necessary elements of the vectors in the registers <code>zmm05-zmm07</code> , after which you can perform multiplication and addition operations to obtain the result.  This method can be applied to processors with support for instructions AVX-512 and KNC. </p><br><p>  You may notice that the elements of the vectors must be partially duplicated by the fours in the corresponding quarters of the registers.  To do this, the KNC instruction set for Intel¬Æ Xeon Phi ‚Ñ¢ has a special <code>swizzle</code> operation that allows you to rearrange and replicate elements in quarters of registers.  In the case of loading 4 vectors with successive elements following, the following code converts it into the necessary factors: </p><br><pre> <code class="cpp hljs">zmm04 = _mm512_swizzle_ps(zmm08, _MM_SWIZ_REG_AAAA); zmm05 = _mm512_swizzle_ps(zmm08, _MM_SWIZ_REG_BBBB); zmm06 = _mm512_swizzle_ps(zmm08, _MM_SWIZ_REG_CCCC); zmm07 = _mm512_swizzle_ps(zmm08, _MM_SWIZ_REG_DDDD);</code> </pre> <br><p>  How the <code>swizzle</code> works is shown. </p><br><p></p><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/files/ff8/7da/275/ff87da2751f1483892efa52c63dd9f2b.png"></div><br><p></p><br><p>  If we had in the register <code>zmm08</code> vector of consecutive elements vec, then as a result of the work of these instructions, we will need <code>zmm04-zmm07</code> .  The latter option contains significantly fewer instructions, but is not applicable to the AVX-512 instruction set, so you should select the most appropriate instructions for your particular architecture and use them. </p><br><p>  To demonstrate the advantage of the execution time obtained as a result of code optimization, we tested multiplications of a large number of transformation matrices for vectors filled with random numbers.  For comparison, a multiplication test of 300,000 transformation matrices was used on the Intel¬Æ Xeon Phi ‚Ñ¢ 5120D coprocessor. </p><br><p>  Comparison of optimization results was carried out after a preliminary run of the same code.  After warming up, the average time was selected from 10 subsequent launches.  To avoid optimization of unused code, an artificial data dependency was also created (the results of the operation were recorded in a vector factor for the next operation).  The results for 32 and 64-bit real numbers are shown in Table 1, where the auto-vectorization result is in the auto column. </p><br><p>  <strong>Table 1 - Time Measurements and Optimized Code Acceleration</strong> </p><br><table><thead><tr><th>  type of </th><th>  time with car </th><th>  time with a blend </th><th>  time with swizzle </th><th>  acceleration blend </th><th>  speeding up swizzle </th></tr></thead><tbody><tr><td>  32 </td><td>  1.15 </td><td>  0.34 </td><td>  0.2 </td><td>  3.38 </td><td>  5.75 </td></tr><tr><td>  64 </td><td>  1.41 </td><td>  0.55 </td><td>  0.45 </td><td>  2.56 </td><td>  3.13 </td></tr></tbody></table><br><h1>  Optimization with ordinary storage matrices </h1><br><p>  In some cases, the matrix can not be reordered, so you need to do their loading in a special way.  Consider the matrix transformation algorithm for double precision (for a single one, everything is similar, but very cumbersome).  In the case of double precision, we need to place 2 sets of 4 elements in each multiplied register with matrix elements.  In the KNC instruction set, there is a right-to-right gluing operation, which combines 2 registers into a sequence of 1024 bits, then shifts it by a specified number of elements to the right and writes the lower 512 bits into the resulting register.  The operation is presented only for the integer data type, although in the AVX-512 instructions it is also present for real data, but due to the same bit representation, the data type is not important to us.  It is necessary to bear in mind only that the number of shifted elements must be 2 times larger for double precision numbers.  The path has two matrices that are stored as a sequence of columns: <br></p><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/mtx%3D(A_1%2CA_2%2CA_3%2CA_4%2CB_1%2CB_2%2CB_3%2CB_4)" alt="mtx = (A_1, A_2, A_3, A_4, B_1, B_2, B_3, B_4)"></div><p></p><br><p>  To begin with, we load matrices that are stored in columns sequentially: </p><br><pre> <code class="cpp hljs">__m512i izmm10 = _mm512_load_epi64(pmtx); <span class="hljs-comment"><span class="hljs-comment">//A2-A1 __m512i izmm11 = _mm512_load_epi64(pmtx+8); //A4-A3 __m512i izmm12 = _mm512_load_epi64(pmtx+16); //B2-B1 __m512i izmm13 = _mm512_load_epi64(pmtx+24); //B4-B3</span></span></code> </pre> <br><p>  As a result of the operation in <code>izmm10-izmm11</code> - the first matrix, in <code>izmm12-izmm13</code> - the second.  We need to place the first rows of both matrices in the <code>zmm00</code> register, the second in <code>zmm01</code> , the third in <code>zmm02</code> , the fourth in <code>zmm03</code> .  For this we use gluing with a shift of 256 bits (8 integers or 4 real double precision numbers): </p><br><pre> <code class="cpp hljs">izmm14 = _mm512_alignr_epi32(izmm10, izmm12, <span class="hljs-number"><span class="hljs-number">8</span></span>); <span class="hljs-comment"><span class="hljs-comment">//A1-B2 zmm00 = (__m512d)_mm512_alignr_epi32(izmm12, izmm14, 8); //B1-A1 zmm01 = (__m512d)_mm512_alignr_epi32(izmm14, izmm10, 8); //B2-A2 izmm14 = _mm512_alignr_epi32(izmm11, izmm13, 8); //A3-B4 zmm02 = (__m512d)_mm512_alignr_epi32(izmm13, izmm14, 8); //B3-A3 zmm03 = (__m512d)_mm512_alignr_epi32(izmm14, izmm11, 8); //B4-A4</span></span></code> </pre> <br><p>  The picture shows how it works. </p><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/files/c8a/a18/3b3/c8aa183b35a4459695a05bd9800add49.png"></div><br><p>  As a result of the operation in the registers <code>zmm00-zmm03</code> we obtain the required sequence of matrix columns.  After that, using the <code>swizzle</code> to load vectors into the <code>zmm04-zmm07</code> and the subsequent use of FMA instructions, as was done earlier, allows to obtain the result for two operations of multiplying the transformation matrices by two vectors in the <code>zmm00</code> register. </p><br><pre> <code class="cpp hljs">zmm04 = _mm512_swizzle_pd(zmm08, _MM_SWIZ_REG_AAAA); zmm05 = _mm512_swizzle_pd(zmm08, _MM_SWIZ_REG_BBBB); zmm06 = _mm512_swizzle_pd(zmm08, _MM_SWIZ_REG_CCCC); zmm07 = _mm512_swizzle_pd(zmm08, _MM_SWIZ_REG_DDDD); zmm00=_mm512_mul_pd(zmm00, zmm04); zmm00=_mm512_fmadd_pd(zmm01, zmm05, zmm00); zmm00=_mm512_fmadd_pd(zmm02, zmm06, zmm00); zmm00=_mm512_fmadd_pd(zmm03, zmm07, zmm00);</code> </pre> <br><p>  Similarly, you can implement the code for the four operations of multiplying matrices by four vectors with 32-bit real numbers.  If we compare the time measurements, we can estimate the losses associated with the preliminary transformation of matrices (Table 2); nevertheless, even this code gives an acceleration of more than 2 times, compared to the code generated by the compiler. </p><br><p>  <strong>Table 2 - Measurements of time and acceleration with different storage matrices</strong> </p><br><table><thead><tr><th>  storage of matrices </th><th>  time with car </th><th>  time with <br>  swizzle </th><th>  speeding up swizzle </th></tr></thead><tbody><tr><td>  interlaced </td><td>  1.41 </td><td>  0.45 </td><td>  3.13 </td></tr><tr><td>  ordinary </td><td>  1.25 </td><td>  0.56 </td><td>  2.23 </td></tr></tbody></table><br><h1>  Conclusion </h1><br><p>  Optimizing code for spatial transformations with matrix-vector multiplication for the Intel¬Æ Xeon Phi ‚Ñ¢ coprocessor allows you to get faster code than compiler optimization if you use the available set of vector instructions and organize efficient storage of data in memory. </p><br><h1>  Links </h1><br><p>  [one].  Intel¬Æ C ++ Compiler 16.0 User and Reference Guide // <br>  <a href="https://software.intel.com/en-us/intel-cplusplus-compiler-16.0-user-and-reference-guide">https://software.intel.com/en-us/intel-cplusplus-compiler-16.0-user-and-reference-guide</a> <br>  [2].  Intel¬Æ Xeon Phi ‚Ñ¢ Coprocessor Instruction Set Architecture Reference // <br>  <a href="https://software.intel.com/sites/default/files/forum/278102/327364001en.pdf">https://software.intel.com/sites/default/files/forum/278102/327364001en.pdf</a> <br>  [3].  Intel¬Æ Intrinsic Guide // <br>  <a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/">https://software.intel.com/sites/landingpage/IntrinsicsGuide/</a> </p><p></p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/305558/">https://habr.com/ru/post/305558/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../305544/index.html">Expansion of functionality Scene View in Unity3D. Interception of events</a></li>
<li><a href="../305546/index.html">Who is aggregated by Meduza?</a></li>
<li><a href="../305548/index.html">Fighting BEM: 10 major mistakes and how to avoid them</a></li>
<li><a href="../305550/index.html">TP-LINK lost the right to the domain used to configure routers and amplifiers</a></li>
<li><a href="../305556/index.html">9 ways to split your work so as to maintain concentration and productivity</a></li>
<li><a href="../305560/index.html">Steve Wozniak - an enthusiast who changed the world of personal computers</a></li>
<li><a href="../305564/index.html">Elixir: Deploying Applications with Edeliver</a></li>
<li><a href="../305566/index.html">Deploying Django 1.9 on IIS 7+</a></li>
<li><a href="../305568/index.html">The work of the technical support department of the local positioning system</a></li>
<li><a href="../305570/index.html">Tour of the world fintech hubs</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>