<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Deployment of NetUP UTM5 billing system part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Under the cat, you will learn about setting up a failover cluster, to run the NetUP UTM5 billing system, setting up encryption and backing up "valuabl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Deployment of NetUP UTM5 billing system part 1</h1><div class="post__text post__text-html js-mediator-article"> Under the cat, you will learn about setting up a failover cluster, to run the NetUP UTM5 billing system, setting up encryption and backing up "valuable" data. <br><a name="habracut"></a><br>  Why cluster? <br>  I hope that I will not reveal a big secret saying that billing should be as fault-tolerant as possible.  And as you know, the main way to achieve fault tolerance is redundancy.  We will achieve this through Ganeti. <br>  Ganeti is a virtualization cluster management system built on Xen or KVM virtualization systems.  Uses DRBD for the organization of failover clusters. <br>  To implement this idea, we need 2 servers, with the preinstalled Debian lenny.  In fact, one ‚Äúbilling will be launched‚Äù, the second will be in the hot standby. <br>  Let one of the servers be node1, then the rest will be node2. <br>  All of the following must be done on each of the nodes. <br>  Let's start with the partitioning of disks.  It is assumed that we have 2 identical hard drives in each server.  When installing, I created a software ‚Äúmirror‚Äù raid of 2 GB in size and installed the base system on it. <br>  It looks like this. <br><br> <code>node1:~# fdisk -l /dev/hda <br> Device Boot Start End Blocks Id System <br> /dev/hda1 1 243 1951866 fd Linux raid autodetect <br> node1:~# fdisk -l /dev/hdb <br> /dev/hdb1 1 243 1951866 fd Linux raid autodetect <br> node1:~# cat /proc/mdstat <br> Personalities : [raid1] <br> md0 : active raid1 hda1[0] hdb1[1] <br> 1951744 blocks [2/2] [UU]</code> <br> <br>  Now we will create another mirrored raid array, for example, dimensions of 5 GB, the size of the array being created, but both nodes must be the same. <br><br>  node1: ~ # fdisk -l / dev / hda <br>  / dev / hda1 1,243 1951866 fd Linux raid autodetect <br>  / dev / hda2 244 865 4996215 83 Linux <br>  node1: ~ # fdisk -l / dev / hdb <br>  / dev / hdb1 1,243 1951866 fd Linux raid autodetect <br>  / dev / hdb2 244 865 4996215 83 Linux 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Create a raid. <br><br>  node1: ~ # mdadm --create / dev / md1 --level 1 --raid-devices = 2 / dev / hda2 / dev / hdb2 <br><br>  We register in autoload. <br><br>  node1: ~ # mdadm --examine --scan | grep -v / dev / md0 &gt;&gt; /etc/mdadm/mdadm.conf <br><br>  Add to / etc / hosts the description of the hosts, it is necessary for the correct authorization <br><br>  node1: ~ # mcedit / etc / hosts <br><br>  192.168.0.111 node1.name.org node1 <br>  192.168.0.112 node2.name.org node2 <br>  192.168.0.113 cluster1.name.org cluster1 <br>  192.168.0.114 inst1.name.org inst1 <br><br>  I will omit the description of the interface settings, I believe that you will cope with this yourself. <br><br>  Install the LVM2 package <br><br>  node1: ~ # apt-get install lvm2 <br>  node1: ~ # pvcreate / dev / md1 <br>  Physical volume "/ dev / md1" successfully created <br>  node1: ~ # vgcreate xenvg / dev / md1 <br>  Volume group "xenvg" successfully created <br><br>  We put ganeti <br>  node1: ~ # apt-get install ganeti <br><br>  Configuring Xen <br><br>  node1: ~ # mcedit /etc/xen/xend-config.sxp <br><br>  (xend-relocation-server yes) <br>  (xend-relocation-port 8002) <br>  (xend-relocation-address '') <br>  (network-script network-bridge) <br>  # (network-script network-dummy) <br>  (vif-script vif-bridge) <br>  (dom0-min-mem 0) <br><br>  Customize grub <br><br>  node1: ~ # mcedit /boot/grub/menu.lst <br><br>  ## Xen hypervisor options to use with the default Xen boot option <br>  # xenhopt = dom0_mem = 256M <br><br>  Update the bootloader <br>  node1: ~ # / sbin / update-grub <br><br>  We reboot and if everything works we proceed to create the cluster. <br>  Install DRBD <br><br>  node1: ~ # apt-get install drbd8-modules-2.6.26-2-xen-686 drbd8-utils <br><br>  Add to startup <br><br>  node1: ~ # echo drbd minor_count = 64 &gt;&gt; / etc / modules <br><br>  Load the module (of course, if you did not reload the node) <br><br>  node1: ~ # modprobe drbd minor_count = 64 <br><br>  Customize LVM <br><br>  node1: ~ # mcedit /etc/lvm/lvm.conf <br><br>  filter = ["r | / dev / cdrom |", "r | / dev / drbd [0-9] + |"] <br><br>  Now everything is ready to initiate the cluster.  Let the cluster1 be the node1. <br>  On the first node (node1) we execute the command <br><br>  node1: ~ # gnt-cluster init -b eth0 -g xenvg --master-netdev eth0 cluster1.name.org <br><br>  Add node2 to the cluster. <br><br>  node1: ~ # gnt-node add node2.name.org <br><br>  If everything went well, create an instance (the virtual machine in which the billing will work) <br><br>  node1: ~ # gnt-instance add -t drbd -n node2.name.org:node1.name.org -o debootstrap -s 2g --swap-size 128 -m 256 --kernel /boot/vmlinuz-2.6.26 -2-xen-686 --ip 192.168.0.114 inst1.name.org <br><br>  And so, what this team suggests. <br>  1.We create a new virtual machine in the cluster called inst1.name.org <br>  2. Allocating 2 GB of disk, 128 MB of swap file and 256 MB of RAM to it. <br>  3. The distribution for the virtual machine - Debian (debootstrap), kernel xen <br>  4. Be sure to pay attention to the -n option.  It determines which node will be the main, and which will be in reserve. <br>  So the entry node2.name.org:node1.name.org indicates that node2 is primary, and node1 is auxiliary (in hot spare). <br>  If the cluster owner is the first node (node1), it is preferable to start the instance on the second node (node2).  In case of failure of the first node (node1), the billing will continue to work as soon as the first node (node1) returns to the system - the network raid will be synchronized and the full-time cluster operation will be restored.  When the second node (node2) fails, we retain cluster management and have the opportunity to transfer the instance to the first node (node1) with minimal downtime, and quietly put the second one into operation without fear of a ‚Äúsplit-brian‚Äù. <br><br>  To gain access to the instance we will carry out several manipulations. <br><br>  node1: ~ # gnt-instance shutdown inst1.name.org <br>  node1: ~ # gnt-instance startup --extra "xencons = tty1 console = tty1" inst1.name.org <br><br>  Only now can we get full access. <br><br>  node1: ~ # gnt-instance console inst1.name.org <br><br>  List of ‚Äúinside‚Äù instance manipulations (by default, root has an empty password). <br>  Configure the network.  (again, I hope for your intellect, or perseverance) <br>  Configure apt. <br>  Install openssh-server and udev <br>  Add a line to / etc / fstab <br>  none / dev / pts devpts gid = 5, mode = 620 0 0 <br><br>  echo inst1&gt; / etc / hostname (Configuring hostname) <br>  apt-get install locales (set locale) <br>  dpkg-reconfigure locales (Configuring Locales) <br>  tasksel install standard (We are reinstalling packages included in the ‚Äústandard build‚Äù) <br><br>  Initial cluster setup is complete. <br>  I recommend to study the documentation for ganeti without fail.  It should be clear to you how to transfer instance from node to node in normal mode, how to transfer in case of an accident, etc.  Again, without fail it is necessary to make a reminder: how to act in an emergency situation, laminate and hang before your eyes, because accidents rarely happen, and the brain tends to forget. <br>  What should you think about first after starting the cluster?  Personally, it seems to me that you need to be extremely ready for the unexpected visit of harsh masked men who are very hard to refuse anything.  Willingness will be to encrypt all valuable data and back it up in a timely manner. <br>  All further settings relate directly to the virtual machine (instance). <br>  Let's start with encryption.  We will carry it out this way.  When the instance starts, the base system is loaded, then the system waits for the pass phrase to mount the file with the encrypted partition, after which we start the database, billing, and other ‚Äúnecessary services‚Äù. <br>  Due to this, everything is stored inside the instance, but for the migration we need to extinguish the machine (unmount the encrypted partition), otherwise the data on the encrypted partition will not be fully synchronized. <br>  Install the necessary packages. <br><br>  inst1: ~ # apt-get install loop-aes-modules-2.6.26-2-xen <br>  inst1: ~ # apt-get install loop-aes-utils <br><br>  We are unloading the "old" module (you can reload the instance, which would be for sure) <br><br>  inst1: ~ # modprobe -r loop <br><br>  Download the updated module <br><br>  inst1: ~ # insmod /lib/modules/2.6.26-2-xen/updates/loop.ko <br><br>  Create file for encrypted partition <br><br>  inst1: ~ # dd if = / dev / zero of = / var / encrypt_file bs = 4k count = 10000 <br><br>  We mount the created file.  I will not give recommendations on the pass phrase, this is a matter of taste. <br><br>  inst1: ~ # losetup -e AES256 / dev / loop1 / var / encrypt_file <br><br>  Create a file system, with our ext2 duplication level will be enough <br><br>  inst1: ~ # mkfs -t ext2 / dev / loop1 <br><br>  Unmount the file <br><br>  inst1: ~ # losetup -d / dev / loop1 <br><br>  An example of mounting an encrypted partition <br><br>  inst1: ~ # mount volume -o loop = / dev / loop1, encryption = AES256 / var / secure / -t ext2 <br><br>  Encrypting the swap file is at your discretion, but note that paranoia is a contagious thing. <br><br>  How to organize data backup?  You can run wonderful samopisnyi backup scripts on the crown.  Or you can approach responsibly and set up a centralized backup system, for example bacula, recommendations for setting up and examples can be found here <a href="http://habrahabr.ru/blogs/sysadm/86526/">Setting up and understanding Bacula</a> .  Backups should also be stored on an encrypted partition, and the server with backups should be kept somewhere far away, in a place known to a narrow circle of people, and for reliability you can pour plenty of pigeon poop on it so that you can‚Äôt even kick thoughts. <br>  To back up the database, I advise you to add lines to the job description (job) <br>  ClientRunBeforeJob = "/ usr / local / bin / create_mysql_dump" <br>  ClientRunAfterJob = "/ bin / rm -f /var/lib/bacula/mysql.dump" <br>  It should be borne in mind that the database is in the habit of growing in size, which increases the time of the database dump.  To avoid this problem, you can use the table archiving function, which was implemented starting from build 006. The logic is simple, we store archive tables in a separate database, and we can create views of archive tables to access the billing core.  Backup is done significantly faster, as only the structure of the view is saved, without the data itself.  Data from the archive tables can not be backed up, it is enough to store only 2 copies in case of fire. <br><br>  The result of all of the above can be the creation of a fault-tolerant cluster, with increased redundancy of the disk subsystem, the possibility of geographical diversity of nodes, encryption and backup of valuable data. <br>  PS Why the choice fell on NetUP UTM5? <br>  When the company bought the license, I first thought it was because of the detailed documentation, which at the time of purchase (late 2006) turned out to be as many as 260 pages, although it was frankly crude.  Then I thought that because of the literate.  support  I don‚Äôt argue, the support was high, but it was exceptionally ‚Äúunfriendly‚Äù, which, coupled with raw documentation, overshadowed the joy of purchasing the ‚Äúproduct‚Äù and stretched the transition for more than a year.  In the end, everything turned out to be simple, our director went to see her friends, and as you know, the monkey will devour red berries with frenzy only if she sees how other monkeys do the same.  This is how we became the proud owners of this ‚Äúproduct‚Äù. <br>  If you are already honest to the end, the ‚Äúproduct‚Äù proved to be extremely capricious, not well documented, but as it turned out with the right settings, it was very stable, so the configured system worked for over a year without any special interventions, the only thing was to clean the base. </div><p>Source: <a href="https://habr.com/ru/post/92221/">https://habr.com/ru/post/92221/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../92208/index.html">A quick educational program for JavaFX</a></li>
<li><a href="../92209/index.html">Fresh idea or not, that is the question?</a></li>
<li><a href="../92213/index.html">Kiev presentation CS5 - report with pictures</a></li>
<li><a href="../92216/index.html">Hide terminal servers. Budget solution</a></li>
<li><a href="../92217/index.html">Commercialization msk.ru/spb.ru, etc.</a></li>
<li><a href="../92225/index.html">The secret of kells</a></li>
<li><a href="../92226/index.html">GameDev: where to go to work?</a></li>
<li><a href="../92227/index.html">San Francisco city system administrator found guilty of stealing passwords from city WAN network</a></li>
<li><a href="../92228/index.html">She is alive!</a></li>
<li><a href="../92231/index.html">RAO again zhhot</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>