<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>3D-reconstruction of people on the photos and their animation using video. Lecture in Yandex</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the film Mission Impossible 3, the process of creating famous spy masks was shown, thanks to which some characters become indistinguishable from ot...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>3D-reconstruction of people on the photos and their animation using video. Lecture in Yandex</h1><div class="post__text post__text-html js-mediator-article">  In the film Mission Impossible 3, the process of creating famous spy masks was shown, thanks to which some characters become indistinguishable from others.  In the story, it was first required to photograph the person into whom the hero wanted to turn into, from several angles.  In 2018, a simple 3D model of a person can be, if not printed, but at least created in digital form - moreover, based on just one photo.  Researcher VisionLabs described in detail the process at the Yandex event ‚ÄúThe <a href="https://events.yandex.ru/events/ds/18-aug-2018/">World through the Eyes of Robots</a> ‚Äù from the Data &amp; Science series - with details on specific methods and formulas. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/zy9fxYDgUQw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  - Good day.  My name is Nikolai, I work in the company VisionLabs, which is engaged in computer vision.  Our main profile is face recognition, but we also have technologies that are applicable in augmented and virtual reality.  In particular, we have the technology to build a 3D face in one photo, and today I will talk about it. <br><br><a name="habracut"></a><img src="https://habrastorage.org/webt/hi/ks/fu/hiksfukx8cknqrbhf5kv-c9xwbs.jpeg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Let's start with a story about what it is.  On the slide you can see the original photo of Jack Ma and the 3D model, built on this photo in two variations: with texture and without texture, just geometry.  This is a task that we solve. <br><br><img src="https://habrastorage.org/webt/qo/sy/10/qosy10ijsqqmfgyqiemsloakdqg.jpeg"><br><br>  We also want to be able to animate this model, change the direction of gaze, facial expression, add facial expressions, etc. <br><br>  The application is in different areas.  The most obvious is games, including VR.  You can also make virtual fitting rooms - try on glasses, beards and hairstyles.  You can do 3D printing, because some people are interested in personalized accessories under their face.  And you can make faces for robots: both print and show on any display on the robot. <br><br><img src="https://habrastorage.org/webt/uh/d4/1b/uhd41bqzarsfib5oao3zven7gxk.jpeg"><br><br>  I will start with a story about how you can generate 3D faces in general, and then we move on to the 3D reconstruction task as the inverse generation task.  After that, we will focus on the animation and move on to the teamwork that occurs in this area. <br><br><img src="https://habrastorage.org/webt/bk/yb/sj/bkybsja0tr11pleorczx4okgk3g.jpeg"><br><br>  What is the task of generating faces?  We would like to have some way to generate three-dimensional faces that differ in shape and expression.  Here are two rows with examples.  The first row shows persons who are different in form and belong to different people.  And below is the same person with a different expression. <br><br><img src="https://habrastorage.org/webt/_s/md/94/_smd94kpe-kgilpu8c2ncguhgve.jpeg"><br><br>  One of the ways to solve the generation problem is deformable models.  The leftmost face on the slide is a kind of averaged model to which we can apply deformations by adjusting the sliders.  Here are three sliders.  In the upper row are the faces in the direction of increasing the intensity of the slider, in the lower row - in the direction of decreasing.  Thus, we will have several customizable parameters.  By installing them, you can give people different forms. <br><br><img src="https://habrastorage.org/webt/rs/gj/bd/rsgjbdm2ekfclog-l9r24yj05zq.jpeg"><br><br>  An example of a deformable model is the famous Basel Face Model, built from face scans.  To build a deformable model, you first need to take a few people, bring them to a special laboratory and shoot their faces with special equipment, transferring them to 3D.  Then, based on this, you can make new faces. <br><br><img src="https://habrastorage.org/webt/da/lm/zi/dalmziafwox1_wosdsp25pjphiu.jpeg"><br><br>  How does it work mathematically?  We can imagine a three-dimensional model of a face as a vector in a 3n-dimensional space.  Here n is the number of vertices in the model, each vertex corresponds to three coordinates in 3D, and thus we get 3n coordinates. <br><br><img src="https://habrastorage.org/webt/fr/h2/ek/frh2ekb0ts7xoeymvoxdthm7_ak.jpeg"><br><br>  If we have a set of scans, then each scan is represented by such a vector, and we have a set of n such vectors. <br><br>  Next, we can build new faces as linear combinations of vectors from our base.  In this case, we would like the coefficients to be some meaningful.  Obviously, they cannot be completely arbitrary, and I will soon show why.  One of the limitations can be set so that all coefficients lie in the interval from 0 to 1. This must be done, because if the coefficients are completely arbitrary, then the persons will be implausible. <br><br><img src="https://habrastorage.org/webt/43/x3/m_/43x3m_fwzwj8xcsvywqjusbijmm.jpeg"><br><br>  Here I would like to give the parameters some probabilistic meaning.  That is, we want to look at a set of parameters and understand whether a person is likely to turn out or not.  By this we want to ensure that distorted people meet low probabilities. <br><br><img src="https://habrastorage.org/webt/ge/m4/yb/gem4ybi9ha0puz8tcjv3nomiwiy.jpeg"><br><br>  Here's how to do it.  We can apply the main component method to a set of scans.  At the output, we obtain the average face S0, obtain the matrix V, the set of principal components, and also obtain the variation of the data along the principal components.  Then we can take a fresh look at the generation of faces, we will see faces as some average face, plus the matrix of main components multiplied by the vector of parameters. <br><br>  The value of the parameters is the very intensity of the sliders, which I talked about on one of the early slides.  And also we can assign a probabilistic value to the vector of parameters.  In particular, we can agree that this vector is Gaussian. <br><br><img src="https://habrastorage.org/webt/pv/bk/qq/pvbkqqdess7l175ginfuz9vkzmw.jpeg"><br><br>  Thus, we have a method that allows us to generate 3D faces, and this generation is controlled by the following parameters.  As in the previous slide, we have two sets of parameters, two vectors Œ± id and Œ± exp, they are the same as in the previous slide, but Œ± id is responsible for the shape of the face, and Œ± exp will be responsible for the emotion. <br><br>  Also there is a new vector T - vector texture.  It has the same dimension as the shape vector, and each vertex in this vector has three RGB values.  Similarly, a texture vector is generated using the parameter vector Œ≤.  There are no formalized parameters that will be responsible for the lighting of the face and for its position, but they also exist. <br><br><img src="https://habrastorage.org/webt/ab/bx/23/abbx2327nyov76ewuzl_s_fa9qo.jpeg"><br><br>  Here are examples of individuals that can be generated using a deformed model.  Please note that they differ in shape, skin color, and are also traced in different lighting conditions. <br><br><img src="https://habrastorage.org/webt/-n/ht/eu/-nhteuzhzcomppfvup80sevyhhc.jpeg"><br><br>  Now we can go to 3D reconstruction.  This is called the inverse problem, because we want to select such parameters for the deformable model so that the face we draw from it is as close as possible to the original.  This slide differs from the first one in that the face on the right is completely synthetic.  If on the first slide our texture was taken from a photo, here the texture was taken from a deformable model. <br><br>  At the output we will have all the parameters, on the slide are Œ± id and Œ± exp, and also we will have lighting, texture parameters, etc. <br><br><img src="https://habrastorage.org/webt/qa/ci/ql/qaciqlkbflxmqo1a1pzzn01g4d8.jpeg"><br><br>  We said that we want to ensure that the generated model was similar to the photo.  This similarity is determined by the energy function.  Here we simply take the pixel-by-pixel difference of the images in those pixels where we consider that the face is visible.  For example, if the face is rotated, there will be overlaps.  For example, part of the cheekbones will be closed nose.  And the visibility matrix M should display such an overlap. <br><br>  In essence, 3D reconstruction is to minimize this energy function.  But in order to solve this minimization problem, it would be good to have initialization and regularization.  Regularization is needed for an understandable reason, as we said that if we do not regularize the parameters and make them very arbitrary, then distorted faces may turn out.  Initialization is necessary, because the task as a whole is complex, it has local minima, and you don‚Äôt want to deal with them. <br><br><img src="https://habrastorage.org/webt/jq/i-/ns/jqi-nskft-v7pvl8tadsmrldaco.jpeg"><br><br>  How can I do the initialization?  For this you can use 68 key points of the face.  Since 2013-2014, a lot of algorithms have appeared that allow detecting 68 points with fairly good accuracy, and now they are approaching the saturation of their accuracy.  Therefore, we have a way to reliably detect 68 points of the face. <br><br>  We can add to our function of energy a new addend, which will say that we want the projections of the same 68 points of the model to coincide with the key points of the face.  We mark these points on the model, then we somehow deform the model, twist, projecting points, and make sure that the positions of the points coincide.  In the left photo of the point of two colors, purple and yellow.  Some points were detected by the algorithm, while others were projected from the model.  On the right there is a marking of points on the model, but for points on the edge of the face, not one point is marked, but a whole line.  This is done because when the face rotates, the marking of these points must change, and the point is selected with a line. <br><br><img src="https://habrastorage.org/webt/sb/kg/n0/sbkgn0oajeqf_z-bleo0qfhzguy.jpeg"><br><br>  Here is the term about which I spoke, it is the coordinatewise difference of two vectors that describe the key points of the face and the key points projected from the model. <br><br><img src="https://habrastorage.org/webt/wj/ku/eq/wjkueqaneclilvcxhbdsimu-gik.jpeg"><br><br>  Let us return to the regularization and consider the whole problem from the perspective of the Bayesian inference.  The probability that the vector Œ± is equal to something given a known image is proportional to the product of the probability of observing an image for a given Œ± multiplied by the probability Œ±.  If we take the negative logarithm of this expression, which we will have to minimize, we will see that the term responsible for regularization will have a specific form here.  In particular, this is the second term.  Recalling that earlier we made the assumption that the Œ± vector is Gaussian, we will see that the term responsible for regularization is the sum of the squares of the parameters, reduced to variations along the principal components. <br><br><img src="https://habrastorage.org/webt/9t/hj/y2/9thjy2x8qmvgcqxayk7rrddusra.jpeg"><br><br>  So, we can write out the full function of energy containing three terms.  The first term is responsible for the texture, for the difference in pixels between the generated image and the target image.  The second term is responsible for the key points, and the third is responsible for regularization. <br><br>  The coefficients of the terms in the process of minimization are not optimized, they are simply given. <br>  Here, the energy function is represented as a function of all parameters.  Œ± id - face shape parameters, Œ± exp - expression parameters, Œ≤ - texture parameters, p - other parameters that we talked about, but did not formalize them, these are position and lighting parameters. <br><br><img src="https://habrastorage.org/webt/tn/ki/np/tnkinpazftdmbqj3cyiij4vxn8s.jpeg"><br><br>  Let us dwell on such a remark.  This energy function can be simplified.  From it, you can throw away the addendum, which is responsible for the texture, and use only the information transmitted by 68 points.  And this will allow you to build some kind of 3D model.  However, pay attention to the model profile.  On the left is a model built only on key points.  On the right is a model using texture when building.  Please note that on the right, the profile produces a more appropriate central photograph that represents the frontal view of the face. <br><br><img src="https://habrastorage.org/webt/r2/kp/p9/r2kpp94wnxg7vhfz1nha-ewl4xg.jpeg"><br><br>  Animation with the existing algorithm for constructing a 3D-face model works quite simply.  Recall that when building a 3D model, we get two parameter vectors, one responsible for the form, the other for the expression.  These parameter vectors for the user and the avatar will always be their own.  The user has one vector of form parameters, the avatar has another.  However, we can make them so that the vectors responsible for the expression become the same.  We will take the parameters that are responsible for the user's facial expression, and simply substitute them into the avatar model.  Thus, we will transfer the user's facial expression to the avatar. <br><br>  Let's talk about two shifts in this area: the speed of work and the limitations of the deformable model. <br><br><img src="https://habrastorage.org/webt/d8/s6/q8/d8s6q8yjlm76pcdoa2p4pavf-vw.jpeg"><br><br>  Work speed is really a problem.  Minimizing the total energy function is a very computationally intensive task.  In particular, it can take from 20 to 40, an average of 30 seconds.  It is long enough.  If we build a three-dimensional model only at key points, it will turn out much faster, but the quality will suffer from this. <br><br><img src="https://habrastorage.org/webt/wk/se/zq/wksezqkjmdzs3v25feuanneaf84.jpeg"><br><br>  How to deal with this problem?  You can use more resources, some people solve this problem on the GPU.  Only key points can be used, but quality will suffer.  And you can use machine learning methods. <br><br><img src="https://habrastorage.org/webt/p2/8m/fa/p28mfavfwitizfwss5fhreauviq.jpeg"><br><br>  We will see in order.  Here is the work of 2016, in which the user's expression is transferred to a specific video, you can manage the video with the help of your face.  Here, the construction of a 3D model is performed in real time using a GPU. <br><br><img src="https://habrastorage.org/webt/dn/qx/yj/dnqxyjywcjbx5yus2v7pvcyq8vm.jpeg"><br><br>  Here are the techniques that use machine learning.  The idea is that we can first take a large database of individuals, build a 3D model for each person with a long but accurate algorithm, present each model as a set of parameters, and continue to train the grid to predict these parameters.  In particular, in this work in 2016, ResNet is used, which takes the image as input, and gives the model parameters as output. <br><br><img src="https://habrastorage.org/webt/p6/ng/t6/p6ngt6gzn-4mps27709zhmeuzwk.jpeg"><br><br>  The three-dimensional model can be presented in a different way.  In this 2017 paper, the 3D model is presented not as a set of parameters, but as a set of voxels.  The network predicts voxels, turning the image into some three-dimensional representation.  It is worth noting that network training options are possible, for which 3D models are not required at all. <br><br><img src="https://habrastorage.org/webt/yw/__/ot/yw__otysknuehp7kdjgikvziijc.jpeg"><br><br>  This works as follows.  Here the most important part is the layer, which can take as input the parameters of the deformable model and render the picture.  It has such a wonderful property that through it you can back propagate the error.  The network accepts an image as input, predicts the parameters, feeds these parameters to the layer that renders the image, compares this image with the input image, gets an error, back propagates the error and continues to learn.  Thus, the network learns to predict the parameters of a three-dimensional model, having only images as training data.  And it is very interesting. <br><br><img src="https://habrastorage.org/webt/m_/kx/vc/m_kxvcoekvrrgp41k410ikd_viu.jpeg"><br><br>  We talked a lot about accuracy - in particular, that it suffers if we throw away some of the components of the energy function.  Let's formalize what this means, how to evaluate the accuracy of 3D face reconstruction.  This requires a base of ground truth scans obtained using special equipment, using methods for which there are some guarantees of accuracy.  If there is such a base, then we can compare our reconstructed models with ground truth.  This is done simply: we consider the average distance from the vertices of our model, which we built, to the vertices in ground truth, and we normalize to the size of the scan.  This needs to be done because there are different faces, some more, some less, and on a small face the error would be less, simply because the face itself is smaller.  Therefore, we need a normalization. <br><br><img src="https://habrastorage.org/webt/a0/us/rq/a0usrqlogscnks2yygnrestgf9o.jpeg"><br><br>  I would like to tell about our work, it will be in workshops, there is an ECCV.  We do similar things, we teach MobileNet to predict the parameters of a deformable model.  We use 3D models built for 300 dataset photos as training data.  We estimate the accuracy on the basis of BU4DFE scans. <br><br><img src="https://habrastorage.org/webt/oa/db/ha/oadbhaivdnukffi7vcdwye8vsmu.jpeg"><br><br>  That's what happens.  We compare our two algorithms with state of the art.  The yellow curve on this graph is an algorithm that takes 30 seconds and consists in minimizing the total energy function.  Here, on the X axis, is the error that we just talked about, the average distance between the vertices.  On the Y axis, the proportion of images in which this error is less than that on the X axis. In this graph, the higher the curve, the better.  The next curve is our network based on the MobileNet architecture.  Then three works that we talked about.  Network predicting parameters and network predicting voxel. <br><br><img src="https://habrastorage.org/webt/p3/ku/82/p3ku82pa5bgvee5_k7mj2w5bfne.jpeg"><br><br>  We also compared our network with analogues in terms of model size and speed of operation.  It‚Äôs a win here, since we‚Äôre using MobileNet, a fairly easy one. <br><br>  The second challenge is the boundedness of the deformable model. <br><br><img src="https://habrastorage.org/webt/u1/6a/9j/u16a9jeq3vc6lvwvc0hu82ymqd8.jpeg"><br><br>  Pay attention to the left face, look at the wings of the nose.  Here are the shadows on the wings of the nose.  The borders of the shadows do not coincide with the borders of the nose in the photograph, thus resulting in a defect.  The reason for this may be that the deformable model is in principle incapable of building the nose of the required shape, because this deformable model was obtained from scans of only 200 persons.  We would like the nose to be correct, as in the right photo.  Thus, we need to somehow go beyond the deformable model. <br><br><img src="https://habrastorage.org/webt/eh/83/a5/eh83a5w3ovjb0t3v0d3slmazsgc.jpeg"><br><br>  This can be done with nonparametric deformation of the mesh.  Here are three tasks that we would like to solve: modify the local part of the face, such as the nose, then embed it into the original face model, and leave everything else unchanged. <br><br><img src="https://habrastorage.org/webt/kr/dt/j5/krdtj5freppwfivlapbiqtlyz-y.jpeg"><br><br>  This can be done as follows.  Let us return to the designation of the mesh as a vector in the 3n-dimensional space and look at the averaging operator.  This is an operator, which in S with a cap replaces each vertex with the average of its neighbors.  Neighbors of the top are those that are connected to it by an edge. <br><br>  We define some energy function that describes the position of the vertex relative to its neighbors.  We want the position of the vertex relative to its neighbors to remain unchanged or at least not change much.  But at the same time, we will somehow modify S. This energy function is called internal, because there will also be some external component that will say that, for example, the nose should take a given shape. <br><br><img src="https://habrastorage.org/webt/9r/0g/-o/9r0g-o4tyxpzkzgfl_3bx7_xjue.jpeg"><br><br>  Such techniques were used, for example, in the work of 2015.  They did 3D-reconstruction of persons on several photos.  They took several photos from the phone, got a cloud of points, and then adapted the face model to this cloud using non-parametric modification. <br><br><img src="https://habrastorage.org/webt/1q/9c/yh/1q9cyhw6dspqkz57ese7dwbezyc.jpeg"><br><br>  Beyond the deformable model, you can go in another way.  Let us dwell on the action of the smoothing operator.  Here, for simplicity, the two-dimensional mesh is presented, to which this operator has been applied.  On the model on the left there are a lot of details, on the model on the right these details have been smoothed out.  Can we do something to add details and not to remove them? <br><br><img src="https://habrastorage.org/webt/5k/je/ir/5kjeirpdi5wtjeapi2v6h7tsxm8.jpeg"><br><br>  For the answer, we can look at the basis of the vectors of the smoothing operator.  The smoothing operator modifies the coefficients of the mesh in the expansion over this basis. <br><br>  Is it necessary to solve the problem in this way?  You can do it another way: just modify these coefficients in some external way.  Let's just take the first few vectors of the smoothing operator and add it to our deformable model as a new set of sliders.  This technique really allows you to get improvements, so it is done in the work of 2016.  This concludes my report, thank you all. </div><p>Source: <a href="https://habr.com/ru/post/421353/">https://habr.com/ru/post/421353/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../421341/index.html">Valve wants to run all Windows games on Linux</a></li>
<li><a href="../421343/index.html">Protecting Microsoft Office 365 Cloud Data with Veeam Solution</a></li>
<li><a href="../421345/index.html">Angular: unobvious directive selectors</a></li>
<li><a href="../421347/index.html">CORS, CSP, HTTPS, HSTS: About Web Security Technologies</a></li>
<li><a href="../421351/index.html">The theory of happiness. The curse of the director and damn printers</a></li>
<li><a href="../421355/index.html">Go 1.11 is released - WebAssembly and Native modules</a></li>
<li><a href="../421357/index.html">To the question of the impossible. Part 3</a></li>
<li><a href="../421359/index.html">Festival as a game. Taxonomy IT</a></li>
<li><a href="../421361/index.html">AMD has opened the source code for V-EZ, a cross-platform low-level Vulkan API shell</a></li>
<li><a href="../421365/index.html">The evolution of a single startup. Agile from Yaytselova to Chiken Invaders</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>