<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Batch Normalization to accelerate neural network learning</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the modern world, neural networks find more and more applications in various fields of science and business. Moreover, the more complex the task, t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Batch Normalization to accelerate neural network learning</h1><div class="post__text post__text-html js-mediator-article"><p>  In the modern world, neural networks find more and more applications in various fields of science and business.  Moreover, the more complex the task, the more complex is the neural network. </p><br><p>  Learning complex neural networks can sometimes take days and weeks for only one configuration.  And in order to choose the optimal configuration for a specific task, it is required to start training several times - this can take months of calculations even on a really powerful machine. </p><br><p>  At some point, while <a href="https://arxiv.org/pdf/1502.03167.pdf">familiarizing</a> myself with the <a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization</a> method from Google presented in 2015, for solving the problem of face recognition, I was able to significantly improve the speed of the neural network. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/edf/a44/ef2/edfa44ef289e496bad3ebc18a391431f.png"></div><br><p>  For details, I ask under the cat. </p><a name="habracut"></a><br><p>  In this article I will try to combine two urgent tasks for today - this is the task of computer vision and machine learning.  As I have already specified, Batch Normalization will be used as a neural network learning design to accelerate neural network learning.  The training of the neural network (written using the Caffe library, which is popular within the computer vision), was conducted on the basis of 3 million faces of 14 thousand different people. </p><br><p>  In my task, a classification into 14,700 classes was necessary.  The base is divided into two parts: training and test samples.  Known classification accuracy on the test sample: 94.5%.  At the same time, this required 420 thousand iterations of training - which is almost 4 days (95 hours) on the NVidia Titan X video card. </p><br><p>  Initially, for this neural network, some standard methods of learning acceleration were used: </p><br><ul><li>  Increase learning rate </li><li>  Reducing the number of network parameters </li><li>  Change learning rate in the learning process in a special way </li></ul><br><p>  All these methods were applied to this network but then the ability to apply them came to naught, because  began to decrease the accuracy of classification. </p><br><p>  At this point, I discovered a new method of accelerating neural networks - Batch Normalization.  The authors of this method tested it on the standard Inception network based on <a href="http://www.image-net.org/challenges/LSVRC/2012/">LSVRC2012</a> and got good results: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/128/0f8/673/1280f8673aa94a8ea3cfdf4ebdae2f12.png"></div><br><p>  From the graph and the table it is clear that the network has learned 15 times faster and even achieved higher accuracy in the end. </p><br><h3>  What is Batch Normalization? </h3><br><p>  Consider a classic neural network with several layers.  Each layer has multiple inputs and multiple outputs.  The network is trained by the method of back propagation of the error, by the batch, that is, the error is considered for some subset of the training sample. </p><br><p>  The standard method of normalization - for each k, consider the distribution of elements of the batch.  Let us subtract the average and divide by the variance of the sample, obtaining the distribution with the center at 0 and the variance 1. Such a distribution will allow the network to learn faster, because  all numbers are of the same order.  But it is even better to introduce two variables for each attribute, summarizing the normalization as follows: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/005/d19/2bd/005d192bd6274c298f75896498aea377.png"></div><br><p>  We obtain the mean, variance.  These parameters will be included in the backpropagation algorithm. <br>  Thus, we get a batch normalization layer with 2 * k parameters, which we will add to the architecture of the proposed network for face recognition. </p><br><p>  The input to my task is a black and white image of a 50x50 pixel face.  At the exit, we have 14,000 probability classes.  The class with the maximum probability is considered the result of prediction. </p><br><p>  The original network looks like this: </p><br><p>  8 convolutional layers are used, each 3x3 in size.  After each convolution, ReLU: max (x, 0) is used.  After a block of two bundles, there is a max-pooling with a cell size of 2x2 (without overlapping cells).  The last pooling layer has a cell size of 7x7, which averages the values, but does not take the maximum.  The result is an array of 1x1x320, which is served on a fully connected layer. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d77/d9a/299/d77d9a2999b94793a15bb1d2567acc1b.png"></div><br><p>  The network with Batch Normalization looks a bit more complicated: </p><br><p>  In the new architecture, each block of two bundles contains a Batch Normalization layer between them.  We also had to remove one convolutional layer from the second block, as the addition of new layers increased the memory consumption of the graphics card. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/ebb/656/b92/ebb656b927034339a32006b289de8371.png"></div><br><p>  At the same time, I removed Dropout in accordance with the recommendations on the use of BN authors of the original article. </p><br><h3>  Experimental evaluation </h3><br><p>  The main metric is accuracy, we divide the number of correctly classified images by the number of all images in the test sample. </p><br><p>  The main difficulty in optimizing a neural network with the help of the Batch Normalization layer is to select the learning rate and correctly change it during the network learning process.  In order for the network to converge faster, the initial learning rate must be greater, and then decrease, so that the result is more accurate. </p><br><p>  Several options for changing the rate of learning were tested: </p><br><table><thead><tr><th>  Name </th><th>  Formula for changing learning rate </th><th>  Iterations to 80% accuracy </th><th>  Iterations to complete convergence </th><th>  Maximum accuracy </th></tr></thead><tbody><tr><td>  original </td><td>  0.01 * 0.1 <sup>[ <i>#iter</i> / 150000]</sup> </td><td>  64000 </td><td>  420000 </td><td>  94.5% </td></tr><tr><td>  short step </td><td>  0.055 * 0.7 <sup>[ <i>#iter</i> / 11000]</sup> </td><td>  45,000 </td><td>  180000 </td><td>  86.7% </td></tr><tr><td>  multistep without dropout </td><td>  0.055 * 0.7 <sup><i>Nsteps</i></sup> </td><td>  45,000 </td><td>  230000 </td><td>  91.3% </td></tr></tbody></table><br><p>  [x] - the whole part <br>  <i>#iter</i> - iteration number <br>  <i>Nsteps</i> - the steps specified manually at the iterations: 14000, 28000, 42000, 120000 (x4), 160000 (x4), 175000, 190,000, 210,000. </p><br><p>  A graph showing the learning process: accuracy on a test sample depending on the number of iterations of neural network training performed: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/b3e/ed3/3e3/b3eed33e3b05456fa05fe358edbf1314.png"></div><br><p>  The original network converges in 420000 iterations, and the learning rate for all time changes only 2 times at the 150000th iteration and at the 300000th.  This approach was proposed by the author of the original network, and my experiments with this network showed that this approach is optimal. </p><br><p>  But if the Batch Normalization layer is present, this approach gives poor results - the long_step graph.  Therefore, my idea was to change the learning rate smoothly at the initial stage, and then make a few jumps (multistep_no_dropout schedule).  The short_step chart shows that just a smooth change in the learing rate works worse.  In fact, here I rely on the recommendations of the article and try to apply them to the original approach. </p><br><p>  As a result of the experiments, I came to the conclusion that learning can be accelerated, but the accuracy will be a little worse anyway.  You can compare the task of face recognition with the task described in the Inception articles ( <a href="http://arxiv.org/pdf/1512.00567v3">Inception-v3</a> , <a href="http://arxiv.org/pdf/1602.07261v1.pdf">Inception-v4</a> ): the authors compared the classification results for different architectures and it turned out that Inception-BN is still inferior to the new Inception versions without using Batch Normalization.  In our problem, we get the same problem.  But still, if the task is to get acceptable accuracy as quickly as possible, then BN can help: in order to achieve 80% accuracy, it takes 1.4 times less time compared to the original network (45000 iterations against 64000).  This can be used, for example, for designing new networks and selecting parameters. </p><br><h3>  Software implementation </h3><br><p>  As I already wrote, in my work Caffe is used - a convenient tool for deep learning.  Everything is implemented in C ++ and CUDA, which ensures optimal use of computer resources.  For this task it is especially important, since  if the program were not written optimally there would be no point in speeding up learning by changing the network architecture. </p><br><p>  Caffe has a modularity - it is possible to connect any neural network layer.  For the Batch Normalization layer, 3 implementations were found: </p><br><ol><li>  <a href="">BatchNorm</a> in the original caffe (dated February 25, 2015) </li><li>  <a href="">BN</a> in caffe windows (from March 31, 2015) </li><li>  <a href="">BatchNorm</a> at CUDNN (November 24, 2015) </li></ol><br><p>  To quickly compare these implementations, I took the standard <a href="https://www.cs.toronto.edu/~kriz/cifar.html">Cifar10</a> base and tested everything under the Linux x64 operating system using the NVidia GeForce 740M (2GB) graphics card, Intel¬Æ Core (TM) i5-4200U CPU @ 1.60GHz processor and 4GB RAM. </p><br><p>  The implementation from caffe windows is built into the standard caffe, because  tested under linux. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/bef/6f4/89e/bef6f489e73e42989336ea30b7471e9d.png"></div><br><p>  The graph shows that the 1st implementation is inferior to the 2nd and 3rd in accuracy, in addition, in the 1st and 2nd cases, the prediction accuracy changes jumps from iteration to iteration (in the first case, jumps are much stronger).  Therefore, the 3rd implementation (from NVidia) was chosen as the most stable and newer for the facial recognition task. </p><br><p>  These neural network modifications by adding layers of Batch Normalization show that it will take 1.4 times less time to achieve acceptable accuracy (80%). </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/309302/">https://habr.com/ru/post/309302/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../309292/index.html">GPS Spoofing in Practice: How to Catch All Pokemon Without Leaving Home</a></li>
<li><a href="../309294/index.html">Life in the conditions of total network connection</a></li>
<li><a href="../309296/index.html">Finite automata in the SimInTech dynamic simulation environment. Part 3. Go to the C code.</a></li>
<li><a href="../309298/index.html">The book "ES6 and not only"</a></li>
<li><a href="../309300/index.html">What to read about Angular 2</a></li>
<li><a href="../309304/index.html">Analysis of the modifications of the extortioner TorrentLocker</a></li>
<li><a href="../309306/index.html">Webpack manual</a></li>
<li><a href="../309308/index.html">Wi-Fi positioning "cheap and cheerful." About the frequency of measurements or is Wi-Fi positioning possible in real time?</a></li>
<li><a href="../309310/index.html">The main business of Google began to suffer due to the high popularity of mobile applications.</a></li>
<li><a href="../309312/index.html">The network published a database of 98 million Rambler accounts</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>