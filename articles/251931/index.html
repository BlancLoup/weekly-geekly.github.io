<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We test content distribution in GlusterFS</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I found several articles on Habr√© about the basic installation and configuration of GlusterFS, but did not find anything about the types of content di...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We test content distribution in GlusterFS</h1><div class="post__text post__text-html js-mediator-article">  I found several articles on Habr√© about the basic installation and configuration of GlusterFS, but did not find anything about the types of content distribution that it supports and with which <a href="http://sysadm.pp.ua/linux/glusterfs-setup.html">I have been playing for a long time</a> .  What will be discussed in this article. <br><img src="https://habrastorage.org/getpro/habr/post_images/f76/768/8e6/f767688e6ee45db8a127e40bce9ba26b.png" alt="image"><br><a name="habracut"></a><br>  GlusterFS is a very convenient and easy to use and set up distributed file system that works in user space using FUSE technology, i.e.  runs on top of the main file system.  For installation and configuration we will use the OS Ubuntu 12.04. <br><br>  We will configure a cluster of two servers and mount (test) on one client.  We have the following settings: <br><table border="5" cellspacing="3" cellpadding="3"><tbody><tr><td width="120">  <b>IP</b> <br></td><td width="174">  <b>Hostname</b> <br></td><td width="96">  <b>Short name</b> <br></td></tr><tr><td width="120">  192.168.1.100 </td><td width="174">  server1.example.com </td><td width="96">  server1 </td></tr><tr><td width="120">  192.168.1.101 </td><td width="174">  server2.example.com </td><td width="96">  server2 </td></tr><tr><td width="120">  192.168.1.102 </td><td width="174">  client1.example.com </td><td width="96">  client1 </td></tr></tbody></table><br><br><h1>  1. General settings </h1><br>  First, for convenience, add all the addresses to the hosts on each machine. <br><pre><code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># cat /etc/hosts 127.0.0.1 localhost 192.168.1.100 server1.example.com server1 192.168.1.101 server2.example.com server2 192.168.1.102 client1.example.com client1 root@server2:~# cat /etc/hosts 127.0.0.1 localhost 192.168.1.100 server1.example.com server1 192.168.1.101 server2.example.com server2 192.168.1.102 client1.example.com client1 root@client1:~# cat /etc/hosts 127.0.0.1 localhost 192.168.1.100 server1.example.com server1 192.168.1.101 server2.example.com server2 192.168.1.102 client1.example.com client1</span></span></code> </pre> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      If you use a firewall, you need to open TCP ports 111, 24007-24050 on all machines that act as servers (two, in our cases). <br><br><h1>  2. Configuring the server part (cluster) </h1><br>  On all servers you need to install gluster-server.  At the moment, the latest version is 3.5.2.  Installation will be done from ppa repositories. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment">#apt-get install python-software-properties root@server1:~#add-apt-repository ppa:semiosis/ubuntu-glusterfs-3.5 root@server1:~#apt-get update root@server1:~#apt-get install glusterfs-server - root@server2:~#apt-get install python-software-properties root@server2:~#add-apt-repository ppa:semiosis/ubuntu-glusterfs-3.5 root@server2:~#apt-get update root@server2:~# apt-get install glusterfs-server -</span></span></code> </pre><br>  Now we connect to one of the servers and create a cluster. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># gluster peer probe server2.example.com Probe successful</span></span></code> </pre><br><br>  Check status: <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># gluster peer status Number of Peers: 1 Hostname: server2.example.com Uuid: 0f3aa4f4-f9dd-4cd2-a7a5-126606fbab33 State: Peer in Cluster (Connected)</span></span></code> </pre><br><br>  As you can see, server2 was added to the cluster and its status is Connected.  You can also make sure that on the second server everything is ok. <br><pre> <code class="bash hljs">root@server2:~<span class="hljs-comment"><span class="hljs-comment"># gluster peer status Number of Peers: 1 Hostname: 192.168.1.100 Uuid: ae4e6766-787f-4d0c-8b96-8fc9523ef346 State: Peer in Cluster (Connected)</span></span></code> </pre><br><br>  GlusterFS supports 5 types of distribution of content combined in volume: <br><ul><li>  Distributed </li><li>  Replicated </li><li>  Striped (divided in parts) </li><li>  Distributed Striped (distributed and divided in parts) </li><li>  Distributed Replicated (distributed and replicable) </li></ul><br><br>  First, we configure each type of content distribution separately and then mount all 5 volums on the client for tests. <br><br><h2>  2.1 Configuring distributed volume </h2><br>  With this setting, the data will be distributed in a random order between each folder that entered the volume. <br>  Create dist1, dist2 packs on the first server, dist3, dist4 packs on the second server. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># mkdir /mnt/dist{1,2} root@server2:~# mkdir /mnt/dist{3,4}</span></span></code> </pre><br>  Now you can create and start volume. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># gluster volume create distributed transport tcp server1:/mnt/dist1 server1:/mnt/dist2 server2:/mnt/dist3 server2:/mnt/dist4 force Creation of volume distributed has been successful. Please start the volume to access data. root@server1:~# gluster volume start distributed Starting volume distributed has been successful</span></span></code> </pre><br><br><h2>  2.2 Configuring replicated volume </h2><br>  With this setting, the data will be mirrored (like RAID1) copied between each folder (server) that entered volume.  Of course, the number of folders (servers) included in the volume must be a multiple of the replica. <br>  Create repl1, repl2 on the first server, repl3, repl4 on the second server. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># mkdir /mnt/repl{1,2} root@server2:~# mkdir /mnt/repl{3,4}</span></span></code> </pre><br><br>  Now you can create and start volume. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># gluster volume create replicated replica 4 transport tcp server1:/mnt/repl1 server2:/mnt/repl3 server1:/mnt/repl2 server2:/mnt/repl4 force Multiple bricks of a replicate volume are present on the same server. This setup is not optimal. Do you still want to continue creating the volume? (y/n) y volume create: replicated: success: please start the volume to access data root@server1:~# gluster volume start replicated volume start: replicated: success</span></span></code> </pre><br><br>  In this case, all files will be replicated to all 4 folders.  Those.  all folders will contain the same content. <br><br><h2>  2.3 Configuring striped volume </h2><br>  With this setting, the files will be divided into parts and each piece will be stored in each of the folders included in the volume.  This type of content distribution is suitable for storing capacitive data with a very large size (video content in good quality, for example).  Of course, the number of folders included in the volume should be equivalent to the value of stripe.  Those.  if we have 5 servers, the file will be divided into 5 parts and each of its pieces will be stored on all 5 servers. <br>  Let's create pack1 strip1, strip2 on the first server, strip3, strip4 on the second. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># mkdir /mnt/strip{1,2} root@server2:~# mkdir /mnt/strip{3,4}</span></span></code> </pre><br><br>  Now you can create and start volume. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># gluster volume create striped stripe 4 transport tcp server1:/mnt/strip1 server1:/mnt/strip2 server2:/mnt/strip3 server2:/mnt/strip4 force Creation of volume striped has been successful. Please start the volume to access data. root@server1:~# gluster volume start striped Starting volume striped has been successful</span></span></code> </pre><br><br>  In this case, on each of the 4 servers (folders) there will be one piece of the uploaded file. <br><br><h2>  2.4 Configuring distributed striped volume </h2><br>  With this setting, the files will be divided into parts and the pieces of the files will be divided into folders and stored in one of them.  Of course, the number of folders included in the volume must be a multiple of the stripe value.  Those.  if we have 8 servers and the stripe value is 4, then the files will be split into 4 parts and stored either on the first 4 servers or on the next 4 servers.  In the same way, if we have 4 servers and the stripe is 2, then the files will be split into 2 parts and stored on the first 2 or on the next 2 servers. <br>  Let's create dist-strip1, dist-strip2 packs on the first server, dist-strip3, dist-strip4 on the second. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># mkdir /mnt/dist-strip{1,2} root@server2:~# mkdir /mnt/dist-strip{3,4}</span></span></code> </pre><br><br>  Now you can create and start volume.  In this case, the order of servers plays a very important role: parts of files will be stored on the first pair of servers (with stripe = 2), or on the first four of servers (with stripe = 4), and the content will be distributed between the first and second pair or four and t .P. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># gluster volume create distributed-striped stripe 2 transport tcp server1:/mnt/dist-strip1 server2:/mnt/dist-strip3 server1:/mnt/dist-strip2 server2:/mnt/dist-strip4 force Creation of volume distributed-striped has been successful. Please start the volume to access data. root@server1:~# gluster volume start distributed-striped Starting volume distributed-striped has been successful</span></span></code> </pre><br><br><h2>  2.5 Configuring distributed replicated volume </h2><br>  With this setting, data will be randomly distributed between folders and each folder has its own mirror copy.  Of course, the number of folders included in the volume should be a multiple of the replica value.  Those.  if we have 4 servers and the replica value is 2, then the files will be distributed to the 2 servers in random order, and the 2 remaining servers will store an identical copy of the content of the first two.  If we have 8 servers and replica is 4, then we will have one mirror of 4 servers. <br>  Let's create dist-repl1, dist-repl2 packs on the first server, dist-repl3, dist-repl4 on the second server. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># mkdir /mnt/dist-repl{1,2} root@server2:~# mkdir /mnt/dist-repl{3,4}</span></span></code> </pre><br><br>  Now you can create and start volume.  At the same time, the order of servers plays a very important role: the first pair (foursome) of servers recorded sequentially is one replica.  Those.  if we have 8 servers and replica is equal to 4, then the first 4 servers will have one identical content, and the second four will have another identical content, etc. <br><br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># gluster volume create distributed-replicated replica 2 transport tcp server1:/mnt/dist-repl1 server2:/mnt/dist-repl3 server1:/mnt/dist-repl2 server2:/mnt/dist-repl4 force Creation of volume distributed-replicated has been successful. Please start the volume to access data. root@server1:~# gluster volume start distributed-replicated Starting volume distributed-replicated has been successful</span></span></code> </pre><br><br>  Now let's check what volum we created. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># gluster volume info Volume Name: distributed Type: Distribute Volume ID: 01622619-fd93-4ee1-85ad-ca8cf1d85f7e Status: Started Number of Bricks: 4 Transport-type: tcp Bricks: Brick1: server1:/mnt/dist1 Brick2: server1:/mnt/dist2 Brick3: server2:/mnt/dist3 Brick4: server2:/mnt/dist4 Volume Name: replicated Type: Replicate Volume ID: 67afcb89-7e5d-4a02-b4ac-0c2de7cd97be Status: Started Number of Bricks: 1 x 4 = 4 Transport-type: tcp Bricks: Brick1: server1:/mnt/repl1 Brick2: server2:/mnt/repl3 Brick3: server1:/mnt/repl2 Brick4: server2:/mnt/repl4 Volume Name: striped Type: Stripe Volume ID: e9ef42bf-8265-4973-85de-4cafd2a68fec Status: Started Number of Bricks: 1 x 4 = 4 Transport-type: tcp Bricks: Brick1: server1:/mnt/strip1 Brick2: server1:/mnt/strip2 Brick3: server2:/mnt/strip3 Brick4: server2:/mnt/strip4 Volume Name: distributed-striped Type: Distributed-Stripe Volume ID: aa70dd67-3ca9-48cb-865b-b10f8ca1ccad Status: Started Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: server1:/mnt/dist-strip1 Brick2: server2:/mnt/dist-strip3 Brick3: server1:/mnt/dist-strip2 Brick4: server2:/mnt/dist-strip4 Volume Name: distributed-replicated Type: Distributed-Replicate Volume ID: 59a819c4-6e84-4c49-9e90-23daa59d12ee Status: Started Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: server1:/mnt/dist-repl1 Brick2: server2:/mnt/dist-repl3 Brick3: server1:/mnt/dist-repl2 Brick4: server2:/mnt/dist-repl4</span></span></code> </pre> <br><br>  As you can see, we have 5 volums with different types of content distribution.  You can go to the client side. <br><br><h1>  3. Configure the client part </h1><br>  All clients need to install gluster-client from ppa repositories. <br><pre> <code class="bash hljs">root@client1:~<span class="hljs-comment"><span class="hljs-comment">#apt-get install python-software-properties root@client1:~#add-apt-repository ppa:semiosis/ubuntu-glusterfs-3.5 root@client1:~#apt-get update root@client1:~#apt-get install glusterfs-client</span></span></code> </pre><br><br>  Then you just need to mount the network folder by the name of the created volum-a.  In this case, the IP or domain name of the server is not important  if we have 10 servers added to one cluster, then on the client you can mount the ball using any of the 10 IP, the name volum-a remains the same. <br><br><h2>  3.1 Mounting and distributed volume test </h2><br>  Connect to the client, create a folder for the new disk and mount the distributed volume. <br><pre> <code class="bash hljs">root@client1:~<span class="hljs-comment"><span class="hljs-comment"># mkdir /mnt/distrib root@client1:~# mount.glusterfs server1:/distributed /mnt/distrib/ root@client1:~# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda1 7.3G 1.5G 5.5G 21% / udev 236M 4.0K 236M 1% /dev tmpfs 49M 280K 49M 1% /run none 5.0M 0 5.0M 0% /run/lock none 245M 0 245M 0% /run/shm server1:/distributed 30G 6.3G 22G 23% /mnt/distrib</span></span></code> </pre><br><br>  As you can see, we have successfully mounted a new network drive with a size of 30GB (the total size of the disks of all servers included in the volume).  Now create a dozen files. <br><pre> <code class="bash hljs">root@client1:~<span class="hljs-comment"><span class="hljs-comment"># seq 1 10 | while read line; do echo "This is File${line}" &gt; /mnt/distrib/file${line};done root@client1:~# ls /mnt/distrib/ file1 file10 file2 file3 file4 file5 file6 file7 file8 file9</span></span></code> </pre><br><br>  We look how the content is distributed across servers. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># ls /mnt/dist[0-9]/ /mnt/dist1/: file10 file3 file4 file9 /mnt/dist2/: file7 root@server2:~# ls /mnt/dist[0-9]/ /mnt/dist3/: file1 file2 /mnt/dist4/: file5 file6 file8</span></span></code> </pre><br><br>  As you can see, the files are scattered in all four folders that were included in the distributed volume. <br><br><h2>  3.2 Mounting and replicated volume test </h2><br>  Connect to the client, create a folder for the new disk and mount the replicated volume. <br><pre> <code class="bash hljs">root@client1:~<span class="hljs-comment"><span class="hljs-comment"># mkdir /mnt/replica root@client1:~# mount.glusterfs server1:/replicated /mnt/replica/ root@client1:~# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda1 7.3G 1.5G 5.5G 21% / udev 131M 4.0K 131M 1% /dev tmpfs 28M 280K 28M 1% /run none 5.0M 0 5.0M 0% /run/lock none 140M 0 140M 0% /run/shm server1:/replicated 7.3G 1.9G 5.1G 28% /mnt/replica</span></span></code> </pre><br><br>  As you can see, we have successfully mounted a new network drive with a size of 7.3GB (the total size of the disks of all servers included in the volume divided by the number of replicas).  Now create a dozen files. <br><pre> <code class="bash hljs">root@client1:~<span class="hljs-comment"><span class="hljs-comment"># seq 1 10 | while read line; do echo "This is File${line}" &gt; /mnt/replica/file${line};done root@client1:~# ls /mnt/replica/ file1 file10 file2 file3 file4 file5 file6 file7 file8 file9</span></span></code> </pre><br><br>  We look how the content is distributed across servers. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># ls /mnt/repl* /mnt/repl1: file1 file10 file2 file3 file4 file5 file6 file7 file8 file9 /mnt/repl2: file1 file10 file2 file3 file4 file5 file6 file7 file8 file9 root@server2:~# ls /mnt/repl* /mnt/repl3: file1 file10 file2 file3 file4 file5 file6 file7 file8 file9 /mnt/repl4: file1 file10 file2 file3 file4 file5 file6 file7 file8 file9</span></span></code> </pre><br><br>  As you can see, the files were mirrored into each folder that was included in the replicated volume. <br><br><h2>  3.3 Mounting and test striped volume </h2><br>  Connect to the client, create a folder for the new disk and mount striped volume. <br><pre> <code class="bash hljs">root@client1:~<span class="hljs-comment"><span class="hljs-comment"># mkdir /mnt/strip root@client1:~# mount.glusterfs server1:/striped /mnt/strip/ root@client1:~# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda1 7.3G 1.5G 5.5G 21% / udev 131M 4.0K 131M 1% /dev tmpfs 28M 280K 28M 1% /run none 5.0M 0 5.0M 0% /run/lock none 140M 0 140M 0% /run/shm server1:/striped 30G 6.3G 22G 23% /mnt/strip</span></span></code> </pre><br><br>  As you can see, we have successfully mounted a new network drive with a size of 30GB (the total size of the disks of all servers included in the volume).  Now create a couple of large files and copy to the folder. <br><pre> <code class="bash hljs">root@client1:~<span class="hljs-comment"><span class="hljs-comment"># dd if=/dev/urandom of=test2.bin bs=30M count=10 root@client1:~# dd if=/dev/urandom of=test1.bin bs=30M count=10 root@client1:~# cp test* /mnt/strip/ root@client1:~# ls -lh /mnt/strip/ total 601M -rw-r--r-- 1 root root 300M Mar 2 14:13 test1.bin -rw-r--r-- 1 root root 300M Mar 2 14:13 test2.bin</span></span></code> </pre><br><br>  As you can see, we have 2 files in the folder with a size of 300MB.  Now let's see how the content is distributed across servers. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># ls -lh /mnt/strip* /mnt/strip1: total 151M -rw-r--r-- 2 root root 75M Mar 2 14:13 test1.bin -rw-r--r-- 2 root root 75M Mar 2 14:13 test2.bin /mnt/strip2: total 151M -rw-r--r-- 2 root root 75M Mar 2 14:13 test1.bin -rw-r--r-- 2 root root 75M Mar 2 14:13 test2.bin root@server2:~# ls -lh /mnt/strip* /mnt/strip3: total 151M -rw-r--r-- 2 root root 75M Mar 2 14:13 test1.bin -rw-r--r-- 2 root root 75M Mar 2 14:13 test2.bin /mnt/strip4: total 151M -rw-r--r-- 2 root root 75M Mar 2 14:13 test1.bin -rw-r--r-- 2 root root 75M Mar 2 14:13 test2.bin</span></span></code> </pre><br><br>  As you can see, the files were divided into equal parts of 75 MB and scattered in all four folders, which were included in the striped volume. <br><br><h2>  3.4 Mounting and test distributed volume </h2><br>  Connect to the client, create a folder for the new disk and mount the distributed volume. <br><pre> <code class="bash hljs">root@client1:~<span class="hljs-comment"><span class="hljs-comment"># mkdir /mnt/distrib-strip root@client1:~# mount.glusterfs server1:/distributed-striped /mnt/distrib-strip/ root@client1:~# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda1 7.3G 1.8G 5.2G 25% / udev 131M 4.0K 131M 1% /dev tmpfs 28M 280K 28M 1% /run none 5.0M 0 5.0M 0% /run/lock none 140M 0 140M 0% /run/shm server1:/distributed-striped 30G 6.9G 21G 26% /mnt/distrib-strip</span></span></code> </pre><br><br>  As you can see, we have successfully mounted a new network drive with a size of 30GB (the total size of the disks of all servers included in the volume).  Now create a couple of large files and copy to the folder. <br><pre> <code class="bash hljs">root@client1:~<span class="hljs-comment"><span class="hljs-comment"># dd if=/dev/urandom of=test2.bin bs=30M count=10 root@client1:~# dd if=/dev/urandom of=test1.bin bs=30M count=10 root@client1:~# cp test* /mnt/distrib-strip/ root@client1:~# ls -lh /mnt/distrib-strip/ total 600M -rw-r--r-- 1 root root 300M Mar 2 14:35 test1.bin -rw-r--r-- 1 root root 300M Mar 2 14:34 test2.bin</span></span></code> </pre><br><br>  As you can see, we have 2 files in the folder with a size of 300MB.  Now let's see how the content is distributed across servers. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># ls -lh /mnt/dist-strip* /mnt/dist-strip1: total 151M -rw-r--r-- 2 root root 150M Mar 2 14:35 test1.bin /mnt/dist-strip2: total 151M -rw-r--r-- 2 root root 150M Mar 2 14:34 test2.bin root@server2:~# ls -lh /mnt/dist-strip* /mnt/dist-strip3: total 151M -rw-r--r-- 2 root root 150M Mar 2 14:35 test1.bin /mnt/dist-strip4: total 151M -rw-r--r-- 2 root root 150M Mar 2 14:34 test2.bin</span></span></code> </pre><br><br>  As you can see, the files were scattered in different folders and divided into equal parts of 150 MB. <br><br><h2>  3.5 Mounting and test distributed replicated volume </h2><br>  Connect to the client, create a folder for the new disk and mount the distributed replicated volume. <br><pre> <code class="bash hljs">root@client1:~<span class="hljs-comment"><span class="hljs-comment"># mkdir /mnt/distrib-repl root@client1:~# mount.glusterfs server1:/distributed-replicated /mnt/distrib-repl/ root@client1:~# df -h Filesystem Size Used Avail Use% Mounted on /dev/sda1 7.3G 1.8G 5.1G 27% / udev 131M 4.0K 131M 1% /dev tmpfs 28M 280K 28M 1% /run none 5.0M 0 5.0M 0% /run/lock none 140M 0 140M 0% /run/shm server1:/distributed-replicated 15G 4.4G 9.5G 32% /mnt/distrib-repl</span></span></code> </pre><br><br>  As you can see, we have successfully mounted a new network drive with a size of 15GB (the total size of the disks of all servers included in the volume divided by the number of replicas).  Now create a dozen files. <br><pre> <code class="bash hljs">root@client1:~<span class="hljs-comment"><span class="hljs-comment"># seq 1 10 | while read line; do echo "This is File${line}" &gt; /mnt/distrib-repl/file${line};done root@client1:~# ls /mnt/distrib-repl/ file1 file10 file2 file3 file4 file5 file6 file7 file8 file9</span></span></code> </pre><br><br>  We look how the content is distributed across servers. <br><pre> <code class="bash hljs">root@server1:~<span class="hljs-comment"><span class="hljs-comment"># ls /mnt/dist-repl* /mnt/dist-repl1: file10 file3 file4 file7 file9 /mnt/dist-repl2: file1 file2 file5 file6 file8 root@server2:~# ls /mnt/dist-repl* /mnt/dist-repl3: file10 file3 file4 file7 file9 /mnt/dist-repl4: file1 file2 file5 file6 file8</span></span></code> </pre><br><br>  As you can see, the first server has the same content as the second one scattered across folders. <br><br><h2>  4. Conclusion </h2><br>  In reality, each folder must be a separately mounted disk (not a root file system).  You are now familiar with each type of content distribution that is used in glusterFS. <br><br>  GlusterFS shows itself well in local networks at basic settings, if you go to data centers distributed across countries, you need to tune the settings both on the server side and when mounted on the client, which will be discussed in the next article. </div><p>Source: <a href="https://habr.com/ru/post/251931/">https://habr.com/ru/post/251931/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../251919/index.html">Big pitfalls of a small controller</a></li>
<li><a href="../251921/index.html">New server park Fujitsu</a></li>
<li><a href="../251923/index.html">Cubieboard customization highlights with linux</a></li>
<li><a href="../251927/index.html">PHP and Erlang interaction through RabbitMQ</a></li>
<li><a href="../251929/index.html">Detection of mobile malware in the wild</a></li>
<li><a href="../251933/index.html">The magic of one div. Masterclass from the creator of a.singlediv.com</a></li>
<li><a href="../251935/index.html">Ciklum Dnepropetrovsk C ++ Saturday: The first subbotnik of the year for C ++ fans is happy to meet everyone</a></li>
<li><a href="../251937/index.html">Reading GATT-characteristics of a Bluetooth device</a></li>
<li><a href="../251939/index.html">We use web application downtime for background tasks.</a></li>
<li><a href="../251941/index.html">If Seagate was dusty ...</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>