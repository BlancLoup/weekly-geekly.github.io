<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>blk-mq and I / O schedulers</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the field of data storage devices, in recent years serious changes have taken place: new technologies are being introduced, the volume and speed of...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>blk-mq and I / O schedulers</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/yi/wz/t0/yiwzt0gul_tovyeyjhobag6m0zw.png"><br><br>  In the field of data storage devices, in recent years serious changes have taken place: new technologies are being introduced, the volume and speed of disk operations are growing.  In this case, the following situation develops, in which the bottleneck becomes not the device, but the software.  The Linux kernel mechanisms for working with the disk subsystem are completely unsuitable for new, fast block devices. <br><a name="habracut"></a><br>  In recent versions of the Linux kernel, much work has been done to solve this problem.  With the release of version 4.12, users have the opportunity to use several new schedulers for block devices.  These schedulers are based on a new request transfer method - the <a href="https://lwn.net/Articles/552904/" rel="nofollow noopener noreferrer">multiqueue block layer</a> (blk-mq). <br><br>  In this article, we would like to consider in detail the features of the work of new planners and their impact on the performance of the disk subsystem. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The article will be of interest to everyone involved in testing, implementing and administering high-load systems.  We will talk about traditional and new planners, analyze the results of synthetic tests and draw some conclusions about testing and choosing a scheduler for the load.  Let's start with the theory. <br><br><h2>  Some theory </h2><br>  Traditionally, the subsystem for working with block device drivers in Linux has provided two options for sending a request to a driver: one with a queue, and the second without a queue. <br><br>  In the first case, a queue is used for requests to the device, common to all processes.  On the one hand, this approach allows the use of different planners, who in the common queue can rearrange requests in places, slowing down one and speeding up others, combining several requests to neighboring blocks into a single request to reduce costs.  On the other hand, while accelerating the processing of requests from the block device, this queue becomes a bottleneck: one for the entire operating system. <br><br>  In the second case, the request is sent directly to the device driver.  Of course, this makes it possible to pass a general queue.  But at the same time, it becomes necessary to create a turn and schedule requests inside the device driver.  This option is suitable for virtual devices that process requests and redirect them to real devices. <br><br>  In the past few years, the speed of processing requests by block devices has increased, and therefore the requirements for the operating system, as well as for the data interface, have changed. <br><br>  In 2013, the third option was developed - multiqueue block layer (blk-mq). <br>  We describe the solution architecture in general terms.  The request first enters the program queue.  The number of these queues is equal to the number of processor cores.  After passing through the program queue, the request goes to the send queue.  The number of send queues depends on the device driver, which can support from 1 to 2048 queues.  Since the work of the scheduler is carried out at the level of program queues, a request from any program queue can go to any send queue provided by the driver. <br><br><h2>  Standard Schedulers </h2><br>  In most modern Linux distributions, three standard schedulers are available: <strong>noop</strong> , <strong>CFQ</strong> and <strong>deadline</strong> . <br><br>  The name <strong>noop</strong> is short for no operation, which suggests that this scheduler does not do anything.  In fact, this is not the case: noop implements a simple FIFO queue and combining requests to neighboring blocks.  It does not change the order of requests and relies on the underlying level in this ‚Äî for example, the Raid controller's scheduling capabilities. <br><br>  <strong>CFQ</strong> (Completely fair queuing) works harder.  It divides bandwidth between all processes.  For synchronous requests it is created in turn per process.  Asynchronous requests are combined in a priority queue.  In addition, CFQ sorts queries to minimize disk sector searches.  The time to execute requests between queues is distributed according to priority, which can be configured using the <a href="https://linux.die.net/man/1/ionice" rel="nofollow noopener noreferrer">ionice</a> utility.  When setting up, you can change both the process class and the priority within the class.  We will not dwell on the <a href="https://www.kernel.org/doc/Documentation/block/cfq-iosched.txt" rel="nofollow noopener noreferrer">settings</a> in detail; we only note that this scheduler has a great deal of flexibility, and setting priority on processes is a very useful thing to limit the execution of administrative tasks for the sake of users. <br><br>  The <strong>deadline</strong> scheduler uses a different strategy.  The basis is the time spent in the queue.  In this way, it ensures that every request will be served by the scheduler.  Since most applications are blocked on reading, the deadline by default gives priority to requests for reading. <br><br>  The <strong>blk-mq</strong> mechanism was developed after the advent of NVMe disks, when it became clear that the standard kernel tools do not provide the necessary performance.  It was added to kernel 3.13, however, the planners were written later.  The latest Linux kernels (‚â•4.12) have the following schedulers for blk-mq: none, bfq, mq-deadline, and kyber.  Consider each of these planners in more detail. <br><br><h2>  Blk-mq schedulers: an overview </h2><br>  Using blk-mq, you can really disable the scheduler, for this it is enough to set it to none. <br><br>  <a href="https://habrahabr.ru/post/337102/" rel="nofollow noopener noreferrer">Much has been</a> written about <strong>BFQ</strong> , I can only say that it inherits some of the settings and the main algorithm from CFQ, introducing the concept of a budget and a few more parameters for tuning. <br><br>  <strong>Mq-deadline</strong> is, as you might guess, implementing a deadline using blk-mq. <br><br>  And the last option is <a href="https://patchwork.kernel.org/patch/9672023/" rel="nofollow noopener noreferrer">kyber</a> .  It was written to work with fast devices.  Using two queues - write and read requests, kyber gives priority to read requests, over write requests.  The algorithm measures the completion time of each request and adjusts the actual queue size to achieve the delays specified in the settings. <br><br><h2>  Tests </h2><br><h3>  Introductory notes </h3><br>  Check the work scheduler is not so easy.  A simple single-threaded test will not show convincing results.  There are two options for testing - simulate multi-threaded load using, for example, fio;  install a real application and check how it will show itself under load. <br><br>  We conducted a series of synthetic tests.  All of them were carried out with the standard settings of the schedulers themselves (such settings can be found in / sys / block / sda / queue / iosched /). <br><br><h4>  What case will we test? </h4><br>  We will create a multi-threaded load on the block device.  We will be interested in the smallest delay (the smallest value of the latency parameter) at the highest data transfer rate.  We assume that the priority is read requests. <br><br><h3>  HDD tests </h3><br>  Let's start with testing schedulers with hdd-drive. <br>  For HDD tests the server was used: <br><br><ul><li>  2 x Intel¬Æ Xeon¬Æ CPU E5-2630 v2 @ 2.40GHz </li><li>  128 GB RAM </li><li>  Disk 8TB HGST HUH721008AL </li><li>  OS Ubuntu linux 16.04, kernel 4.13.0-17-generic from official repositories </li></ul><br><h4>  Fio options </h4><br><pre><code class="python hljs">[<span class="hljs-keyword"><span class="hljs-keyword">global</span></span>] ioengine=libaio blocksize=<span class="hljs-number"><span class="hljs-number">4</span></span>k direct=<span class="hljs-number"><span class="hljs-number">1</span></span> buffered=<span class="hljs-number"><span class="hljs-number">0</span></span> iodepth=<span class="hljs-number"><span class="hljs-number">1</span></span> runtime=<span class="hljs-number"><span class="hljs-number">7200</span></span> random_generator=lfsr filename=/dev/sda1 [writers] numjobs=<span class="hljs-number"><span class="hljs-number">2</span></span> rw=randwrite [reader_40] rw=randrw rwmixread=<span class="hljs-number"><span class="hljs-number">40</span></span></code> </pre> <br>  From the settings you can see that we go around caching, set queue depth 1 for each process.  Two processes will write data to random blocks, one will write and read.  The process described in the <strong>reader_40</strong> section will send 40% of requests for reading, the remaining 60% for writing (option <strong>rwmixread</strong> ). <br><br>  More fio options are described in the <a href="https://www.mankier.com/1/fio" rel="nofollow noopener noreferrer">man page</a> . <br><br>  The test duration is two hours (7200 seconds). <br><br><div class="spoiler">  <b class="spoiler_title">Test results</b> <div class="spoiler_text">  CFQ: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/d0b/2b8/603/d0b2b8603fa90b6e824bb4aa64d81dc2.png" width="1079" height="450"></a> <br>  Deadline: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/3f8/928/eff/3f8928eff8c9cc28285afe06f8ce76f4.png" width="1084" height="451"></a> <br>  Noop: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/23b/f7f/b7e/23bf7fb7ed8d40ec21c94b705a34f865.png" width="1080" height="449"></a> <br>  BFQ: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/cab/784/8cf/cab7848cf1caec70bc640252889739df.png" width="1083" height="450"></a> <br>  Mq-deadline: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/06b/cb9/637/06bcb9637149cb1dcf82f5fef833c296.png" width="1079" height="451"></a> <br>  Kyber: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/968/308/d62/968308d6280c2db3dabf1238047e3274.png" width="1085" height="452"></a> <br>  None: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/e93/640/ce9/e93640ce978c5a8c59abdfd1710e85e4.png" width="1084" height="453"></a> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">Test results in the table</b> <div class="spoiler_text"><table><tbody><tr><td colspan="2"></td><td>  writers randwrite </td><td>  reader_40 randwrite </td><td>  reader_40 read </td></tr><tr><td rowspan="3">  CFQ </td><td>  bw </td><td>  331 KB / s </td><td>  210 KB / s </td><td>  140 KB / s </td></tr><tr><td>  iops </td><td>  80 </td><td>  51 </td><td>  34 </td></tr><tr><td>  avg lat </td><td>  12.36 </td><td>  7.17 </td><td>  18.36 </td></tr><tr><td rowspan="3">  deadline </td><td>  bw </td><td>  330 KB / s </td><td>  210 KB / s </td><td>  140 KB / s </td></tr><tr><td>  iops </td><td>  80 </td><td>  51 </td><td>  34 </td></tr><tr><td>  avg lat </td><td>  12.39 </td><td>  7.2 </td><td>  18.39 </td></tr><tr><td rowspan="3">  noop </td><td>  bw </td><td>  331 KB / s </td><td>  210 KB / s </td><td>  140 KB / s </td></tr><tr><td>  iops </td><td>  80 </td><td>  51 </td><td>  34 </td></tr><tr><td>  avg lat </td><td>  12.36 </td><td>  7.16 </td><td>  18.42 </td></tr><tr><td rowspan="3">  Bfq </td><td>  bw </td><td>  384 KB / s </td><td>  208 KB / s </td><td>  139 KB / s </td></tr><tr><td>  iops </td><td>  93 </td><td>  50 </td><td>  33 </td></tr><tr><td>  avg lat </td><td>  10.65 </td><td>  6.28 </td><td>  20.03 </td></tr><tr><td rowspan="3">  mq-deadline </td><td>  bw </td><td>  333 KB / s </td><td>  211 KB / s </td><td>  142 KB / s </td></tr><tr><td>  iops </td><td>  81 </td><td>  51 </td><td>  34 </td></tr><tr><td>  avg lat </td><td>  12.29 </td><td>  7.08 </td><td>  18.32 </td></tr><tr><td rowspan="3">  kyber </td><td>  bw </td><td>  385 KB / s </td><td>  193 KB / s </td><td>  129 KB / s </td></tr><tr><td>  iops </td><td>  94 </td><td>  47 </td><td>  31 </td></tr><tr><td>  avg lat </td><td>  10.63 </td><td>  9.15 </td><td>  18.01 </td></tr><tr><td rowspan="3">  none </td><td>  bw </td><td>  332 KB / s </td><td>  212 KB / s </td><td>  142 KB / s </td></tr><tr><td>  iops </td><td>  81 </td><td>  51 </td><td>  34 </td></tr><tr><td>  avg lat </td><td>  12.3 </td><td>  7.1 </td><td>  18.3 </td></tr></tbody></table><br>  <em>* here and further in the writers column the median from the stream of writers is taken, similarly in the reader_40 columns.</em>  <em>The delay value in milliseconds.</em> <br></div></div><br>  Let's look at the test results of traditional (single-queue) planners.  The values ‚Äã‚Äãobtained as a result of tests, do not differ from each other.  It should be noted that latency bursts that occur on the charts of deadline and noop tests also occurred during CFQ tests, although less often.  When testing blk-mq schedulers, this was not observed, the maximum latency reached as much as 11 seconds, regardless of the type of requests - writing or reading. <br><br>  Everything is much more interesting when using blk-mq-schedulers.  We are primarily interested in the delay in processing requests for reading data.  In the context of such a task, BFQ is worse for the worse.  The maximum delay value for this scheduler reached 6 seconds per write and 2.5 seconds per read.  The smallest maximum delay value showed kyber - 129ms for writing and 136 for reading.  20ms more maximum delay with none for all streams.  For mq-deadline, it was 173ms for writing and 289ms for reading. <br><br>  As the results show, it was not possible to achieve some significant reduction of the delay by changing the scheduler.  But you can select the BFQ scheduler, which showed a good result in terms of the number of recorded / read data.  On the other hand, when looking at the graph obtained when testing BFQ, it seems strange to unevenly distribute the load on the disk, despite the fact that the load from fio is quite uniform and uniform. <br><br><h3>  SSD tests </h3><br>  For SSD tests the server was used: <br><br><ul><li>  Intel¬Æ Xeon¬Æ CPU E5-1650 v3 @ 3.50GHz </li><li>  64 GB RAM </li><li>  1.6TB INTEL Intel SSD DC S3520 Series </li><li>  OS Ubuntu linux 14.04, kernel 4.12.0-041200-generic ( <a href="http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.12/" rel="nofollow noopener noreferrer">kernel.ubuntu.com</a> ) </li></ul><br><h4>  Fio options </h4><br><pre> <code class="python hljs">[<span class="hljs-keyword"><span class="hljs-keyword">global</span></span>] ioengine=libaio blocksize=<span class="hljs-number"><span class="hljs-number">4</span></span>k direct=<span class="hljs-number"><span class="hljs-number">1</span></span> buffered=<span class="hljs-number"><span class="hljs-number">0</span></span> iodepth=<span class="hljs-number"><span class="hljs-number">4</span></span> runtime=<span class="hljs-number"><span class="hljs-number">7200</span></span> random_generator=lfsr filename=/dev/sda1 [writers] numjobs=<span class="hljs-number"><span class="hljs-number">10</span></span> rw=randwrite [reader_20] numjobs=<span class="hljs-number"><span class="hljs-number">2</span></span> rw=randrw rwmixread=<span class="hljs-number"><span class="hljs-number">20</span></span></code> </pre><br>  The test is similar to the previous one for hdd, but it differs in the number of processes that will access the disk - 10 per record and two for writing and reading and the write / read ratio of 80/20, as well as the queue depth.  A partition with a capacity of 1598GB was created on the drive, two gigabytes are left unused. <br><br><div class="spoiler">  <b class="spoiler_title">Test results</b> <div class="spoiler_text">  CFQ: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/2f0/895/a42/2f0895a42585d274cee5212f450a9bb2.png" width="1237" height="415"></a> <br>  Deadline: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/a20/8a4/f75/a208a4f75a51ed20adbbe60ae995b4e3.png" width="1243" height="415"></a> <br>  Noop: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/358/1a7/6aa/3581a76aa83a2b9fe6c594bf78fa53e2.png" width="1236" height="422"></a> <br>  BFQ: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/0d1/940/319/0d194031992fbf4efbd23b65d33153e2.png" width="1237" height="423"></a> <br>  Mq-deadline: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/128/b6d/d8e/128b6dd8ed84640fd04e8309429c6e85.png" width="1237" height="420"></a> <br>  Kyber: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/5f0/c4a/498/5f0c4a49850dba47dfbd1c75ca66c11c.png" width="1242" height="421"></a> <br>  None: <br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/de7/c72/0c0/de7c720c071e1b69ae39e6e8250906dc.png" width="1240" height="419"></a> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">Test results in the table</b> <div class="spoiler_text"><table><tbody><tr><td colspan="2"></td><td>  writers randwrite </td><td>  reader_20 randwrite </td><td>  reader_20 read </td></tr><tr><td rowspan="3">  CFQ </td><td>  bw </td><td>  13065 KB / s </td><td>  6321 KB / s </td><td>  1578 KB / s </td></tr><tr><td>  iops </td><td>  3265 </td><td>  1580 </td><td>  394 </td></tr><tr><td>  avg lat </td><td>  1.223 </td><td>  2,000 </td><td>  2.119 </td></tr><tr><td rowspan="3">  deadline </td><td>  bw </td><td>  12690 KB / s </td><td>  10279 KB / s </td><td>  2567 KB / s </td></tr><tr><td>  iops </td><td>  3172 </td><td>  2569 </td><td>  641 </td></tr><tr><td>  avg lat </td><td>  1.259 </td><td>  1.261 </td><td>  1.177 </td></tr><tr><td rowspan="3">  noop </td><td>  bw </td><td>  12509 KB / s </td><td>  9807 KB / s </td><td>  2450 KB / s </td></tr><tr><td>  iops </td><td>  3127 </td><td>  2451 </td><td>  613 </td></tr><tr><td>  avg lat </td><td>  1.278 </td><td>  1.278 </td><td>  1.405 </td></tr><tr><td rowspan="3">  Bfq </td><td>  bw </td><td>  12803 KB / s </td><td>  10,000 KB / s </td><td>  2497 KB / s </td></tr><tr><td>  iops </td><td>  3201 </td><td>  2499 </td><td>  624 </td></tr><tr><td>  avg lat </td><td>  1.248 </td><td>  1.248 </td><td>  1.398 </td></tr><tr><td rowspan="3">  mq-deadline </td><td>  bw </td><td>  12650 KB / s </td><td>  9715 KB / s </td><td>  2414 KB / s </td></tr><tr><td>  iops </td><td>  3162 </td><td>  2416 </td><td>  604 </td></tr><tr><td>  avg lat </td><td>  1.264 </td><td>  1.298 </td><td>  1.423 </td></tr><tr><td rowspan="3">  kyber </td><td>  bw </td><td>  8764 KB / s </td><td>  8572 KB / s </td><td>  2141 KB / s </td></tr><tr><td>  iops </td><td>  2191 </td><td>  2143 </td><td>  535 </td></tr><tr><td>  avg lat </td><td>  1.824 </td><td>  1.823 </td><td>  0.167 </td></tr><tr><td rowspan="3">  none </td><td>  bw </td><td>  12835 KB / s </td><td>  10174 KB / s </td><td>  2541 KB / s </td></tr><tr><td>  iops </td><td>  3208 </td><td>  2543 </td><td>  635 </td></tr><tr><td>  avg lat </td><td>  1.245 </td><td>  1.227 </td><td>  1.376 </td></tr></tbody></table><br></div></div><br>  Pay attention to the average read latency.  Among all the schedulers, the kyber is the most prominent, showing the lowest latency, and the CFQ is the highest.  Kyber was designed to work with fast devices and aims to reduce latency in general, with priority for synchronous requests.  For read requests, the delay is very low, while the amount of data read is less than when using other schedulers (with the exception of CFQ). <br><br>  Let's try to compare the difference in the amount of data read per second between kyber and, for example, the deadline, as well as the difference in the read delay between them.  We see that kyber showed 7 times less read latency than the deadline, with a decrease in throughput per reading of just 1.2 times.  At the same time, kyber showed worse results for write requests - by 1.5 an increase in latency and a decrease in throughput by 1.3 times. <br><br>  Our initial task is to get the least read latency with the least amount of bandwidth damage.  According to the test results, it can be considered that kyber is better than other planners for solving this task. <br><br>  It is interesting to note that CFQ and BFQ showed a relatively low recording delay, but at the same time, with CFQ, processes that performed only writing to the disk received the highest priority.  What conclusion can be made from this?  Probably, BFQ more ‚Äúhonestly‚Äù distributes the priority for requests, as stated by the developers. <br><br>  The maximum latency value was much higher for * FQ schedulers and mq-deadline - up to ~ 3.1 seconds for CFQ and up to ~ 2.7 seconds for BFQ and mq-deadline.  For other schedulers, the maximum delay during the tests was 35-50 ms. <br><br><h3>  NVMe tests </h3><br>  For the NVMe tests, the Micron 9100 drive was installed in the server on which the SSD tests were carried out. The disk layout is similar to SSD - the section for the 1598GB test and 2GB of unused space.  The fio settings used were the same as in the previous test, only the queue depth (iodepth) was increased to 8. <br><br><pre> <code class="python hljs">[<span class="hljs-keyword"><span class="hljs-keyword">global</span></span>] ioengine=libaio blocksize=<span class="hljs-number"><span class="hljs-number">4</span></span>k direct=<span class="hljs-number"><span class="hljs-number">1</span></span> buffered=<span class="hljs-number"><span class="hljs-number">0</span></span> iodepth=<span class="hljs-number"><span class="hljs-number">8</span></span> runtime=<span class="hljs-number"><span class="hljs-number">7200</span></span> random_generator=lfsr filename=/dev/nvme0n1p1 [writers] numjobs=<span class="hljs-number"><span class="hljs-number">10</span></span> rw=randwrite [reader_20] numjobs=<span class="hljs-number"><span class="hljs-number">2</span></span> rw=randrw rwmixread=<span class="hljs-number"><span class="hljs-number">20</span></span></code> </pre><br><div class="spoiler">  <b class="spoiler_title">Test results</b> <div class="spoiler_text"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/da6/82d/e15/da682de155204f7f59636c1f4f709381.png" width="1240" height="417"></a> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">Test results in the table</b> <div class="spoiler_text"><table><tbody><tr><td colspan="2"></td><td>  writers randwrite </td><td>  reader_20 write </td><td>  reader_20 read </td></tr><tr><td rowspan="3">  Bfq </td><td>  bw </td><td>  45752 KB / s </td><td>  30541 KB / s </td><td>  7634 KB / s </td></tr><tr><td>  iops </td><td>  11437 </td><td>  7635 </td><td>  1908 </td></tr><tr><td>  avg lat </td><td>  0.698 </td><td>  0.694 </td><td>  1.409 </td></tr><tr><td rowspan="3">  mq-deadline </td><td>  bw </td><td>  46321 KB / s </td><td>  31112 KB / s </td><td>  7777 KB / s </td></tr><tr><td>  iops </td><td>  11580 </td><td>  7777 </td><td>  1944 </td></tr><tr><td>  avg lat </td><td>  0.690 </td><td>  0.685 </td><td>  1.369 </td></tr><tr><td rowspan="3">  kyber </td><td>  bw </td><td>  30460 KB / s </td><td>  27709 KB / s </td><td>  6926 KB / s </td></tr><tr><td>  iops </td><td>  7615 </td><td>  6927 </td><td>  1731 </td></tr><tr><td>  avg lat </td><td>  1.049 </td><td>  1,000 </td><td>  0.612 </td></tr><tr><td rowspan="3">  none </td><td>  bw </td><td>  45940 KB / s </td><td>  30867 KB / s </td><td>  7716 KB / s </td></tr><tr><td>  iops </td><td>  11484 </td><td>  7716 </td><td>  1929 </td></tr><tr><td>  avg lat </td><td>  0.695 </td><td>  0.694 </td><td>  1.367 </td></tr></tbody></table><br></div></div><br>  The graph shows the test results with the change of the scheduler and pause.  Test order: none, kyber, mq-deadline, BFQ. <br><br>  The table and the graph again show the active work of the kyber algorithm to reduce latency: 0.612ms versus 1.3-1.4 for other planners.  It is generally believed that for NVMe disks there is no point in using any scheduler, but if the priority is to reduce latency and you can sacrifice the number of I / O operations, then it makes sense to consider kyber.  Looking at the graphics, you can notice an increase in CPU load when using BFQ (last tested). <br><br><h2>  Conclusions and recommendations </h2><br>  Our article is a very general introduction to the topic.  Yes, and all the data of our tests should be taken in view of the fact that in real practice everything is much more complicated than in experimental conditions.  It all depends on a number of factors: the type of load, the file system used and much more.  Very much depends on the hardware component: disk model, RAID / HBA / JBOD. <br><br>  Speaking in general terms: if an application uses ioprio to prioritize specific processes, then the choice of a scheduler in the direction of CFQ / BFQ will be justified.  But you should always proceed from the type of load and the composition of the entire disk subsystem.  For some solutions, developers give very specific recommendations: for example, for clickhouse they <a href="https://clickhouse.yandex/docs/ru/operations/tips.html" rel="nofollow noopener noreferrer">recommend</a> using CFQ for HDD and noop for SSD drives.  If the disk requires a large bandwidth, the ability to perform more I / O operations and the average delay is not important, then you should look towards BFQ / CFQ, and for ssd disks also noop and none. <br><br>  If it is necessary to reduce the delay and each operation separately, and especially the read operation, should be performed as quickly as possible, then in addition to using ssd, you should use a specially designed deadline scheduler, or one of the new ones - mq-deadline, kyber. <br><br>  Our recommendations are general in nature and are not suitable for all cases.  When choosing a scheduler, in our opinion, the following points should be considered: <br><br><ol><li>  The hardware component also plays an important role: a situation is quite possible when a RAID controller uses embedded algorithms for scheduling queries ‚Äî in this case, the choice of a scheduler of none or noop is fully justified. </li><li>  The type of load is very important: not in experimental, but in ‚Äúcombat conditions‚Äù application can show other results.  The fio utility used for tests keeps the same load at all times, but in actual practice applications rarely access the disk in a consistent and consistent way.  The actual queue depth, average per minute, can be kept in the range of 1-3, but in peaks rise to 10-13-150 requests.  It all depends on the type of load. </li><li>  We only tested writing / reading random data blocks, and when schedulers are working, consistency is important.  With a linear load, you can get more bandwidth if the scheduler is well grouped requests. </li><li>  Each scheduler has options that can be configured additionally.  All of them are described in the <a href="https://www.kernel.org/doc/Documentation/block/" rel="nofollow noopener noreferrer">documentation</a> for schedulers. </li></ol><br>  The attentive reader probably noticed that we used different kernels to test the schedulers for HDD and SSD / NVMe drives.  The fact is that during testing on kernel 4.12 with HDD, the work of the BFQ and mq-deadline planners looked rather strange - the delay then decreased, then grew and kept very high values ‚Äã‚Äãin a few minutes.  Since this behavior did not look quite adequate and the 4.13 core came out, we decided to run tests with the HDD on the new core. <br><br>  Do not forget that the code of planners may change.  In the next release of the kernel, it turns out that the mechanism due to which a specific scheduler showed a performance degradation on your workload has already been rewritten and now, at a minimum, does not reduce performance. <br><br>  The peculiarities of the work of input-output schedulers in Linux is a complex topic, and it can hardly be considered in one publication.  We plan to return to this topic in the following articles.  If you have experience testing planners in combat, we will be happy to read about it in the comments. </div><p>Source: <a href="https://habr.com/ru/post/346844/">https://habr.com/ru/post/346844/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../346832/index.html">Forsyth study of blockchain technology</a></li>
<li><a href="../346834/index.html">We calculate the real effect of paid advertising on Youtube</a></li>
<li><a href="../346836/index.html">What rules of English do our foreign colleagues violate? Part 3</a></li>
<li><a href="../346838/index.html">10 most popular sites for the competition programmers at the beginning of 2018</a></li>
<li><a href="../346840/index.html">Seminar "Ecosystems for business. Application Performance in the Cloud, January 25, St. Petersburg</a></li>
<li><a href="../346848/index.html">How to cache AVURLAsset data downloaded by AVPLayer</a></li>
<li><a href="../346850/index.html">More about validation in ASP.NET</a></li>
<li><a href="../346852/index.html">Simulating iridissension: shader CD-ROM</a></li>
<li><a href="../346862/index.html">IPsec vs TLS / SRTP for VoIP Security</a></li>
<li><a href="../346864/index.html">Connect Fanvil phones to 3CX via L2TP tunnel on Mikrotik</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>