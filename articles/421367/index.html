<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>AI, practical course. Configure the model and hyperparameters to recognize emotions in images</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In previous articles of this training series, possible options for data preparation were described. Preprocessing and addition of data with images , a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>AI, practical course. Configure the model and hyperparameters to recognize emotions in images</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/zq/7s/el/zq7selxswjsrmplg_pxw2xilqi4.jpeg"><br><br>  In previous articles of this training series, possible options for data preparation were described. <a href="https://habr.com/company/intel/blog/414635/">Preprocessing and addition of data with images</a> , also in these articles a <a href="https://habr.com/company/intel/blog/420635/">Basic model of emotion recognition</a> based on convolutional neural network images was built. <br>  In this article, we will build an improved convolutional neural network model for recognizing emotions in images using a technique called <i>inductive learning</i> . <br><a name="habracut"></a><br>  First you need to read the article on the <a href="https://habr.com/company/intel/blog/420635/">Basic Model of Emotion Recognition on Images</a> , you can also refer to it during the reading, since some sections, including the study of source data and the description of network indicators, will not be given here in detail. <br><br><h2>  <font color="#0071c5">Data</font> </h2><br>  The data set contains 1630 images with emotions of two classes: <i>Negative</i> (class 0) and <i>Positive</i> (class 1).  A few examples of such images are shown below. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>Negative</b> <br><img src="https://habrastorage.org/webt/zr/pf/ki/zrpfkiqvgcxw6kpsv777v9d1t6w.jpeg"><br><br><img src="https://habrastorage.org/webt/52/8x/s6/528xs6k7jnimfgvhagwolru8mi4.jpeg"><br><br><img src="https://habrastorage.org/webt/no/p0/vb/nop0vbapr4ep7o5dps1wvpkncoa.jpeg"><br><br>  <b>Positive</b> <br><img src="https://habrastorage.org/webt/uc/ua/oh/ucuaohklcwcgotqf4uryopnt_qg.jpeg"><br><br><img src="https://habrastorage.org/webt/u5/hp/rb/u5hprb1cjig28b4xk8urdljb8lm.jpeg"><br><br><img src="https://habrastorage.org/webt/ru/_a/xo/ru_axoahw4h4xytx_-yphxk56ii.jpeg"><br><br>  Some of the examples contain an obvious positive or negative emotion, while others may not be classified - even with human participation.  Based on a visual check of such cases, we estimate that the maximum possible accuracy should be about 80 percent.  Note that the random classifier provides approximately 53 percent accuracy due to a small imbalance of classes. <br><br>  To train the model, we use the technique of <i>holding part of the samples</i> and divide the initial data set into two parts, one of which (20 percent of the initial set) will be used by us for testing.  Partitioning is performed using <i>stratification</i> : this means that the balance between classes is maintained in the training and test sets. <br><br><h2>  <font color="#0071c5">Solving the problem of insufficient data</font> </h2><br>  The base model demonstrated results, only slightly better random class prediction of images.  There may be many possible reasons for this behavior.  We believe that the main reason is that the available amount of data is decidedly insufficient for such training of the convolutional part of the network, which would allow to obtain the characteristics based on the input image. <br>  There are many different ways to solve the problem of data failure.  Here are a few of them: <br><br><ul><li>  <b>Re-sample</b>  The idea of ‚Äã‚Äãthe method is to evaluate the distribution of data and select <i>new examples</i> from this distribution. </li><li>  <b>Teaching without a teacher</b> .  Everyone can find large amounts of data of the same nature as the labeled examples in a given data set.  For example, it can be movies for video recognition or audio books for speech recognition.  The next step on this path is to use this data for preliminary training of the model (for example, with the help of autocoders). </li><li>  <b>Data augmentation</b> .  During this process, the sample data is modified randomly using a given set of transforms. </li><li>  <b>Inductive learning</b> .  This topic is of great interest to us, so let's take a look at it in more detail. </li></ul><br><h2>  <font color="#0071c5">Inductive learning</font> </h2><br>  The term <i>inductive learning</i> refers to a set of techniques that use models (often very large) that are trained on different data sets of approximately the same nature. <br><br><img src="https://habrastorage.org/webt/wl/jb/qi/wljbqidbfmpj1yfddtvjlkqt9ma.png"><br><br><img src="https://habrastorage.org/webt/bq/ji/ah/bqjiahabshkakrv2jcilp5pa1y8.png"><br><br>  Comparison of traditional machine learning and inductive learning methods.  The image is taken from S. Ruder <i>'s</i> blog post <i>‚ÄúWhat is inductive learning?‚Äù</i> . <br>  There are <a href="http://cs231n.github.io/transfer-learning/">three</a> main scenarios for the use of inductive learning: <br><br><ul><li>  <b>Pre-trained models</b> .  Any user can simply take a model trained by someone else and use it for their tasks.  Such a scenario is possible if the tasks are very similar. </li><li>  <b>The block selection features</b> .  At this point, we know that the architecture of the model can be divided into two main parts: <i>The feature extraction unit</i> , which is responsible for selecting features from the input data, and the <i>classification module</i> , which classifies examples based on the features obtained.  Usually, the feature extraction unit is the main part of the model.  The idea of ‚Äã‚Äãthe method is to take a block of feature extraction from a model trained in another task, fix its weights (make them obsolete), and then build on its basis new classification modules for the problem in question.  The classification module is usually not very deep and consists of several fully connected layers, so this model is much easier to train. </li><li>  <b>Accurate and deep tuning</b> .  This method is like a script using the feature selection block.  The same actions are performed with the exception of ‚Äúfreezing‚Äù the feature extraction unit.  For example, you can take the <a href="https://arxiv.org/pdf/1409.1556v6.pdf">VGG</a> network as a feature selection block and freeze only the first three (out of four) convolutional blocks in it.  In this case, the feature extraction unit can better adapt to the current task.  For more information, see F. Chollet's blog entry. <a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html">Building powerful image classification models using a very small amount of data</a> . </li></ul><br>  A detailed description of the scenarios for the use of inductive learning can be found in the Stanford University course <a href="http://cs231n.github.io/transfer-learning/">Convolutional neural networks CS231n for visual recognition</a> by Fei-Fei Li and S. Ruder's blog entry (S. Ruder) <a href="http://ruder.io/transfer-learning/index.html">Inductive learning is the next milestone in development machine learning</a> (the subject is considered more comprehensively). <br><br>  You may have questions: why are all these methods needed and why can they work?  We will try to answer them. <br><br><ul><li>  The benefits of using large data sets.  For example, we can take a feature extraction block from a model trained on 14 million images contained in the <a href="http://image-net.org/">ImageNet</a> contest dataset.  These models are complex enough to ensure the <i>selection of very high-quality features</i> from the input data. </li><li>  Considerations related to time.  Training large models can take weeks or even months.  In this case, everyone can <i>save a huge amount of time and computing resources</i> . </li><li>  The weighty assumption underlying why all this can work is as follows: Signs obtained as a result of training in one task may be useful and suitable for another task.  In other words, signs have the property of invariance with respect to a problem.  Please note that the <i>domain of the</i> new task should be similar to the domain of the original task.  Otherwise, the feature extraction unit may even worsen the results obtained. </li></ul><br><h2>  <font color="#0071c5">Advanced Model Architecture</font> </h2><br>  Now we are familiar with the concept of inductive learning.  We also know that ImageNet is a major event, in which almost all modern advanced architecture of convolutional neural networks were tested.  Let's try to take a feature extraction block from one of these networks. <br><br>  Fortunately, the Keras library provides us with <a href="https://keras.io/applications/">several</a> pre-trained (within ImageNet) models that were created within this platform.  We import and use one of these models. <br><br><img src="https://habrastorage.org/webt/jz/3t/ry/jz3trysk11d88hbmbxvlnoungxy.png"><br><br>  In this case, we will use a network with a VGG architecture.  To select only the feature selection block, remove the classification module (three upper fully connected layers) of the network by setting the <i>include_top</i> parameter to <i>False</i> .  We also want to initialize our network with the weights of the network trained within ImageNet.  The last parameter is the size of the input data. <br><br>  Please note that the size of the original images in the ImageNet competition is (224, 224, 3), while our images are sized (400, 500, 3).  However, we use convolutional layers ‚Äî this means that the weights of the network are the weights of the sliding cores in the convolution operation.  Coupled with the parameter sharing property (a discussion of this is found in our theoretical article <a href="https://habr.com/company/intel/blog/415811/">Overview of convolutional neural networks for image classification</a> ) - this leads to the fact that the size of the input data can be almost arbitrary, since the convolution is performed by means of a sliding window, and this window can slide image of arbitrary size.  The only limitation is that the size of the input data must be large enough so that it does not collapse into one point (spatial dimensions) in any intermediate layer, since otherwise it will be impossible to perform further calculations. <br><br>  Another trick we use is <i>caching</i> .  VGG is a very large network.  One straight pass for all images (1630 examples) through the feature extraction block takes approximately 50 seconds.  However, it should be remembered that the weights of the feature extraction block are fixed, and a straight pass always gives the same result for the same image.  We can use this fact to perform a direct pass through the feature extraction block only <i>once</i> and then cache the results in the intermediate array.  To implement this scenario, we first create an instance of the <a href="https://keras.io/preprocessing/image/">ImageDataGenerator</a> class to load files from the hard disk directly (for more information, see the basic article <a href="https://habr.com/company/intel/blog/420635/">Basic Model of Emotion Recognition in Images</a> ). <br><br><img src="https://habrastorage.org/webt/o-/wo/7h/o-wo7hel-_kgurcvwcs2v2smecc.png"><br><br>  In the next step, in the prediction mode, we use the previously created feature extraction block in the model to obtain the image features. <br><br><img src="https://habrastorage.org/webt/gl/no/p7/glnop717aagtytzitgpdtbl_11c.png"><br><br>  It takes about 50 seconds.  Now we can use the results for a very fast learning of the upper classification part of the model - one epoch lasts about 1 second.  Imagine now that each epoch lasts 50 seconds longer.  Thus, this simple caching method allowed us to speed up the process of network training by 50 times!  In this scenario, we save all the signs for all the examples in the RAM, since its volume is enough for this.  When using a larger data set, you can calculate the properties, write them to the hard disk, and then read them using the same approach associated with the generator class. <br><br>  Finally, consider the architecture of the classification part of the model: <br><br><img src="https://habrastorage.org/webt/6x/hq/qb/6xhqqbgjol6dfxyn47rufbuc_ag.png"><br><br><img src="https://habrastorage.org/webt/ss/4v/3t/ss4v3to-rinafik2lthu5ecszt8.png"><br><br>  Recall that a four-dimensional tensor (examples, height, width, and channels) is output at the output of a feature extraction block of a convolutional neural network, and a two-dimensional tensor (examples, features) is taken for classification.  One of the ways to transform a four-dimensional tensor with attributes is simply its alignment around the last three axes (we used a similar technique in the basic model).  In this scenario, we use a different approach, called <i>global average</i> - <i>based subsampling</i> (GAP).  Instead of aligning four-dimensional vectors, we will take an average based on two spatial dimensions.  In fact, we take a feature map and just average all the values ‚Äã‚Äãin it.  The GAP method was first introduced in the excellent work of Min Lin (Min Lin) <a href="https://arxiv.org/pdf/1312.4400.pdf">Network in a network</a> (this book is really worth exploring it, because it covers some important concepts - for example, convolutions of 1 √ó 1 size).  One of the obvious advantages of the GAP approach is a significant reduction in the number of parameters.  When using GAP, we get only 512 signs for each example, while aligning the raw data, the number of signs will be 15 √ó 12 √ó 512 = 92 160. This can lead to serious overhead, since in this case the classification part of the model will have about 50 millions of options!  Other elements of the classification part of the model, such as fully connected layers and layers that implement the exclusion method, are discussed in detail in the article <a href="https://habr.com/company/intel/blog/420635/">Basic Model of Emotion Recognition in Images</a> . <br><br><h2>  <font color="#0071c5">Settings and learning options</font> </h2><br>  After we have prepared the architecture of our model with the help of Keras, it is necessary to configure the entire model for training using the compilation method. <br><br><img src="https://habrastorage.org/webt/dl/x5/by/dlx5byabpmih_ocdviw_8ngvatw.png"><br><br>  In this case, we use settings that are almost the same as those of the base model, with the exception of the choice of optimizer.  To optimize learning, <a href="https://en.wikipedia.org/wiki/Cross_entropy">binary cross entropy</a> will be used as a loss function, and an accuracy metric will additionally be tracked.  We use the <a href="https://arxiv.org/pdf/1412.6980.pdf">Adam</a> method as an optimizer.  Adam is a type of stochastic gradient descent algorithm with momentum and adaptive <i>learning rate</i> (for more information, see the blog post by S. Ruder <a href="http://ruder.io/optimizing-gradient-descent/index.html">Review of gradient descent optimization algorithms</a> ). <br><br>  The learning rate is an optimizer hyperparameter that must be configured to ensure that the model works.  Recall what the formula for ‚Äúvanilla‚Äù gradient descent looks like, which does not contain additional functionality: <br><br><img src="https://habrastorage.org/webt/qy/iu/ny/qyiunyjwn2bjmvzkkd_jg5050ay.png"><br><br>  Œò is the vector of model parameters (in our case, weights of the neural network), ¬£ is the objective function, ‚àá is the gradient operator (calculated using the error back-propagation algorithm), Œ± is the learning rate.  Thus, the gradient of the objective function is the direction of the optimization step in the parameter space, and the learning rate is its size.  When using an unreasonably high learning rate, there is a chance that the optimal point will constantly slip over due to an excessively large step size.  On the other hand, if the learning rate is too low, then the optimization will take too much time and can provide convergence only in low-quality local minima instead of global extremum.  Therefore, in each specific situation it is necessary to seek an appropriate compromise.  Using the default settings for the Adam algorithm is a good starting point to get started. <br><br>  However, in this task, the default Adam settings show poor results.  We need to reduce the initial learning rate to a value of 0.0001.  Otherwise, the training will not be able to provide convergence. <br><br>  Ultimately, we can start learning for 100 epochs and then save the model and the history of learning.  The <i>% time</i> command is an Ipython * magic command that allows you to measure code execution time. <br><br><img src="https://habrastorage.org/webt/tp/qr/h3/tpqrh3zk0u7oxnxg77cbsmigxc8.png"><br><br><h2>  <font color="#0071c5">Evaluation</font> </h2><br><br><img src="https://habrastorage.org/webt/ij/w6/a-/ijw6a-rdyl9fd5rhlsuybmu24vm.png"><br><br>  Let's evaluate the effectiveness of the model during training.  In our case, the verification accuracy is 73 percent (compared to 55 percent when using the base model).  This result is much better than the base model. <br><br>  Let's also look at the distribution of errors using a matrix of inaccuracies.  Errors are distributed almost evenly between the classes with a slight shift toward incorrectly classified negative examples (upper left cell of the inaccuracy matrix).  This can be explained by a <i>small imbalance in the data set</i> towards a positive class. <br><br>  Another metric we track is the receiver performance curve (ROC curve) and the area under this curve (AUC).  For a detailed description of these metrics, see the article <a href="https://habr.com/company/intel/blog/420635/">Basic Model of Emotion Recognition in Images</a> . <br><br><img src="https://habrastorage.org/webt/if/lx/cf/iflxcf-2pxzdcuicg8im4us-9yg.png"><br><br>  The closer the ROC curve is to the upper left of the graph and the larger the area under it (AUC metric), the better the classifier works.  This figure clearly shows that the improved and pre-trained model demonstrates better results than the basic model created from scratch.  The AUC value for a pre-trained model is 0.82, which is a good result. <br><br><img src="https://habrastorage.org/webt/7o/dy/hu/7odyhuvi5j09ajzquknecv_ch2s.png"><br><br><h2>  <font color="#0071c5">Conclusion</font> </h2><br>  In this article, we met with a powerful technique - inductive learning.  We also built a convolutional neural network classifier using a pre-trained feature extraction block based on the VGG architecture.  This classifier has surpassed the basic convolutional model trained from scratch in terms of its performance.  The gain was exactly 18 percent, and the gain in the AUC metric was 0.25, which demonstrates a very significant improvement in the quality of the system. </div><p>Source: <a href="https://habr.com/ru/post/421367/">https://habr.com/ru/post/421367/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../421355/index.html">Go 1.11 is released - WebAssembly and Native modules</a></li>
<li><a href="../421357/index.html">To the question of the impossible. Part 3</a></li>
<li><a href="../421359/index.html">Festival as a game. Taxonomy IT</a></li>
<li><a href="../421361/index.html">AMD has opened the source code for V-EZ, a cross-platform low-level Vulkan API shell</a></li>
<li><a href="../421365/index.html">The evolution of a single startup. Agile from Yaytselova to Chiken Invaders</a></li>
<li><a href="../421369/index.html">What do interns at ABBYY actually do</a></li>
<li><a href="../421371/index.html">Invisible needles: scientists have developed a method of masking nanosensors for optics and biomedicine</a></li>
<li><a href="../421373/index.html">Python made programming available to a wide audience.</a></li>
<li><a href="../421375/index.html">How uncertainty kills commerce</a></li>
<li><a href="../421377/index.html">7 misconceptions of a novice project manager in game dev</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>