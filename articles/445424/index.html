<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Experience in developing the Refund Tool service with an asynchronous API on Kafka</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="What can make such a big company like Lamoda with a streamlined process and dozens of interrelated services significantly change the approach? The mot...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Experience in developing the Refund Tool service with an asynchronous API on Kafka</h1><div class="post__text post__text-html js-mediator-article">  What can make such a big company like Lamoda with a streamlined process and dozens of interrelated services significantly change the approach?  The motivation can be completely different: from the legislative to the inherent desire of all programmers to experiment. <br><br>  But this does not mean that you can not count on additional benefits.  What exactly can be won if you implement the events-driven API on Kafka, will tell Sergey Zaika ( <a href="https://habr.com/ru/users/fewald/" class="user_link">fewald</a> ).  There will also be about stuffed bumps and interesting discoveries - there is no way to experiment without them. <br><br><img src="https://habrastorage.org/webt/kq/o2/ye/kqo2yemmmu-wiqi9vtpcrbolroq.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <em>Disclaimer: This article is based on the materials of the mitap that Sergey spent in November 2018 on HighLoad ++.</em>  <em>Lamoda‚Äôs lively experience of working with Kafka attracted listeners no less than to other reports from the schedule.</em>  <em>It seems to us that this is a great example of what you can and should always find like-minded people, and the organizers of HighLoad ++ will continue to try to create an atmosphere conducive to this.</em> <br><a name="habracut"></a><br><h2>  About the process </h2><br>  Lamoda is a large e-commerce platform that has its own contact center, a delivery service (and many partnerships), a photo studio, a huge warehouse and it all works on its software.  There are dozens of payment methods, b2b-partners who can use part or all of these services and want to know the latest information on their products.  In addition, Lamoda works in three countries besides the Russian Federation and everything is a bit different in its own way.  In total, there are probably more than a hundred ways to configure a new order, which must be processed in its own way.  All this works with the help of dozens of services that sometimes communicate in an unobvious way.  There is also a central system, whose main responsibility is the status of orders.  We call her BOB, I work with her. <br><br><h2>  Refund Tool with events-driven API </h2><br>  The word events-driven is pretty jaded, a little further we will define in more detail what is meant by this.  I will start with the context in which we decided to try out the event-driven API approach in Kafka. <br><br><img src="https://habrastorage.org/webt/8y/0i/kp/8y0ikp8ebxzglwhw1s-dhrubivw.png"><br><br>  In any store, in addition to orders for which buyers pay, there are times when the store is required to return money, because the goods did not fit the customer.  This is a relatively short process: we clarify information, if there is such a need, and transfer money. <br><br>  But the return became more complicated due to the change of legislation, and we had to implement a separate microservice for it. <br><br><img src="https://habrastorage.org/webt/nn/_x/xr/nn_xxr2xlqfpn-kmgc5y4rzj9a8.png"><br><br>  Our motivation: <br><br><ol><li>  <strong>Law FZ-54</strong> - briefly, the law requires you to report to the tax office about every monetary transaction, whether it be a refund or a return, in a rather short SLA in a few minutes.  We, as e-commerce, carry out quite a lot of operations.  Technically, this means a new responsibility (and therefore a new service) and improvements in all the involved systems. </li><li>  <strong>BOB split</strong> is an internal project of the company to rid BOB of a large number of non-core responsibilities and reduce its total complexity. </li></ol><br><img src="https://habrastorage.org/webt/u7/k8/7x/u7k87xo4jzxcjmoeanxdzhy_yag.png"><br><br>  In this diagram, the main Lamoda systems are drawn.  Now most of them are more like a <strong>constellation of 5-10 microservices around a decreasing monolith</strong> .  They are slowly growing, but we are trying to make them smaller, because deploying the fragment selected in the middle is scary - we should not allow it to fall.  We have to reserve all exchanges (arrows) and pledge that any of them may be unavailable. <br><br>  There are also quite a lot of exchanges in BOB: payment systems, delivery, notifications, etc. <br><br>  Technically, BOB is: <br><br><ul><li>  ~ 150k lines of code + ~ 100k lines of tests; </li><li>  php7.2 + Zend 1 &amp; Symfony Components 3; </li><li>  &gt; 100 API &amp; ~ 50 outgoing integrations; </li><li>  4 countries with their business logic. </li></ul><br>  Deploying BOB is expensive and painful, the amount of code and the tasks it solves is such that no one can put it in its head entirely.  In general, there are many reasons to simplify it. <br><br><h2>  Return process </h2><br>  Initially, two systems are involved in the process: BOB and Payment.  Now two more appear: <br><br><ul><li>  Fiscalization Service, which will take on problems with fiscalization and communication with external services. </li><li>  Refund Tool, in which new exchanges are simply taken out so as not to inflate BOB. </li></ul><br>  Now the process looks like this: <br><br><img src="https://habrastorage.org/webt/pe/e6/0q/pee60qj1tdefewqgdaw22bww-ag.png"><br><br><ol><li>  To BOB comes a request for a refund. </li><li>  BOB talks about this refund tool. </li><li>  Refund Tool says Payment: ‚ÄúReturn the money.‚Äù </li><li>  Payment returns money. </li><li>  The Refund Tool and BOB synchronize the statuses with each other, because for the time being, both of them need it.  We are not yet ready to switch completely to the Refund Tool, since BOB has UI, reports for bookkeeping, and in general a lot of data that you can‚Äôt transfer so easily.  We have to sit on two chairs. </li><li>  The request for fiscalization is leaving. </li></ol><br>  As a result, we made a kind of event bus at Kafka - the event-bus, on which everything started.  Hooray, now we have a single point of failure (sarcasm). <br><br><img src="https://habrastorage.org/webt/8k/zg/qc/8kzgqcfju7ybv8imzajjjykpz10.png"><br><br>  The pros and cons are pretty obvious.  We did the bus, so now all the services depend on it.  This simplifies the design, but introduces a single point of failure into the system.  Kafka will fall, the process will rise. <br><br><h2>  What is a events-driven API? </h2><br>  A good answer to this question is in the report of Martin Fowler (GOTO 2017) <a href="https://youtu.be/STKCRSUsyPO">‚ÄúThe Many Meanings of Event-Driven Architecture‚Äù</a> . <br><br>  Briefly, what we have done: <br><br><ol><li>  Wrapped up all asynchronous exchanges via <strong>events storage</strong> .  Instead of reporting the status change to each interested consumer on the network, we write a state change event to the centralized repository, and the consumers interested in the topic read from there everything that appears. </li><li>  The event (event) in this case is the notification ( <strong>notifications</strong> ) that something has changed somewhere.  For example, the order status has changed.  A consumer who is interested in some kind of accompanying status change data and who is not in the notification can find out their status by himself. </li><li>  The maximum option is a full event sourcing, <strong>state transfer</strong> , in which event contains all the information needed for processing: from where and in what status it was transferred, how exactly the data changed, etc. The only question is expediency and the amount of information you can afford to store. </li></ol><br>  As part of the launch of the Refund Tool, we used the third option.  This simplified event handling, since there is no need to extract detailed information, plus eliminated the scenario when each new event generates a surge of refinement get-requests from consumers. <br><br>  The Refund Tool service is <strong>not loaded</strong> , so Kafka is more likely to try the pen than the need.  I don‚Äôt think that if the refund service became a highload project, the business would be happy. <br><br><h4>  Async exchange AS IS </h4><br>  For asynchronous exchanges, the PHP department usually uses RabbitMQ.  We collected the data for the request, put it in a queue, and the consumer of the same service counted it and sent it (or did not send it).  For the API itself, Lamoda actively uses Swagger.  We design API, we describe it in Swagger, we generate client and server code.  We also use slightly advanced JSON RPC 2.0. <br><br>  In some places esb-buses are used, someone lives on activeMQ, but, in general, <strong>RabbitMQ is standard</strong> . <br><br><h4>  Async exchange TO BE </h4><br>  Designing the exchange through the events-bus, an analogy can be traced.  We similarly describe the future exchange of data through the descriptions of the structure of the event.  The yaml format, code generation had to be done by ourselves, the generator according to the specification creates DTO and teaches clients and servers to work with them.  The generation goes into two languages ‚Äã‚Äã- <strong>golang and php</strong> .  This allows you to keep libraries consistent.  The generator is written in golang, for which he received the name gogi. <br><br>  Event-sourcing at Kafka is a typical thing.  There is a solution from the main enterprise version of Kafka Confluent, there is <a href="https://github.com/zalando/nakadi">nakadi</a> , a solution from our ‚Äúbrothers‚Äù in the domain domain Zalando.  Our <strong>motivation to start with vanilla Kafka</strong> is to leave the decision free until we finally decide whether we will use it everywhere, and also leave ourselves room for maneuver and improvements: we want support for our <strong>JSON RPC 2.0</strong> , generators for two languages ‚Äã‚Äãand see what else. <br><br>  It is ironic that even in such a happy case, when there is a similar Zalando business, which made a similar decision, we cannot use it effectively. <br><br>  Architecturally, at the launch, the pattern is as follows: we read directly from Kafka, but we write only through the events-bus.  Kafka has a lot of ready-to-read: brokers, balancers and she is more or less ready for horizontal scaling, I wanted to keep it.  Record, we wanted to wrap through one Gateway aka Events-bus, and here's why. <br><br><h3>  Events-bus </h3><br>  Or a bus event.  This is just a stateless http gateway, which assumes several important roles: <br><br><ul><li>  <strong>Production validation</strong> - we check that events meet our specification. </li><li>  <strong>The master system for events</strong> , that is, it is the main and only system in the company that answers the question, which events with which structures are considered valid.  Validation includes simply data types and enums for tight content specifications. </li><li>  <strong>Hash-function</strong> for sharding - the structure of the Kafka message is key-value, and now, according to the hash, the key calculates where to put it. </li></ul><br><h3>  Why </h3><br>  We work in a large company with a streamlined process.  Why change something?  <strong>This is an experiment</strong> , and we expect to get several benefits. <br><br><h4>  1: n + 1 exchanges (one to many) </h4><br>  With Kafka, it is very easy to connect new consumers to the API. <br><br>  Suppose you have a directory that needs to be kept up to date on several systems at once (and in some new ones).  Previously, we invented the bundle that implemented the set-API, and the master system was informed by the addresses of consumers.  Now the master system sends updates to the topic, and everyone who is interested in reading.  There was a new system - they signed it on the topic.  Yes, also bundle, but simpler. <br><br>  In the case of the refund-tool, which is the essence of a BOB piece, it is convenient for us to keep them synchronized through Kafka.  Payment says that the money was returned: BOB, RT found out about it, changed their status, the Fiscalization Service found out about it and knocked out a check. <br><br><img src="https://habrastorage.org/webt/x1/rs/gx/x1rsgx9mbceqzstdmcsg_hapnbs.png"><br><br>  We have plans to make a single Notifications Service, which would notify the client about the news on his order / returns.  Now this responsibility is spread between systems.  It will be enough for us to teach Notifications Service to retrieve relevant information from Kafka and react to it (and disable these notifications in other systems).  No new direct exchanges are required. <br><br><h4>  Data driven </h4><br>  The information between the systems becomes transparent - no matter how ‚Äúbloody enterprise‚Äù you have and no matter how puffy your backlog is.  Lamoda has a Data Analytics department that collects data on systems and brings them into a reusable form, both for business and for intelligent systems.  Kafka allows you to quickly give them a lot of data and keep this information stream current. <br><br><h4>  Replication log </h4><br>  Messages do not disappear after reading, as in RabbitMQ.  When an event contains enough information for processing, we have a history of recent changes to the object, and, if desired, the ability to apply these changes. <br><br>  The shelf life of replication log depends on the intensity of writing to this topic, Kafka allows you to flexibly adjust the limits for storage time and for data volume.  For intensive topics, it is important that all consumers have time to read information before it disappears, even in the case of short-term inoperability.  Usually it turns out to store data for <strong>units of days</strong> , which is quite enough for support. <br><br><img src="https://habrastorage.org/webt/xz/6-/59/xz6-59a1z7vrszrmoowvswxauqc.png"><br><br>  Further a little retelling of the documentation, for those who are not familiar with Kafka (the picture is also from the documentation) <br><br>  There are queues in AMQP: we write messages to the queue for the concierge.  As a rule, one queue is processed by one system with the same business logic.  If you need to notify several systems, you can teach the application to write in several queues or configure the exchange with the fanout mechanism, which itself clones them. <br><br>  In Kafka there is a similar abstraction of the <em>topic</em> , in which you write messages, but they do not disappear after reading.  By default, when you connect to Kafka, you get all the messages, and at the same time it is possible to save the place where you stopped.  That is, you read sequentially, you may not mark the message as read, but save the id from which you continue reading.  The Id at which you stopped is called offset (offset), and the mechanism - commit offset. <br><br>  Accordingly, it is possible to implement different logic.  For example, we have BOB in 4 instances for different countries - Lamoda exists in Russia, Kazakhstan, Ukraine, Belarus.  Since they are deployed separately, they have their own configs and their own business logic.  We indicate in the message to which country it belongs.  Each BOB consumer in each country reads with a different groupId, and, if the message does not apply to it, skip it, i.e.  Immediately commit offset +1.  If the same topic reads our Payment Service, then it does so with a separate group, and therefore the offset does not overlap. <br><br>  <b>Event requirements:</b> <br><br><ul><li>  <strong>Completeness of data.</strong>  I wish there was enough data in the event so that it could be processed. </li></ul><br><ul><li>  <strong>Integrity.</strong>  We delegate Events-bus to check that the event is consistent and can handle it. </li><li>  <strong>The order is important.</strong>  In the case of a return, we are forced to work with history.  With notifications, the order is not important, if it is homogeneous notifications, the email will be the same no matter which order arrived first.  In the case of a return there is a clear process, if you change the order, then exceptions will occur, the refund will not be created or will not be processed - we will fall into another status. </li><li>  <strong>Consistency</strong>  We have a repository, and now instead of the API we create events.  We need a way to quickly and cheaply transfer to our services information about new events and about changes in existing ones.  This is achieved by using a common specification in a separate git repository and code generator.  Therefore, clients and servers in different services are agreed with us. </li></ul><br><h2>  Kafka in Lamoda </h2><br>  We have three Kafka installations: <br><br><ol><li>  Logs; </li><li>  R &amp; D; </li><li>  Events-bus. </li></ol><br>  Today we are talking only about the last paragraph.  In the events-bus we have not very large installations - 3 brokers (servers) and only 27 topics.  As a rule, one topic is one process.  But this is a delicate moment, and now we touch it. <br><br><img src="https://habrastorage.org/webt/1v/5s/nj/1v5snjoqmrg2sb1grok5snkzowu.png"><br><br>  The above rps chart.  The process of refunds is marked with a turquoise line (yes, the one that lies on the X axis), and the pink line indicates the process of updating the content. <br><br>  The Lamoda catalog contains millions of products, and the data is updated all the time.  Some collections go out of fashion, new ones are released instead, new models constantly appear in the catalog.  We try to predict what will be interesting to our customers tomorrow, so we constantly buy new things, photograph them and update the shop window. <br><br>  Pink peaks are product update, that is, changes by product.  It is evident that the guys took pictures, took pictures, and then again!  - downloaded a pack of events. <br><br><h2>  Lamoda Events use cases </h2><br>  We use the built architecture for such operations: <br><br><ul><li>  <strong>Return status tracking</strong> : call-to-action and status tracking from all involved systems.  Payment, statuses, fiscalization, notifications.  Here we tried the approach, made the tools, collected all the bugs, wrote the documentation and told colleagues how to use it. </li><li>  <strong>Update product cards:</strong> configuration, meta-data, characteristics.  Reads one system (which displays), and write a few. </li><li>  <strong>Email, push and sms</strong> : order is assembled, order is reached, return is accepted, etc., there are a lot of them. </li><li>  <strong>Stock, warehouse update</strong> - quantitative update of items, just numbers: delivery to the warehouse, return.  It is necessary that all systems associated with the reservation of goods, operated with the most relevant data.  Now the system of updating the drain is quite complicated, Kafka will allow to simplify it. </li><li>  <strong>Data Analysis</strong> (R &amp; D department), ML-tools, analytics, statistics.  We want the information to be transparent - for this Kafka is well suited. </li></ul><br>  Now the more interesting part about stuffed bumps and interesting discoveries that occurred in six months. <br><br><h2>  Design problems </h2><br>  Suppose we want to make a new thing - for example, transfer the entire delivery process to Kafka.  Now part of the process is implemented in Order Processing at BOB.  After the transfer of the order to the delivery service, moving to the intermediate warehouse and other things, there is a status model.  There is a whole monolith, even two, plus a bunch of APIs dedicated to delivery.  They know much more about delivery. <br><br>  These seem to be similar areas, but for Order Processing at BOB and for the delivery system, the statuses are different.  For example, some courier services do not send intermediate statuses, but only final ones: ‚Äúdelivered‚Äù or ‚Äúlost‚Äù.  Others, on the contrary, report in great detail on the movement of goods.  Everyone has their own validation rules: for someone, email is valid, so it will be processed;  for others, it is not valid, but the order will still be processed, because there is a telephone for communication, and someone will say that such an order will not be processed at all. <br><br><h3>  Data stream </h3><br>  In the case of Kafka, the question of organizing the data flow arises.  This task is connected with the choice of strategy on several points, we will go over all of them. <br><br><h4>  In one topic or in different? </h4><br>  We have an event specification.  In BOB we write that such an order must be delivered, and we indicate: the order number, its composition, some SKU and bar codes, etc.  When the goods arrive at the warehouse, the delivery will be able to get the status, timestamps and everything you need.  But then we want to receive updates on this data in BOB.  We have a reverse process of receiving data from the delivery.  Is this the same event?  Or is it a separate exchange that deserves a separate topic? <br><br>  Most likely, they will be very similar, and the temptation to make one topic is not unreasonable, because a separate topic is separate concumers, separate configs, a separate generation of all this.  But not a fact. <br><br><h4>  New field or new event? </h4><br>  But if you use the same events, another problem arises.  For example, not all delivery systems can generate a DTO that can generate BOBs.  We send them an id, but they don‚Äôt save them, because they don‚Äôt need them, and from the point of view of starting the event-bus process, this field is mandatory. <br><br>  If we introduce a rule for event-bus that this field is mandatory, then we are forced to set additional validation rules in the BOB or in the handler of the start event.  Validation begins to creep away on service - it is not very convenient. <br><br>  Another problem is the temptation of incremental development.  We are told that we need to add something to the event, and maybe, if you think well, it should have been a separate event.  But in our scheme, a separate event is a separate topic.  A separate topic is the whole process that I described above.  The developer is tempted to simply add another field to the JSON schema and regenerate it. <br><br>  In case of refunds, we came to the event of the event in six months.  We had one meta event called refund update, in which there was a type field describing what this update itself actually is.  From this, we had ‚Äúbeautiful‚Äù switches with validators, who said how to validate this event with this type. <br><br><h4>  Event versioning </h4><br>  <a href="https://docs.confluent.io/current/schema-registry/docs/index.html">Avro</a> can be used to validate messages in Kafka, but it was necessary to immediately lay it out and use Confluent.  In our case, with versioning you have to be careful.  It will not always be possible to re-read messages from replication log, because the model has "left."  Basically, it turns out to build versions so that the model is backward compatible: for example, to make the field temporarily optional.  If the differences are too strong, we start writing in a new topic, and the clients are transplanted when they finish reading the old one. <br><br><h4>  Guaranteed reading order partitions </h4><br>  Topics inside Kafka are divided into partitions.  This is not very important while we design entities and exchanges, but it is important when we decide how to conclude and scale this. <br><br>  In the usual case, you write one topic in Kafka.  By default, one partition is used, and all messages from this topic fall into it.  And the consumer accordingly sequentially reads these messages.  Suppose now that you need to expand the system so that the messages read two different consumer accounts.  If you, for example, send SMS, then you can tell Kafka to make an additional partition, and Kafka will begin to decompose the messages into two parts - half there, half here. <br><br>  How does Kafka divide them?  Each message has a body (in which we store JSON) and there is a key.  To this key, you can attach a hash function that will determine which partition will get the message. <br><br>  In our case with refunds, this is important if we take two partitions, then there is a chance that a parallel account will process the second event before the first and there will be trouble.  The hash function ensures that messages with the same key fall into the same partition. <br><br><h4>  Events vs commands </h4><br>  This is another problem we are facing.  An event is a kind of event: we say that something happened somewhere (something_happened), for example, item canceled or there was a refund.  If someone listens to these events, then by ‚Äúitem canceled‚Äù the essence of the refund will be created, and ‚Äúa refund has occurred‚Äù will be written somewhere in the setup. <br><br>  But usually, when you design events, you do not want to write them in vain - you are laying on the fact that someone will read them.  It is tempting to write not something_happened (item_canceled, refund_refunded), but something_should_be_done.  For example, item is ready to be returned. <br><br>  On the one hand, it tells you how the event will be used.  On the other hand, it is much less like a normal event name.  In addition, it is not far from here to the do_something command.  But you have no guarantee that someone has read this event;  and if read, then read successfully;  and if I read successfully, I did something, and this something went well.  The moment the event becomes do_something, feedback becomes necessary, and this is a problem. <br><br><img src="https://habrastorage.org/webt/gy/xo/vm/gyxovmgvxv3wbwkv_k7mzluobls.png"><br><br>  In the asynchronous exchange in RabbitMQ, when you read the message, went to http, you have a response ‚Äî at least that the message was received.  When you recorded in Kafka, there is a message that you recorded in Kafka, but you know nothing about how it was processed. <br><br>  Therefore, in our case, we had to enter a response event and set up monitoring on the fact that if so many events took off, after a certain time, the same number of response events should arrive.  If this did not happen, then it seems that something went wrong.  For example, if we sent the event ‚Äúitem_ready_to_refund‚Äù, we expect that the refund will be created, the client will get the money back, the event ‚Äúmoney_refunded‚Äù will fly to us.  But this is not accurate, so monitoring is needed. <br><br><h3>  Nuances </h3><br>  There is a fairly obvious problem: if you read from a topic consistently, and you have a bad message, the customer falls and you will not go on.  You need to <strong>stop all the</strong> consummers, commit the offset further to continue reading. <br><br>  We knew about it, it was laid on it, and still it happened.  This happened because the event was valid from the point of view of events-bus, the event was valid from the point of view of the validator of the application, but it was not valid from the point of view of PostgreSQL, because we have UNSIGNED INT in one MySQL system, and The system was PostgreSQL just with int.  His size is a little smaller, and Id did not fit.  Symfony died with an exception.  We, of course, caught the exception, because we laid on it, and were going to commit this offset, but before that we wanted to increment the problem counter, since the message was processed unsuccessfully.  The counters in this project are also in the database, and Symfony has already closed communication with the database, and the second exception killed the whole process without a chance to commit the offset. <br><br>  For a while the service had lain - fortunately, with Kafka it is not so bad, because the messages remain.  When work is restored they can be read.  It's comfortable. <br><br>  Kafka has an opportunity through tooling to set an arbitrary offset.  But in order to do this, you need to stop all consumer accountants - in our case, prepare a separate release, which will not have consumer accounts, redeployments.  Then, with Kafka, tooling can offset the offset, and the message will pass. <br><br>  Another nuance - <strong>replication log vs rdkafka.so</strong> - is related to the specifics of our project.  We have PHP, and in PHP, as a rule, all the libraries communicate with Kafka via the rdkafka.so repository, and then some kind of wrapper follows.  Perhaps these are our personal difficulties, but it turned out that simply re-reading a piece of what has already been read is not so easy.  In general, there were software problems. <br><br>  Coming back to the specifics of working with partitions, <strong>consumers are</strong> right in the documentation for the <strong>&gt;&gt; topic partitions</strong> .  But I learned about it much later than I would like.  If you want to scale and have two consumers, you need at least two partitions.  That is, if you had one partition in which 20 thousand messages accumulated, and you made a fresh one, the number of messages will not even out evenly.  Therefore, to have two parallel concumer, it is necessary to deal with partitions. <br><br><h2>  Monitoring </h2><br>  I think, by the way we monitor, it will be even clearer what problems there are in the existing approach. <br><br>  For example, we consider how many goods in the database have recently changed their status, and, accordingly, events should have happened due to these changes, and send this number to our monitoring system.   Kafka    ,       . ,         . <br><br><img src="https://habrastorage.org/webt/il/on/va/ilonvaqqf3tkfcv9reuyezxpgoq.png"><br><br>  ,  ,    ,   events-bus ,     . ,     Refund Tool  ,   BOB  -  ( ). <br><br><img src="https://habrastorage.org/webt/i4/9b/jf/i49bjfsr_lrypwz0_1qfkhhmcjq.png"><br><br>    consumer-group lag.  ,    .       ,     0,      . Kafka    ,     . <br><br>   <a href="https://github.com/linkedin/Burrow">Burrow</a> ,       Kafka.    API  consumer-group  ,     .    Failed   warning,    ,         ‚Äî    ,  .   ,   . <br><br><img src="https://habrastorage.org/webt/4j/ht/go/4jhtgoqjn_0kdgvagxkg3flceng.png"><br><br>     API.   bob-live-fifa, partition refund.update.v1,  , lag 0 ‚Äî   offset -. <br><br><img src="https://habrastorage.org/webt/mh/gn/aq/mhgnaq2qcxamaejf1fkdkkkqnwu.png"><br><br>  <strong>updated_at SLA (stuck)</strong>   . ,    ,     .   Cron,  ,    5       refund (       ),  -    ,      .    Cron,    ,     0,   . <br><br> <b> ,   , </b> : <br><br><ul><li>    ; </li><li>    ; </li><li>     . </li></ul><blockquote>  ,      ‚Äî  API  Kafka,          . <br> -,  <a href="https://www.highload.ru/">HighLoad++</a>     ,       ,         . <br> -,               <a href="https://knowledgeconf.ru/2019">KnowledgeConf</a> .  ,  26 ,      . <br>      <a href="https://phprussia.ru/2019">PHP Russia</a>  <a href="https://ritfest.ru/2019">++</a> ( DevOpsConf  ) ‚Äî      ,          . </blockquote></div><p>Source: <a href="https://habr.com/ru/post/445424/">https://habr.com/ru/post/445424/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445414/index.html">How Soviet scientific books became an artifact from physicists and engineers in India</a></li>
<li><a href="../445416/index.html">Why vinyl records survive the digital age</a></li>
<li><a href="../445418/index.html">A reasonable person? No longer</a></li>
<li><a href="../445420/index.html">There are 17 billion computers in your brain</a></li>
<li><a href="../445422/index.html">What programming languages ‚Äã‚Äãare least secure?</a></li>
<li><a href="../445428/index.html">High-quality Wi-Fi is the basis of modern hospitality and the engine of business.</a></li>
<li><a href="../445432/index.html">Unity Package Manager</a></li>
<li><a href="../445434/index.html">The best worst job in the world: we are looking for a habra</a></li>
<li><a href="../445436/index.html">Retraining in DevOps - what to prepare for yourself</a></li>
<li><a href="../445438/index.html">Introduction to lit-element and web components based on it</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>