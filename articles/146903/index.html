<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Yet another classifier</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Instead of intro 
 Laziness is the engine of progress. You do not want to grind grain yourself - make a mill, you do not want to throw stones at enemi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Yet another classifier</h1><div class="post__text post__text-html js-mediator-article"><h4>  Instead of intro </h4><br>  Laziness is the engine of progress.  You do not want to grind grain yourself - make a mill, you do not want to throw stones at enemies - build a catapult, get tired of burning inquisition on bonfires and bend your back under the feudal lord - muddle with the guys renaissance ... however, what is it about me. <br>  Automation, gentlemen.  You take some useful process in which a person participates, you replace a person with a complex mechanism, you get a profit.  Relatively recently, it has also become fashionable to replace a person with a piece of code.  Oh, how many noble professions can fall under the onslaught of informatization.  Especially when you consider that a piece of code in our time is capable not only of predefined behavior, but also of ‚Äúlearning‚Äù some behavior. <br><a name="habracut"></a><br><h4>  Generalize a little </h4><br>  Generally speaking, such superpowers for the algorithm, which embodies our piece of code, are called not just training, but machine learning (machine learning).  Usually we mean that only something endowed with intelligence is capable of learning.  And, not surprisingly, the discipline of machine learning is included in a much more general discipline of artificial intelligence (artificial intelligence aka AI) <br><img src="https://dl.dropbox.com/u/20359725/AItree.png" alt="image"><br>  As is clear from the picture, many AI tasks are based on machine learning algorithms.  It remains only to choose something fun and not less fun to solve. <br><br><h4>  And fantasize </h4><br>  Suppose you,% username%, were hired by a machine learning specialist for a company that owns the <a href="http://www.imdb.com/">IMDb</a> site.  And they say they say, now we want aki from Russian Ivanov at their Kinopoisk, user reviews are marked with a color green or red depending on whether a positive or negative review.  And here you seem to understand that it is necessary to take an indecent size base with reviews and issue a algorithm that will be well trained and will divide all of their sets into two non-intersecting subsets (classes), so that the probability of error is minimal. <br>  Like a person, our algorithm can learn either from its mistakes (learning without a teacher, unsupervised learning), or using someone else's example (learning with a teacher, supervised learning).  But, in this case, he himself cannot distinguish between positive and negative, since  he was never called to parties and he had little contact with other people.  Therefore, it is necessary to obtain a sample of reviews already marked up on classes (training sample) in order to carry out training on ready-made examples (precedents). <br>  So, we want to solve the problem of the analysis of opinions (sentiment analysis) related to the tasks of processing natural language (natural language processing), having trained a certain classifier, which for each new review will tell us whether it is positive or negative.  Now you need to choose a classifier and code the whole thing. <br><br><h4>  God how naive </h4><br>  In general, a huge number of different classifiers are found in the wild.  The simplest thing we could think of here is to attribute all the articles, for example, to positive ones.  Result: no need to learn, classifier accuracy 50%, interest 0%. <br>  But if you think about it ... How do we choose the class to which we will place the next object?  We choose it so that the probability of the object belonging to this class is maximum (captain!), I.e. <br><img src="http://tp.mail.ru/uploads/images/00/00/20/2012/05/06/f01da6.png" alt="image"><br>  where C is a class and d is an object, in our case it is a review.  Thus, we are trying to maximize the a posteriori probability of a review belonging to a positive or negative class. <br>  Having seen the conditional probability, it immediately suggests itself to apply the Bayes theorem to it, and since we are still looking for the maximum, it still suggests that we should reject, as unnecessary, the denominator, having received as a result: <br><img src="http://tp.mail.ru/uploads/images/00/00/20/2012/05/06/172417.png" alt="image"><br>  All this is cool, you say, but where do you get these miracle probabilities in real life?  Well, with P everything is simple: if you have a training sample that adequately reflects the distribution of classes, you can evaluate: <br><img src="http://tp.mail.ru/uploads/images/00/00/20/2012/05/06/31dc89.png" alt="image"><br>  where n is the number of objects belonging to the class C, N is the sample size. <br>  With P (d | C) everything is a bit more complicated.  Here we must make a ‚Äúnaive‚Äù assumption about the independence of individual words in a document from each other: <br><img src="https://habrastorage.org/getpro/habr/post_images/d80/75d/b3d/d8075db3d22ec73bf46ecffc0775dd59.png" alt="image"><br>  and understand that the recovery of n one-dimensional densities is a much simpler task than one n-dimensional one. <br>  In this case, the training sample can be estimated: <br><img src="http://tp.mail.ru/uploads/images/00/00/20/2012/05/06/a91598.png" alt="image"><br>  where m is the number of words w in the class C, M is the number of words (or more precisely, tokens) in the class C. So we got: <br><img src="https://habrastorage.org/getpro/habr/post_images/229/6b5/91f/2296b591f774128aefb2189162e9b297.png" alt="image"><br>  such a classifier is called the naive Bayes classifier. <br>  A few words about the validity of the naive assumption of independence.  Such an assumption is rarely true, in this case, for example, it is clearly incorrect.  But, as practice shows, naive Bayes classifiers show surprisingly good results in many problems, including those where the assumption of independence, strictly speaking, is not fulfilled. <br>  The advantages of the naive Bayes classifier are insensitivity to the size of the training sample, high learning speed and resistance to the so-called retraining (overfitting is a phenomenon in which the algorithm works very well on a training sample and is bad on a test one).  Therefore, they are used either when the initial data is very small, or when there is a lot of such data, and the speed of learning comes first.  Disadvantages: not the best accuracy, especially if the assumption of independence is violated. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Developers, developers, developers </h4><br>  And now for those who are desperate to see the <a href="https://bitbucket.org/Fitzgerald/tp.mail.ru/src/9df783d6f8f3/NaiveBayes.py">code</a> .  The IMDb review collection, prepared by Cornell University (whose employees themselves conducted an <a href="http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf">interesting study</a> with this data), was taken as a <a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/">training sample</a> . <br>  The most interesting methods are add_example and classify.  The first, respectively, is engaged in training, restoring the necessary probability density, the second - applies the resulting model. <br><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">add_example</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, klass, words)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" * Builds a model on an example document with label klass ('pos' or 'neg') and * words, a list of words in document. """</span></span> self.docs_counter_for_klass[klass] += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: self.token_counter_for_klass[klass] += <span class="hljs-number"><span class="hljs-number">1</span></span> self.word_in_klass_counter_for[klass][word] += <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> <br>  Education is very straightforward - we get the words from the review and its labeled class.  We count the number of documents in the class n, the number of words in the class M, and the number of words w in the class are m.  All further classify. <br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">classify</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, words)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" 'words' is a list of words to classify. Return 'pos' or 'neg' classification. """</span></span> score_for_klass = collections.defaultdict(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span>: <span class="hljs-number"><span class="hljs-number">0</span></span>) num_of_docs = sum(self.docs_counter_for_klass.values()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> klass <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.klasses: klass_prior = float(self.docs_counter_for_klass[klass]) / num_of_docs score_for_klass[klass] += math.log(klass_prior) klass_vocabulary_size = len(self.word_in_klass_counter_for[klass].keys()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> words: score_for_klass[klass] += math.log(self.word_in_klass_counter_for[klass][word] + <span class="hljs-number"><span class="hljs-number">1</span></span>) score_for_klass[klass] -= math.log(self.token_counter_for_klass[klass] + klass_vocabulary_size) sentiment = max(self.klasses, key=<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> klass: score_for_klass[klass]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> sentiment</code> </pre><br>  Because  the probabilities can be very small numbers, and we are still engaged in maximization, we can, without a twinge of conscience, predict everything: <br><img src="https://habrastorage.org/getpro/habr/post_images/35c/2d9/1d4/35c2d91d48baabecc258068152e3daea.png" alt="image"><br>  - this explains what is happening.  The resulting classes, respectively, is the one that received the maximum score. <br><br><pre> <code class="python hljs"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">cross_validation_splits</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, train_dir)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""Returns a lsit of TrainSplits corresponding to the cross validation splits."""</span></span> <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'[INFO]\tPerforming %d-fold cross-validation on data set:\t%s'</span></span> % (self.num_folds, train_dir) splits = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> fold <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>, self.num_folds): split = TrainSplit() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> klass <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> self.klasses: train_file_names = os.listdir(<span class="hljs-string"><span class="hljs-string">'%s/%s/'</span></span> % (train_dir, klass)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> file_name <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> train_file_names: example = Example() example.words = self.read_file(<span class="hljs-string"><span class="hljs-string">'%s/%s/%s'</span></span> % (train_dir, klass, file_name)) example.klass = klass <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> file_name[<span class="hljs-number"><span class="hljs-number">2</span></span>] == str(fold): split.test.append(example) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: split.train.append(example) splits.append(split) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> splits</code> </pre><br>  For the most complete use of available data, cross-validation was applied (k-fold cross-validation): the available data is divided into k parts, then the model is trained in k ‚àí 1 data parts, and the remaining data is used for testing.  The procedure is repeated k times, as a result, each of the k parts of the data is used for testing.  The resulting efficiency of the classifier is simply calculated as the average. <br><br>  Well, in order to fully achieve Zen, I attach a picture depicting the learning process with the teacher (in general): <br><div style="text-align:center;"><img src="https://dl.dropbox.com/u/20359725/SupLer.png" alt="image"></div><br><br>  What is the result of all this? <br><br>  $ python NaiveBayes.py ../data/imdb1 <br>  [INFO] Performing 10-fold cross-validation on data set: ../data/imdb1 <br>  [INFO] Fold 0 Accuracy: 0.765000 <br>  [INFO] Fold 1 Accuracy: 0.825000 <br>  [INFO] Fold 2 Accuracy: 0.810000 <br>  [INFO] Fold 3 Accuracy: 0.800000 <br>  [INFO] Fold 4 Accuracy: 0.815000 <br>  [INFO] Fold 5 Accuracy: 0.810000 <br>  [INFO] Fold 6 Accuracy: 0.825000 <br>  [INFO] Fold 7 Accuracy: 0.825000 <br>  [INFO] Fold 8 Accuracy: 0.765000 <br>  [INFO] Fold 9 Accuracy: 0.820000 <br>  [INFO] Accuracy: 0.806000 <br><br>  With filtering stop words: <br>  $ python NaiveBayes.py -f ../data/imdb1 <br>  [INFO] Performing 10-fold cross-validation on data set: ../data/imdb1 <br>  [INFO] Fold 0 Accuracy: 0.770000 <br>  [INFO] Fold 1 Accuracy: 0.815000 <br>  [INFO] Fold 2 Accuracy: 0.810000 <br>  [INFO] Fold 3 Accuracy: 0.825000 <br>  [INFO] Fold 4 Accuracy: 0.810000 <br>  [INFO] Fold 5 Accuracy: 0.800000 <br>  [INFO] Fold 6 Accuracy: 0.815000 <br>  [INFO] Fold 7 Accuracy: 0.830000 <br>  [INFO] Fold 8 Accuracy: 0.760000 <br>  [INFO] Fold 9 Accuracy: 0.815000 <br>  [INFO] Accuracy: 0.805000 <br>  I leave the reader to think about why filtering stop words has affected the result so much (well, that is why it almost didn‚Äôt have any effect at all). <br><br><h4>  Examples </h4><br>  If we move away from stingy numbers, what did we really get? <br>  For seed: <br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">"Yes, it IS awful! The books are good, but this movie is a travesty of epic proportions! 4 people in front of me stood up and walked out of the movie. It was that bad. Yes, 50 minutes of wedding...very schlocky accompanied by sappy music. I actually timed the movie with a stop watch I was so bored. There is about 20 minutes of action and the rest is Bella pregnant and dying. The end. No, seriously. One of the worst movies I have ever see"</span></span></code> </pre><br>  Such a review is clearly recognized as negative, which is not surprising.  It is more interesting to look at errors. <br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">" `Strange Days' chronicles the last two days of 1999 in los angeles. as the locals gear up for the new millenium, lenny nero (ralph fiennes) goes about his business of peddling erotic memory clips. He pines for his ex-girlfriend, faith (juliette lewis ), not noticing that another friend, mace (angela bassett) really cares for him. This film features good performances, impressive film-making technique and breath-taking crowd scenes. Director kathryn bigelow knows her stuff and does not hesitate to use it. But as a whole, this is an unsatisfying movie. The problem is that the writers, james cameron and jay cocks ,were too ambitious, aiming for a film with social relevance, thrills, and drama. Not that ambitious film-making should be discouraged; just that when it fails to achieve its goals, it fails badly and obviously. The film just ends up preachy, unexciting and uninvolving."</span></span></code> </pre><br>  This review has been marked as positive, but has been recognized as negative.  Here you can ask a question about the correctness of the original mark, because even a person, in my opinion, is difficult to determine whether this review is positive or negative.  It is not surprising, at least, why the classifier referred it to the negative: unsatisfying, fails, badly, unexciting, uninvolving, are quite strong indicators of the negative, outweighing the rest. <br>  And, of course, the classifier does not understand sarcasm and in this case easily takes the negative for the positive: <br><pre> <code class="python hljs"><span class="hljs-string"><span class="hljs-string">"This is what you call a GREAT movie? AWESOME acting you say?Oh yeah! Robert is just so great actor that my eyes bleeding when I'm see him. And the script is so good that my head is exploding. And the ending..ohhh, you gonna love it! Everything is so perfect and nice - I dont got words to describe it!"</span></span></code> </pre><br><h4>  Conclusion </h4><br>  Classifiers, including naive Bayes, are used in many areas besides analyzing opinions: spam filtering, speech recognition, credit scoring, biometric identification, etc. <br>  In addition to the naive Bayesian, there are many other classifiers: perceptrons, decision trees, classifiers based on the support vector method (SVM), nuclear classifiers (kernel classifiers), etc.  But besides learning with a teacher, there is also learning without a teacher, reinforcement learning (reinforcement learning), learning to rank, and much more ... but that is another story <br><br><hr><br>  This article was written as part of the Technopark @ Mail.Ru student article competition. </div><p>Source: <a href="https://habr.com/ru/post/146903/">https://habr.com/ru/post/146903/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../146897/index.html">The illusion of reality, a note on human engineering</a></li>
<li><a href="../146898/index.html">Artificial Intelligence. Forecast 2029</a></li>
<li><a href="../146899/index.html">Book purchases can be tracked on Google Maps</a></li>
<li><a href="../146900/index.html">About emotions, programs and artificial intelligence</a></li>
<li><a href="../146901/index.html">Programmers misconceptions about names</a></li>
<li><a href="../146905/index.html">#Boycottapple entered the top three most popular trends on Google+</a></li>
<li><a href="../146910/index.html">Ruby NoName Podcast S04E12</a></li>
<li><a href="../146911/index.html">Internet and IPTV in the cottage: a difficult but successful experience</a></li>
<li><a href="../146912/index.html">We are looking for along with Google</a></li>
<li><a href="../146914/index.html">Droider Show # 47. Google I / O and patent lawlessness!</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>