<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Kubernetes Cluster for $ 20 per month</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tl; DR 


 Raising a cluster for servicing web applications without stateless web applications along with ingress , letsencrypt , without using automa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Kubernetes Cluster for $ 20 per month</h1><div class="post__text post__text-html js-mediator-article"><h1 id="tl-dr">  Tl;  DR </h1><br><p>  Raising a cluster for servicing web applications without <a href="https://whatis.techtarget.com/definition/stateless-app">stateless</a> web applications along with <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">ingress</a> , <a href="https://letsencrypt.org/">letsencrypt</a> , without using automation tools like kubespray, kubeadm, and any others. <br>  Time for reading: ~ 45-60 minutes, for reproducing actions: from 3 hours. </p><br><h1 id="preambula">  Preamble </h1><br><p>  Writing an article led me to the need for my own cluster of kubernetes for experimentation.  The automatic installation and configuration solutions that are publicly available did not work in my case, since I used non-mainstream Linux distributions.  Dense work with kubernetes in IPONWEB stimulates to have such a platform, solving their problems in a comfortable way, including for home projects. </p><br><h1 id="komponenty">  Components </h1><br><p>  The article will feature the following components: </p><br><p>  - <em>Your favorite</em> Linux - I used Gentoo (node-1: systemd / node-2: openrc), Ubuntu 04/18/1. <br>  - <a href="https://kubernetes.io/docs/setup/release/notes/">Kubernetes Server</a> - kube-apiserver, kube-controller-manager, kube-scheduler, kubelet, kube-proxy. <br>  - <a href="https://containerd.io/downloads/">Containerd</a> + <a href="">CNI Plugins (0.7.4)</a> - to organize containerization, we take containerd + CNI instead of docker (although initially the entire configuration was raised to the docker, so nothing prevents you from using it if necessary). <br>  - <a href="https://coredns.io/">CoreDNS</a> - for organizing service discovery of components operating within the kubernetes cluster.  The recommended version is not lower than 1.2.5, since from this version appears implicit support for the work of coredns as a process running outside the cluster. <br>  - <a href="https://github.com/coreos/flannel">Flannel</a> - for the organization of a network stack, communication podov and containers among themselves. <br>  - <em>Your favorite</em> db. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/423/a71/5fc/423a715fc68fed1101c86d3335b0a8a8.jpg" alt="For all"></p><a name="habracut"></a><br><h1 id="ogranicheniya-i-dopuscheniya">  Limitations and assumptions </h1><br><ul><li>  The article does not consider the cost of solutions vps / vds, presented on the market, as well as the possibility of deploying machines on these services.  It is assumed that you already have something expanded, or you are able to do it yourself.  Also not covered the process of installing / configuring your favorite database and private docker repository, if you need one. </li><li>  We can use both containerd + cni plugins and docker.  The article does not consider the use of docker as a means of containerization.  If you want to use docker, you will be able to configure the <a href="">flannel accordingly</a> , in addition, you will need to configure kubelet, namely, remove all options related to containerd.  As shown by my experiments, docker and containerd on different nodes as means for containerization will work correctly. </li><li> We cannot use <code>host-gw</code> backend for flannel, read more in the <a href="https://habr.com/ru/company/iponweb/blog/435228/">Flannel Configuration</a> section. </li><li>  We will not use anything for monitoring, backups, saving user files (state), storing configuration files and application code (git / hg / svn / etc) </li></ul><br><h1 id="vvedenie">  Introduction </h1><br><p>  In the course of the work, I used a large number of sources, but I want to mention separately the rather detailed guide <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way/">Kubernetes the hard way</a> , which covers about 90% of the questions of the basic configuration of my own cluster.  If you have already read this manual, feel free to skip straight to the <a href="https://habr.com/ru/company/iponweb/blog/435228/">Flannel Configuration</a> section. </p><br><div class="spoiler">  <b class="spoiler_title">Legend</b> <div class="spoiler_text"><h2 id="spisok-terminov--glossariy">  List of Terms / Glossary </h2><br><ul><li>  The api-server is a physical or virtual machine, on which a set of applications is located for running and correctly functioning kubernetes kube-apiserver.  In this article, it is etcd, kube-apiserver, kube-controller-manager, kube-scheduler. </li><li>  master - dedicated workstation or VPS installation, synonymous with api-server. </li><li>  node-X is a dedicated workstation or VPS installation, <code>X</code> indicates the ordinal number of the station.  In this article, all the numbers are unique and are key to understanding: <br><ul><li>  node-1 - car number 1 </li><li>  node-2 - car number 2 </li></ul></li><li>  vCPU - virtual CPU, processor core.  The number corresponds to the number of cores: 1vCPU - one core, 2vCPU - two, and so on. </li><li>  user - user or user space.  When using <code>user$</code> in command line instructions, the term refers to any client machine. </li><li>  worker - a worker node on which direct calculations will be performed, synonymous with <code>node-X</code> </li><li>  A resource is an entity operated by a cluster of kubernetes.  The resources of kubernetes include a large number of <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/">interconnected entities</a> . </li></ul></div></div><br><h1 id="setevaya-arhitektura-resheniya">  Network solution architecture </h1><br><p>  In the process of raising the cluster, I did not set the goal to optimize iron resources in such a way as to fit into the budget of $ 20 per month.  It was necessary to simply assemble a working cluster with at least two working nodes (nodes).  Therefore, initially the cluster looked like this: </p><br><ul><li>  machine with 2 vCPU / 4G RAM: api-server + node-1 [$ 20] </li><li>  machine with 2 vCPU / 4G RAM: node-2 [$ 20] </li></ul><br><p>  After the first version of the cluster was working, I decided to rebuild it so as to distinguish between the nodes responsible for running applications within the cluster (the working nodes, they are also workers), and the master server API. </p><br><p>  As a result, I received an answer to the question: "How to get a more or less inexpensive, but functioning cluster, if I want to host there are not the thickest applications." </p><br><div class="spoiler">  <b class="spoiler_title">$ 20 solution</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/216/89c/a88/21689ca889156d11108e5f5327c606cc.png" alt="Design"><br>  (It is planned to be such) </p></div></div><br><div class="spoiler">  <b class="spoiler_title">General Information Kubernetes Architecture</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/getpro/habr/post_images/820/462/882/820462882e7cc92479190c067ac4a4f8.jpg" alt="Design"><br>  (Stolen from the Internet, if someone suddenly does not know or have not seen) </p></div></div><br><h2 id="komponenty-i-ih-proizvoditelnost">  Components and their performance </h2><br><p>  First of all, it was worth understanding how much resources I need to run software systems that relate directly to the cluster.  The search for "hardware requirements" did not give specific results, so I had to approach the task from a practical point of view.  As a measurement of MEM and CPU, I took statistics from systemd - we can assume that the measurements were made in a very amateur way, but I didn‚Äôt have the task of obtaining accurate values, since I still did not find any options cheaper than $ 5 for an instance. </p><br><div class="spoiler">  <b class="spoiler_title">Why exactly $ 5?</b> <div class="spoiler_text"><p>  It was possible to find VPS / VDS cheaper when hosting servers in Russia or the CIS, but the sad stories associated with the RKN and its actions create certain risks and give rise to a natural desire to avoid them. </p></div></div><br><p>  So: </p><br><ul><li>  Master server / servers configuration (Master Nodes): <br><ul><li>  etcd (3.2.17): 80 - 100M, metrics were shot at randomly selected time.  On average, memory consumption etcd did not exceed 300M; </li><li>  kube-apiserver (1.12.x - 1.13.0): 237.6M ~ 300M; </li><li>  kube-controller-manager (1.12.x - 1.13.0): approximately 90M, did not rise above 100M; </li><li>  kube-scheduler (1.12.x - 1.13.0): approximately 20M, consumption above 30-50M is not fixed. </li></ul></li><li>  Worker Nodes configuration: <br><ul><li>  kubelet (1.12.3 - 1.13.1): approximately 35 Mb, consumption above 50M is not fixed; </li><li>  kube-proxy (1.12.3 - 1.13.1): approximately 7.5 - 10M; </li><li>  flannel (0.10.0): approximately 15-20M; </li><li>  coredns (1.3.0): approximately 25M; </li><li>  containerd (1.2.1): consumption of containerd is low, but the statistics also show the container processes started by the daemon. </li></ul></li></ul><br><div class="spoiler">  <b class="spoiler_title">Do I need containerd / docker on master nodes?</b> <div class="spoiler_text"><p>  <strong>No, not needed</strong> .  The master node does not require docker or containerd as such, although there are a large number of tutorials on the Internet that, for various needs, include using the containerization environment.  In this configuration, turning off containerd from the list of dependencies was intentionally done; however, I don‚Äôt single out any obvious advantages of this approach. </p><br><p>  The configuration provided above is minimal and sufficient to start the cluster.  No additional actions / components are required, unless you want to add something on your own. </p></div></div><br><p>  To build a test cluster or cluster for home projects, 1vCPU / 1G RAM will be quite enough for the master node to function.  Of course, the load on the master node will vary depending on the number of workers involved (workers), as well as the presence and volume of third-party requests to the api-server. </p><br><p>  I posted the master and worker configurations as follows: </p><br><ul><li>  1x Master with installed components: etcd, kube-apiserver, kube-controller-manager, kube-scheduler </li><li>  2x Workers with installed components: containerd, coredns, flannel, kubelet, kube-proxy </li></ul><br><h1 id="konfiguraciya">  Configuration </h1><br><p>  To configure the wizard, the following components must function: </p><br><ul><li><p>  etcd - for storing data for the api-server, as well as for the flannel; </p><br></li><li><p>  kube-apiserver - actually, api-server; </p><br></li><li><p>  kube-controller-manager - for generating and processing events; </p><br></li><li><p>  kube-scheduler - for distributing resources registered via api-server - for example, <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">pods</a> . <br>  The configuration of the "workhorse" requires the operation of the following components: </p><br></li><li><p>  kubelet - to start the sweats, to ensure the configuration of network settings; </p><br></li><li><p>  kube-proxy - to organize the routing / balancing of kubernetes <a href="https://kubernetes.io/docs/concepts/services-networking/service/">services</a> ; </p><br></li><li><p>  coredns - for service discovery inside running containers; </p><br></li><li><p>  flannel - for organizing network access for containers running on different nodes, as well as for dynamic distribution of networks across nodes (kubernetes node) of the cluster. </p><br></li></ul><br><div class="spoiler">  <b class="spoiler_title">CoreDNS</b> <div class="spoiler_text"><p>  There should be a small digression: coredns can be run on the master server as well.  There are no restrictions that would force coredns to run on the worker nodes, except for the coredns.service configuration nuance, which simply won't run on the standard / unmodified Ubuntu server due to a conflict with the systemd-resolved service.  I didn‚Äôt want to solve this problem, as I was completely satisfied with the 2 ns servers located on the worker nodes. </p></div></div><br><p>  In order not to waste time on getting acquainted with all the details of the process of configuring components, I suggest that you familiarize yourself with them in the <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way/">Kubernetes manual the hard way</a> .  I will focus on the distinctive features of my configuration option. </p><br><h2 id="fayly">  Files </h2><br><p>  All files for the functioning of cluster components for the wizard and work nodes are placed in <strong>/ var / lib / kubernetes /</strong> for convenience.  If necessary, you can place them in another way. </p><br><h2 id="sertifikaty">  Certificates </h2><br><p>  The basis for generating certificates is the same <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way/">Kubernetes the hard way</a> , there are practically no significant differences.  For the regeneration of subordinate certificates, simple bash scripts were written around <a href="">cfssl</a> applications - this was very useful in the debugging process. </p><br><p>  You can generate certificates for your needs using the following scripts, recipes from <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way/">Kubernetes the hard way,</a> or other suitable tools. </p><br><div class="spoiler">  <b class="spoiler_title">Generating certificates using bash scripts</b> <div class="spoiler_text"><p>  You can <a href="https://github.com/tarvitz/kubernetes">get the</a> scripts here: <a href="https://github.com/tarvitz/kubernetes">kubernetes bootstrap</a> .  Before launching, edit the <a href="">certs / env.sh file</a> , specifying your settings.  Example: </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:   certs$ ./generate-keys.sh # ... certificate generate output #:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><p>  If you have used <code>env.sh</code> and correctly specified all the parameters, then there is no need to touch the generated certificates.  If you make a mistake at some stage, then the certificates can be regenerated in parts.  These bash scripts are trivial, to understand them is not difficult. </p><br><p>  Important note - you should not often re-create <code>ca.pem</code> and <code>ca-key.pem</code> certificates, since they are the root for all subsequent ones, in other words, you will have to recreate all accompanying certificates and deliver them to all machines and to all necessary directories. </p></div></div><br><h3 id="master">  Master </h3><br><p>  The necessary certificates to start the services on the master node should be placed in <code>/var/lib/kubernetes/</code> : </p><br><ul><li>  ca.pem - this certificate is used everywhere, you can generate it only once and then use it unchanged - so be careful.  When re-generating, you will need to copy it to all nodes, as well as update the kubeconfig files using it (also on all machines). </li><li>  ca-key.pem is the same, except for copying by nodes. </li><li>  kube-controller-manager.pem - needed only for kube-controller-manager. </li><li>  kube-controller-manager-key.pem - only needed for kube-controller-manager. </li><li><p>  kubernetes.pem - required for flannel, coredns when connected to etcd, kube-apiserver. </p><br><div class="spoiler">  <b class="spoiler_title">Theoretical retreat</b> <div class="spoiler_text"><p>  This feature is made according to the <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way/">Kubernetes the hard way</a> configuration logic. <br>  Based on this, this file will be needed everywhere - both on the wizard and on the work nodes.  I did not begin to change the approach provided by the original manual, since with its help it is possible to organize the work of the cluster more quickly and more clearly and understand the whole bunch of dependencies. </p><br><p>  My personal opinion is that for etcd we need separate certificates that do not overlap with the certificates used for the work of kubernetes. </p><br></div></div><br></li></ul><br><ul><li>  kubernetes-key.pem - remains on the master servers. </li><li>  service-account.pem - needed only for kube-controller-manager daemons. </li><li>  service-account-key.pem is the same. </li></ul><br><h3 id="rabochie-uzly">  Work nodes </h3><br><ul><li>  ca.pem - needed for all services involved in working nodes (kubelet, kube-proxy), as well as for flannel, coredns.  Among other things, its contents are included in kubeconfig files when they are generated using kubectl. </li><li>  kubernetes-key.pem - only needed for flannel and coredns to connect to etcd, which is located on the api master node. </li><li>  kubernetes.pem - similar to the previous one, needed only for flannel and coredns. </li><li>  kubelet / node-1.pem is the key for authorizing node-1. </li><li>  kubelet / node-1-key.pem is the key for authorizing node-1. </li></ul><br><p>  <strong>Important!</strong>  If you have more than one node, then each node will include <code>node-X-key.pem</code> , <code>node-X.pem</code> and <code>node-X.kubeconfig</code> files inside the kubelet. </p><br><div class="spoiler">  <b class="spoiler_title">Debugging Certificates</b> <div class="spoiler_text"><h4 id="otladka-sertifikatov">  Debugging Certificates </h4><br><p>  Sometimes it may be necessary to look at how the certificate is configured to find out which IP / DNS nodes were used during its generation.  The <code>cfssl-certinfo -cert &lt;cert&gt;</code> command will help us with this.  For example, learn this information for <code>node-1.pem</code> : </p><br><pre> <code class="bash hljs">$ cfssl-certinfo -cert node-1.pem</code> </pre> <br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"subject"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:nodes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure Unit"</span></span>, <span class="hljs-string"><span class="hljs-string">"system:node:node-1"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"issuer"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"common_name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"country"</span></span>: <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organization"</span></span>: <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"organizational_unit"</span></span>: <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"locality"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"province"</span></span>: <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"names"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"RU"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Moscow"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span>, <span class="hljs-string"><span class="hljs-string">"Infrastructure"</span></span>, <span class="hljs-string"><span class="hljs-string">"Kubernetes"</span></span> ] }, <span class="hljs-attr"><span class="hljs-attr">"serial_number"</span></span>: <span class="hljs-string"><span class="hljs-string">"161113741562559533299282037709313751074033027073"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sans"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"w40k.net"</span></span>, <span class="hljs-string"><span class="hljs-string">"node-1"</span></span>, <span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>, <span class="hljs-string"><span class="hljs-string">"192.168.164.230"</span></span> ], <span class="hljs-attr"><span class="hljs-attr">"not_before"</span></span>: <span class="hljs-string"><span class="hljs-string">"2019-01-04T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"not_after"</span></span>: <span class="hljs-string"><span class="hljs-string">"2029-01-01T14:24:00Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"sigalg"</span></span>: <span class="hljs-string"><span class="hljs-string">"SHA256WithRSA"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"authority_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"6:C8:94:67:59:55:19:82:AD:ED:6D:50:F1:89:B:8D:46:78:FD:9A"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"subject_key_id"</span></span>: <span class="hljs-string"><span class="hljs-string">"A1:5E:B3:3C:45:14:3D:C6:C:A:97:82:1:D5:2B:75:1A:A6:9D:B0"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"pem"</span></span>: <span class="hljs-string"><span class="hljs-string">"&lt;pem content&gt;"</span></span> }</code> </pre> </div></div><br><p>  All other certificates for kubelet and kube-proxy are sewn directly into the corresponding kubeconfig. </p><br><h2 id="kubeconfig">  kubeconfig </h2><br><p>  All the necessary kubeconfig'i can be done with the help of <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way/">Kubernetes the hard way</a> , however, here are some differences.  The manual uses the <code>kubedns</code> and <code>cni bridge</code> configuration, but also <a href="https://coredns.io/">coredns</a> and <a href="https://github.com/coreos/flannel">flannel</a> .  These two services, in turn, use <code>kubeconfig</code> for authorization in the cluster. </p><br><pre> <code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> certs <span class="hljs-comment"><span class="hljs-comment">#:  kubeconfig     certs$ ./generate-configkube.sh</span></span></code> </pre> <br><h3 id="master-1">  Master </h3><br><p>  The wizard requires the following kubeconfig files (as mentioned above, after generating them, you can take them to <code>certs/kubeconfig</code> ): </p><br><pre> <code class="plaintext hljs">master /var/lib/kubernetes/$ tree -L 2 . +-- kube-controller-manager.kubeconfig L-- kube-scheduler  L-- kube-scheduler.kubeconfig</code> </pre> <br><p>  These files are required to run each of the service components. </p><br><h3 id="rabochie-uzly-1">  Work nodes </h3><br><p>  For work nodes the following kubeconfig files are needed: </p><br><pre> <code class="plaintext hljs">node-1 /var/lib/kubernetes/$ tree -L 2 . +-- coredns ¬¶  L-- coredns.kubeconfig +-- flanneld ¬¶  L-- flanneld.kubeconfig +-- kubelet ¬¶  L-- node-1.kubeconfig L-- kube-proxy  L-- kube-proxy.kubeconfig</code> </pre> <br><h2 id="zapusk-servisov">  Starting services </h2><br><div class="spoiler">  <b class="spoiler_title">Services</b> <div class="spoiler_text"><p>  Despite the fact that on my work nodes different initialization systems are used, examples and the repository give options using systemd.  With their help, the easiest way to understand what process and with what parameters you need to run, in addition, they should not cause major problems when learning services with destination flags. </p></div></div><br><p>  To start the services, you need to copy the <code>service-name.service</code> to <code>/lib/systemd/system/</code> or any other directory where systemd services are located, and then turn on and start the service.  Example for kube-apiserver: </p><br><pre> <code class="bash hljs">$ systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> kube-apiserver.service $ systemctl start kube-apiserver.service</code> </pre> <br><p>  Of course, all services must be <em>green</em> (that is, running and functioning).  If suddenly you encounter any error, the <code>journalct -xe</code> or <code>journal -f -t kube-apiserver</code> will help you understand what went wrong. </p><br><p>  Do not rush to start all the servers at once, for a start it will be enough to enable etcd and kube-apiserver.  If everything went well, and you immediately earned all four services on the wizard, the launch of the wizard can be considered successful. </p><br><h3 id="master-2">  Master </h3><br><p>  You can use the <a href="https://github.com/tarvitz/kubernetes/tree/master/systemd">systemd</a> settings or generate init scripts for the configuration you are using.  As already mentioned, for the master you need: </p><br><p>  - <a href="https://github.com/tarvitz/kubernetes/blob/master/systemd/etcd.service">systemd / etcd</a> <br>  - <a href="https://github.com/tarvitz/kubernetes/blob/master/systemd/kube-apiserver.service">systemd / kube-apiserver</a> <br>  - <a href="https://github.com/tarvitz/kubernetes/blob/master/systemd/kube-controller-manager.service">systemd / kube-controller-manager</a> <br>  - <a href="https://github.com/tarvitz/kubernetes/blob/master/systemd/kube-scheduler.service">systemd / kube-scheduler</a> </p><br><h3 id="rabochie-uzly-2">  Work nodes </h3><br><p>  - <a href="https://github.com/tarvitz/kubernetes/blob/master/systemd/containerd.service">systemd / containerd</a> <br>  - <a href="https://github.com/tarvitz/kubernetes/blob/master/systemd/kubelet.service">systemd / kubelet</a> <br>  - <a href="https://github.com/tarvitz/kubernetes/blob/master/systemd/kube-proxy.service">systemd / kube-proxy</a> <br>  - <a href="https://github.com/tarvitz/kubernetes/blob/master/systemd/coredns.service">systemd / coredns</a> <br>  - <a href="https://github.com/tarvitz/kubernetes/blob/master/systemd/kubelet.service">systemd / flannel</a> </p><br><h3 id="klient">  Customer </h3><br><p>  For the client to work, just copy the <code>certs/kubeconfig/admin.kubeconfig</code> (after you generate it or write it yourself) in <code>${HOME}/.kube/config</code> </p><br><p>  Download <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">kubectl</a> and test kube-apiserver.  Let me remind you once again that at this stage only etcd should work for the kube-apiserver to work.  The remaining components will be needed for the full operation of the cluster a little later. </p><br><p>  Check that kube-apiserver and kubectl work: </p><br><pre> <code class="bash hljs">$ kubectl version Client Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>} Server Version: version.Info{Major:<span class="hljs-string"><span class="hljs-string">"1"</span></span>, Minor:<span class="hljs-string"><span class="hljs-string">"13"</span></span>, GitVersion:<span class="hljs-string"><span class="hljs-string">"v1.13.0"</span></span>, <span class="hljs-string"><span class="hljs-string">"extra info"</span></span>: <span class="hljs-string"><span class="hljs-string">"..."</span></span>}</code> </pre> <br><h1 id="konfiguraciya-flannel">  Flannel configuration </h1><br><p>  As a flannel configuration, I settled on a <code>vxlan</code> backend.  Read more about backends <a href="">here</a> . </p><br><div class="spoiler">  <b class="spoiler_title">host-gw and why it won't work</b> <div class="spoiler_text"><p>  At once I will say that launching the kubernetes cluster on a VPS will most likely limit you in using the <code>host-gw</code> backend.  Not being an experienced network engineer, I spent about two days debugging to understand what the problem was with using it on popular VDS / VPS providers. </p><br><p>  <a href="https://www.linode.com/community/questions/17540/kubernetesflannel-routing-issue-with-linode-private-networks">Linode.com</a> and <a href="https://cloudsupport.digitalocean.com/s/case/5001N00000lUsNxQAK">digitalocean</a> have been tested.  The essence of the problem lies in the fact that providers do not provide honest L2 for private network (private networking).  This, in turn, makes it impossible to walk network traffic between nodes in this configuration: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e0c/c7e/add/e0cc7eadd6253cf4921df17ca6fe5d47.png" alt="Traffic"></p><br><p>  In order for the network traffic to work between the nodes, normal routing is enough.  Do not forget that net.ipv4.ip_forward should be set to 1, and the FORWARD chain in the filter table should not contain any prohibiting rules for nodes. </p><br><pre> <code class="bash hljs">node1$ ip route add 10.200.12.0/24 via 192.168.1.2 node2$ ip route add 10.200.8.0/24 via 192.168.1.1</code> </pre> <br><pre> <code class="plaintext hljs">[10.200.80.23 container-1]-&gt;[192.168.1.1 node-1]-&gt;[192.168.1.2 node-2]-&gt;[10.200.12.5 container-2]</code> </pre> <br><p>  This is what does not work on the specified (and, most likely, and in general at all) VPS / VDS. </p><br><p>  Therefore, if the configuration of a solution with high network performance between nodes <strong>is important to</strong> you, you still have to spend more than $ 20 to organize the cluster. </p></div></div><br><p>  To set the desired flannel configuration, you can use <code>set-flannel-config.sh</code> from <a href="https://github.com/tarvitz/kubernetes/tree/master/etc/flanneld">etc / flannel</a> .  <strong>It is important to remember</strong> : if you decide to change the backend, you will need to delete the configuration in etcd and restart all flannel daemons on all nodes - so choose it wisely.  The default is vxlan. </p><br><pre> <code class="bash hljs">master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CA_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/ca.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_CERT_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_KEY_FILE=<span class="hljs-string"><span class="hljs-string">'/var/lib/kubernetes/kubernetes-key.pem'</span></span> master$ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> ETCDCTL_ENDPOINTS=<span class="hljs-string"><span class="hljs-string">'https://127.0.0.1:2379'</span></span> master$ etcdctl ls /coreos.com/network/subnets/ /coreos.com/network/subnets/10.200.8.0-24 /coreos.com/network/subnets/10.200.12.0-24 master$ etcdctl get /coreos.com/network/subnets/10.200.8.0-24 {<span class="hljs-string"><span class="hljs-string">"PublicIP"</span></span>:<span class="hljs-string"><span class="hljs-string">"178.79.168.130"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendType"</span></span>:<span class="hljs-string"><span class="hljs-string">"vxlan"</span></span>,<span class="hljs-string"><span class="hljs-string">"BackendData"</span></span>:{<span class="hljs-string"><span class="hljs-string">"VtepMAC"</span></span>:<span class="hljs-string"><span class="hljs-string">"22:ca:ac:15:71:59"</span></span>}}</code> </pre> <br><p>  After you have registered the necessary configuration in etcd, you need to configure the service to start it on each of the work nodes. </p><br><h2 id="flannelservice">  flannel.service </h2><br><p>  An example for the service can be taken here: <a href="https://github.com/tarvitz/kubernetes/blob/master/systemd/kubelet.service">systemd / flannel</a> </p><br><div class="spoiler">  <b class="spoiler_title">flannel.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Flanneld overlay address etcd agent After=network.target [Service] Type=notify #: current host ip. don't change if ip have not changed Environment=PUBLIC_IP=178.79.168.130 Environment=FLANNEL_ETCD=https://192.168.153.60:2379 ExecStart=/usr/bin/flanneld \ -etcd-endpoints=${FLANNEL_ETCD} -etcd-prefix=${FLANNEL_ETCD_KEY} \ -etcd-cafile=/var/lib/kubernetes/ca.pem \ -etcd-certfile=/var/lib/kubernetes/kubernetes.pem \ -etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \ -etcd-prefix=/coreos.com/network \ -healthz-ip=127.0.0.1 \ -subnet-file=/run/flannel/subnet.env \ -public-ip=${PUBLIC_IP} \ -kubeconfig-file=/var/lib/kubernetes/config/kubeconfig/flanneld.kubeconfig \ $FLANNEL_OPTIONS ExecStartPost=/usr/libexec/flannel/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/docker Restart=on-failure RestartSec=5 [Install] RequiredBy=docker.service</code> </pre> </div></div><br><h2 id="nastroyka">  Customization </h2><br><p>  As described earlier, we need ca.pem, kubernetes.pem, and kubernetes-key.pem files for authorization in etcd.  All other parameters do not carry any sacred meaning.  The only thing that is really important is to configure the global ip address through which the network packets will go between networks: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/037/e54/803/037e5480319cedd1e662c925bce23b3e.png" alt="Flannel networking"><br>  ( <a href="https://docker-k8s-lab.readthedocs.io/en/latest/docker/docker-flannel.html">Multi-Host Networking Overlay with Flannel</a> ) </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable flanneld.service #:  node-1$ systemctl start flanneld</span></span></code> </pre> <br><p>  After successfully launching the flannel, you should discover the flannel.N network interface in your system: </p><br><pre> <code class="plaintext hljs">node-1$ ifconfig flannel.100: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1450 inet 10.200.8.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::20ca:acff:fe15:7159 prefixlen 64 scopeid 0x20&lt;link&gt; ether 22:ca:ac:15:71:59 txqueuelen 0 (Ethernet) RX packets 18853 bytes 1077085 (1.0 MiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 11856 bytes 264331154 (252.0 MiB) TX errors 0 dropped 47 overruns 0 carrier 0 collisions 0</code> </pre> <br><p>  Verifying that your interfaces are working correctly on all nodes is quite simple.  In my case, node-1 and node-2 have 10.200.8.0/24 and 10.200.12.0/24 networks, respectively, so we check availability with the usual icmp request: </p><br><pre> <code class="plaintext hljs">#:  node-2  node-1 node-1 $ ping -c 1 10.200.12.0 PING 10.200.12.0 (10.200.12.0) 56(84) bytes of data. 64 bytes from 10.200.12.0: icmp_seq=1 ttl=64 time=4.58 ms #:  node-1  node-2 node-2 $ ping -c 1 10.200.8.0 PING 10.200.8.0 (10.200.8.0) 56(84) bytes of data. 64 bytes from 10.200.8.0: icmp_seq=1 ttl=64 time=1.44 ms</code> </pre> <br><p>  In case of any problems, it is recommended to check if there are any cutting rules in iptables over UDP between the hosts. </p><br><h1 id="konfiguraciya-containerd">  Containerd Configuration </h1><br><p>  Place <a href="">etc / containerd / config.toml</a> in <code>/etc/containerd/config.toml</code> or where it is convenient for you, the main thing is to remember to change the path to the configuration file in the service (containerd.service, described below). </p><br><p>  Configuration with some modifications of the standard.  <strong>It is important not to set</strong> <code>enable_tls_streaming = true</code> if you do not understand why you are doing this.  Otherwise, <code>kubectl exec</code> will stop working and will <code>kubectl exec</code> error stating that the certificate was signed by an unknown party. </p><br><h2 id="containerdservice">  containerd.service </h2><br><div class="spoiler">  <b class="spoiler_title">containerd.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target [Service] ; uncomment this if your overlay module are built as module ; ExecStartPre=/sbin/modprobe overlay ExecStart=/usr/bin/containerd \ -c /etc/containerd/config.toml Restart=always RestartSec=5 Delegate=yes KillMode=process OOMScoreAdjust=-999 LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-1">  Customization </h2><br><p>  ,   ,   <a href="https://github.com/kubernetes-sigs/cri-tools">cri-tools</a> . <br>  <a href="">etc/crictl.yaml</a>  <code>/etc/crictl.yaml</code> .      : </p><br><pre> <code class="bash hljs">node-1$ CONTAINERD_NAMESPACE=k8s.io crictl ps CONTAINER ID IMAGE CREATED STATE NAME ATTEMPT POD ID</code> </pre> <br><p>  ,    -    kubernetes , crictl       , ,    . </p><br><h1 id="konfiguraciya-cni-plugins">  CNI Plugins </h1><br><p>  CNI    ,      <a href="https://habr.com/company/flant/blog/329830/"></a>   ,    ,   . </p><br><h1 id="nastroyka-2">  Customization </h1><br><p>       <a href="https://github.com/containernetworking/plugins/releases">cni plugins</a>        <code>/opt/cni/bin/</code> </p><br><p>  <a href="">/etc/cni/net.d</a>      : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/10-flannel.conflist</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "name": "cbr0", "plugins": [ { "type": "flannel", "name": "kubenet", "delegate": { "hairpinMode": true, "isDefaultGateway": true } }, { "type": "portmap", "capabilities": { "portMappings": true }, "externalSetMarkChain": "KUBE-MARK-MASQ" } ] }</code> </pre> </div></div><br><div class="spoiler"> <b class="spoiler_title">/etc/cni/net.d/99-loopback.conf</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">{ "cniVersion": "0.3.0", "type": "loopback" }</code> </pre> </div></div><br><p>       ,    .  ,       ,    <a href="https://habr.com/company/flant/blog/426141/">Red Hat  Docker  Podman</a> , <a href="https://developers.redhat.com/blog/2018/08/29/intro-to-podman/">Intro to Podman</a> </p><br><h1 id="konfiguraciya-kubelet">  Kubelet </h1><br><p>     kubelet  (     cni) ‚Äî    .   kubelet    hostname.         ,      ""   <code>kubectl logs</code> , <code>kubectl exec</code> , <code>kubectl port-forward</code> . </p><br><div class="spoiler"> <b class="spoiler_title"> kubelet-config.yaml</b> <div class="spoiler_text"><p>  ,   <a href="">etc/kubelet-config.yaml</a>   ,        ,     .     : </p><br><pre> <code class="plaintext hljs">systemReserved: cpu: 200m memory: 600Mi</code> </pre> <br><p>  ,        GO  kubernetes,  ,       .        .           0.2 vCPU  600 MB     . </p><br><p>   ,  , kubelet, kube-proxy, coredns, flannel    . ,               ‚Äî     2 vCPU / 4G ram,           ,     kubernetes + postgresql . </p><br><p>    - (micro nodes)        . </p></div></div><br><h2 id="kubeletservice"> kubelet.service </h2><br><p>  service    : <a href="https://github.com/tarvitz/kubernetes/blob/master/systemd/kubelet.service">systemd/kubelet</a> </p><br><div class="spoiler"> <b class="spoiler_title">kubelet.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes Requires=containerd.service [Service] #Environment=NODE_IP=192.168.164.230 Environment=NODE_IP=178.79.168.130 #: node name given by env Environment=NODE_NAME=w40k.net ExecStart=kubelet \ --allow-privileged \ --root-dir=/var/lib/kubernetes/kubelet \ --config=/var/lib/kubernetes/kubelet/kubelet-config.yaml \ --kubeconfig=/var/lib/kubernetes/kubelet/node-1.kubeconfig \ --cni-bin-dir=/opt/cni/bin \ --cni-conf-dir=/etc/cni/net.d/ \ --network-plugin=cni \ --container-runtime=remote \ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \ --image-pull-progress-deadline=10m \ --node-ip=${NODE_IP} \ --hostname-override=${NODE_NAME} \ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-3">  Customization </h2><br><p>      ,   <a href="https://www.cncf.io/blog/2018/08/01/demystifying-rbac-in-kubernetes/">RBAC</a> ,                kubelet. </p><br><p>  <a href="">etc/kubelet-default-rbac.yaml</a>  ,  kubelet        : </p><br><pre> <code class="bash hljs">user$ kubectl apply -f etc/kubelet-default-rbac.yaml</code> </pre> <br><p>  ,    ,        . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kubelet.service #:  node-1$ systemctl start kubelet</span></span></code> </pre> <br><p>    ,           api : </p><br><pre> <code class="plaintext hljs">$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME w40k.net Ready &lt;none&gt; 5m v1.13.1 178.79.168.130 &lt;none&gt; Gentoo/Linux 4.18.16-x86_64-linode118 containerd://1.2.1</code> </pre> <br><h1 id="konfiguraciya-kube-proxy">  Kube Proxy </h1><br><p> : <a href="https://github.com/tarvitz/kubernetes/blob/master/systemd/kubelet.service">systemd/kubelet</a> .   ,   , <code>kube-proxy-config.yaml</code>     : <a href="">etc/kube-proxy</a> </p><br><h2 id="kube-proxyservice"> kube-proxy.service </h2><br><div class="spoiler"> <b class="spoiler_title">kube-proxy.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=Kubernetes Proxy Documentation=https://github.com/kubernetes/kubernetes After=network.target [Service] ExecStart=kube-proxy \ --config=/var/lib/kubernetes/kube-proxy/kube-proxy-config.yaml Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-4">  Customization </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable kube-proxy.service #:  node-1$ systemctl start kube-proxy</span></span></code> </pre> <br><p>   kube-proxy   ""   iptables,         ,   -   kubernetes  (- ).   . </p><br><h1 id="konfiguraciya-coredns">  CoreDNS </h1><br><p> Corefile   : <a href="https://github.com/tarvitz/kubernetes/blob/master/etc/coredns/Corefile">etc/coredns/Corefile</a> ,    : </p><br><div class="spoiler"> <b class="spoiler_title">/etc/coredns/Corefile</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">.:53 { errors log stdout health :8081 kubernetes cluster.local 10.200.0.0/16 { endpoint https://178.79.148.185:6443 tls /var/lib/kubernetes/kubernetes.pem /var/lib/kubernetes/kubernetes-key.pem /var/lib/kubernetes/ca.pem pods verified upstream /etc/resolv.conf kubeconfig /var/lib/kubernetes/config/kubeconfig/coredns.kubeconfig default } proxy . /etc/resolv.conf cache 30 }</code> </pre> </div></div><br><p>     coredns.kubeconfig  pem- (    )   worker . , coredns      systemd-resolved. ,         Ubuntu ,  ,  ,  ,  .        . </p><br><h2 id="corednsservice"> coredns.service </h2><br><div class="spoiler"> <b class="spoiler_title">coredns.service</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">[Unit] Description=CoreDNS Documentation=https://coredns.io/ After=network.target [Service] ExecStart=/usr/bin/coredns -conf /etc/coredns/Corefile Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target</code> </pre> </div></div><br><h2 id="nastroyka-5">  Customization </h2><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#:   node-1$ systemctl enable coredns.service #:  node-1$ systemctl start coredns</span></span></code> </pre> <br><p> ,   ,   : </p><br><pre> <code class="plaintext hljs">node-1$ dig kubernetes.default.svc.cluster.local @127.0.0.1 #:    ;kubernetes.default.svc.cluster.local. IN A ;; ANSWER SECTION: kubernetes.default.svc.cluster.local. 5 IN A 10.32.0.1</code> </pre> <br><p>   , coredns   ip   kubernetes . <br> <strong></strong> , <a href="https://kubernetes.io/docs/concepts/services-networking/service/">kubernetes.default </a>   <strong></strong> kube-controller-manager,      : </p><br><pre> <code class="plaintext hljs">$ kubectl get svc -n default NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.32.0.1 &lt;none&gt; 443/TCP 26h</code> </pre> <br><h1 id="nginx-ingress--cert-manager"> nginx-ingress &amp; cert-manager </h1><br><p>   ,    .        nginx-ingress  cert-manager. </p><br><p> ‚Äî <a href="https://github.com/nginxinc/kubernetes-ingress">nginx kubernetes ingress</a> (master),  : </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/nginxinc/kubernetes-ingress.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> kubernetes-ingress/deployments user$ kubectl apply -f common/ns-and-sa.yaml user$ kubectl apply -f common/nginx-config.yaml user$ kubectl apply -f common/default-server-secret.yaml user$ kubectl apply -f daemon-set/nginx-ingress.yaml user$ kubectl apply -f rbac/rbac.yaml</code> </pre> <br><p> ‚Äî <a href="">cert manager</a> (v0.5.2) </p><br><pre> <code class="bash hljs"> user$ git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> https://github.com/jetstack/cert-manager.git user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> cert-manager &amp;&amp; git co v0.5.2 user$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> contrib/manifests/cert-manager user$ kubectl apply -f with-rbac.yaml</code> </pre> <br><p>  ,    ,  ,    : </p><br><pre> <code class="plaintext hljs">NAMESPACE NAME READY STATUS RESTARTS AGE cert-manager cert-manager-554c76fbb7-t9762 1/1 Running 0 3h38m nginx-ingress nginx-ingress-sdztf 1/1 Running 0 10h nginx-ingress nginx-ingress-vrf85 1/1 Running 0 10h</code> </pre> <br><p>  cert-manager  nginx-ingress    running state,   ,    .          ,         <code>Running</code> .            . </p><br><h1 id="zapuskaem-prilozhenie">   </h1><br><p>   ,     .      ,   kubernetes resource : <a href="https://github.com/tarvitz/kubernetes/tree/master/app/k8s">app/k8s</a> </p><br><pre> <code class="bash hljs">user$ kube apply -f ns-and-sa.yaml user$ kube apply -f configmap.yaml <span class="hljs-comment"><span class="hljs-comment">#:  secret-example.yaml       #: secret.yaml user$ kube apply -f secret.yaml user$ kube apply -f tls-production.yaml user$ kube apply -f deployment.yaml user$ kube apply -f service.yaml user$ kube apply -f ingress-production.yaml</span></span></code> </pre> <br><p>   ,     - .  ,    (      kubernetes-example.w40k.net),     ,    ,  cert-manager    nginx-ingress              .   ,    ingress  tls/ssl. </p><br><p>      : </p><br><ul><li> <a href="https.kubernetes-example.w40k.net/">http://no-https.kubernetes-example.w40k.net/</a> ‚Äî  ssl;  ,  -   ,     . </li><li> <a href="https://kubernetes-example.w40k.net/">https://kubernetes-example.w40k.net/</a> ‚Äî   (,   ,   ),  ,     ,       kubernetes     . </li></ul><br><p>       ,      -   .    -       ,    <a href="https://habr.com/ru/company/iponweb/blog/435228/"> </a> ,        . </p><br><h1 id="ssylki">  Links </h1><br><p> ,     ,   : </p><br><p> ‚Äî <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way/">Kubernetes the hard way</a> <br> ‚Äî <a href="https://docker-k8s-lab.readthedocs.io/en/latest/docker/docker-flannel.html">Multi-Host Networking Overlay with Flannel</a> <br> ‚Äî <a href="https://developers.redhat.com/blog/2018/08/29/intro-to-podman/">Intro to Podman</a> <br> ‚Äî <a href="https://whatis.techtarget.com/definition/stateless-app">Stateless Applications</a> <br> ‚Äî <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">What is ingress</a> </p><br><p>   : </p><br><p> ‚Äî <a href="https://itnext.io/kubernetes-networking-behind-the-scenes-39a1ab1792bb">Kubernetes Networking: Behind the scenes</a> ( <a href="https://habr.com/company/flant/blog/420813/"></a> ) <br> ‚Äî <a href="https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/">A Guide to the Kubernetes Networking Model</a> <br> ‚Äî <a href="https://medium.com/google-cloud/understanding-kubernetes-networking-services-f0cb48e4cc82">Understanding kubernetes networking: services</a> ( <a href="https://medium.com/google-cloud/understanding-kubernetes-networking-services-f0cb48e4cc82"></a> ) </p><br><h1 id="qa"> Q&amp;A </h1><br><p> &lt;tbd&gt;,           . </p><br><h1 id="otladochnaya-informaciya">   </h1><br><p>     , ,     .    ,       ,  -  ,    ,  . </p><br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><h2 id="api-server"> Api Server </h2><br><p>   <code>kube-apiserver.service</code>    ,       api-server'   curl    http .            - . <br>     admin.kubeconfig  ${HOME}/.kube/config,   kubectl      api-server (kube-apiserver). </p><br><p>    (   )  HTTP 200 OK + ,  api-server  : </p><br><pre> <code class="plaintext hljs">curl -H "Authorization: Bearer e5qXNAtwwCHUUwyLilZmAoFPozrQwUpw" -k -L https://&lt;api-server-address&gt;:6443/api/v1/</code> </pre> <br><h2 id="kube-controller-manager"> Kube Controller Manager </h2><br><p>  ,  controller manager   api    ,      .        ,    <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">service account'</a> : </p><br><pre> <code class="plaintext hljs">$ kubectl get sa NAME SECRETS AGE default 1 19h</code> </pre> <br><p>    ,   ,  kube-controller-manager  . </p><br><h2 id="kube-scheduler"> Kube Scheduler </h2><br><p>       .  ,    ,    <a href="">debug/job.yaml</a>        <code>kubectl describe &lt;type/resource&gt;</code> . <br>    <strong> </strong>  ,  kube controller manager . </p><br><pre> <code class="plaintext hljs">#:   job user$ kubectl apply -f debug/job.yaml job.batch/app created #:  ,   job user$ kubectl get pods -l job-name=app NAME READY STATUS RESTARTS AGE app-9kr9z 0/1 Completed 0 54s #: ,        #:   user$ kubectl describe pods app-9kr9z # ...   ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 12s default-scheduler Successfully assigned example/app-9kr9z to w40k.net</code> </pre> <br><p>   , default-scheduler   pod   w40k.net.    -  ,            ‚Äî    . </p><br><p>              . , ,   , ‚Äî      "".       systemd        . </p><br><p>   kube scheduler  <a href="https://jvns.ca/blog/2017/07/27/how-does-the-kubernetes-scheduler-work/"></a>   <a href="https://habr.com/company/flant/blog/335552/"></a> </p><br><h2 id="kubelet"> Kubelet </h2><br><p> Kubelet    kubernetes     .  kubelet       .     kubernetes event ( <code>kubectl get events -o wide</code> )         . </p><br><p>     <a href="https://github.com/jamiehannaford/what-happens-when-k8s"></a> (  <a href="https://habr.com/company/flant/blog/342822/"></a> ) </p><br><h2 id="kube-proxy-i-servisy"> Kube Proxy   </h2><br><p>     kube-proxy    : </p><br><ul><li>      (     <a href="https://habr.com/ru/company/iponweb/blog/435228/"> Flannel</a> ,      ); </li><li>  iptables,   filter  nat . </li></ul><br><p> <strong></strong> , 10.32.0.0/24   "".  ,        .     iptables,     ,   ,     -    +.  <strong> </strong>  icmp    ,      ping'  .        ,     . </p><br><p>  ,     kube-proxy,               : </p><br><pre> <code class="plaintext hljs">#:    user$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE backend ClusterIP 10.32.0.195 &lt;none&gt; 80/TCP 5m #:     user$ kubectl get pods -o wide #:     ' NAME READY STATUS RESTARTS AGE IP NODE backend-896584448-4r94s 1/1 Running 0 11h 10.200.8.105 w40k.net backend-896584448-np992 1/1 Running 0 11h 10.200.12.68 docker.grart.net #:  10   /status/ endpoint ,       #:       node-1$ for i in `seq 10`; do curl -L http://10.32.0.195/status/; done okokokokokokokokokok node-1$ conntrack -L -d 10.32.0.195 tcp 6 62 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62158 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62158 [ASSURED] mark=0 use=1 tcp 6 60 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62144 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62144 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62122 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62122 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62142 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62142 [ASSURED] mark=0 use=1 tcp 6 58 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62130 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62130 [ASSURED] mark=0 use=1 tcp 6 61 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62150 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62150 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62116 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62116 [ASSURED] mark=0 use=1 tcp 6 57 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62118 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62118 [ASSURED] mark=0 use=1 tcp 6 59 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62132 dport=80 src=10.200.12.68 dst=10.200.8.0 sport=8000 dport=62132 [ASSURED] mark=0 use=1 tcp 6 56 TIME_WAIT src=178.79.168.130 dst=10.32.0.195 sport=62114 dport=80 src=10.200.8.105 dst=10.200.8.1 sport=8000 dport=62114 [ASSURED] mark=0 use=1</code> </pre> <br><p>      src/dst (9  10 ).   ,  src      : </p><br><ul><li> 10.200.8.105 </li><li> 10.200.12.68 </li></ul><br><p>  ,    .      ,  -  ( ,    )  .         . </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#: node-1   10.200.8.105, node-2 10.200.12.68, #:      8000  #:  node-1 node-1$ curl -L http://10.200.8.105:8000/status/ ok node-1$ curl -L http://10.200.12.68:8000/status/ ok #:  node-2 node-2$ curl -L http://10.200.8.105:8000/status/ ok node-2$ curl -L http://10.200.12.68:8000/status/ ok</span></span></code> </pre> <br><p>    ,    ,    conntrack        ,  ,      kube-proxy.   ,       nat : </p><br><p> <code>node-1$ iptables -t nat -vnL</code> </p> <br><p>          . </p><br><p>                  .  ,    ,      .   ,       .  -       , ,   . </p><br><p>     <a href="https://learnk8s.io/blog/kubernetes-chaos-engineering-lessons-learned"></a>   <a href="https://habr.com/company/flant/blog/359120/"></a> </p></div></div></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/435228/">https://habr.com/ru/post/435228/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../435214/index.html">Linux 4.20 released - what has changed in the new kernel version</a></li>
<li><a href="../435216/index.html">How to make 200 lines from two lines of code, and why do you need to do that</a></li>
<li><a href="../435220/index.html">Kotlin Native: Watch Files</a></li>
<li><a href="../435224/index.html">How to communicate in an English-speaking office: 14 useful idioms</a></li>
<li><a href="../435226/index.html">Recover data from empty space</a></li>
<li><a href="../435234/index.html">Smarter, more, more precisely: how does AI change flights into space</a></li>
<li><a href="../435236/index.html">Byte-machine for the fort (and not only) in Indian (Part 3)</a></li>
<li><a href="../435240/index.html">Unreal Engine4 - PostProcess scan effect</a></li>
<li><a href="../435242/index.html">Why am I afraid of becoming a "pumped man"</a></li>
<li><a href="../435244/index.html">ITER Project in 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>