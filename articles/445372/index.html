<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Machine Vision vs Human Intuition: Algorithms for Disruption of Object Recognition</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The logic of machines is flawless, they do not make mistakes if their algorithm is working properly and the specified parameters meet the required sta...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Machine Vision vs Human Intuition: Algorithms for Disruption of Object Recognition</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/s9/kf/68/s9kf68iyyoikag7yeexxfaxrp1e.jpeg"><br><br>  The logic of machines is flawless, they do not make mistakes if their algorithm is working properly and the specified parameters meet the required standards.  Ask the car to choose a route from point A to point B, and it will build the most optimal one, taking into account the distance, fuel consumption, availability of gas stations, etc.  This is a pure calculation.  The car will not say: "We will go on this road, I feel this route better."  Maybe cars are better than us in speed of calculation, but intuition still remains one of our trump cards.  Mankind has spent dozens of years in order to create a machine similar to the human brain.  But is it much in common between them?  Today we will look at a study in which scientists, having doubted the unsurpassed machine ‚Äúview‚Äù on the basis of convolutional neural networks, conducted an experiment on duping the object recognition system with an algorithm whose task was to create ‚Äúfake‚Äù images.  How successful was the sabotage activity of the algorithm, did people cope with the recognition of better machines and what would this research bring to the future of this technology?  We will find the answers in the scientists' report.  Go. <a name="habracut"></a><br><br><h3>  The basis of the study </h3><br>  Object recognition technologies using convolutional neural networks (SNS) allow the machine, roughly speaking, to distinguish a swan from a number 9 or a cat from a bicycle.  This technology is developing quite rapidly and is currently being used in various fields, the most obvious of which is the production of unmanned vehicles.  Many express the opinion that the SNA of the object recognition system can be considered as a model of human vision.  However, this statement is too loud, due to the human factor.  The whole point is that it was easier to fool a car than a man (at least in matters of object recognition).  SNA systems are very vulnerable to the effects of malicious algorithms (hostile, if you will) that will in every way prevent them from properly performing their task, creating images that will be incorrectly classified by the SNA system. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Researchers divide these images into two categories: ‚Äúduping‚Äù (completely changing the target object) and ‚Äúembarrassing‚Äù (partially changing the target object).  The first are meaningless images that are recognized by the system as something familiar.  For example, a set of lines can be classified as a ‚Äúbaseball ball‚Äù, and a multi-colored digital noise as a ‚Äúbattleship‚Äù.  The second category of images (‚Äúembarrassing‚Äù) are images that would normally be classified correctly, but the malicious algorithm slightly distorts them, speaking exaggeratedly, in the eyes of the SNA system.  For example, handwritten number 6 will be classified as number 5 due to a small addition of several pixels. <br><br>  Just imagine what harm such algorithms can do.  It is necessary to swap the classification of road signs for autonomous transport and accidents will be inevitable. <br><br>  Below are the "fake" images that fool the SNA system, trained to recognize objects, and how they were classified by a similar system. <br><br><img src="https://habrastorage.org/webt/bl/wf/ze/blwfzeyvqwjwhtqmk65ykrbulrc.jpeg"><br>  <i>Image number 1</i> <br><br>  Explanation of the series: <br><br><ul><li>  <b>a</b> - indirectly coded "fraudulent" images; </li><li>  <b>b</b> - directly coded fraudulent images; </li><li>  <b></b> - ‚Äúembarrassing‚Äù images, forcing the system to classify one digit as another; </li><li>  <b>d</b> - LaVAN attack (localized and visible competitive / harmful noise) can lead to misclassification, even when ‚Äúnoise‚Äù is located only at one point (in the lower right corner). </li><li>  <b>e</b> - three-dimensional objects that are misclassified from different angles. </li></ul><br>  But the most curious thing about this is that a person may not succumb to fooling a malicious algorithm and classify images correctly, based on intuition.  Previously, as scientists say, no one made a practical comparison of the capabilities of a machine and a person in an experiment to counter harmful algorithms of fake images.  That is what the researchers decided to do. <br><br>  To this end, several images were prepared, made by malicious algorithms.  The subjects were told that the machine classified these images (dummy) as familiar objects, i.e.  the machine recognized them incorrectly.  The task of the subjects was to determine exactly how the machine classified these images, i.e.  what they think the car saw on the images, whether such a classification is correct, etc. <br><br>  A total of 8 experiments were conducted in which 5 types of malicious images were used that were created without taking into account human vision.  In other words, they are made by machine for cars.  The results of these experiments were very entertaining, but we will not spoil and consider everything in order. <br><br><h3>  Experimental results </h3><br><h2>  Experiment number 1: fooling images with incorrect tags </h2><br>  In the first experiment, 48 duplicate images were used, created by an algorithm to counteract the recognition system based on the SNA called AlexNet.  This system has classified these images as ‚Äúgear wheel‚Äù and ‚Äúdonut‚Äù ( <b>2a</b> ). <br><br><img src="https://habrastorage.org/webt/fg/yt/0h/fgyt0hewtb9akm-4nlbb-movqxo.jpeg"><br>  <i>Image number 2</i> <br><br>  During each attempt, the test person, of whom there were 200, saw one image fool and two tags, i.e.  classification labels: system SNS label and random from other 47 images.  The subjects had to choose the label that was created by the machine. <br><br>  As a result, most of the subjects chose to choose the label created by the machine, rather than the label of the malicious algorithm.  Classification accuracy, i.e.  the degree of agreement of the subject with the machine was 74%.  Statistically, 98% of the subjects chose machine labels at a level higher than statistical randomness ( <b>2d</b> , "% of subjects who agreed with the machine").  94% of the images showed a very high human-machine matching, that is, out of 48, only 3 images were classified by people differently from the machine. <br><br>  Thus, the subjects showed that a person is able to separate the real image and the one who is fooling, that is, to act according to a program based on the SNA. <br><br><h2>  Experiment number 2: the first choice against the second </h2><br>  The researchers wondered if the subjects were able to recognize the images so well and separate them from the erroneous marks and the dupes?  Probably, the subjects marked the orange-yellow ring as a ‚Äúbagel‚Äù, because in reality a bagel of precisely this shape and approximately the same color.  Associations and intuitive choice based on experience and knowledge could help a person to recognize. <br><br>  To check this, the random tag was replaced with the one chosen by the machine as the second possible classification option.  For example, AlexNet classified the image of the orange-yellow ring as a ‚Äúbagel‚Äù, and the second version of this program was a ‚Äúpretzel‚Äù. <br><br>  The subjects had a task to choose the first tag of the car or the one that occupied the second place, for all 48 images ( <b>2c</b> ). <br><br>  The graph in the center of image <b>2d</b> shows the results of this test: 91% of the subjects chose the first variant of the label, and the man-machine agreement level was 71%. <br><br><h2>  Experiment number 3: multithreaded classification </h2><br>  The above experiments are quite simple due to the fact that the subjects have a choice between two answer choices (a car tag and a random tag).  In fact, the machine in the process of image recognition goes through hundreds and even thousands of options for tags, before choosing the most suitable one. <br><br>  In this test, all the labels for 48 images were immediately in front of the subjects.  They had to choose from this set the most suitable for each image. <br><br>  As a result, 88% of the subjects chose exactly those tags as the machine, and the degree of agreement was 79%.  Curious is the fact that even when choosing the wrong label that the car chose, subjects in 63% of such cases chose one of the 5 most top marks.  That is, the car all the tags are arranged in a list from the most suitable to the most inappropriate (exaggerated example: "donut", "pretzel", "rubber ring", "tire", etc. up to "hawk in the night sky" ). <br><br><h2>  Experiment number 3b: "what is it?" </h2><br>  In this test, scientists have slightly changed the rules.  Instead of asking to "guess" which machine the machine would choose for a particular image, the subjects simply asked what they saw in front of them. <br><br>  Recognition systems based on convolutional neural networks select the appropriate label for a particular image.  This is a fairly clear and logical process.  In this test, the subjects exhibit intuitive thinking. <br><br>  As a result, 90% of the subjects chose a label that the machine also chose.  The level of human-machine matching among the images was 81%. <br><br><h2>  Experiment No. 4: TV static noise </h2><br>  Scientists note that in previous experiments, the images, although unusual, have distinct features that can push test subjects to the correct (or wrong) choice of label.  For example, the image ‚Äúbaseball ball‚Äù is not a ball, but there are lines and colors on it that are present on a real baseball ball.  This is a bright distinctive feature.  But if the image does not have such features, but is essentially a static noise, can a person recognize at least something on it?  That was what was decided to check. <br><br><img src="https://habrastorage.org/webt/je/9o/sm/je9osmgxp99wmexrvqmzndviqpk.jpeg"><br>  <i>Image number 3a</i> <br><br>  In this test, before the subjects there were 8 images with statics, which are recognized by the SNA system as a specific object (for example, a bird charge).  Also in front of the subjects was a label and normal images related to it (8 images of static, 1 charge label and 5 photos of this bird).  The subject had to choose 1 out of 8 static images that best fit one mark or another. <br><br>  You can check yourself.  Above, you see an example of such a test.  Which of the three images best fits the charge label and why? <br><br>  81% of subjects chose the label that the machine chose.  In this case, 75% of the images were labeled by the subjects with the most appropriate label in the opinion of the machine (from a number of variants, which we have already talked about earlier). <br><br>  For this particular test, you may have questions, like me.  The fact is that in the suggested images of statics (above) I, personally, see three distinct features that distinguish them from each other.  And only in one image this feature strongly resembles that very charge (I think you understand exactly which image of the three).  Therefore, my personal and very subjective opinion is that such a test is not particularly revealing.  Although perhaps among the other options, static images were really indistinguishable and unrecognizable. <br><br><h2>  Experiment number 5: "doubtful" numbers </h2><br>  The above tests were based on images that cannot be immediately and fully classified without a drop of doubt as a particular object.  There is always a bit of doubt.  Stupid images are quite straightforward in their business - to spoil the image beyond recognition.  But there is a second type of malicious algorithms that add (or remove) only a small detail in the image, which can completely disrupt the recognition procedure by the SNS system.  Add a few pixels, and the number 6 magically turns into the number 5 ( <b>1c</b> ). <br><br>  Scientists consider such algorithms among the most dangerous.  You can slightly change the image-tag, and the unmanned vehicle incorrectly considers the sign of the speed limit (for example, 75 instead of 45), which can lead to sad consequences. <br><br><img src="https://habrastorage.org/webt/9k/wr/h9/9kwrh9bcsqbqyolfpdgc1crqcfg.jpeg"><br>  <i>Image number 3b</i> <br><br>  In this test, scientists suggested that subjects choose the wrong answer, but rather the wrong one.  In the test, 100 digitally modified images were used by the malicious algorithm (the SAN system LeNet changed their classification, that is, the malicious algorithm worked successfully).  The subjects had to say what number they thought the car saw.  As expected, 89% of the subjects successfully passed this test. <br><br><h2>  Experiment number 6: photos and localized "distortion" </h2><br>  Scientists note that not only object recognition systems are developed, but also malicious algorithms that prevent them from doing so.  Previously, in order for the image to be classified incorrectly, it was necessary to distort (change, delete, damage, etc.) 14% of all pixels in the target image.  Now this figure has become much smaller.  It is enough to add a small image inside the target and the classification will be broken. <br><br><img src="https://habrastorage.org/webt/6i/sz/1p/6isz1pvm_j-zzxt1mkeq2krmlts.jpeg"><br>  <i>Image number 4</i> <br><br>  In this test, a rather new malicious algorithm LaVAN was used, which places a small image localized at one point on the target photo.  As a result, the object recognition system can recognize a subway train as a milk can ( <b>4a</b> ).  The most significant features of this algorithm are just a small fraction of the damaged pixels (only 2%) of the target image and no need to distort it in whole or the main (most significant) part of it. <br><br>  In the test, 22 images damaged by LaVAN were used (the Inception V3 recognition system SNS was successfully hacked by this algorithm).  The subjects had to classify the malicious insert in the photo.  87% of the subjects were able to successfully do this. <br><br><h2>  Experiment number 7: three-dimensional objects </h2><br>  The images we saw before are two-dimensional, like any photo, picture or newspaper clipping.  Most malicious algorithms successfully manipulate such images.  However, these pests can work only under certain conditions, that is, they have a number of restrictions: <br><br><ul><li>  complexity: only 2D images; </li><li>  practical application: malicious changes are possible only on systems that read the resulting digital images, and not images from sensors and sensors; </li><li>  resilience: the malicious attack loses its power if the two-dimensional image is rotated (resize, crop, change sharpness, etc.); </li><li>  person: we see the world and objects around us in 3D from different angles, lighting, and not in the form of two-dimensional digital images taken from a single angle. </li></ul><br><br>  But, as we know, progress has not bypassed malicious algorithms.  Among them appeared the one who is capable of not only distorting two-dimensional images, but also three-dimensional ones too, which leads to an incorrect classification by the object recognition system.  When using software for three-dimensional graphics, such an algorithm is misleading classifiers based on the SNA (in this case, the program Inception V3) from different distances and viewing angles.  The most amazing thing is that such fooling 3D images can be printed on the corresponding printer, i.e.  to create a real physical object, and the object recognition system will still mistakenly classify it (for example, an orange as an electric drill).  And all due to minor changes in the texture of the target image ( <b>4b</b> ). <br><br>  For the object recognition system, such a malicious algorithm is a serious adversary.  But man is not a car, he sees and thinks differently.  In this test, before the subjects there were images of three-dimensional objects, in which there were the above-described changes in texture, from three angles.  Also, the subjects were given the label correct and erroneous.  They had to determine which labels are correct, which are not, and why, i.e.  Do subjects see changes in textures on images. <br><br>  As a result, 83% of subjects successfully coped with the task. <br><br>  For more detailed acquaintance with the nuances of the study I strongly recommend to look into the <a href="https://www.nature.com/articles/s41467-019-08931-6.pdf">report of scientists</a> . <br><br>  And by <a href="https://osf.io/uknbh/">this link</a> you will find the image, data and code files that were used in the study. <br><br><h3>  Epilogue </h3><br>  The work carried out gave scientists the opportunity to make a simple and fairly obvious conclusion - human intuition can be a source of very important data and a tool in making the right decision and / or perception of information.  A person is able to intuitively understand how the object recognition system will behave, which labels it will choose and why. <br><br>  The reasons why it is easier for a person to see a real image and recognize it correctly is somewhat.  The most obvious is the method of obtaining information: the machine receives the image in digital form, and the person sees it with his own eyes.  For a machine, a picture is a data set, by making changes to which one can distort its classification.  For us, the image of a subway train will always be a subway train, and not a can of milk, because we see it. <br><br>  Scientists also emphasize that such tests are difficult to evaluate, because a person is not a machine, and a machine is not a person.  For example, the researchers talk about the dough with the "donut" and "wheel".  These images are similar to "donut" and "wheel", because the recognition system so classifies them.  A person sees that they are similar to the "donut" and "wheel", but they are not.  This is the fundamental difference in the perception of visual information between a person and a program. <br><br>  Thank you for your attention, remain curious and have a good working week, guys. <br><br>  Thank you for staying with us.  Do you like our articles?  Want to see more interesting materials?  Support us by placing an order or recommending to friends, <b>30% discount for Habr's users on a unique analogue of the entry-level servers that we invented for you:</b> <a href="https://habr.com/company/ua-hosting/blog/347386/">The whole truth about VPS (KVM) E5-2650 v4 (6 Cores) 10GB DDR4 240GB SSD 1Gbps from $ 20 or how to share the server?</a>  (Options are available with RAID1 and RAID10, up to 24 cores and up to 40GB DDR4). <br><br>  <b>VPS (KVM) E5-2650 v4 (6 Cores) 10GB DDR4 240GB SSD 1Gbps before summer for free</b> if you pay for a period of six months, you can order <a href="https://ua-hosting.company/vpsnl">here</a> . <br><br>  <b>Dell R730xd 2 times cheaper?</b>  Only we have <b><a href="https://ua-hosting.company/serversnl">2 x Intel Dodeca-Core Xeon E5-2650v4 128GB DDR4 6x480GB SSD 1Gbps 100 TV from $ 249</a> in the Netherlands and the USA!</b>  Read about <a href="https://habr.com/company/ua-hosting/blog/329618/">How to build an infrastructure building.</a>  <a href="https://habr.com/company/ua-hosting/blog/329618/">class c using servers Dell R730xd E5-2650 v4 worth 9000 euros for a penny?</a> </div><p>Source: <a href="https://habr.com/ru/post/445372/">https://habr.com/ru/post/445372/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445360/index.html">5 typical tasks for interviews on JavaScript: analysis and solutions</a></li>
<li><a href="../445362/index.html">The book "Distributed Systems. Design Patterns ¬ª</a></li>
<li><a href="../445366/index.html">How to speed up encryption in accordance with GOST 28147-89 on a Baikal-T1 processor due to a SIMD unit</a></li>
<li><a href="../445368/index.html">Load testing games with a couple of hundreds of thousands of virtual users</a></li>
<li><a href="../445370/index.html">TSDB Analysis in Prometheus 2</a></li>
<li><a href="../445378/index.html">Labyrinths: classification, generation, search for solutions</a></li>
<li><a href="../445380/index.html">Modern PHP is beautiful and productive.</a></li>
<li><a href="../445384/index.html">Mission "Chang'e-4" - scientific equipment on the landing module and the satellite transponder</a></li>
<li><a href="../445390/index.html">Idea normal person or why we chose Monaco</a></li>
<li><a href="../445392/index.html">MyTarget Dynamic Remarketing: Non-Personal Product Tips</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>