<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Airflow is a tool to conveniently and quickly develop and maintain batch data processing processes.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! In this article, I want to talk about one great tool for developing batch data processing processes, for example, in an enterprise DWH infra...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Airflow is a tool to conveniently and quickly develop and maintain batch data processing processes.</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/getpro/habr/post_images/2f7/ef3/ac4/2f7ef3ac4894959ce6bf8e3e7cbbe373.jpg" alt="image"></p><br><p>  Hi, Habr!  In this article, I want to talk about one great tool for developing batch data processing processes, for example, in an enterprise DWH infrastructure or your DataLake.  It will be a question of Apache Airflow (further Airflow).  It is unfairly deprived of attention on Habr√©, and in the main part I will try to convince you that at least on Airflow you should look when choosing a scheduler for your ETL / ELT processes. </p><br><p>  Earlier, I wrote a series of articles on the topic of DWH, when I worked at Tinkoff Bank.  Now I have become part of the Mail.Ru Group team and am developing a platform for analyzing data on the game direction.  Actually, as news and interesting solutions appear, we and the team will talk here about our platform for data analytics. </p><a name="habracut"></a><br><h2 id="prolog">  Prologue </h2><br><p>  So, let's begin.  What is Airflow?  This is a library (or a <a href="https://github.com/apache/incubator-airflow">set of libraries</a> ) for developing, planning and monitoring workflows.  The main feature of Airflow: for describing (developing) processes, Python code is used.  This results in a lot of advantages for organizing your project and development: in fact, your (for example) ETL project is just a Python project, and you can organize it as you like, taking into account the particular infrastructure, team size and other requirements.  Instrumental everything is simple.  Use, for example, PyCharm + Git.  It is beautiful and very comfortable! </p><br><p>  Now consider the main essence of Airflow.  Having understood their essence and purpose, you optimally organize the process architecture.  Perhaps the main entity is the Directed Acyclic Graph (hereinafter referred to as DAG). </p><br><h2 id="dag">  DAG </h2><br><p>  DAG is a certain semantic association of your tasks that you want to perform in a strictly defined sequence according to a specific schedule.  Airflow provides a convenient web-interface for working with DAGs and other entities: </p><br><p><img src="https://habrastorage.org/webt/59/d6/03/59d603d6ae55c528277668.png"></p><br><p>  DAG might look like this: </p><br><p><img src="https://habrastorage.org/webt/59/d6/03/59d603d6bbc66112650773.png"></p><br><p>  The developer, designing DAG, lays a set of operators on which tasks will be built inside DAG'a.  Here we come to another important entity: Airflow Operator. </p><br><h2 id="operatory">  Operators </h2><br><p>  An operator is an entity on the basis of which task instances are created, which describes what will happen during the execution of an instance of a task.  <a href="https://github.com/apache/incubator-airflow/releases">Airflow releases from GitHub</a> already contain a set of operators ready to use.  Examples: </p><br><ul><li>  BashOperator - an operator to execute a bash command. </li><li>  PythonOperator is an operator for calling Python code. </li><li>  EmailOperator is the operator for sending email. </li><li>  HTTPOperator is an operator for working with http requests. </li><li>  SqlOperator is an operator for executing SQL code. </li><li>  Sensor - the operator of the event wait (the desired time, the appearance of the desired file, the string in the database, the response from the API - and so on, etc.). </li></ul><br><p>  There are more specific operators: DockerOperator, HiveOperator, S3FileTransferOperator, PrestoToMysqlOperator, SlackOperator. </p><br><p>  You can also develop operators, focusing on their features, and use them in the project.  For example, we created MongoDBToHiveViaHdfsTransfer, an operator for exporting documents from MongoDB to Hive, and several operators for working with <a href="https://clickhouse.yandex/">ClickHouse</a> : CHLoadFromHiveOperator and CHTableLoaderOperator.  As a matter of fact, as soon as a frequently used code arises in a project, built on basic operators, you can think about collecting it into a new operator.  This will simplify further development, and you will replenish your library of operators in the project. </p><br><p>  Further, all these instances of the tasks need to be performed, and now it will be a question of the scheduler. </p><br><h2 id="planirovschik">  Scheduler </h2><br><p>  Airflow Task Scheduler is built on <a href="http://www.celeryproject.org/">Celery</a> .  Celery is a Python library that allows you to organize a queue plus asynchronous and distributed execution of tasks.  From the Airflow side, all tasks are divided into pools.  Pools are created manually.  As a rule, their goal is to limit the workload with the source or to type tasks inside DWH.  Pools can be controlled via the web interface: </p><br><p><img src="https://habrastorage.org/webt/59/d6/03/59d603d6d5fa2858230533.png"></p><br><p>  Each pool has a limit on the number of slots.  When creating a DAG, it is given a pool: </p><br><pre><code class="hljs pgsql">ALERT_MAILS = Variable.<span class="hljs-keyword"><span class="hljs-keyword">get</span></span>("gv_mail_admin_dwh") DAG_NAME = <span class="hljs-string"><span class="hljs-string">'dma_load'</span></span> OWNER = <span class="hljs-string"><span class="hljs-string">'Vasya Pupkin'</span></span> DEPENDS_ON_PAST = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> EMAIL_ON_FAILURE = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> EMAIL_ON_RETRY = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> RETRIES = <span class="hljs-type"><span class="hljs-type">int</span></span>(Variable.<span class="hljs-keyword"><span class="hljs-keyword">get</span></span>(<span class="hljs-string"><span class="hljs-string">'gv_dag_retries'</span></span>)) POOL = <span class="hljs-string"><span class="hljs-string">'dma_pool'</span></span> PRIORITY_WEIGHT = <span class="hljs-number"><span class="hljs-number">10</span></span> start_dt = datetime.today() - timedelta(<span class="hljs-number"><span class="hljs-number">1</span></span>) start_dt = datetime(start_dt.year, start_dt.month, start_dt.day) default_args = { <span class="hljs-string"><span class="hljs-string">'owner'</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">OWNER</span></span>, <span class="hljs-string"><span class="hljs-string">'depends_on_past'</span></span>: DEPENDS_ON_PAST, <span class="hljs-string"><span class="hljs-string">'start_date'</span></span>: start_dt, <span class="hljs-string"><span class="hljs-string">'email'</span></span>: ALERT_MAILS, <span class="hljs-string"><span class="hljs-string">'email_on_failure'</span></span>: EMAIL_ON_FAILURE, <span class="hljs-string"><span class="hljs-string">'email_on_retry'</span></span>: EMAIL_ON_RETRY, <span class="hljs-string"><span class="hljs-string">'retries'</span></span>: RETRIES, <span class="hljs-string"><span class="hljs-string">'pool'</span></span>: POOL, <span class="hljs-string"><span class="hljs-string">'priority_weight'</span></span>: PRIORITY_WEIGHT } dag = DAG(DAG_NAME, default_args=default_args) dag.doc_md = __doc__</code> </pre> <br><p>  The pool defined at the DAG level can be overridden at the task level. <br>  A separate process, Scheduler, is responsible for scheduling all tasks in Airflow.  Actually, Scheduler deals with all the mechanics of setting tasks for execution.  The task, before getting into execution, goes through several stages: </p><br><ol><li>  In DAG, the previous tasks are completed, a new one can be queued. </li><li>  The queue is sorted according to the priority of tasks (priorities can also be controlled), and if there is a free slot in the pool, the task can be taken to work. </li><li>  If there is a free worker celery, the task is sent to it;  the work that you have programmed in the problem begins, using one or another operator. </li></ol><br><p>  Simple enough. </p><br><p>  Scheduler runs on the set of all DAGs and all tasks within DAGs. </p><br><p>  In order for Scheduler to start working with DAG, DAG needs to set a schedule: </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">dag</span></span> = DAG(DAG_NAME, default_args=default_args, schedule_interval=<span class="hljs-string"><span class="hljs-string">'</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">@hourly</span></span></span><span class="hljs-string">'</span></span>)</code> </pre> <br><p>  There is a set of ready-made presets: <code>@once</code> , <code>@hourly</code> , <code>@daily</code> , <code>@weekly</code> , <code>@monthly</code> , <code>@yearly</code> . </p><br><p>  You can also use cron expressions: </p><br><pre> <code class="hljs lisp">dag = DAG(<span class="hljs-name"><span class="hljs-name">DAG_NAME</span></span>, default_args=default_args, schedule_interval='*/10 * * * *')</code> </pre> <br><h2 id="execution-date">  Execution Date </h2><br><p>  To understand how Airflow works, it is important to understand what Execution Date is for DAG.  In Airflow, DAG has a Execution Date dimension, i.e., depending on the DAG work schedule, task instances are created for each Execution Date.  And for each Execution Date, tasks can be executed repeatedly - or, for example, the DAG can work simultaneously on several Execution Date.  This is graphically displayed here: </p><br><p><img src="https://habrastorage.org/webt/59/d6/03/59d603d70b4e9720285070.png"></p><br><p>  Unfortunately (and perhaps, fortunately: it depends on the situation), if the implementation of the problem is being corrected in DAG, then the execution in the previous Execution Date will go after adjustments.  This is good if you need to recalculate the data in past periods with a new algorithm, but badly, because the reproducibility of the result is lost (of course, no one bothers to return the necessary version of the source code from Git and to calculate what you need, as needed). </p><br><h2 id="generaciya-zadach">  Task generation </h2><br><p>  The implementation of DAG is Python code, so we have a very convenient way to reduce the amount of code when working, for example, with shardirovannyh sources.  Let you have three MySQL shards as your source, you need to slag into each one and pick up some data.  And independently and in parallel.  Python code in DAG can look like this: </p><br><pre> <code class="hljs python">connection_list = lv.get(<span class="hljs-string"><span class="hljs-string">'connection_list'</span></span>) export_profiles_sql = <span class="hljs-string"><span class="hljs-string">''' SELECT id, user_id, nickname, gender, {{params.shard_id}} as shard_id FROM profiles '''</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> conn_id <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> connection_list: export_profiles = SqlToHiveViaHdfsTransfer( task_id=<span class="hljs-string"><span class="hljs-string">'export_profiles_from_'</span></span> + conn_id, sql=export_profiles_sql, hive_table=<span class="hljs-string"><span class="hljs-string">'stg.profiles'</span></span>, overwrite=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, tmpdir=<span class="hljs-string"><span class="hljs-string">'/data/tmp'</span></span>, conn_id=conn_id, params={<span class="hljs-string"><span class="hljs-string">'shard_id'</span></span>: conn_id[<span class="hljs-number"><span class="hljs-number">-1</span></span>:], }, compress=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, dag=dag ) export_profiles.set_upstream(exec_truncate_stg) export_profiles.set_downstream(load_profiles)</code> </pre> <br><p>  DAG is obtained as follows: </p><br><p><img src="https://habrastorage.org/webt/59/d6/03/59d603d7051c9579606120.png"></p><br><p>  In this case, you can add or remove the shard by simply adjusting the setting and updating the DAG.  Conveniently! </p><br><p>  You can also use more complex code generation, for example, work with sources in the form of a database or describe a table structure, algorithm for working with a table and taking into account the features of the DWH infrastructure, generate the process of loading N tables to you in the repository.  Or, for example, working with an API that does not support working with a parameter in the form of a list, you can generate N tasks in the DAG for this list, limit the parallelism of requests in the API to a pool and retrieve the necessary data from the API.  Flexibly! </p><br><h2 id="repozitoriy">  Repository </h2><br><p>  Airflow has its own backend repository, a database (maybe MySQL or Postgres, we have Postgres) that stores the status of tasks, DAGs, connection settings, global variables, etc., etc. say that the repository in Airflow is very simple (about 20 tables) and convenient if you want to build your own process over it.  I remember 100500 tables in the Informatica repository, which had to be consumed for a long time before understanding how to build a query. </p><br><h2 id="monitoring">  Monitoring </h2><br><p>  Given the simplicity of the repository, you can build yourself a convenient process for monitoring tasks.  We use a notebook in Zeppelin, where we look at the status of the tasks: </p><br><p><img src="https://habrastorage.org/webt/59/d6/03/59d603d735dea232372208.png"></p><br><p>  This could be the Airflow web interface itself: </p><br><p><img src="https://habrastorage.org/webt/59/d6/03/59d603d763d01142932907.png"></p><br><p>  The Airflow code is open, so we added an alert to the Telegram.  Each running instance of the task, if an error occurs, spam to the group in Telegram, where the entire development and support team consists. </p><br><p>  We get a quick response via Telegram (if such is required), through Zeppelin - a general picture of the tasks in Airflow. </p><br><h2 id="itogo">  Total </h2><br><p>  Airflow is primarily open source, and no need to wait for miracles from it.  Be prepared to spend time and energy building a working solution.  A goal from the category of achievable, believe me, it's worth it.  The speed of development, flexibility, ease of adding new processes - you will like it.  Of course, you need to pay a lot of attention to the organization of the project, the stability of the Airflow itself: there are no miracles. </p><br><p>  Now we have Airflow working out <strong>about 6.5 thousand tasks</strong> daily.  By the nature they are quite different.  There are tasks for loading data into the main DWH from many different and very specific sources, there are tasks for calculating windows in the main DWH, there are tasks for publishing data into a fast DWH, there are many, many different tasks - and Airflow chews them all day after day.  Speaking in numbers, these are <strong>2.3 thousand</strong> ELT tasks of varying complexity within DWH (Hadoop), about <strong>2.5 hundred</strong> source <strong>databases</strong> , this is a team of <strong>4 ETL developers</strong> who are divided into ETL data processing in DWH and ELT data processing inside DWH and of course another <strong>admin</strong> who deals with the infrastructure of the service. </p><br><h2 id="plany-na-buduschee">  Future plans </h2><br><p>  The number of processes is inevitably growing, and the main thing we will be doing in terms of the Airflow infrastructure is scaling.  We want to build an Airflow cluster, select a pair of legs for Celery workers and make a duplicate of the head with the tasks planning process and repository. </p><br><h2 id="epilog">  Epilogue </h2><br><p>  This, of course, is not all that I would like to tell about Airflow, but I tried to highlight the main points.  Appetite comes with eating, try it - and you will like it :) </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/339392/">https://habr.com/ru/post/339392/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../339382/index.html">Say no to Electron! Writing a fast JavaFX desktop application</a></li>
<li><a href="../339384/index.html">How is CatBoost? Interviews with developers</a></li>
<li><a href="../339386/index.html">Creator of Open Data Science about Slack, xgboost and GPU</a></li>
<li><a href="../339388/index.html">"Back to the Future": Blockchain Today and Tomorrow</a></li>
<li><a href="../339390/index.html">Jenkins Task Management</a></li>
<li><a href="../339394/index.html">APDEX and 1C Performance Measurements</a></li>
<li><a href="../339396/index.html">iOS Safari 11 now can WebRTC</a></li>
<li><a href="../339398/index.html">Leading from big and smart data fields: SmartData 2017 Piter conference program</a></li>
<li><a href="../339400/index.html">Kotlin in Action released in Russian</a></li>
<li><a href="../339402/index.html">Unified Repository for Enterprise Architecture Management</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>