<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The risks of AI from LessWrong.com, part 1: an interview with Shane Legge from DeepMind</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello to readers Habrahabra! Recently, I learned that DeepMind, a company engaged in the development of artificial intelligence (AI), was acquired by ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The risks of AI from LessWrong.com, part 1: an interview with Shane Legge from DeepMind</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/b12/cab/99f/b12cab99f524e8e37ffe4d2a59e61fbf.jpg" alt="image" align="left">  <i>Hello to readers Habrahabra!</i>  <i>Recently, I learned that DeepMind, a company engaged in the development of artificial intelligence (AI), was acquired by Google for $ 500 million.</i>  <i>I began searching the internet for something about DeepMind's researchers, interviewing them and found Q &amp; A with Western experts, including Shane Legg from DeepMind, collected on the <a href="http://lesswrong.com/">LessWrong.com</a> website.</i>  <i>Below I will give the translation of the interview with Shane Legg, which seemed interesting to me.</i>  <i>The second part of the article will include interviews with ten other AI researchers.</i> <br><br>  Shane Legg is a Computer Science scientist and AI researcher who works on theoretical models of super intelligent machines (AIXI).  His PhD thesis ‚ÄúMachine Super Intelligence‚Äù was completed in 2008. He was awarded the ‚ÄúArtificial Intelligence‚Äù award and $ 10,000 from the Canadian Institute of Singularity.  The list of works of Shane can be found <a href="http://www.vetta.org/publications/">on the link</a> . <br><a name="habracut"></a><br><h2>  Interview with Shane Legge, July 17, 2011 </h2><br>  <a href="http://lesswrong.com/lw/691/qa_with_shane_legg_on_risks_from_ai/">Original article</a> <br><br>  Abbreviations used: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>CII</b> : human-level AI (hereinafter - human AI, CII, also Artificial Generic Intelligence, human-level AI) <br>  <b>SCHII</b> : <b>ai</b> super-human level <br><br>  <b>Q1</b> : <i>Assuming that PII research will not be suspended by a global catastrophe, how do you think, in what year will PII be developed with a probability of 10% / 50% / 90%?</i> <br><br>  Explanation: <br><br><pre>  P (CII (year) | no wars ‚àß no catastrophes ‚àß political and economic support) = 10% / 50% / 90% </pre><br><br>  <b>Shane Legg</b> : 2018, 2028, 2050. <br><br><br><br>  <b>Q2</b> : <i>With what probability the result of the development of AI will fail and absolutely fail?</i> <br><br>  Explanation: <br><pre> P (Terrible consequences | unsuccessful AI) =?  # Disappearance of humanity
 P (Absolutely terrible consequences | unsuccessful AI) =?  # Torment of mankind </pre><br>  <b>Shane Legg</b> : It depends a lot on how you define the terms.  True, it seems to me that the disappearance of humanity will nevertheless come, and technology will play its part in this.  ( <i>Probably, this means disappearing species of homo sapience, and not intelligent life in general - comment of the translator</i> ).  But there is a big difference when this happens - a year after the invention of the PII or a million years after it.  I don't know about probability.  Maybe 5%, maybe 50%.  I do not think anyone can give a good rating. <br><br>  If by torment you mean long torments, I think this is unlikely.  If a super intelligent machine wants to get rid of us, it will do it quite effectively.  I don‚Äôt think that we will voluntarily create a machine to maximize the suffering of humanity. <br><br><br><br>  <b>Q3</b> : <i>How likely is it that CII will rebuild itself to a massive superhuman intellectual machine (UCHI) within hours / days / &lt;5 years?</i> <br><br>  Explanation: <br><br><pre> P (CHII within hours | CII, running at person speed, 100 Gb network connectivity) =?
 P (CHII for days | CII, running at person speed, 100 Gb network connectivity) =?
 P (CHRII for &lt;5 years | CII, working at person speed, 100 Gb network connection) =?
</pre><br><br>  <b>Shane Legg</b> : ‚Äúthe rate of human level CIA‚Äù is a rather vague term.  Without a doubt, the car will be in something better than a man, and in something worse.  What it can be better at ‚Äî it can lead to a big difference in the results. <br><br>  In any case, I suspect that, by creating CII, the developers will independently scale it up to SMII, the machine itself will not do it.  After that, the car will be engaged in self-improvement, most likely. <br><br>  How quickly can this be done?  Perhaps very quickly, but it is also possible that there may never be ‚Äî there may be limitations of nonlinear complexity, which means that even theoretically optimal algorithms give a decreasing increase in intelligence when adding computing power. <br><br><br><br>  <b>Q4:</b> <i>Is it important to figure out and prove how to make AI friendly for us and our values ‚Äã‚Äã(safe) before solving the AI ‚Äã‚Äãproblem?</i> <br><br>  <b>Shane Legg</b> : I think that sounds like <a href="http://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25BE%25D0%25B1%25D0%25BB%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25BA%25D1%2583%25D1%2580%25D0%25B8%25D1%2586%25D1%258B_%25D0%25B8_%25D1%258F%25D0%25B9%25D1%2586%25D0%25B0">a chicken and an egg question</a> .  At the moment, we cannot reach a general opinion on what intelligence is and how to measure it, and we cannot even reach a general opinion on how the PII should work.  How can we make something safe if we don‚Äôt really know how it will work?  It may be useful to consider some theoretical questions.  But without a concrete and basic understanding of AI, I think that an abstract analysis of these issues will become very variable. <br><br><br><br>  <b>Q5</b> : <i>How much money is needed now to eliminate possible risks from AI (contributing to the achievement of your personal distant goals, for example, to survive this century), less / suffices / slightly more / much more / incommensurable more.</i> <br><br>  <b>Shane Legg</b> : A lot more.  However, as is the case with charity, it is unlikely that the cause of pumping problems with money, and indeed it can worsen the situation.  I really think that the <b>main question is not financial, it is a question of culture</b> (singled. Transl.).  I think that here in the society, changes will begin to occur when there is progress with the AI ‚Äã‚Äãand people will begin to take the possibility of the emergence of PII during their lifetimes more seriously.  I think up to this point a serious study of the risks of AI will remain optional. <br><br><br><br>  <b>Q6</b> : <i>Are AI risks prioritized than other existential risks, such as those related to advanced nanotechnology capabilities?</i>  <i>Explanation: which existential risks (such as the disappearance of humanity) are most likely to have the greatest negative impact on your personal long-term goals, if nothing is done to reduce the risk.</i> <br><br>  <b>Shane Legg</b> : For me, this is the number one risk in this century, next to it, but in second place is the creation of a biological pathogen. <br><br><br>  <b>Q7</b> : <i>What is the current level of risk awareness of AI, regarding the ideal?</i> <br><br>  <b>Shane Legg</b> : Too low ... but it‚Äôs a double-edged sword: by the time the main research community begins to worry about the problem, we may face some kind of arms race if large companies and / or governments secretly panic.  In this case, most likely, everything will be bad. <br><br><br>  <b>Q8</b> : <i>Can you say something about Milestone, after which we will probably reach CII within five years?</i> <br><br>  <b>Shane Legg</b> : This is a tough question!  When the machine can play a fairly extensive set of games, having a stream of perception at the entrance and exit, and will also be able to reuse the experience between different games.  I think in this case, we will begin to approach. <br><br>  July 17, 2011 <br><br><br><hr><br>  Translator‚Äôs note: I allowed myself to single out Shane Legg‚Äôs opinion on culture - he considers the problem of resources for the invention of AI less important than the issue of cultural exchange.  When I try to transfer this opinion to the Russian reality, I have different thoughts - negative, due to the fact that there is almost no cultural exchange within the whole, big Russia, and rather positive ones, related to the fact that developers who seriously evaluate their lives will either leave the country, or contribute to the development of the social sphere. </div><p>Source: <a href="https://habr.com/ru/post/213069/">https://habr.com/ru/post/213069/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../213055/index.html">Facebook knows who you fell in love with</a></li>
<li><a href="../213057/index.html">Air conditioner controller (fan)</a></li>
<li><a href="../213061/index.html">Prototype. From idea to trial lot</a></li>
<li><a href="../213063/index.html">Chinese video cameras and TCP: a bug or feature?</a></li>
<li><a href="../213065/index.html">Typing (defining properties) of an object by the hands of site users</a></li>
<li><a href="../213071/index.html">Telephony integration in distributed call centers</a></li>
<li><a href="../213073/index.html">Get out of the room</a></li>
<li><a href="../213075/index.html">Backup and Restore Graylog Server</a></li>
<li><a href="../213077/index.html">Rails: DRY style ajax validation</a></li>
<li><a href="../213081/index.html">BSA offers to earn up to $ 200,000, reporting on the use of pirated software</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>