<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Achievements in deep learning over the past year</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr. In my article, I will tell you what interesting things happened in the world of machine learning over the past year (mainly in Deep Learning...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Achievements in deep learning over the past year</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/web/375/0d8/9c3/3750d89c365c4fd58780fa4a9f9c97cf.jpg"></div><br><p>  Hi, Habr.  In my article, I will tell you what interesting things happened in the world of machine learning over the past year (mainly in Deep Learning).  A lot has happened, so I settled on the most, in my opinion, spectacular and / or significant achievements.  Technical aspects of improving network architectures are not given in the article.  Expand your horizons! </p><a name="habracut"></a><br><h2 id="1-tekst">  1. Text </h2><br><h3 id="11-google-neural-machine-translation">  1.1.  Google Neural Machine Translation </h3><br><p>  Almost a year ago, Google <a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html">announced the</a> launch of a new model for Google Translate.  The company described the network architecture in detail - the Recurrent Neural Network (RNN) - in its <a href="https://arxiv.org/abs/1609.08144">article</a> . </p><br><p><img src="https://habrastorage.org/files/5d3/d48/157/5d3d48157b02496a9e66078834ea5142.gif"></p><br><p>  The main result: a reduction in the lag from a person in terms of accuracy of translation by 55-85% (people rated on a 6-point scale)  It is difficult to reproduce the high results of this model without the huge dataset that Google has. </p><br><p><img src="https://habrastorage.org/web/214/e29/cae/214e29cae1264bb5807442b9fa1a4488.jpg" alt="image"></p><br><h3 id="12-peregovory-budet-sdelka">  1.2.  Conversation.  Will there be a deal? </h3><br><p>  You may have heard the stupid news that Facebook turned off its chat bot, which was out of control and made up its own language.  This chat bot company has created for negotiations.  His goal is to conduct text negotiations with another agent and reach a deal: how to divide items into two (books, hats ...).  Each agent has his own goal in the negotiations, the other does not know her.  Just leave the negotiations without a deal is impossible. </p><br><p>  For training, they collected datasets of human talks, trained a supervised recurrent network, then already trained agent using reinforcement learning (reinforcement learning) trained to talk with himself, putting a restriction: the similarity of a language to a human. </p><br><p>  Bot has learned one of the strategies of real negotiations - to show a fake interest in some aspects of the transaction, then to give way to them, getting benefit for their real purposes.  This is the first attempt to create such a negotiation bot, and quite successful. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b99/f8a/35f/b99f8a35f20f9b6cf72a380ae69ef66f.gif" alt="image"></p><br><p>  Details - in the <a href="https://code.facebook.com/posts/1686672014972296/deal-or-no-deal-training-ai-bots-to-negotiate">article</a> , the <a href="https://github.com/facebookresearch/end-to-end-negotiator">code</a> is laid out in open access. </p><br><p>  Of course, the news that the bot allegedly invented a language, inflated from scratch.  During training (when negotiating with the same agent), the restriction of text similarity to human was turned off, and the algorithm modified the interaction language.  Nothing unusual. </p><br><hr><br><p>  Over the past year, recurrent networks have been actively developed and used in many tasks and applications.  The architecture of recurrent networks has become much more complicated, however, in some areas, simple feedforward networks, <a href="https://www.microsoft.com/en-us/research/project/dssm/">DSSM,</a> also achieve similar results.  For example, Google for its mail feature Smart Reply has reached the same <a href="https://arxiv.org/pdf/1705.00652.pdf">quality</a> as with LSTM before.  And Yandex has <a href="https://habrahabr.ru/company/yandex/blog/314222/">launched a</a> new search engine based on such networks. </p><br><h2 id="2-rech">  2. Speech </h2><br><h3 id="21-wavenet-generiruyuschaya-model-dlya-neobrabotannogo-audio">  2.1.  WaveNet: generating model for unprocessed audio </h3><br><p>  DeepMind employees (a company known for its go-go bot, now owned by Google) told in their <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">article</a> about audio generation. </p><br><p>  In short, the researchers made the WaveRead auto-regression full-convolutional model based on previous approaches to image generation ( <a href="https://arxiv.org/abs/1601.06759">PixelRNN</a> and <a href="https://arxiv.org/abs/1606.05328">PixelCNN</a> ). </p><br><p><img src="https://habrastorage.org/web/6fc/5c9/839/6fc5c9839a7041ae9a62a43c8a0b7a23.gif"></p><br><p>  The network was trained end-to-end: text input, audio output.  The result is excellent, the difference with the person was reduced by 50%. </p><br><p><img src="https://habrastorage.org/web/5e7/077/9f7/5e70779f71d94bb7ab55bcd4921c56c9.png" alt="image"></p><br><p>  The main drawback of the network is poor performance, because, due to autoregression, sounds are generated sequentially; it takes about 1-2 minutes to create one second of audio. </p><br><p>  English: <a href="">example</a> </p><br><p>  If we remove the dependence of the network on the input text and leave only the dependence on the previously generated phoneme, the network will generate phoneme-like, but meaningless, human language. </p><br><p>  Voice generation: <a href="https://cloud.mail.ru/public/FkG2/Hjj2xNyhn">an example</a> </p><br><p>  The same model can be applied not only to speech, but also, for example, to the creation of music.  An example of audio generated by a model that was taught in piano dataset (again, without any dependence on the input data). </p><br><p>  <a href="https://cloud.mail.ru/public/DN2S/YPPfHf3qJ">Piano example</a> </p><br><p>  Details - in the <a href="https://arxiv.org/pdf/1609.03499.pdf">article</a> . </p><br><h3 id="22-chtenie-po-gubam">  2.2.  Lip reading </h3><br><p>  Another victory of machine learning over man;) This time is lip-reading. </p><br><p>  Google Deepmind, in collaboration with the University of Oxford, tells in the <a href="https://arxiv.org/pdf/1611.05358v1.pdf">article</a> "Lip Reading Sentences in the Wild" how their model, trained on a television dataset, could outperform a professional lips reader from the BBC channel. </p><br><p><img src="https://habrastorage.org/web/9c3/bf5/2b2/9c3bf52b291f479a8ebf6a7888ab0e59.png" alt="image"></p><br><p>  In 100 thousand offers with audio and video.  Model: LSTM on audio, CNN + LSTM on video, these two state-vectors are fed to the final LSTM, which generates the result (characters). </p><br><p><img src="https://habrastorage.org/web/cc5/35f/c19/cc535fc192ba43a5874554ce77878b63.jpg" alt="image"></p><br><p>  The training used different input data options: audio, video, audio + video, that is, the omnichannel model. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/c11/479/250/c11479250da8567ec2bae3d0e9ef2a0d.gif" alt="image"></p><br><h3 id="23-sinteziruya-obamu-sinhronizaciya-dvizheniya-gub-s-audio">  2.3.  Synthesizing Obama: Syncing Lips With Audio </h3><br><p><img src="https://habrastorage.org/web/2d4/400/fe4/2d4400fe4c1c46cb92c0ec0254ddaf18.png" alt="image"></p><br><p> The University of Washington <a href="http://grail.cs.washington.edu/projects/AudioToObama/siggraph17_obama.pdf">has done a serious job</a> generating the lips movement of the former US President Obama.  The choice fell on him, including because of the huge number of recordings of his performances on the network (17 hours of HD-video). </p><br><p>  It was not possible to manage with one network, it turned out too many artifacts.  Therefore, the authors of the article made several crutches (or tricks, if you like) to improve the texture and timing. </p><br><p><img src="https://habrastorage.org/web/703/950/1de/7039501dee2f4253b6e0eb7a83ae34ca.png" alt="image"></p><br><p>  The result is <a href="https://www.youtube.com/watch%3Fv%3D9Yq67CjDqvw%26t%3D0m25s">impressive</a> .  Soon it will be impossible to believe even the video with the president;) </p><br><h2 id="3-kompyuternoe-zrenie">  3. Computer vision </h2><br><h3 id="31-ocr-google-maps-i-street-view">  3.1.  OCR: Google Maps and Street View </h3><br><p>  In their <a href="https://research.googleblog.com/2017/05/updating-google-maps-with-deep-learning.html">post</a> and <a href="https://arxiv.org/abs/1704.03549">article</a> , the Google Brain team tells how they introduced a new OCR (Optical Character Recognition) engine into their Maps, which is used to identify street signs and store signs. </p><br><p><img src="https://habrastorage.org/web/511/816/642/5118166425674d21b241c32fd4e3b620.png" alt="image"></p><br><p><img src="https://habrastorage.org/web/3b3/874/1dc/3b38741dc0774fdc8c1f5b64ffa6cd31.png" alt="image"></p><br><p>  In the process of developing technology, the company has compiled a new <a href="https://github.com/tensorflow/models/tree/master/research/street">FSNS</a> (French Street Name Signs), which contains many complex cases. </p><br><p>  The network uses up to four of its photographs to recognize each character.  Using CNN, features are retrieved, weighted using spatial attention (pixel coordinates are taken into account), and the result is sent to LSTM. </p><br><p><img src="https://habrastorage.org/web/da5/09b/2e0/da509b2e09e84842b9e055dea8387dc1.jpg" alt="image"></p><br><p>  The authors apply the same approach to the task of recognizing the names of stores on signboards (there may be a lot of ‚Äúnoise‚Äù data, and the network itself must ‚Äúfocus‚Äù in the right places).  The algorithm was applied to 80 billion photos. </p><br><h3 id="32-visual-reasoning">  3.2.  Visual reasoning </h3><br><p>  There is such a type of tasks as Visual Reasoning, that is, the neural network should answer a question through a photo.  For example: ‚ÄúAre there rubber items of the same size as the yellow metal cylinder in the picture?‚Äù The question is really non-trivial, and until recently the problem was solved with an accuracy of only 68.5%. </p><br><p><img src="https://habrastorage.org/web/33a/185/9ec/33a1859ec86c45d48674de5423ea0200.png" alt="image"></p><br><p>  And again, the team from Deepmind achieved a breakthrough: they reached a super-human accuracy of 95.5% on the CLEVR data sheet. </p><br><p>  The network architecture is <a href="https://arxiv.org/pdf/1706.01427.pdf">quite interesting</a> : </p><br><ol><li>  For a textual question, with the help of pretrained LSTM, we get the embedding (representation) of the question. </li><li>  From the picture using CNN (there are four layers in all) we get feature maps (features that characterize the picture). </li><li>  Next, we create pairwise combinations of coordinate maps feature maps (yellow, blue, red in the picture below), add to each coordinate and text embedding. </li><li>  We run all these threes through another network, we summarize. </li><li><p>  We get the resulting representation through another feedforward network, which already gives a response on the softmax. </p><br><p><img src="https://habrastorage.org/web/8c7/500/ee7/8c7500ee7a7045a8a7e062382f08ed5f.jpg" alt="image"></p><br></li></ol><br><h3 id="33-pix2code">  3.3.  Pix2Code </h3><br><p>  An interesting application for neural networks was <a href="https://arxiv.org/abs/1705.07962">invented</a> by Uizard: from the screenshot from the interface designer to generate the layout code. </p><br><p><img src="https://habrastorage.org/web/095/b2c/8ee/095b2c8eee52433088981ecacd8de7a4.jpg" alt="image"></p><br><p>  Extremely useful application of neural networks, which can make life easier when developing software.  The authors claim that they obtained 77% accuracy.  It is clear that this is still a research work and there is no talk about combat use. </p><br><p>  There is no code and dataset in open source yet, but they promise to post it. </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/pqKeXkhFA3I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3 id="34-sketchrnn-uchim-mashinu-risovat">  3.4.  SketchRNN: teach the car to draw </h3><br><p>  You may have seen the <a href="https://quickdraw.withgoogle.com/">Quick Draw</a> page <a href="https://quickdraw.withgoogle.com/">!</a>  from Google with the call to draw sketches of various objects in 20 seconds.  The corporation collected this dataset in order to train the neural network to draw, as Google <a href="https://research.googleblog.com/2017/04/teaching-machines-to-draw.html">described</a> in its blog and <a href="https://arxiv.org/pdf/1704.03477.pdf">article</a> . </p><br><p><img src="https://habrastorage.org/web/673/566/bf9/673566bf90b74af49fe8c09b2b422997.jpg" alt="image"></p><br><p>  The assembled dataset consists of 70 thousand sketches, it was eventually <a href="https://github.com/googlecreativelab/quickdraw-dataset">laid out</a> in open access.  Sketches are not pictures, but detailed vector views of pictures (at which point the user clicked a pencil, let go, where to draw a line, and so on). </p><br><p>  Researchers trained Sequence-to-Sequence Variational Autoencoder (VAE) using RNN as the encoding / decoding mechanism. </p><br><p><img src="https://habrastorage.org/web/843/646/0dd/8436460dde8f4a9c8a6bd09e82772271.jpg" alt="image"></p><br><p>  As a result, as befits an autoencoder, the model gets a latent vector that characterizes the original image. </p><br><p><img src="https://habrastorage.org/web/49e/967/663/49e96766336a475aae5d7bd56dbea34c.jpg" alt="image"></p><br><p>  Since the decoder is able to extract a picture from this vector, it is possible to change it and get new sketches. </p><br><p><img src="https://habrastorage.org/web/c27/ae8/0ff/c27ae80ffbbd45809ae44df6f8698bf7.jpg" alt="image"></p><br><p>  And even perform vector arithmetic to build a kotosvina: </p><br><p><img src="https://habrastorage.org/web/46b/9cc/125/46b9cc12507e42ea9bb85274bd43aa51.png" alt="image"></p><br><h3 id="35-gan">  3.5.  Gan </h3><br><p>  One of the hottest topics in Deep Learning is Generative Adversarial Networks (GAN).  Most often this idea is used for working with images, so I will explain the concept on them. </p><br><p>  The essence of the idea is the competition of two networks - the Generator and the Discriminator.  The first network creates a picture, and the second one tries to understand whether the picture is real or generated. </p><br><p>  Schematically it looks like this: </p><br><p><img src="https://habrastorage.org/web/dce/80f/354/dce80f35406f48c492f0fe4cbf065758.png" alt="image"></p><br><p>  During training, the generator from a random vector (noise) generates an image and inputs the discriminator to the input that says if it is fake or not.  The real-time images from the dataset are also served to the discriminator. </p><br><p>  Teaching such a structure is often difficult because it is difficult to find the balance point of two networks, most often the discriminator defeats and learning stagnates.  However, the advantage of the system is that we can solve problems in which it is difficult for us to set a loss function (for example, improving the quality of a photo), we give it to the discriminator. </p><br><p>  A classic example of a GAN learning outcome is pictures of bedrooms or individuals. </p><br><p><img src="https://habrastorage.org/web/09a/d9a/7c9/09ad9a7c9feb4c62b6c9258eb7b13aa1.png" alt="image"></p><br><p><img src="https://habrastorage.org/web/e01/8bc/f77/e018bcf779184187aca68f3316c7d4f8.png" alt="image"></p><br><p>  Earlier, we considered autocoders (Sketch-RNN), which encode the source data into a latent view.  With the generator it turns out the same. </p><br><p>  Very clearly, the idea of ‚Äã‚Äãgenerating images by a vector is shown <a href="http://carpedm20.github.io/faces/">here</a> as an example of faces (you can change the vector and see which faces go out). </p><br><p><img src="https://habrastorage.org/web/4de/8c9/230/4de8c923037c48fd997b1f03a7d6978f.jpg" alt="image"></p><br><p>  All the same arithmetic works on the latent space: ‚Äúa man with glasses‚Äù minus ‚Äúman‚Äù plus ‚Äúwoman‚Äù is equal to ‚Äúa woman with glasses‚Äù. </p><br><p><img src="https://habrastorage.org/web/c59/638/9cb/c596389cb8d0420b98cdd10d3b01886c.png" alt="image"></p><br><h3 id="36-izmenenie-vozrasta-lica-s-pomoschyu-gan">  3.6.  Changing the age of the face using the GAN </h3><br><p>  If, during training, a controlled parameter is slipped into the latent vector, then during generation it can be changed and so controlled as needed in the picture.  This approach is called Conditional GAN. </p><br><p>  So did the authors of the <a href="https://arxiv.org/pdf/1702.01983.pdf">article</a> "Face Aging With Conditional Generative Adversarial Networks".  Having trained the car on dataset IMDB with a known age of actors, the researchers were able to change the age of the face. </p><br><p><img src="https://habrastorage.org/web/261/782/4f1/2617824f1933456b8bbf98f30e198ed3.jpg" alt="image"></p><br><h3 id="37-fotografii-professionalnogo-urovnya">  3.7.  Professional level photos </h3><br><p>  Google also <a href="https://research.googleblog.com/2017/07/using-deep-learning-to-create.html">found</a> another interesting application of GAN - the selection and improvement of photos.  GAN trained professional data on a dataset: a generator tries to improve bad photos (professionally taken and degraded with the help of special filters), and the discriminator - to distinguish between ‚Äúimproved‚Äù photos and real professional ones. </p><br><p>  The trained algorithm went through the Google Street View panoramas in search of the best tracks and got some photos of professional and semi-professional quality (according to the photographers' estimates). </p><br><p><img src="https://habrastorage.org/web/c2d/471/e7b/c2d471e7be7443fda5a4cbb26851c16f.jpg" alt="image"></p><br><p><img src="https://habrastorage.org/web/b03/53a/c5d/b0353ac5d0a14b2981deaff570ef61b0.jpg" alt="image"></p><br><h3 id="38-sintezirovanie-iz-tekstovogo-opisaniya-v-izobrazhenie">  3.8.  Synthesizing from text description to image </h3><br><p>  An impressive example of using GAN is the generation of pictures by text. </p><br><p><img src="https://habrastorage.org/web/25c/58e/74b/25c58e74bee34775bb277ad17fe36efd.png" alt="image"></p><br><p>  The authors of the <a href="https://arxiv.org/pdf/1605.05396.pdf">article</a> propose to submit embedding text to the input not only to the generator (conditional GAN), but also to the discriminator, so that it checks the conformity of the text to the picture.  In order for the discriminator to learn how to perform its function, additionally pairs were added to the training with incorrect text for real pictures. </p><br><p><img src="https://habrastorage.org/web/cc4/968/fe1/cc4968fe16ae4888b56649423c3870f5.jpg" alt="image"></p><br><h3 id="39-pix2pix">  3.9.  Pix2pix </h3><br><p>  One of the brightest <a href="https://arxiv.org/pdf/1611.07004.pdf">articles of the</a> end of 2016 is ‚ÄúImage-to-Image Translation with Conditional Adversarial Networks‚Äù by Berkeley AI Research (BAIR).  Researchers solved the problem of image-to-image generation, for example, when it is required to create a map from a satellite image or to sketch objects - their realistic texture. </p><br><p><img src="https://habrastorage.org/web/ad4/ff9/061/ad4ff9061a77450cbc32f1bec0f1816d.png" alt="image"></p><br><p>  This is another example of successful work conditional GAN, in this case, condition goes to the whole picture.  UNet, popular in image segmentation, was used as a generator architecture, and to combat blurred images, the PatchGAN classifier was used as a discriminator (the picture is cut into N patches, and the fake / real prediction goes for each of them separately). </p><br><p>  The authors have released an <a href="https://affinelayer.com/pixsrv/">online demo of</a> their networks, which aroused great interest among users. </p><br><p><img src="https://habrastorage.org/web/070/720/7ca/0707207caad64ae0a2852a6d84824e14.png" alt="image"></p><br><p>  <a href="https://github.com/phillipi/pix2pix">Source code</a> </p><br><h3 id="310-cyclegan">  3.10.  Cyclegan </h3><br><p>  To apply Pix2Pix, dataset is required with corresponding pairs of images from different domains.  In the case, for example, with maps to collect such dataset is not a problem.  But if you want to do something more complicated, like ‚Äútransfiguring‚Äù objects or styling, then pairs of objects cannot be found in principle.  Therefore, the authors of Pix2Pix decided to develop their idea and came up with <a href="https://arxiv.org/pdf/1703.10593.pdf">CycleGAN</a> for transferring images without specific pairs between different domains - Unpaired Image-to-Image Translation. </p><br><p><img src="https://habrastorage.org/web/d5e/35a/1f7/d5e35a1f769a413ebd03ec6a4be7308f.png" alt="image"></p><br><p>  The idea is as follows: we teach two pairs of the generator-discriminator from one domain to another and vice versa, while we require cycle consistency - after successive use of the generators, we should get an image similar to the original L1 loss.  Cyclical loss is required so that the generator does not start to simply transfer pictures of one domain to completely unrelated to the original image. </p><br><p><img src="https://habrastorage.org/web/0ae/31d/8bd/0ae31d8bd96f4da2964dd56c561d80e9.jpg" alt="image"></p><br><p>  This approach allows you to learn horse mapping -&gt; zebra. </p><br><p><img src="https://habrastorage.org/web/693/41a/396/69341a3967d44ab2bbcfeb288fdeac49.gif"></p><br><p>  Such transformations are unstable and often create unsuccessful options: </p><br><p><img src="https://habrastorage.org/web/d31/a6f/7ed/d31a6f7edf2548898ad7177b2bb19953.png" alt="image"></p><br><p>  <a href="https://github.com/junyanz/CycleGAN">Source code</a> </p><br><h3 id="311-razrabotka-molekul-v-onkologii">  3.11.  Development of molecules in oncology </h3><br><p>  Machine learning is now coming to medicine.  In addition to the recognition of ultrasound, MRI and diagnostics it can be used to search for new drugs to fight cancer. </p><br><p>  We have already written in detail about this study <a href="https://habrahabr.ru/company/mailru/blog/325908/">here</a> , so briefly: with the help of the Adversarial Auto Encoder (AAE), you can learn the latent representation of molecules and continue to look for new ones with it.  As a result, 69 molecules were found, half of which are used to fight cancer, the rest have serious potential. </p><br><p><img src="https://habrastorage.org/web/b97/f20/774/b97f2077469941709496d5bbe0a436ed.png" alt="image"></p><br><h3 id="312-adversarial-ataki">  3.12.  Adversarial attacks </h3><br><p>  The topic with adversarial attacks is being actively explored.  What it is?  Standard networks, trained, for example, on ImageNet, are completely unstable to add special noise to the classified picture.  In the example below, we see that the picture with the noise for the human eye remains almost unchanged, but the model goes crazy and predicts a completely different class. </p><br><p><img src="https://habrastorage.org/web/ed5/6a7/143/ed56a7143da04bc9a8d51ddd5322fae8.png" alt="image"></p><br><p>  Stability is achieved using, for example, the Fast Gradient Sign Method (FGSM): having access to the parameters of the model, you can <a href="">take</a> one or several gradient steps towards the desired class and change the original image. </p><br><p>  One of the tasks on <a href="https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack">Kaggle</a> for the upcoming NIPS is precisely related to this: participants are encouraged to create universal attacks / defenses that end up running all against all to determine the best. </p><br><p>  Why do we even need to investigate these attacks?  Firstly, if we want to protect our products, we can add noise to the captcha to prevent spammers from recognizing it automatically.  Secondly, algorithms are more and more involved in our lives - face recognition systems, unmanned vehicles.  In this case, attackers can use the disadvantages of the algorithms.  Here is an example when special glasses allow you to deceive the facial recognition system and ‚Äúintroduce yourself‚Äù as a different person.  So the model will need to be taught with regard to possible attacks. </p><br><p><img src="https://habrastorage.org/web/3c5/1b3/3f8/3c51b33f8d6542b39e8bd58f21b8c981.png" alt="image"></p><br><p>  Such manipulations with signs also do not allow to correctly recognize them. </p><br><p><img src="https://habrastorage.org/web/10f/55e/639/10f55e6398d8440aa725ebd292df0659.png" alt="image"></p><br><p>  <a href="https://www.kaggle.com/c/nips-2017-non-targeted-adversarial-attack/discussion/35840">A set of</a> articles from the organizers of the competition. </p><br><p>  Already written libraries for attacks: <a href="https://github.com/tensorflow/cleverhans">cleverhans</a> and <a href="https://github.com/bethgelab/foolbox">foolbox</a> . </p><br><h2 id="4-obuchenie-s-podkrepleniem">  4. Training with reinforcements </h2><br><p>  Reinforcement learning (RL), or reinforcement learning, is also now one of the most interesting and actively developing topics in machine learning. </p><br><p>  The essence of the approach is to learn the successful behavior of the agent in an environment that, when interacting, provides feedback (reward).  In general, through experience - just as people learn during life. </p><br><p><img src="https://habrastorage.org/web/717/1da/291/7171da291f1242819e12a697530cceb8.png" alt="image"></p><br><p>  RL is actively used in games, robots, system management (traffic, for example). </p><br><p>  Of course, everyone heard about AlphaGo's victories from DeepMind in the game of go over the best professionals.  The authors' article was <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">published</a> in Nature "Mastering the game of Go."  In training, the developers used RL: the bot played with itself to improve its strategies. </p><br><h3 id="41-obuchenie-s-podkrepleniem-s-nekontroliruemymi-vspomogatelnymi-zadachami">  4.1.  Reinforced training with unsupervised auxiliary tasks </h3><br><p>  In previous years, DeepMind learned using <a href="https://deepmind.com/research/dqn/">DQN</a> to play arcade games better than humans.  Now algorithms are taught to play more complex games like <a href="http://vizdoom.cs.put.edu.pl/">Doom</a> . </p><br><p>  Much attention is paid to accelerating learning, because the accumulation of agent experience in interaction with the environment requires many hours of training on modern GPUs. </p><br><p>  Deepmind in his blog <a href="https://deepmind.com/blog/reinforcement-learning-unsupervised-auxiliary-tasks/">says</a> that the introduction of additional loss'ov (auxiliary tasks, auxiliary tasks), such as the prediction of frame changes (pixel control), so that the agent better understands the consequences of actions, significantly speeds up training. </p><br><p><img src="https://habrastorage.org/web/8d6/0b2/d87/8d60b2d875ab4206a47bc2f1e19eb53e.gif"></p><br><p>  Learning outcomes: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Uz-zGYrYEjA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h3 id="42-obuchayuschiesya-roboty">  4.2.  Learning robots </h3><br><p>  OpenAI is actively exploring human agent training in a virtual environment, which is more secure for experiments than in real life;) </p><br><p>  In one of the <a href="https://blog.openai.com/robots-that-learn/">studies, the</a> team showed that one-shot learning is possible: a person shows in VR how to perform a specific task, and a single demonstration is enough for the algorithm to learn it and then reproduce it in real conditions. </p><br><p><img src="https://habrastorage.org/web/93f/ffb/158/93fffb1585f04216986d82f5c7bde42f.png" alt="image"></p><br><p>  Eh, if people were so easy;) </p><br><h3 id="43-obuchenie-na-chelovecheskih-predpochteniyah">  4.3.  Learning on human preferences </h3><br><p>  OpenAI and DeepMind work on the same topic.  The bottom line is this: the agent has a certain task, the algorithm provides the person with two possible solutions, and the person indicates which one is better.  The process is repeated iteratively, and the algorithm learned to solve the problem from a person for 900 bits of feedback (binary markup) from a person. </p><br><p><img src="https://habrastorage.org/web/e79/6c7/66a/e796c766aa3246ef96f38fae86aa4c94.png" alt="image"></p><br><p><img src="https://habrastorage.org/web/afc/d52/f83/afcd52f83280477483e4cad8a1c7379d.gif"></p><br><p>  As always, a person needs to be careful and think about what he teaches the car.  For example, the appraiser decided that the algorithm really wanted to take the object, but in fact he only <strong>imitated</strong> this action. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/9fb/fa6/e4f/9fbfa6e4fc5935ea334f25f6f8135fe0.gif" alt="image"></p><br><h3 id="44-dvizhenie-v-slozhnyh-okruzheniyah">  4.4.  Movement in difficult environments </h3><br><p>  Another <a href="https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/">study</a> from DeepMind.  To teach the robot complex behavior (walk / jump / ...), and even similar to human, you need to be very confused with the choice of the loss function, which will encourage the desired behavior.  But I would like the algorithm itself to learn complex behavior, relying on simple reward. </p><br><p>  The researchers managed to achieve this: they taught agents (body emulators) to perform complex actions by constructing a complex environment with obstacles and with a simple reward for the progress in movement. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1e5/6cb/dd9/1e56cbdd9381f7911d59ba8600ddda0b.gif" alt="image"></p><br><p>  Impressive <a href="https://www.youtube.com/watch%3Fv%3Dhx_bgoTF7bs">video</a> with results.  But it is much more fun with the superimposed sound;) </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/itACOKJHYmw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><hr><br><p>  Finally, I will give a <a href="https://github.com/openai/baselines">link</a> to the recently published RL learning algorithms from OpenAI.  Now you can use more advanced solutions than the already standard DQN. </p><br><h2 id="5-prochee">  5. Other </h2><br><h3 id="51-ohlazhdenie-data-centra">  5.1.  Cooling the data center </h3><br><p>  In July 2017, Google <a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/">told us</a> that it took advantage of the development of DeepMind in machine learning to reduce the energy costs of its data center. </p><br><p>  Based on information from thousands of sensors in the data center, Google developers have trained a network of neural networks to predict PUE (Power Usage Effectiveness) and more efficiently manage the data center.  This is an impressive and significant example of the practical application of ML. </p><br><p><img src="https://habrastorage.org/web/b83/cff/755/b83cff755d1446c58ab1d94a386106c2.png" alt="image"></p><br><h3 id="52-odna-model-na-vse-zadachi">  5.2.  One model for all tasks </h3><br><p>  As you know, the trained models are poorly transferred from task to task, for each task you have to train / retrain a specific model.  A small step towards the universality of models made Google Brain in its <a href="https://arxiv.org/abs/1706.05137">article</a> ‚ÄúOne Model To Learn Them All‚Äù. </p><br><p>  Researchers trained a model that performs eight tasks from different domains (text, speech, images).  For example, translation from different languages, text parsing, image and sound recognition. </p><br><p><img src="https://habrastorage.org/web/f33/8d8/41c/f338d841c329472b84ff03125dcce5c9.png" alt="image"></p><br><p>  To achieve this goal, we have made a complex network architecture with various blocks for processing different input data and generating a result.  The blocks for encoder / decoder are divided into three types: convolutional, attention, <a href="https://arxiv.org/abs/1701.06538">gated mixture of experts</a> (MoE). </p><br><p><img src="https://habrastorage.org/web/d37/25e/e58/d3725ee58cd64eac80aac267c0eaa681.png" alt="image"></p><br><p><img src="https://habrastorage.org/web/0e4/bb8/838/0e4bb8838b5247cfb91b60660af65a99.jpg" alt="image"></p><br><p>  The main results of the training: </p><br><ul><li>  almost perfect models were obtained (the authors did not tweak the hyperparameters); </li><li>  there is a transfer of knowledge between different domains, that is, on tasks with a large amount of data, the performance will be almost the same.  And on small tasks (for example, on parsing) it is better; </li><li>  the blocks needed for different tasks do not interfere with each other and sometimes even help, for example, MoE for Imagenet tasks. </li></ul><br><p>  By the way, this model <a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/multimodel.py">is</a> in tensor2tensor. </p><br><h3 id="53-obuchenie-na-imagenet-za-odin-chas">  5.3.  Imagenet training in one hour </h3><br><p>  In their post, Facebook employees told how their engineers were able to achieve training for the Resnet-50 model at Imagenet in just one hour.  True, this required a cluster of 256 GPUs (Tesla P100). </p><br><p>  For distributed learning, Gloo and Caffe2 were used.  To make the process effective, we had to adapt the learning strategy with a huge batch (8192 elements): averaging gradients, the warm-up phase, special learning rates, and the like.  <a href="https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf">More in the article</a> . </p><br><p>  As a result, it was possible to achieve an efficiency of 90% when scaling from 8 to 256 GPU.  Now, researchers from Facebook can experiment even faster, unlike ordinary mortals without such a cluster;) </p><br><h2 id="6-novosti">  6. News </h2><br><h3 id="61-bespilotnye-avtomobili">  6.1.  Unmanned vehicles </h3><br><p>  The sphere of unmanned vehicles is developing intensively, and the machines are actively tested in combat conditions.  Of the relatively recent events, we can mention the purchase of Intel's MobilEye, the <a href="https://www.theguardian.com/technology/2017/feb/25/uber-google-lawsuit-self-driving-car-threat-anthony-levandowski">scandal</a> around Uber and the technologies stolen by the ex-Google employee, the first <a href="https://www.theguardian.com/technology/2016/jun/30/tesla-autopilot-death-self-driving-car-elon-musk">death</a> of the autopilot and much more. </p><br><p>  I will note one point: Google <a href="https://waymo.com/">Waymo</a> <a href="https://www.macrumors.com/2017/04/25/waymo-self-driving-program-phoenix/">launches</a> beta program.  Google is a pioneer in this area, and it is assumed that their technology is very good, because the cars have already been driven over 3 million miles. </p><br><p>  Also, most recently, unmanned vehicles <a href="https://geektimes.ru/post/292743/">were allowed to</a> travel in all US states. </p><br><h3 id="62-zdravoohranenie">  6.2.  Health care </h3><br><p>  As I said, the modern ML is beginning to be introduced into medicine.  For example, Google is <a href="https://research.googleblog.com/2017/03/assisting-pathologists-in-detecting.html">working</a> with a medical center to help diagnosticians. </p><br><p><img src="https://habrastorage.org/web/4e6/beb/c6f/4e6bebc6fff241b785b30c7c561a355e.png" alt="image"></p><br><p>  Deepmind <a href="https://deepmind.com/applied/deepmind-health/">has</a> even <a href="https://deepmind.com/applied/deepmind-health/">created a</a> separate division. </p><br><p><img src="https://habrastorage.org/web/f30/189/522/f3018952286443acb645fff5a9d4beec.png" alt="image"></p><br><p>  This year, within the framework of the Data Science Bowl, a <a href="https://www.kaggle.com/c/data-science-bowl-2017">contest</a> was held for predicting lung cancer in a year on the basis of detailed images, the prize fund was one million dollars. </p><br><h3 id="63-investicii">  6.3.  Investments </h3><br><p>  Now they are investing a lot in ML, as before - in BigData. </p><br><p>  China is investing $ 150 billion in AI to become the world leader in the industry. </p><br><p>  For comparison, Baidu Research employs 1,300 people, and in the same FAIR (Facebook) - 80. At the last KDD, Alibaba employees talked about their parameter server <a href="http://www.kdd.org/kdd2017/papers/view/kunpeng-parameter-server-based-distributed-learning-systems-and-its-applica">KungPeng</a> , which runs on 100 billionths of samples with a trillion parameters, which ‚Äúbecomes an ordinary task‚Äù ( with). </p><br><p><img src="https://habrastorage.org/web/a8b/ee9/5c8/a8bee95c8f38495093c537460c851084.jpg" alt="image"></p><br><hr><br><p>  Draw conclusions, study ML.  Anyway, over time, all developers will use machine learning, which will become one of the competencies, as today - the ability to work with databases. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/338248/">https://habr.com/ru/post/338248/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../338236/index.html">ServiceNow Platform: Starter Kit</a></li>
<li><a href="../338238/index.html">Sales at the Unity Asset Store. Personal experience</a></li>
<li><a href="../338240/index.html">How to increase service indicators by 7 times in three months using HADI-cycles and prioritization of hypotheses</a></li>
<li><a href="../338242/index.html">How to start young mobile game developers from Russia [Part 3]</a></li>
<li><a href="../338246/index.html">Acronis design system. Part one. Unified Component Library</a></li>
<li><a href="../338250/index.html">Talk about Virtual Reality. Conversation number 2. Practical about virtuality</a></li>
<li><a href="../338254/index.html">learnopengl. Lesson 2.6 - Multiple Illumination Sources</a></li>
<li><a href="../338256/index.html">Recommendations for neutralizing threats related to the vulnerability CVE-2017-8759</a></li>
<li><a href="../338262/index.html">Superjob IT meetup. System Business Analysis</a></li>
<li><a href="../338264/index.html">We write for UEFI BIOS in Visual Studio. Part 1 - Deploying the development environment, compiling and running to debug</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>