<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Testing software RAIDs for NVMe devices using the SNIA technique</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A few months ago, in the course of working on the next project, the guys from our research lab conducted research on NVMe disks and software solutions...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Testing software RAIDs for NVMe devices using the SNIA technique</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/4j/ib/lz/4jiblzb0pamckpjzolpdiq1gr8s.jpeg"><br><br>  A few months ago, in the course of working on the next project, the guys from our research lab conducted research on NVMe disks and software solutions in order to find the best option for building a software array. <br><br>  The test results were surprisingly discouraging - the huge potential of the speed of NVMe-disks did not correspond at all to the demonstrated performance of the available software products. <br><a name="habracut"></a><br>  Our developers did not like it.  And they decided to write their product ... The product, which marketers later happily came up with the name RAIDIX ERA. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Today, more than a dozen companies produce servers adapted for using NVMe drives.  The market for products that support and develop this technology has great potential.  The G2M analyst report presents fairly convincing figures that convince us that this data transfer protocol will dominate in the near foreseeable future. <br><br><img src="https://habrastorage.org/webt/wd/f6/ol/wdf6olgkq9vam7sfinx9sb843ek.png"><br>  <i><font color="#99999">Chart from <a href="http://g2minc.com/g2m-research-nvme-ecosystem-market-sizing-report">the G2M report</a></font></i> <br><br>  Currently, Intel is one of the leaders in manufacturing NVMe components.  It was on their equipment that we carried out tests in order to evaluate the capabilities of existing software products for managing this ‚Äúinnovative‚Äù hardware. <br><br>  Together with our partner, Promobit (manufacturer of servers and data storage systems under the brand name BITBLAZE), we organized testing of Intel NVMe-drives and common software for managing such devices.  Testing was performed by the method of SNIA. <br><br>  In this article, we will share figures obtained during testing of Intel's NVMe hardware system, software arrays from MDRAID, Zvol over ZFS RAIDZ2 and, in fact, our new development. <br><br><h2>  Hardware configuration </h2><br>  The test platform is based on the Intel Server System R2224WFTZS server system.  It has 2 sockets for installing Intel Xeon Scalable processors and 12 memory channels (24 DIMMs in total) DDR 4 with frequencies up to 2666 MHz. <br><br>  More information about the server platform can be found <a href="https://ark.intel.com/products/89012/Intel-Server-System-R2224WFTZS">on the manufacturer's website</a> . <br><br>  All NVMe-drives connected via 3 backplane <a href="https://ark.intel.com/ru/products/124465/2U-8x2_5inch-SASNVMe-Hot-Swap-Backplane-F2U8X25S3PHS">F2U8X25S3PHS</a> . <br><br>  In total, we had 12 NVMe-disks INTEL SSDPD2MD800G4 with firmware CVEK6256004E1P0BGN. <br><br>  The server platform was equipped with two Intel¬Æ Xeon¬Æ Gold 6130 CPU @ 2.10GHz processors with Hyper-Threading enabled, allowing two threads to run from each core.  Thus, at the output we received 64 computational streams. <br><br><h2>  Preparation for testing </h2><br>  All tests in this article were performed in accordance with the specification of the SNIA SSS PTSe v 1.1 method.  In particular, preliminary preparation of the repository was carried out in order to get a stable and honest result. <br><br>  SNIA allows the user to set the parameters for the number of threads and the queue depth, so we set 64/32, possessing 64 computing threads on 32 cores. <br><br>  Each test was performed in 16 rounds in order to bring the system to a stable level of indicators and exclude random values. <br><br>  Before running the tests, we made a preliminary preparation of the system: <br><br><ol><li>  Installing kernel version 4.11 on CentOS 7.4. </li><li>  Shut down C-STATES and P-STATES. </li><li>  Run the tuned-adm utility and set the latency-performance profile. </li></ol><br>  Testing of each product and element took place in the following stages: <br><br>  Device preparation according to the SNIA specification (dependent and independent of the type of load). <br><br><ol><li>  IOps test in 4k, 8k, 16k, 32k, 64k, 128k, 1m blocks with variations of 0/100, 5/95, 35/65, 50/50, 65/35, 95/5, 100/0 read / write combinations . </li><li>  Tests of latency with 4k, 8k, 16k blocks with variations of 0/100, 65/35 and 100/0 read / write combinations.  The number of threads and the queue depth is 1-1.  Results are recorded in the form of the average and maximum value of delays. </li><li>  Bandwidth test (throughtput) with blocks of 128k and 1M, in 64 queues of 8 teams. </li></ol><br>  We started by testing performance, delays and throughput of the hardware platform.  This allowed us to assess the potential of the proposed equipment and compare it with the capabilities of the applied software solutions. <br><br><h2>  Test 1. Hardware Testing </h2><br>  To begin with, we decided to see what a single Intel DCM D3700 NVMe drive is capable of. <br>  In the specification, the manufacturer states the following performance parameters: <br><blockquote>  Random Read (100% Span) 450000 IOPS <br>  Random Write (100% Span) 88000 IOPS </blockquote><br><h3>  Test 1.1 One NVMe drive.  IOPS Test </h3><br>  Performance result (IOps) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R5% / W95% </th><th>  R35% / W65% </th><th>  R50% / W50% </th><th>  R65% / W35% </th><th>  R95% / W5% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  84017.8 </td><td>  91393.8 </td><td>  117271.6 </td><td>  133059.4 </td><td>  175086.8 </td><td>  281131.2 </td><td>  390969.2 </td></tr><tr><td>  8k </td><td>  42602.6 </td><td>  45735.8 </td><td>  58980.2 </td><td>  67321.4 </td><td>  101357.2 </td><td>  171316.8 </td><td>  216551.4 </td></tr><tr><td>  16k </td><td>  21618.8 </td><td>  22834.8 </td><td>  29703.6 </td><td>  33821.2 </td><td>  52552.6 </td><td>  89731.2 </td><td>  108347 </td></tr><tr><td>  32k </td><td>  10929.4 </td><td>  11322 </td><td>  14787 </td><td>  16811 </td><td>  26577.6 </td><td>  47185.2 </td><td>  50670.8 </td></tr><tr><td>  64k </td><td>  5494.4 </td><td>  5671.6 </td><td>  7342.6 </td><td>  8285.8 </td><td>  13130.2 </td><td>  23884 </td><td>  27249.2 </td></tr><tr><td>  128k </td><td>  2748.4 </td><td>  2805.2 </td><td>  3617.8 </td><td>  4295.2 </td><td>  6506.6 </td><td>  11997.6 </td><td>  13631 </td></tr><tr><td>  1m </td><td>  351.6 </td><td>  354.8 </td><td>  451.2 </td><td>  684.8 </td><td>  830.2 </td><td>  1574.4 </td><td>  1702.8 </td></tr></tbody></table><br>  Performance result (IOps) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/ja/gw/al/jagwaldo3cvyw5bxk9uxgeaqvtq.png"><br><br>  At this stage, we obtained results that very little fall short of the factory ones.  Most likely, NUMA played its role (the computer memory implementation scheme used in multiprocessor systems, when the access time to memory is determined by its location relative to the processor), but for now we will not pay attention to it. <br><br><h3>  Test 1.2 One NVMe-drive.  Delay tests </h3><br>  <b>Average response time</b> (ms) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R65% / W35% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  0.02719 </td><td>  0.072134 </td><td>  0.099402 </td></tr><tr><td>  8k </td><td>  0.029864 </td><td>  0.093092 </td><td>  0.121582 </td></tr><tr><td>  16k </td><td>  0.046726 </td><td>  0.137016 </td><td>  0.16405 </td></tr></tbody></table><br>  <b>Average response time</b> (ms) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/di/dg/cy/didgcy1xldxrxa8e9qec9cfri8y.png"><br><br>  <b>Maximum response time</b> (ms) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R65% / W35% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  6.9856 </td><td>  4.7147 </td><td>  1.5098 </td></tr><tr><td>  8k </td><td>  7.0004 </td><td>  4.3118 </td><td>  1.4086 </td></tr><tr><td>  16k </td><td>  7.0068 </td><td>  4.6445 </td><td>  1.1064 </td></tr></tbody></table><br>  <b>Maximum response time</b> (ms) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/j0/sp/1y/j0sp1ynjubpq8yfuikeiogvhzz0.png"><br><br><h3>  Test 1.3 Throughput </h3><br>  The final step is throughput assessment.  Here we have such indicators: <br><blockquote>  1MB sequential write - 634 MBps. <br>  1MB sequential read - 1707 MBps. <br>  128KB sequential write - 620 MBps. <br>  128KB sequential read - 1704 MBps. </blockquote><br>  Having dealt with a single disk, we proceed to the evaluation of the entire platform, which consists of 12 drives. <br><br><h3>  Test 1.4 System in 12 drives.  IOPS Test </h3><br>  It was a volitional decision to save time and show results only for working with block 4k, which is by far the most common and significant performance evaluation scenario. <br><br>  Performance result (IOps) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R5% / W95% </th><th>  R35% / W65% </th><th>  R50% / W50% </th><th>  R65% / W35% </th><th>  R95% / W5% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  1363078.6 </td><td>  1562345 </td><td>  1944105 </td><td>  2047612 </td><td>  2176476 </td><td>  3441311 </td><td>  4202364 </td></tr></tbody></table><br>  Performance result (IOps) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/lz/nb/za/lznbza47oi8rjwokg-eobx2rfnc.png"><br><br><h3>  Test 1.5 System in 12 drives.  Bandwidth tests </h3><br><blockquote>  1MB sequential write - 8612 MBps. <br>  1MB sequential read - 20481 MBps. <br>  128KB sequential write - 7500 MBps. <br>  128KB sequential read - 20400 MBps. </blockquote><br>  We will look at the obtained performance indicators of hardware at the end of the article, comparing them with the numbers of the software tested on it. <br><br><h2>  Test 2. MDRAID testing </h2><br>  When we talk about a software array, MDRAID comes to mind first.  Recall that this is a basic software RAID for Linux, which is distributed completely free of charge. <br><br>  Let's see how MDRAID will cope with the 12-disk system offered to it at the level of RAID 0. We all understand that building RAID 0 on 12 disks requires special courage, but now we need this level of array to demonstrate the maximum capabilities of this solution. <br><br><h3>  Test 2.1 MDRAID.  RAID 0. IOPS Test </h3><br>  Performance result (IOps) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R5% / W95% </th><th>  R35% / W65% </th><th>  R50% / W50% </th><th>  R65% / W35% </th><th>  R95% / W5% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  1010396 </td><td>  1049306.6 </td><td>  1312401.4 </td><td>  1459698.6 </td><td>  1932776.8 </td><td>  2692752.8 </td><td>  2963943.6 </td></tr><tr><td>  8k </td><td>  513627.8 </td><td>  527230.4 </td><td>  678140 </td><td>  771887.8 </td><td>  1146340.6 </td><td>  1894547.8 </td><td>  2526853.2 </td></tr><tr><td>  16k </td><td>  261087.4 </td><td>  263638.8 </td><td>  343679.2 </td><td>  392655.2 </td><td>  613912.8 </td><td>  1034843.2 </td><td>  1288299.6 </td></tr><tr><td>  32k </td><td>  131198.6 </td><td>  130947.4 </td><td>  170846.6 </td><td>  216039.4 </td><td>  309028.2 </td><td>  527920.6 </td><td>  644774.6 </td></tr><tr><td>  64k </td><td>  65083.4 </td><td>  65099.2 </td><td>  85257.2 </td><td>  131005.6 </td><td>  154839.8 </td><td>  268425 </td><td>  322739 </td></tr><tr><td>  128k </td><td>  32550.2 </td><td>  32718.2 </td><td>  43378.6 </td><td>  66999.8 </td><td>  78935.8 </td><td>  136869.8 </td><td>  161015.4 </td></tr><tr><td>  1m </td><td>  3802 </td><td>  3718.4 </td><td>  3233.4 </td><td>  3467.2 </td><td>  3546 </td><td>  6150.8 </td><td>  8193.2 </td></tr></tbody></table><br>  Performance result (IOps) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/jf/d1/kl/jfd1klb3wv6px-vjbxypjff8nmu.png"><br><br><h3>  Test 2.2 MDRAID.  RAID 0. Delay Tests </h3><br>  <b>Average response time</b> (ms) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R65% / W35% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  0.03015 </td><td>  0.067541 </td><td>  0.102942 </td></tr><tr><td>  8k </td><td>  0.03281 </td><td>  0.082132 </td><td>  0.126008 </td></tr><tr><td>  16k </td><td>  0.050058 </td><td>  0.114278 </td><td>  0.170798 </td></tr></tbody></table><br>  <b>Average response time</b> (ms) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/6l/_o/im/6l_oimj5-72qy9d6w9tobclq3n4.png"><br><br>  <b>Maximum response time</b> (ms) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R65% / W35% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  6.7042 </td><td>  3.7257 </td><td>  0.8568 </td></tr><tr><td>  8k </td><td>  6.5918 </td><td>  2.2601 </td><td>  0.9004 </td></tr><tr><td>  16k </td><td>  6.3466 </td><td>  2.7741 </td><td>  2.5678 </td></tr></tbody></table><br>  <b>Maximum response time</b> (ms) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/ms/-s/or/ms-sorgvctpuijgx_jf2uomifty.png"><br><br><h3>  Test 2.3 MDRAID.  RAID 0. Bandwidth tests </h3><br><blockquote>  1MB sequential write - 7820 MBPS. <br>  1MB sequential read - 20418 MBPS. <br>  128KB sequential write - 7622 MBPS. <br>  128KB sequential read - 20380 MBPS. </blockquote><br><h3>  Test 2.4 MDRAID.  RAID 6. IOPS Test </h3><br>  Let's now see what happens with this system at the level of RAID 6. <br><br>  Array Creation Options <br><blockquote>  mdadm --create --verbose --chunk 16K / dev / md0 --level = 6 --raid-devices = 12 / dev / nvme0n1 / dev / nvme1n1 / dev / nvme2n1 / dev / nvme3n1 / dev / nvme4n1 / dev / nvme5n1 / dev / nvme8n1 / dev / nvme9n1 / dev / nvme10n1 / dev / nvme11n1 / dev / nvme6n1 / dev / nvme7n1 </blockquote><br>  The total volume of the array was <b>7450.87</b> GiB. <br><br>  Run the test after pre-initializing the RAID. <br><br>  Performance result (IOps) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R5% / W95% </th><th>  R35% / W65% </th><th>  R50% / W50% </th><th>  R65% / W35% </th><th>  R95% / W5% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  39907.6 </td><td>  42849 </td><td>  61609.8 </td><td>  78167.6 </td><td>  108594.6 </td><td>  641950.4 </td><td>  1902561.6 </td></tr><tr><td>  8k </td><td>  19474.4 </td><td>  20701.6 </td><td>  30316.4 </td><td>  39737.8 </td><td>  57051.6 </td><td>  394072.2 </td><td>  1875791.4 </td></tr><tr><td>  16k </td><td>  10371.4 </td><td>  10979.2 </td><td>  16022 </td><td>  20992.8 </td><td>  29955.6 </td><td>  225157.4 </td><td>  1267495.6 </td></tr><tr><td>  32k </td><td>  8505.6 </td><td>  8824.8 </td><td>  12896 </td><td>  16657.8 </td><td>  23823 </td><td>  173261.8 </td><td>  596857.8 </td></tr><tr><td>  64k </td><td>  5679.4 </td><td>  5931 </td><td>  8576.2 </td><td>  11137.2 </td><td>  15906.4 </td><td>  109469.6 </td><td>  320874.6 </td></tr><tr><td>  128k </td><td>  3976.8 </td><td>  4170.2 </td><td>  5974.2 </td><td>  7716.6 </td><td>  10996 </td><td>  68124.4 </td><td>  160453.2 </td></tr><tr><td>  1m </td><td>  768.8 </td><td>  811.2 </td><td>  1177.8 </td><td>  1515 </td><td>  2149.6 </td><td>  4880.4 </td><td>  5499 </td></tr></tbody></table><br>  Performance result (IOps) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/p7/as/5s/p7as5s0v49uhmbhznnvjrpmfgf0.png"><br><br><h3>  Test 2.5 MDRAID.  RAID 6. Delay Tests </h3><br>  <b>Average response time</b> (ms) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R65% / W35% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  0.193702 </td><td>  0.145565 </td><td>  0.10558 </td></tr><tr><td>  8k </td><td>  0.266582 </td><td>  0.186618 </td><td>  0.127142 </td></tr><tr><td>  16k </td><td>  0.426294 </td><td>  0.281667 </td><td>  0.169504 </td></tr></tbody></table><br>  <b>Average response time</b> (ms) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/ls/zv/3f/lszv3flgx0nduvygoxigvjfdfgk.png"><br><br>  <b>Maximum response time</b> (ms) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R65% / W35% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  6.1306 </td><td>  4.5416 </td><td>  4.2322 </td></tr><tr><td>  8k </td><td>  6.2474 </td><td>  4.5197 </td><td>  3.5898 </td></tr><tr><td>  16k </td><td>  5.4074 </td><td>  5.5861 </td><td>  4.1404 </td></tr></tbody></table><br>  <b>Maximum response time</b> (ms) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/fv/id/ri/fvidriesmmcwdstzhrsnmwd-kqa.png"><br><br>  It is worth noting that here MDRAID showed a very good level of delays. <br><br><h3>  Test 2.6 MDRAID.  RAID 6. Bandwidth Tests </h3><br><blockquote>  1MB sequential write - 890 MBPS. <br>  1MB sequential read - 18,800 MBPS. <br>  128KB sequential write - 870 MBPS. <br>  128KB sequential read - 10400 MBPS. </blockquote><br><h2>  Test 3. Zvol over ZFS RAIDZ2 </h2><br>  ZFS has a built-in RAID creation function and a built-in volume manager that creates virtual block devices, which many storage vendors use.  We will also use these features, creating a RAIDZ2-protected pool (analogous to RAID 6) and a virtual block volume on top of it. <br><br>  Version 0.79 (ZFS) has been compiled.  Parameters for creating an array and volume: <br><blockquote>  ashift = 12 / compression - off / dedup - off / recordsize = 1M / atime = off / cachefile = none / Type RAID = RAIDZ2 </blockquote><br>  ZFS shows very good results with the newly created pool.  However, with multiple re-recordings, performance is significantly reduced. <br><br>  The SNIA approach is good enough, which allows you to see real results from testing similar file systems (the one that is at the heart of ZFS) after repeated rewrites on them. <br><br><h3>  Test 3.1 ZVOL (ZFS).  RAIDZ2.  IOps test </h3><br>  Performance result (IOps) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R5% / W95% </th><th>  R35% / W65% </th><th>  R50% / W50% </th><th>  R65% / W35% </th><th>  R95% / W5% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  15719.6 </td><td>  15147.2 </td><td>  14190.2 </td><td>  15592.4 </td><td>  17965.6 </td><td>  44832.2 </td><td>  76314.8 </td></tr><tr><td>  8k </td><td>  15536.2 </td><td>  14929.4 </td><td>  15140.8 </td><td>  16551 </td><td>  17898.8 </td><td>  44553.4 </td><td>  76187.4 </td></tr><tr><td>  16k </td><td>  16696.6 </td><td>  15937.2 </td><td>  15982.6 </td><td>  17350 </td><td>  18546.2 </td><td>  44895.4 </td><td>  75549.4 </td></tr><tr><td>  32k </td><td>  11859.6 </td><td>  10915 </td><td>  9698.2 </td><td>  10235.4 </td><td>  11265 </td><td>  26741.8 </td><td>  38167.2 </td></tr><tr><td>  64k </td><td>  7444 </td><td>  6440.2 </td><td>  6313.2 </td><td>  6578.2 </td><td>  7465.6 </td><td>  14145.8 </td><td>  19099 </td></tr><tr><td>  128k </td><td>  4425.4 </td><td>  3785.6 </td><td>  4059.8 </td><td>  3859.4 </td><td>  4246.4 </td><td>  7143.4 </td><td>  10052.6 </td></tr><tr><td>  1m </td><td>  772 </td><td>  730.2 </td><td>  779.6 </td><td>  784 </td><td>  824.4 </td><td>  995.8 </td><td>  1514.2 </td></tr></tbody></table><br>  Performance result (IOps) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/0j/cf/fr/0jcffrp4gkeayltgnp5us5rg2q8.png"><br><br>  Performance figures are quite unimpressive.  At the same time, pure zvol (before rewrites) gives significantly better results (5-6 times higher).  Here, the test showed that after the first rewrite, performance drops. <br><br><h3>  Test 3.2 ZVOL (ZFS).  RAIDZ2.  Delay tests </h3><br>  <b>Average response time</b> (ms) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R65% / W35% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  0.332824 </td><td>  0.255225 </td><td>  0.218354 </td></tr><tr><td>  8k </td><td>  0.3299 </td><td>  0.259013 </td><td>  0.225514 </td></tr><tr><td>  16k </td><td>  0.139738 </td><td>  0.180467 </td><td>  0.233332 </td></tr></tbody></table><br>  <b>Average response time</b> (ms) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/ys/fj/hb/ysfjhbkrepf-zzj6bvdtx_hhrlm.png"><br><br>  <b>Maximum response time</b> (ms) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R65% / W35% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  90.55 </td><td>  69.9718 </td><td>  84.4018 </td></tr><tr><td>  8k </td><td>  91.6214 </td><td>  86.6109 </td><td>  104.7368 </td></tr><tr><td>  16k </td><td>  108.2192 </td><td>  86.2194 </td><td>  105.658 </td></tr></tbody></table><br>  <b>Maximum response time</b> (ms) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/du/ue/_1/duue_1fwxcx-n4dmyd455pdrc40.png"><br><br><h3>  Test 3.3 ZVOL (ZFS).  RAIDZ2.  Bandwidth tests </h3><br><blockquote>  1MB sequential write - 1150 MBPS. <br>  1MB sequential read - 5500 MBPS. <br>  128KB sequential write - 1100 MBPS. <br>  128KB sequential read - 5300 MBPS. </blockquote><br><h2>  Test 4. RAIDIX ERA </h2><br>  Let's now look at the tests of our new product - RAIDIX ERA. <br>  We created RAID6.  Stripe size: 16kb.  After the initialization is complete, run the test. <br><br>  Performance result (IOps) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R5% / W95% </th><th>  R35% / W65% </th><th>  R50% / W50% </th><th>  R65% / W35% </th><th>  R95% / W5% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  354887 </td><td>  363830 </td><td>  486865.6 </td><td>  619349.4 </td><td>  921403.6 </td><td>  2202384.8 </td><td>  4073187.8 </td></tr><tr><td>  8k </td><td>  180914.8 </td><td>  185371 </td><td>  249927.2 </td><td>  320438.8 </td><td>  520188.4 </td><td>  1413096.4 </td><td>  2510729 </td></tr><tr><td>  16k </td><td>  92115.8 </td><td>  96327.2 </td><td>  130661.2 </td><td>  169247.4 </td><td>  275446.6 </td><td>  763307.4 </td><td>  1278465 </td></tr><tr><td>  32k </td><td>  59994.2 </td><td>  61765.2 </td><td>  83512.8 </td><td>  116562.2 </td><td>  167028.8 </td><td>  420216.4 </td><td>  640418.8 </td></tr><tr><td>  64k </td><td>  27660.4 </td><td>  28229.8 </td><td>  38687.6 </td><td>  56603.8 </td><td>  76976 </td><td>  214958.8 </td><td>  299137.8 </td></tr><tr><td>  128k </td><td>  14475.8 </td><td>  14730 </td><td>  20674.2 </td><td>  30358.8 </td><td>  40259 </td><td>  109258.2 </td><td>  160141.8 </td></tr><tr><td>  1m </td><td>  2892.8 </td><td>  3031.8 </td><td>  4032.8 </td><td>  6331.6 </td><td>  7514.8 </td><td>  15871 </td><td>  19078 </td></tr></tbody></table><br>  Performance result (IOps) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/px/on/md/pxonmd9exhzvhwqse0wugnzkc9o.png"><br><br><h3>  Test 4.2 RAIDIX ERA.  RAID 6. Delay Tests </h3><br>  <b>Average response time</b> (ms) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R65% / W35% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  0.16334 </td><td>  0.136397 </td><td>  0.10958 </td></tr><tr><td>  8k </td><td>  0.207056 </td><td>  0.163325 </td><td>  0.132586 </td></tr><tr><td>  16k </td><td>  0.313774 </td><td>  0.225767 </td><td>  0.182928 </td></tr></tbody></table><br>  <b>Average response time</b> (ms) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/hv/ad/xq/hvadxqbbwsigpbfi2d40nhsfevo.png"><br><br>  <b>Maximum response time</b> (ms) in tabular form.  Read / Write Mix%. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R65% / W35% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  5.371 </td><td>  3.4244 </td><td>  3.5438 </td></tr><tr><td>  8k </td><td>  5.243 </td><td>  3.7415 </td><td>  3.5414 </td></tr><tr><td>  16k </td><td>  7.628 </td><td>  4.2891 </td><td>  4.0562 </td></tr></tbody></table><br>  <b>Maximum response time</b> (ms) graphically.  Read / Write Mix%. <br><br><img src="https://habrastorage.org/webt/pk/ys/bb/pkysbbysl2ldmme0fcbptzjh0ug.png"><br><br>  Delays are similar to what MDRAID issues.  But for more accurate conclusions, you should evaluate the delays under a more serious load. <br><br><h3>  Test 4.3 RAIDIX ERA.  RAID 6. Bandwidth Tests </h3><br><blockquote>  1MB sequential write - 8160 MBPS. <br>  1MB sequential read - 19700 MBPS. <br>  128KB sequential write - 6200 MBPS. <br>  128KB sequential read - 19700 MBPS. </blockquote><br><h2>  Conclusion </h2><br>  As a result of the tests performed, it is worth comparing the figures obtained from software solutions with what the hardware platform provides us. <br><br>  To analyze the performance of a random load, we will compare the speed of RAID 6 (RAIDZ2) when working with a 4k block. <br><table><tbody><tr><th></th><th>  MD RAID 6 </th><th>  RAIDZ2 </th><th>  RAIDIX ERA RAID 6 </th><th>  Hardware </th></tr><tr><td>  4k R100% / W0% </td><td>  1902561 </td><td>  76314 </td><td>  4073187 </td><td>  4494142 </td></tr><tr><td>  4k R65% / W35% </td><td>  108594 </td><td>  17965 </td><td>  921403 </td><td>  1823432 </td></tr><tr><td>  4k R0% / W100% </td><td>  39907 </td><td>  15719 </td><td>  354887 </td><td>  958054 </td></tr></tbody></table><br><img src="https://habrastorage.org/webt/cm/4g/ff/cm4gffjnga2flpdn0nl3yxiwew0.png"><br><br>  To analyze the performance of the serial load, we look at RAID 6 (RAIDZ2) with a 128k block.  Between the streams, we used a shift of 10GB to eliminate the cache hit and show the actual performance. <br><table><tbody><tr><th></th><th>  MD RAID 6 </th><th>  RAIDZ2 </th><th>  RAIDIX ERA RAID 6 </th><th>  Hardware </th></tr><tr><td>  128k seq read </td><td>  10400 </td><td>  5300 </td><td>  19700 </td><td>  20400 </td></tr><tr><td>  128k seq write </td><td>  870 </td><td>  1100 </td><td>  6200 </td><td>  7500 </td></tr></tbody></table><br><img src="https://habrastorage.org/webt/6l/dw/wt/6ldwwti-7dql_njqadpemrz1mxi.png"><br><br><h3>  What is the result? </h3><br>  Popular and available software RAID-arrays for working with NVMe-devices can not show the performance that lies in the hardware potential. <br><br>  Here there is an obvious need for control software that can stir up the situation and show that the software-managing symbiosis with NVMe-drives can be very productive and flexible. <br><br>  Understanding this query, we created a RAIDIX ERA product in our company, the development of which focused on the following tasks: <br><br><ul><li>  High read and write performance (several million IOps) on arrays with Parity in mix mode. </li><li>  Stream performance from 30GBps including during failures and recovery. </li><li>  RAID support levels 5, 6, 7.3. </li><li>  Background initialization and reconstruction. </li><li>  Flexible settings for different types of load (from the user). </li></ul><br>  At the moment, we can say that these tasks have been completed and the product is ready for use. <br><br>  At the same time, understanding the interest of many interested parties in such technologies, we prepared for release not only a paid, but also a <a href="http://www.raidix.ru/products/era/">free license</a> , which can be fully used for solving tasks on both NVMe and SSD drives. <br><br>  Read more about the RAIDIX ERA product <a href="http://www.raidix.ru/products/era/">on our website</a> . <br><br><h2>  UPD.  ZFS reduced testing with recordsize and volblocksize 8k </h2><br>  ZFS Parameters Table <br><table><tbody><tr><th>  NAME </th><th>  PROPERTY </th><th>  Value </th><th>  SOURCE </th></tr><tr><td>  tank </td><td>  recordsize </td><td>  8K </td><td>  local </td></tr><tr><td>  tank </td><td>  compression </td><td>  off </td><td>  default </td></tr><tr><td>  tank </td><td>  dedup </td><td>  off </td><td>  default </td></tr><tr><td>  tank </td><td>  checksum </td><td>  off </td><td>  local </td></tr><tr><td>  tank </td><td>  volblocksize </td><td>  - </td><td>  - </td></tr><tr><td>  tank / raid </td><td>  recordsize </td><td>  - </td><td>  - </td></tr><tr><td>  tank / raid </td><td>  compression </td><td>  off </td><td>  local </td></tr><tr><td>  tank / raid </td><td>  dedup </td><td>  off </td><td>  default </td></tr><tr><td>  tank / raid </td><td>  checksum </td><td>  off </td><td>  local </td></tr><tr><td>  tank / raid </td><td>  volblocksize </td><td>  8k </td><td>  default </td></tr></tbody></table><br>  The recording has become worse, the reading is better. <br>  But all the same, all the results are much worse than other solutions. <br><table><tbody><tr><th>  Block size </th><th>  R0% / W100% </th><th>  R5% / W95% </th><th>  R35% / W65% </th><th>  R50% / W50% </th><th>  R65% / W35% </th><th>  R95% / W5% </th><th>  R100% / W0% </th></tr><tr><td>  4k </td><td>  13703.8 </td><td>  14399.8 </td><td>  20903.8 </td><td>  25669 </td><td>  31610 </td><td>  66955.2 </td><td>  140849.8 </td></tr><tr><td>  8k </td><td>  15126 </td><td>  16227.2 </td><td>  22393.6 </td><td>  27720.2 </td><td>  34274.8 </td><td>  67008 </td><td>  139480.8 </td></tr><tr><td>  16k </td><td>  11111.2 </td><td>  11412.4 </td><td>  16980.8 </td><td>  20812.8 </td><td>  24680.2 </td><td>  48803.6 </td><td>  83710.4 </td></tr></tbody></table></div><p>Source: <a href="https://habr.com/ru/post/420837/">https://habr.com/ru/post/420837/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420827/index.html">Create a simple maven project using Java EE + WildFly10 + JPA (Hibernate) + Postgresql + EJB + IntelliJ IDEA</a></li>
<li><a href="../420829/index.html">Job System. Review on the other hand</a></li>
<li><a href="../420831/index.html">Apparatus for the issuance of cookies "Cooker 3000"</a></li>
<li><a href="../420833/index.html">The main mistakes monetization of the application [and how to fix them]</a></li>
<li><a href="../420835/index.html">"Kalashnikov" showed the concept of a combat robot and electric motorcycle for the civilian market</a></li>
<li><a href="../420841/index.html">Pre-hospital hotfix or ‚ÄúHey, Swagger! Where are my mistakes? ‚Äù</a></li>
<li><a href="../420843/index.html">September 7, Ekaterinburg - mitap for .NET developers</a></li>
<li><a href="../420845/index.html">Taming of the Shrew with the use of a crutch: WF2190 Wi-Fi Hole (Realtek8812AU Wireless LAN 802.11ac USB)</a></li>
<li><a href="../420847/index.html">An introduction to programming shaders for web designers</a></li>
<li><a href="../420853/index.html">Meet the Windows pseudo console (ConPTY)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>