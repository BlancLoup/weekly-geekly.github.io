<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The evolution of neural networks for image recognition in Google: Inception-v3</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I continue to talk about the life of Inception architecture - Google's archneetkury for convnets. 
 (the first part is right here ) 
 So, a year passe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The evolution of neural networks for image recognition in Google: Inception-v3</h1><div class="post__text post__text-html js-mediator-article"><p>  I continue to talk about the life of Inception architecture - Google's archneetkury for convnets. <br>  (the first part is <a href="https://habrahabr.ru/post/301084/">right here</a> ) <br>  So, a year passes, men publish development successes since GoogLeNet. <br>  Here is a scary picture of what the final network looks like: <br><img src="https://habrastorage.org/files/84d/8a8/0a2/84d8a80a299e440c9f1892485c2e0803.png" alt="image"><br>  What kind of horror is happening there? </p><a name="habracut"></a><br><p>  <em>Disclaimer: The post is written on the basis of the edited chat logs <a href="http://closedcircles.com/%3Finvite%3D99b1ac08509c560137b2e3c54d4398b0fa4c175e">closedcircles.com</a> , hence the style of presentation, and clarifying questions.</em> </p><br><p>  This time, the authors are trying to formulate some basic principles for building an effective network architecture (the article itself is <a href="http://arxiv.org/abs/1512.00567">http://arxiv.org/abs/1512.00567</a> ). <br>  (remember, the purpose of the Inception architecture is to be primarily efficient in computing and the number of parameters for real applications, for which we love) <br>  The principles they formulate are as follows: </p><br><ul><li>  A lot of signals are close to each other in space (i.e., in the neighboring "pixels"), and this can be used to make a smaller convolution. <br>  They say that since the neighboring signals are often correlated, it is possible to reduce the dimension before convolution without losing information. </li><li>  For efficient use of resources, you need to increase the width and depth of the network.  Those.  if for example resources become twice as large, the most effective is to make the layers wider and the network deeper.  If you do only deeper, it will be ineffective. </li><li>  It is bad to have sharp bottlenecks, that is, a rail with a sharp decrease in parameters, especially at the beginning. </li><li>  "Wide" layers learn faster, which is especially important at high levels (but locally, that is, it is quite possible to reduce the dimension after them) </li></ul><br><p>  Let me remind you, the previous version of the network building block looked like this: </p><br><img src="https://habrastorage.org/files/623/6a4/fa1/6236a4fa10934720b92734e9df7bc1e6.png" width="400"><br><h2>  What modifications do they make in it </h2><br><ul><li><p>  <em>First</em> , we note that it is possible to replace the big and bold 5x5 convolution by two consecutive 3x3 each, and they say, since the signals are correlated, we lose a little.  It turns out experimentally what to do between these 3x3 nonlinearity is better than not to do. </p><br></li><li>  <em>Secondly</em> , since it's such a booze, let's replace 3x3 with 3x1 + 1x3. </li></ul><br><p><img src="https://habrastorage.org/files/789/9d5/a83/7899d5a839ed451491fd28871675d5cf.png" alt="image"></p><br><p>  Here we find that doing something convolving becomes cheap, and then why not do 3x1 + 1x3 at all, but nx1 + 1xn right away! <br>  And they do, as much as 7, though not at the beginning of the grid.  With all these upgrades, the main brick becomes: </p><br><img src="https://habrastorage.org/files/09b/0fd/4c4/09b0fd4c4f734b13b68ac5c88fa9da7a.png" width="400"><br><ul><li>  <em>Thirdly</em> , following the covenant "do not create bottlenecks", they think about pooling. <br>  What is the problem with pooling? Here let the pool reduce the picture twice, and the number of features after the pool is twice as large. <br>  You can make a pool, and then convolution in a lower resolution, but you can first convolution, and then pool. <br>  Here are the options in the picture: </li></ul><br><img src="https://habrastorage.org/files/78f/5b1/f4c/78f5b1f4c8f747f99ee273a3101484a2.png" width="400"><br><p>  The problem is that the first option - just dramatically reduce the number of activations, and the second - is ineffective in terms of calculations, because you need to carry out convolution at full resolution. </p><br><p>  Therefore, they offer a hybrid scheme - let's make half a pool of features, and half a convolution. </p><br><img src="https://habrastorage.org/files/ae6/f2e/f1f/ae6f2ef1f08a432f87fd6ec4efb709f5.png" width="600"><br><p>  And since after the pool the number of features typically doubles, there will not be a bottleneck.  The pool will compress the previous one without reducing the number of features, some convolutions will be driven out in full resolution, but with fewer features.  The network learns how to divide, which requires full resolution, and why a pool is enough. </p><br><ul><li>  <em>Finally</em> , they slightly modify the brick for the last leierra to be wider, albeit less deep.  To make the mole learn better, at the end of the network this is most important. </li></ul><br><p>  And here is the network - these are a few early convolutions, and then these are the bricks interspersed with a pool.  There are 11 inception layers in total. <br>  Hence the horror in the first figure. </p><br><p>  They also found that the additional classifiers on the sides do not speed up the training much, but rather they help because they work as regularizers - when they connected Batch Normalization to them, the network began to predict better. </p><br><h2>  What else... </h2><br><p>  They offer another trick for additional regularization - the so-called <em>label smoothing</em> . <br>  The idea in brief is this: usually the target label for a particular sample is 1 where the class is correct, and 0 where the class is wrong. <br>  This means that if the network is already very confident in the correctness of the class, the gradient will still push to increase and increase this confidence, because 1 occurs only at infinity due to softmax, which leads to overfitting. </p><br><p>  They propose to mix one-off target with a distribution proportional to the stupid distribution of classes on a dataset, so that in other classes there are not zeros, but some small values.  This gives even more percentages to win, that is a lot. </p><br><h2>  Total </h2><br><p>  And all this machinery eats 2.5 times more computing resources than Inception-v1 and achieves significantly better results. <br>  They call the main architecture Inception-v2, and the version where the additional classifiers work with BN is Inception-v3. <br>  This Inception-v3 achieves 4.2% top5 classification error for Imagenet, and an ensemble of four models - 3.58%. </p><br><p>  And with this good guys from Google gathered to win Imagenet in 2015. <br>  However, ResNets happened and won Kaiming He associates from Microsoft Research Asia with the result ... 3.57% !!! <br>  (it should be noted that in object localization the result is fundamentally better for them) <br>  But about ResNets, I probably tell you another time. </p><br><blockquote>  <em>Interestingly, the average homo sapiens will show what error in these pictures.</em> <br>  The only widely discussed experiment was conducted by Andrey "our all" Karpathy. <br>  <a href="http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/">http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/</a> <br>  He tested himself on some part of dataset and he got 5.1%. <br>  This is also top5, but it may be more difficult for a person to choose top5. <br>  By the way, you can check yourself - <a href="http://cs.stanford.edu/people/karpathy/ilsvrc/">http://cs.stanford.edu/people/karpathy/ilsvrc/</a> <br>  And it is really difficult.  They will show you some subspecies of the Mediterranean finch and guessing. </blockquote></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/302242/">https://habr.com/ru/post/302242/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../302232/index.html">Russian microelectronics and Ukrainian IoT at SVOD and IoT DevCon conferences in Silicon Valley</a></li>
<li><a href="../302234/index.html">Eight ways to demotivate your employees</a></li>
<li><a href="../302236/index.html">Cloud Digest # 6: Data and Security</a></li>
<li><a href="../302238/index.html">How we once again passed the Cisco Gold Certification: features, a couple of tales and pitfalls</a></li>
<li><a href="../302240/index.html">Comparing Dependency Injection Framework Configurations</a></li>
<li><a href="../302244/index.html">Neural networks, typography and layout: Digest of interesting materials about creating letters and working with email in Habrahabr</a></li>
<li><a href="../302246/index.html">DLMS / COSEM is an open protocol for data exchange with metering devices. Part 1: Overview</a></li>
<li><a href="../302248/index.html">Everything is bad: Why Upwork freelance-exchange valuation may soon become zero</a></li>
<li><a href="../302250/index.html">Win-Win: DotNext speakers at the SPB .NET Community meeting and vice versa</a></li>
<li><a href="../302252/index.html">Big interview with Simon Dunlop - founder of Dream Industries: how to bring Russian IT products to the world market</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>