<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚Äú4 weddings and one funerals‚Äù or linear regression for analyzing the open data of the Moscow government</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Despite a lot of great materials on Data Science, for example, from Open Data Science , I continue to collect scraps from the feast of reason and cont...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>‚Äú4 weddings and one funerals‚Äù or linear regression for analyzing the open data of the Moscow government</h1><div class="post__text post__text-html js-mediator-article">  Despite a lot of great materials on Data Science, for example, from <a href="https://habrahabr.ru/company/ods/blog/322626/">Open Data Science</a> , I continue to collect scraps from the feast of reason and continue to share with you my experience in mastering machine learning skills and analyzing data from scratch. <br><br>  In the last articles we looked at a couple of tasks for classification, in the process of obtaining data for ourselves in the process of sweat and blood, now it is time to regress.  Since there was nothing at hand about lighting this time, I decided to scrape the other bottom of the barrel. <br><br>  I remember that in one of the <a href="https://habrahabr.ru/post/337040/">articles</a> I was agitating readers to look towards domestic open data.  But since I am not a young lady from an advertisement for kefir for digestion or a horsepower shampoo, my conscience did not allow me to advise anything without having experienced it. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Where to begin?  Of course, with the open data of the Russian government, there is a whole ministry there.  My acquaintance <a href="http://data.gov.ru/">with the open data of the government of the Russian Federation</a> was approximately the same as in the illustration for this article.  No, it‚Äôs not that I‚Äôm not at all interested in the registry of the movie halls of the city of New Urengoy or the list of rental equipment of the skating rink in Tula, they are not very suitable for the regression task. <br><br>  If you think about it, you can find something worthwhile on the OD site of the Russian government, it‚Äôs just not very easy. <br><br>  <a href="https://www.minfin.ru/ru/OpenData/">Data of the Ministry of Finance,</a> I also decided to leave, for later. <br><br>  Perhaps, I liked the open data of the Moscow government most of all, there I looked at a couple of potential tasks and chose as a result. <a href="https://goo.gl/eVabkm">Information about the civil registration in Moscow by year</a> <br><br>  What came out of the application of minimal skills in the field of linear regression, you can briefly look at <a href="https://github.com/bosonbeard/Funny-models-and-scripts/tree/master/3.Machine_learning">GitHub</a> , and of course, looking under the cat. <br><br><img src="https://habrastorage.org/webt/59/ed/1a/59ed1a8024b00754416084.png"><br><br>  UPD: Added section - "Bonus" <br><a name="habracut"></a><br><h2>  Introduction </h2><br><br>  As usual, at the beginning of the article I will talk about the skills necessary for understanding this article. <br>  You will need: <br><br><ul><li>  Read a tutorial or run a simple Machine Learning course. </li><li>  A little understanding of Python </li><li>  Have almost zero knowledge in mathematics </li></ul><br>  If you are completely new to data analysis and machine learning, look at the previous articles in the cycle in order of their succession, there each article was written with ‚Äúhot pursuit‚Äù and you will understand whether you should spend time on Data Science. <br><br>  All previously prepared articles below the spoiler <br><br><div class="spoiler">  <b class="spoiler_title">Other articles of the cycle</b> <div class="spoiler_text">  1. Learn the basics: <br><ul><li>  ‚Äú <a href="https://habrahabr.ru/post/331118/">Catch data big and small!</a>  "- (Overview of Cognitive Class Data Science Courses) </li><li>  " <a href="https://habrahabr.ru/post/331794/">Now he counted you</a> " or Data Science from Scratch </li><li>  ‚Äú <a href="https://habrahabr.ru/post/331992/">Iceberg instead of Oscar!</a>  "Or as I tried to learn the basics of DataScience on kaggle </li><li>  ‚Äú <a href="https://habrahabr.ru/post/335214/">‚Äú A train that could! ‚ÄùOr‚Äú Specialization Machine learning and data analysis ‚Äù, through the eyes of a newbie in Data Science</a> </li></ul><br>  2. Practice first skills <br><ul><li>  <a href="https://habrahabr.ru/post/337040">‚ÄúRise of Machinery Learning‚Äù or combine a hobby in Data Science and analyzing the spectra of light bulbs</a> </li><li>  <a href="https://habrahabr.ru/post/337438/">‚ÄúAs per the notes!‚Äù Or Machine Learning (Data science) in C # using Accord.NET Framework</a> </li><li>  <a href="https://habrahabr.ru/post/338124/">‚ÄúUse the Power of Machine Learning, Luke!‚Äù Or automatic classification of luminaires according to the CIL</a> </li></ul><br></div></div><br>  Well, as promised earlier now the articles of this cycle will be completed with content. <br><br>  Content: <br><br>  <a href="https://habr.com/ru/post/340698/">Part I: ‚ÄúMarry - Don't Put on My Mouth‚Äù - Obtaining and primary analysis of data.</a> <br>  <a href="https://habr.com/ru/post/340698/">Part II: "One is not a warrior in the field" - Regression on 1 basis</a> <br>  <a href="https://habr.com/ru/post/340698/">Part III: ‚ÄúOne head is good, but much better‚Äù - Regression on several grounds with regularization</a> <br>  <a href="https://habr.com/ru/post/340698/">Part IV: ‚ÄúAll is not gold that glitters‚Äù - Add signs</a> <br>  <a href="https://habr.com/ru/post/340698/">Part V: ‚ÄúCut the new caftan, and try on the old one!‚Äù - Trend forecast</a> <br>  <a href="https://habr.com/ru/post/340698/">Bonus - Increase accuracy, due to a different approach to the months</a> <br><br>  So, we will move on to the task.  Our goal is to dig up any data set sufficient to demonstrate the basic linear regression techniques and determine for ourselves what we will predict. <br><br>  This time I will be brief and I will not deviate from the topic having considered only linear regression (you probably know about the existence of other <a href="https://habrahabr.ru/post/206306/">methods</a> ) <br><br><h2>  Part I <a name="I"></a>  : ‚ÄúMarry - don‚Äôt put on your hands‚Äù - Acquisition and primary analysis of data </h2><br>  Unfortunately, the open data of the Moscow government are not as extensive and endless as the budget spent on improvement under the My Street program, but nevertheless we managed to find something worthwhile. <br><br>  <a href="https://data.mos.ru/opendata/7704111479-dinamika-registratsii-aktov-grajdanskogo-sostoyaniya/description%3FversionNumber%3D2%26releaseNumber%3D33">The dynamics of the registration of acts of civil status</a> , we are quite fit. <br><br>  This is almost a hundred records broken down by months, in which there is data on the number of weddings, births, the establishment of parenthood, deaths, name changes, etc. <br>  To solve the regression problem is quite suitable. <br><br>  The entire code is hosted on <a href="https://github.com/bosonbeard/Funny-models-and-scripts/tree/master/3.Machine_learning">GitHub.</a> <br>  Well, in parts, we dissect it right now. <br><br>  To start importing libraries: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> MinMaxScaler <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> warnings warnings.filterwarnings(<span class="hljs-string"><span class="hljs-string">'ignore'</span></span>) %matplotlib inline</code> </pre> <br>  Then load the data.  The Pandas library allows you to upload files from a remote server, which is by and large just fine, provided, of course, that the page redirection algorithm does not change on the portal. <br><br>  <i>(I hope that the download link in the code will not be covered, if it stops working, please write in "lichku" so that I can update)</i> <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#download df = pd.read_csv('https://op.mos.ru/EHDWSREST/catalog/export/get?id=230308', compression='zip', header=0, encoding='cp1251', sep=';', quotechar='"') #look at the data df.head(12)</span></span></code> </pre><br>  Let's look at the data: <br><table border="1"><thead><tr><th>  ID </th><th>  global_id </th><th>  Year </th><th>  Month </th><th>  StateRegistrationOfBirth </th><th>  StateRegistrationOfDeath </th><th>  StateRegistrationOfMarriage </th><th>  StateRegistrationOfDivorce </th><th>  StateRegistrationOfPaternityExamination </th><th>  StateRegistrationOfAdoption </th><th>  StateRegistrationOfNameChange </th><th>  Totalnumber </th></tr></thead><tbody><tr><td>  one </td><td>  37591658 </td><td>  2010 </td><td>  January </td><td>  9206 </td><td>  10430 </td><td>  4997 </td><td>  3302 </td><td>  1241 </td><td>  95 </td><td>  491 </td><td>  29762 </td></tr><tr><td>  2 </td><td>  37591659 </td><td>  2010 </td><td>  February </td><td>  9060 </td><td>  9573 </td><td>  4873 </td><td>  2937 </td><td>  1326 </td><td>  97 </td><td>  639 </td><td>  28505 </td></tr><tr><td>  3 </td><td>  37591660 </td><td>  2010 </td><td>  March </td><td>  10934 </td><td>  10528 </td><td>  3642 </td><td>  4361 </td><td>  1644 </td><td>  147 </td><td>  717 </td><td>  31973 </td></tr><tr><td>  four </td><td>  37591661 </td><td>  2010 </td><td>  April </td><td>  10140 </td><td>  9501 </td><td>  9698 </td><td>  3943 </td><td>  1530 </td><td>  128 </td><td>  642 </td><td>  35572 </td></tr><tr><td>  five </td><td>  37591662 </td><td>  2010 </td><td>  May </td><td>  9457 </td><td>  9482 </td><td>  3726 </td><td>  3554 </td><td>  1397 </td><td>  96 </td><td>  492 </td><td>  28204 </td></tr><tr><td>  6 </td><td>  62353812 </td><td>  2010 </td><td>  June </td><td>  11253 </td><td>  9529 </td><td>  9148 </td><td>  3666 </td><td>  1570 </td><td>  130 </td><td>  556 </td><td>  35852 </td></tr><tr><td>  7 </td><td>  62353813 </td><td>  2010 </td><td>  July </td><td>  11477 </td><td>  14340 </td><td>  12473 </td><td>  3675 </td><td>  1568 </td><td>  123 </td><td>  564 </td><td>  44220 </td></tr><tr><td>  eight </td><td>  62353814 </td><td>  2010 </td><td>  August </td><td>  10302 </td><td>  15016 </td><td>  10882 </td><td>  3496 </td><td>  1512 </td><td>  134 </td><td>  578 </td><td>  41920 </td></tr><tr><td>  9 </td><td>  62353816 </td><td>  2010 </td><td>  September </td><td>  10140 </td><td>  9573 </td><td>  10736 </td><td>  3738 </td><td>  1480 </td><td>  101 </td><td>  686 </td><td>  36454 </td></tr><tr><td>  ten </td><td>  62353817 </td><td>  2010 </td><td>  October </td><td>  10776 </td><td>  9350 </td><td>  8862 </td><td>  3899 </td><td>  1504 </td><td>  89 </td><td>  687 </td><td>  35167 </td></tr><tr><td>  eleven </td><td>  62353818 </td><td>  2010 </td><td>  November </td><td>  10293 </td><td>  9091 </td><td>  6080 </td><td>  3923 </td><td>  1355 </td><td>  97 </td><td>  568 </td><td>  31407 </td></tr><tr><td>  12 </td><td>  62353819 </td><td>  2010 </td><td>  December </td><td>  10600 </td><td>  9664 </td><td>  6023 </td><td>  4145 </td><td>  1556 </td><td>  124 </td><td>  681 </td><td>  32793 </td></tr></tbody></table><br><br>  If we want to use the data about the months, they need to be converted into an understandable model format, scikit-learn has its own methods, but for reliability we will do it with our hands (for not very much work) and at the same time we will remove a couple of useless in our case columns with ID and rubbish <br><br>  <i>Note: in this case to the column ‚ÄúMonth‚Äù, I think it would be more correct to apply <a href="https://habrahabr.ru/company/ods/blog/326418/">one-hot coding</a> , but since in this case we are not very interested in the quality of predictions, let‚Äôs leave as is.</i> <i><br></i>  <i>UPD: I could not stand it and added a revised version in the <a href="https://habr.com/ru/post/340698/">Bonus</a> section</i> <i><br></i> <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#code months d={'':1, '':2, '':3, '':4, '':5, '':6, '':7, '':8, '':9, '':10, '':11, '':12} df.Month=df.Month.map(d) #delete some unuseful columns df.drop(['ID','global_id','Unnamed: 12'],axis=1,inplace=True) #look at the data df.head(12)</span></span></code> </pre><br>  Since I'm not sure that the tabular view of all will open normally look at the data using the image. <br><br><img src="https://habrastorage.org/webt/59/ed/19/59ed192766b30131894321.png"><br><br>  We construct paired diagrams, from which it will be clear which columns of our table are linearly dependent on each other.  However, we will not immediately consider all the data, so that later there is something to add, therefore, we first remove some of the data. <br>  A fairly simple way to select (‚Äúdelete‚Äù) a part of the columns from the pandas Dataframe is to simply select the right columns: <br><br><pre> <code class="python hljs">columns_to_show = [<span class="hljs-string"><span class="hljs-string">'StateRegistrationOfBirth'</span></span>, <span class="hljs-string"><span class="hljs-string">'StateRegistrationOfMarriage'</span></span>, <span class="hljs-string"><span class="hljs-string">'StateRegistrationOfPaternityExamination'</span></span>, <span class="hljs-string"><span class="hljs-string">'StateRegistrationOfDivorce'</span></span>,<span class="hljs-string"><span class="hljs-string">'StateRegistrationOfDeath'</span></span>] data=df[columns_to_show]</code> </pre><br><br>  Well, now you can build a schedule. <br><br><pre> <code class="python hljs"> grid = sns.pairplot(data)</code> </pre><br><img src="https://habrastorage.org/webt/59/ed/19/59ed1927b7590585965078.png"><br><br>  It is a good practice to scale the signs so as not to compare horses with the weight of hay bales and average atmospheric pressure during the month. <br>  In our case, all data are presented in the same quantities (the number of registered acts), but let's still see what changes the scaling. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># change scale of features scaler = MinMaxScaler() df2=pd.DataFrame(scaler.fit_transform(df)) df2.columns=df.columns data2=df2[columns_to_show] grid2 = sns.pairplot(data2)</span></span></code> </pre><br><img src="https://habrastorage.org/webt/59/ed/19/59ed1927e35c0249166020.png"><br><br>  Almost nothing, but for reliability, we will take scaled data for now. <br><br><h2>  Part II <a name="II"></a>  : ‚ÄúOne soldier is not a warrior‚Äù - Regression by 1 feature </h2><br>  We will look at the pictures and see that the best way to describe the relationship between the two signs of StateRegistrationOfBirth and StateRegistrationOfPaternityExamination can be described in a straight line, which in general is not very surprising (the more often paternity is checked, the more often children are registered). <br>  Prepare the data, namely, select the two columns feature and the objective function, then use the ready-made libraries to divide the data into a training and control sample (the manipulations at the end of the code were needed to bring the data to the desired form) <br><br><pre> <code class="python hljs">X = data2[<span class="hljs-string"><span class="hljs-string">'StateRegistrationOfBirth'</span></span>].values y = data2[<span class="hljs-string"><span class="hljs-string">'StateRegistrationOfPaternityExamination'</span></span>].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="hljs-number"><span class="hljs-number">0.25</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">42</span></span>) X_train=np.reshape(X_train,[X_train.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>],<span class="hljs-number"><span class="hljs-number">1</span></span>]) y_train=np.reshape(y_train,[y_train.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>],<span class="hljs-number"><span class="hljs-number">1</span></span>]) X_test=np.reshape(X_test,[X_test.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>],<span class="hljs-number"><span class="hljs-number">1</span></span>]) y_test=np.reshape(y_test,[y_test.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>],<span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre><br>  It is important to note that now, despite the obvious possibility of linking to chronology, in order to demonstrate, we will consider the data simply as a set of records without reference to time. <br><br>  ‚ÄúFeed‚Äù our model data and look at the coefficient with our attribute, as well as estimate the accuracy of the model fitting with R ^ 2 (coefficient of determination). <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#teach model and get predictions lr = linear_model.LinearRegression() lr.fit(X_train, y_train) print('Coefficients:', lr.coef_) print('Score:', lr.score(X_test,y_test))</span></span></code> </pre><br>  It turned out not very much, on the other hand much better than if we were ‚Äúpoking at a guess‚Äù <br><br> <code>Coefficients: [[ 0.78600258]] <br> Score: 0.611493944197 <br> <br></code> <br><br>  Let's look at the graphs, first on the training data: <br><br><pre> <code class="python hljs">plt.scatter(X_train, y_train, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>) plt.plot(X_train, lr.predict(X_train), color=<span class="hljs-string"><span class="hljs-string">'blue'</span></span>, linewidth=<span class="hljs-number"><span class="hljs-number">3</span></span>) plt.xlabel(<span class="hljs-string"><span class="hljs-string">'StateRegistrationOfBirth'</span></span>) plt.ylabel(<span class="hljs-string"><span class="hljs-string">'State Registration OfPaternity Examination'</span></span>) plt.title=<span class="hljs-string"><span class="hljs-string">"Regression on train data"</span></span></code> </pre><br><img src="https://habrastorage.org/webt/59/ed/19/59ed19281ad25466072897.png"><br><br>  And now on the control: <br><br><pre> <code class="python hljs">plt.scatter(X_test, y_test, color=<span class="hljs-string"><span class="hljs-string">'black'</span></span>) plt.plot(X_test, lr.predict(X_test), color=<span class="hljs-string"><span class="hljs-string">'green'</span></span>, linewidth=<span class="hljs-number"><span class="hljs-number">3</span></span>) plt.xlabel(<span class="hljs-string"><span class="hljs-string">'StateRegistrationOfBirth'</span></span>) plt.ylabel(<span class="hljs-string"><span class="hljs-string">'State Registration OfPaternity Examination'</span></span>) plt.title=<span class="hljs-string"><span class="hljs-string">"Regression on test data"</span></span></code> </pre><br><img src="https://habrastorage.org/webt/59/ed/19/59ed1928383a0521791023.png"><br><br><h2>  Part III <a name="III"></a>  : ‚ÄúOne head is good, but much better‚Äù - Regression on several grounds with regularization </h2><br><br>  To make it more interesting, let's choose another objective function, which is not so obviously linearly dependent on features. <br>  To match the title of the article, we select marriage registration as the target function. <br>  And we will make all the other columns from the set from the pictures of paired charts signs. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#get main data columns_to_show2=columns_to_show.copy() columns_to_show2.remove("StateRegistrationOfMarriage") #get data for a model X = data2[columns_to_show2].values y = data2['StateRegistrationOfMarriage'].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) y_train=np.reshape(y_train,[y_train.shape[0],1]) y_test=np.reshape(y_test,[y_test.shape[0],1])</span></span></code> </pre><br>  First we will teach just a linear regression model. <br><br><pre> <code class="python hljs">lr = linear_model.LinearRegression() lr.fit(X_train, y_train) print(<span class="hljs-string"><span class="hljs-string">'Coefficients:'</span></span>, lr.coef_) print(<span class="hljs-string"><span class="hljs-string">'Score:'</span></span>, lr.score(X_test,y_test))</code> </pre><br>  We get the result a little worse than in the past case (which is not surprising) <br><br> <code>Coefficients: [[-0.03475475 0.97143632 -0.44298685 -0.18245718]] <br> Score: 0.38137432065 <br></code> <br><br>  To combat retraining and / or feature selection, together with a linear regression model, the regularization mechanism is usually used, in this article we will look at the Lasso mechanism ( <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html">L1 - regularization</a> ) <br><br>  The higher the coefficient of regularization of alpha, the more actively the model penalizes large values ‚Äã‚Äãof attributes, up to the reduction of some coefficients of the regression equation to zero. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#let's look at the different alpha parameter: #large Rid=linear_model.Lasso (alpha = 0.01) Rid.fit(X_train, y_train) print(' Appha:', Rid.alpha) print(' Coefficients:', Rid.coef_) print(' Score:', Rid.score(X_test,y_test)) #Small Rid=linear_model.Lasso (alpha = 0.000000001) Rid.fit(X_train, y_train) print('\n Appha:', Rid.alpha) print(' Coefficients:', Rid.coef_) print(' Score:', Rid.score(X_test,y_test)) #Optimal (for these test data) Rid=linear_model.Lasso (alpha = 0.00025) Rid.fit(X_train, y_train) print('\n Appha:', Rid.alpha) print(' Coefficients:', Rid.coef_) print(' Score:', Rid.score(X_test,y_test))</span></span></code> </pre><br>  It should be noted that here I am doing straight things that are not good, adjusted the regularization coefficient to the test data, in reality you shouldn‚Äôt do that, but we‚Äôll get away with it purely for demonstration. <br><br>  Let's look at the conclusion: <br><br> <code>Appha: 0.01 <br> Coefficients: [ 0. 0.46642996 -0. -0. ] <br> Score: 0.222071102783 <br> <br> Appha: 1e-09 <br> Coefficients: [-0.03475462 0.97143616 -0.44298679 -0.18245715] <br> Score: 0.38137433837 <br> <br> Appha: 0.00025 <br> Coefficients: [-0.00387233 0.92989507 -0.42590052 -0.17411615] <br> Score: 0.385551648602 <br></code> <br><br>  In this case, the model with regularization does not greatly improve the quality, we will try to add more features. <br><br><h2>  Part IV <a name="IV"></a>  : ‚ÄúNot all gold glitters‚Äù - Add signs </h2><br>  Add an obviously useless feature ‚Äútotal number of registrations‚Äù, why is it obvious?  Now see for yourself. <br><br><pre> <code class="python hljs">columns_to_show3=columns_to_show2.copy() columns_to_show3.append(<span class="hljs-string"><span class="hljs-string">"TotalNumber"</span></span>) columns_to_show3 X = df2[columns_to_show3].values <span class="hljs-comment"><span class="hljs-comment"># y hasn't changed X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) y_train=np.reshape(y_train,[y_train.shape[0],1]) y_test=np.reshape(y_test,[y_test.shape[0],1])</span></span></code> </pre><br>  To start, look at the results without regularization. <br><br><pre> <code class="python hljs">lr = linear_model.LinearRegression() lr.fit(X_train, y_train) print(<span class="hljs-string"><span class="hljs-string">'Coefficients:'</span></span>, lr.coef_) print(<span class="hljs-string"><span class="hljs-string">'Score:'</span></span>, lr.score(X_test,y_test))</code> </pre><br> <code>Coefficients: [[-0.45286477 -0.08625204 -0.19375198 -0.63079401 1.57467774]] <br> Score: 0.999173764473 <br></code> <br><br>  Wow!  Almost 100% accuracy! <br>  How could this sign be called useless ?! <br>  Well, let's think sensibly, our number of marriages is included in the total, so if we have information about other signs, then accuracy is nearing 100%.  In practice, this is not particularly useful. <br><br>  Let's move on to our "Lasso" <br>  First, choose a small regularization coefficient: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Optimal (for these test data) Rid=linear_model.Lasso (alpha = 0.00015) Rid.fit(X_train, y_train) print('\n Appha:', Rid.alpha) print(' Coefficients:', Rid.coef_) print(' Score:', Rid.score(X_test,y_test))</span></span></code> </pre><br> <code>Appha: 0.00015 <br> Coefficients: [-0.44718703 -0.07491507 -0.1944702 -0.62034146 1.55890505] <br> Score: 0.999266251287 <br></code> <br>  Well, nothing much has changed, it is not interesting to us, let's see what happens if we increase it. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#large Rid=linear_model.Lasso (alpha = 0.01) Rid.fit(X_train, y_train) print('\n Appha:', Rid.alpha) print(' Coefficients:', Rid.coef_) print(' Score:', Rid.score(X_test,y_test))</span></span></code> </pre><br> <code>Appha: 0.01 <br> Coefficients: [-0. -0. -0. -0.05177979 0.87991931] <br> Score: 0.802210158982 <br></code> <br><br>  So, we see that almost all signs were considered useless by the model, and the most useful was a sign of the total number of records, so that if we suddenly had to use only 1-2 signs, we would know what to choose in order to minimize losses. <br><br>  Let's take it out of curiosity to see how the percentage of registration of marriages could be explained only by the total number of records. <br><br><pre> <code class="python hljs">X_train=np.reshape(X_train[:,<span class="hljs-number"><span class="hljs-number">4</span></span>],[X_train.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>],<span class="hljs-number"><span class="hljs-number">1</span></span>]) X_test=np.reshape(X_test[:,<span class="hljs-number"><span class="hljs-number">4</span></span>],[X_test.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>],<span class="hljs-number"><span class="hljs-number">1</span></span>]) lr = linear_model.LinearRegression() lr.fit(X_train, y_train) print(<span class="hljs-string"><span class="hljs-string">'Coefficients:'</span></span>, lr.coef_) print(<span class="hljs-string"><span class="hljs-string">'Score:'</span></span>, lr.score(X_train,y_train))</code> </pre><br> <code>Coefficients: [ 1.0571131] <br> Score: 0.788270672692 <br></code> <br>  Well, not bad, but objectively less than with the rest of the signs. <br>  Let's look at the graphics: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># plot for train data plt.figure(figsize=(8,10)) plt.subplot(211) plt.scatter(X_train, y_train, color='black') plt.plot(X_train, lr.predict(X_train), color='blue', linewidth=3) plt.xlabel('Total Number of Registration') plt.ylabel('State Registration Of Marriage') plt.title="Regression on train data" # plot for test data plt.subplot(212) plt.scatter(X_test, y_test, color='black') plt.plot(X_test, lr.predict(X_test), '--', color='green', linewidth=3) plt.xlabel('Total Number of Registration') plt.ylabel('State Registration Of Marriage') plt.title="Regression on test data"</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/59/ed/19/59ed192878a1a414938109.png"><br><br>  Let's try to add another useless attribute to the original data set. <br>  State Registration Of Name Change, you can build a model yourself and see how much of the data this attribute explains (take a little word for it). <br>  And we will immediately select the data and train the model on the old 4 signs and this useless <br><br><pre> <code class="python hljs">columns_to_show4=columns_to_show2.copy() columns_to_show4.append(<span class="hljs-string"><span class="hljs-string">"StateRegistrationOfNameChange"</span></span>) X = df2[columns_to_show4].values <span class="hljs-comment"><span class="hljs-comment"># y hasn't changed X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) y_train=np.reshape(y_train,[y_train.shape[0],1]) y_test=np.reshape(y_test,[y_test.shape[0],1]) lr = linear_model.LinearRegression() lr.fit(X_train, y_train) print('Coefficients:', lr.coef_) print('Score:', lr.score(X_test,y_test))</span></span></code> </pre><br> <code>Coefficients: [[ 0.06583714 1.1080889 -0.35025999 -0.24473705 -0.4513887 ]] <br> Score: 0.285094398157 <br></code> <br>  We will not try regularization; it will not fundamentally change anything; as you can see, this feature only spoils us. <br><br>  Let's finally pick a useful attribute. <br><br>  Everyone knows that there is a hot season for weddings (summer and early autumn), and there is a quiet season (winter). <br><br>  <i>By the way, I was surprised that in May there are few weddings.</i> <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#get data columns_to_show5=columns_to_show2.copy() columns_to_show5.append("Month") #get data for model X = df2[columns_to_show5].values # y hasn't changed X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) y_train=np.reshape(y_train,[y_train.shape[0],1]) y_test=np.reshape(y_test,[y_test.shape[0],1]) #teach model and get predictions lr = linear_model.LinearRegression() lr.fit(X_train, y_train) print('Coefficients:', lr.coef_) print('Score:', lr.score(X_test,y_test))</span></span></code> </pre><br> <code>Coefficients: [[-0.10613428 0.91315175 -0.55413198 -0.13253367 0.28536285]] <br> Score: 0.472057997208 <br></code> <br>  A good quality improvement and most importantly, everything corresponds to sound logic. <br><br><h2>  Part V <a name="V"></a>  : ‚ÄúCut the new caftan, and try on the old one!‚Äù - Trend forecast </h2><br>  The last thing we probably have left is to look at linear regression as a tool for predicting a trend.  In the previous chapters, we took the data randomly, that is, data from the entire time range fell into the training sample.  This time we will divide the data into past and future and see if we can predict something. <br><br>  For convenience, we will consider the period in months from January 2010, for this we will write a simple anonymous function that converts the data to us, eventually replacing the year column with the number of months. <br><br>  We will be trained on data until 2016, and everything that begins in 2016 will be the future for us. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#get data df3=df.copy() #get new column df3.Year=df.Year.map(lambda x: (x-2010)*12)+df.Month df3.rename(columns={'Year': 'Months'}, inplace=True) #get data for model X=df3[columns_to_show5].values y=df3['StateRegistrationOfMarriage'].values train=[df3.Months&lt;=72] test=[df3.Months&gt;72] X_train=X[train] y_train=y[train] X_test=X[test] y_test=y[test] y_train=np.reshape(y_train,[y_train.shape[0],1]) y_test=np.reshape(y_test,[y_test.shape[0],1]) #teach model and get predictions lr = linear_model.LinearRegression() lr.fit(X_train, y_train) print('Coefficients:', lr.coef_[0]) print('Score:', lr.score(X_test,y_test))</span></span></code> </pre><br> <code>Coefficients: [ 2.60708376e-01 1.30751121e+01 -3.31447168e+00 -2.34368684e-01 <br> 2.88096512e+02] <br> Score: 0.383195050367 <br></code> <br>  As can be seen with such a breakdown of data, the accuracy is somewhat reduced, and yet the quality of the prediction is better than a finger to the sky. <br>  Make sure to look at the graphics. <br><br><pre> <code class="python hljs">plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">9</span></span>,<span class="hljs-number"><span class="hljs-number">23</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># plot for train data plt.subplot(311) plt.scatter(df3.Months.values[train], y_train, color='black') plt.plot(df3.Months.values[train], lr.predict(X_train), color='blue', linewidth=2) plt.xlabel('Months (from 01.2010)') plt.ylabel('State Registration Of Marriage') plt.title="Regression on train data" # plot for test data plt.subplot(312) plt.scatter(df3.Months.values[test], y_test, color='black') plt.plot(df3.Months.values[test], lr.predict(X_test), color='green', linewidth=2) plt.xlabel('Months (from 01.2010)') plt.ylabel('State Registration Of Marriage') plt.title="Regression (prediction) on test data" # plot for all data plt.subplot(313) plt.scatter(df3.Months.values[train], y_train, color='black') plt.plot(df3.Months.values[train], lr.predict(X_train), color='blue', label='train', linewidth=2) plt.scatter(df3.Months.values[test], y_test, color='black') plt.plot(df3.Months.values[test], lr.predict(X_test), color='green', label='test', linewidth=2) plt.title="Regression (prediction) on all data" plt.xlabel('Months (from 01.2010)') plt.ylabel('State Registration Of Marriage') #plot line for link train to test plt.plot([72,73], lr.predict([X_train[-1],X_test[0]]) , color='magenta',linewidth=2, label='train to test') plt.legend()</span></span></code> </pre><br><img src="https://habrastorage.org/webt/59/ed/19/59ed1928a1dc8974980098.png"><br><br>  The graphs in blue represent the ‚Äúpast‚Äù, green ‚Äúthe future‚Äù, and purple a bunch. <br>  So, it is clear that our model imperfectly describes points, but at least takes into account seasonal patterns. <br><br>  Thus, it is hoped that in the future, according to the data available, the model will be able to guide us somehow, in terms of the registration of marriages. <br><br>  Although for the analysis of trends there are much more advanced tools that are beyond the scope of this article (and in my opinion, initial skills in Data Science) <br><br><h2>  Conclusion </h2><br>  Well, we have considered the task of restoring regression, I suggest you look for other dependencies, on the open data portals of the state structures of the country, you may find some interesting dependency.  As a ‚ÄúChallenge‚Äù, I suggest you dig up something on the open data portal of the Republic of Belarus <a href="https://opendata.by/">opendata.by</a> <br>  At the end of the picture, based on <a href="https://www.youtube.com/watch%3Fv%3Dvm_Y7Va1-yA">Alexander G.</a> communication <a href="https://www.youtube.com/watch%3Fv%3Dvm_Y7Va1-yA">with journalists</a> and answers to uncomfortable questions. <br><br><img src="https://habrastorage.org/webt/59/ed/19/59ed19293f8db145938768.png"><br><br><a name="VI"></a><br><h1>  Bonus - Increase accuracy, due to a different approach to the months </h1><br>  Colleagues left useful comments with recommendations for improving the quality of the prediction. <br><br>  In short, all the suggestions came down to the fact that in an attempt to simplify everything, I incorrectly coded the ‚ÄúMonth‚Äù column (and this is indeed so).  We will try to improve this in two ways. <br><br>  <b>Option 1</b> - One-hot coding, when for each value of the month a characteristic is created. <br><br>  First, load the original table without edits. <br><br><pre> <code class="python hljs">df_base = pd.read_csv(<span class="hljs-string"><span class="hljs-string">'https://op.mos.ru/EHDWSREST/catalog/export/get?id=230308'</span></span>, compression=<span class="hljs-string"><span class="hljs-string">'zip'</span></span>, header=<span class="hljs-number"><span class="hljs-number">0</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'cp1251'</span></span>, sep=<span class="hljs-string"><span class="hljs-string">';'</span></span>, quotechar=<span class="hljs-string"><span class="hljs-string">'"'</span></span>)</code> </pre><br>  Then we apply the one-hot encoding implemented in the pandas data frame library (get_dummies function), remove unnecessary columns, and rerun the model's training and graphics drawing. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#get data for model df4=df_base.copy() df4.drop(['Year','StateRegistrationOfMarriage','ID','global_id','Unnamed: 12','TotalNumber','StateRegistrationOfNameChange','StateRegistrationOfAdoption'],axis=1,inplace=True) df4=pd.get_dummies(df4,prefix=['Month']) X=df4.values X_train=X[train] X_test=X[test] #teach model and get predictions lr = linear_model.LinearRegression() lr.fit(X_train, y_train) print('Coefficients:', lr.coef_[0]) print('Score:', lr.score(X_test,y_test)) # plot for all data plt.scatter(df3.Months.values[train], y_train, color='black') plt.plot(df3.Months.values[train], lr.predict(X_train), color='blue', label='train', linewidth=2) plt.scatter(df3.Months.values[test], y_test, color='black') plt.plot(df3.Months.values[test], lr.predict(X_test), color='green', label='test', linewidth=2) plt.title="Regression (prediction) on all data" plt.xlabel('Months (from 01.2010)') plt.ylabel('State Registration Of Marriage') #plot line for link train to test plt.plot([72,73], lr.predict([X_train[-1],X_test[0]]) , color='magenta',linewidth=2, label='train to test')</span></span></code> </pre><br>  Will get <br><br> <code>Coefficients: [ 2.18633008e-01 -1.41397731e-01 4.56991414e-02 -5.17558633e-01 <br> 4.48131002e+03 -2.94754108e+02 -1.14429758e+03 3.61201946e+03 <br> 2.41208054e+03 -3.23415050e+03 -2.73587261e+03 -1.31020899e+03 <br> 4.84757208e+02 3.37280689e+03 -2.40539320e+03 -3.23829714e+03] <br> Score: 0.869208071831 <br></code> <br><br><img src="https://habrastorage.org/webt/59/ef/a3/59efa3ee4a1e3733333406.png"><br><br>  The quality has grown greatly! <br><br>  <b>Option 2</b> - target encoding, we encode each month with the average value of the objective function for this month in the training sample (thanks to <a href="https://habrahabr.ru/users/roryorangepants/" class="user_link">roryorangepants</a> ) <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#get data for pandas data frame df5=df_base.copy() d=dict() #get we obtain the mean value of Registration Of Marriages by months on the training data for mon in df5.Month.unique(): d[mon]=df5.StateRegistrationOfMarriage[df5.Month.values[train]==mon].mean() #d+={} df5['MeanMarriagePerMonth']=df5.Month.map(d) df5.drop(['Month','Year','StateRegistrationOfMarriage','ID','global_id','Unnamed: 12','TotalNumber', 'StateRegistrationOfNameChange','StateRegistrationOfAdoption'],axis=1,inplace=True) #get data for model X=df5.values X_train=X[train] X_test=X[test] #teach model and get predictions lr = linear_model.LinearRegression() lr.fit(X_train, y_train) print('Coefficients:', lr.coef_[0]) print('Score:', lr.score(X_test,y_test)) # plot for all data plt.scatter(df3.Months.values[train], y_train, color='black') plt.plot(df3.Months.values[train], lr.predict(X_train), color='blue', label='train', linewidth=2) plt.scatter(df3.Months.values[test], y_test, color='black') plt.plot(df3.Months.values[test], lr.predict(X_test), color='green', label='test', linewidth=2) plt.title="Regression (prediction) on all data" plt.xlabel('Months (from 01.2010)') plt.ylabel('State Registration Of Marriage') #plot line for link train to test plt.plot([72,73], lr.predict([X_train[-1],X_test[0]]) , color='magenta',linewidth=2, label='train to test')-</span></span></code> </pre><br>  We get: <br><br> <code>Coefficients: [ 0.16556761 -0.12746446 -0.03652408 -0.21649349 0.96971467] <br> Score: 0.875882918435 <br></code> <br><br><img src="https://habrastorage.org/webt/59/ef/a3/59efa3ee5abab919165487.png"><br><br>  Very similar in terms of quality result, with significantly fewer signs used. <br><br>  Well, I hope everything. <br><br>  Here's a goodbye picture with Mr. "Zhoposranchik", I hope she will not offend anyone and will not cause the "holivar" :) <br><br><img src="https://habrastorage.org/webt/59/ef/a0/59efa0fba4e33667047429.png"></div><p>Source: <a href="https://habr.com/ru/post/340698/">https://habr.com/ru/post/340698/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../340688/index.html">Each data center - by virtual reality</a></li>
<li><a href="../340690/index.html">Document generation - personal experience</a></li>
<li><a href="../340692/index.html">What stories will be popular in terms of information noise. Future storytelling</a></li>
<li><a href="../340694/index.html">How we participated in the hackathon M.Video</a></li>
<li><a href="../340696/index.html">array_ * vs foreach or PHP7 vs PHP5</a></li>
<li><a href="../340700/index.html">Language Faylyuren, or what do we do with the language?</a></li>
<li><a href="../340704/index.html">Artificial Intelligence Nemesida WAF</a></li>
<li><a href="../340706/index.html">Testing redux</a></li>
<li><a href="../340708/index.html">Solution of the issue with an Internet in the country</a></li>
<li><a href="../340712/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ285 (October 16 - 22, 2017)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>