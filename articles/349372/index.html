<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How is Alice. Yandex lecture</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this lecture, technological solutions are first considered, on the basis of which Alice works ‚Äî Yandex‚Äôs voice assistant. The leader of the dialogu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How is Alice. Yandex lecture</h1><div class="post__text post__text-html js-mediator-article">  In this lecture, technological solutions are first considered, on the basis of which Alice works ‚Äî Yandex‚Äôs voice assistant.  The leader of the dialogue systems development team Boris Yangel <a href="https://habrahabr.ru/users/hr0nix/" class="user_link">hr0nix</a> tells how his team teaches Alice to understand the desires of the user, to find answers to the most unexpected questions and at the same time behave decently. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/_law_tey0OQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  - I'll tell you what's inside Alice.  Alice is big, there are many components in it, so I‚Äôll go over the surface a little. <br><a name="habracut"></a><br>  Alice is a voice assistant launched by Yandex on October 10, 2017.  It is in the Yandex application on iOS and Android, as well as in a mobile browser and as a separate application under Windows.  There you can solve your problems, find information in a dialogue format, communicating with it with text or voice.  And there is a killer feature that made Alice quite famous in runet.  We use not only pre-known scenarios.  Sometimes, when we do not know what to do, we use the full power of deep learning to generate a response on behalf of Alice.  This is quite funny and allowed us to ride the HYIP train. <br><img src="https://habrastorage.org/webt/gj/3y/xl/gj3yxlqbr7ujuqr9r2akacxmkee.jpeg"><br>  What does Alice look like high-level? <br><br>  The user says: ‚ÄúAlisa, what weather will we expect tomorrow?‚Äù 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      First of all, we stream it into the recognition server, it turns it into text, and this text then goes into the service that my team is developing, into an entity such as an integer classifier.  This is a machine learning thing whose task is to determine what the user wanted to say with his own phrase.  In this example, the intents classifier could say: OK, the user probably needs the weather. <br><br>  Then for each intent there is a special model called a semantic tegger.  The task of the model is to isolate useful nuggets of information in what the user said.  A weather tegger could say that tomorrow is the date for which the user needs weather.  And we turn all these parsing results into some structured representation, which is called a frame.  It will say that it is an intent weather, that the weather is needed for +1 day from the current day, and where it is unknown.  All this information enters the dialog manager module, which, in addition to this, knows the current context of the dialogue, knows what has happened up to this point.  He receives the results of the analysis of the replica at the entrance, and he must decide what to do with them.  For example, he can go to the API, find out the weather for tomorrow in Moscow, because the user's geolocation is Moscow, even though he did not indicate it.  And to say - generate a text that describes the weather, then send it to a speech synthesis module that will speak to the user in Alice‚Äôs beautiful voice. <br><img src="https://habrastorage.org/webt/1p/xr/ek/1pxrekbsu8rzgmyyvmg4gn8ukqs.jpeg"><br>  Dialog Manager.  There is no machine learning, no reinforcement learning, there are only configs, scripts and rules.  It works predictably, and it‚Äôs clear how to change it, if needed.  If the manager comes and says change, then we can do it in a short time. <br><br>  At the heart of the Dialog Manager concept is a concept known to those involved in dialogue systems like form-filling.  The idea is that the user fills in some kind of virtual form with his replicas, and when he fills in all the required fields in it, his need can be met.  Event-driven engine: every time a user does something, some events happen that you can subscribe to, write their handlers in Python and thus construct the logic of the dialogue. <br><br>  When you need to generate a phrase in scripts - for example, we know what the user is talking about the weather and need to answer about the weather - we have a powerful template language that allows us to write these phrases.  This is how it looks. <br><img src="https://habrastorage.org/webt/-l/3x/rn/-l3xrnwjwmgbke8leymci0dgorw.jpeg"><br>  This is an add-on to the Jinja2 python-language templating system, to which all sorts of linguistic tools have been added, such as the ability to incline words or align numerals and nouns, so that coherent text can be easily written, randomized text pieces to increase Alice‚Äôs speech variability. <br><img src="https://habrastorage.org/webt/m_/eg/ni/m_egnidszfysxb8fs9punwoj_fy.jpeg"><br>  In the intents classifier, we managed to try many different models, ranging from logistic regression to gradient boosting, recurrent networks.  As a result, we stopped at the classifier, which is based on the nearest neighbors, because it has a bunch of good properties that other models do not have. <br><br>  For example, you often need to deal with intents for which you have literally a few examples.  It is impossible to just learn the usual multiclass classifiers in this mode.  For example, it turns out that in all the examples, of which there are only five, there was a particle ‚Äúa‚Äù or ‚Äúhow‚Äù, which was not in other examples, and the classifier finds the simplest solution.  He decides that if the word ‚Äúhow‚Äù occurs, then this is exactly the intent.  But this is not what you want.  You want the semantic proximity of what the user said to the phrases that lie in the train for this intent. <br><br>  As a result, we pre-train the metric on a large dataset, which tells how semantically close two phrases are, and then use this metric, looking for the nearest neighbors in our trainset. <br><br>  Another good quality of this model is that it can be quickly updated.  You have new phrases, you want to see how Alice's behavior changes.  All you need is to add their many potential examples for the classifier of the nearest neighbors, you do not need to re-select the entire model.  For example, for our recurrent model, it took several hours.  It is not very convenient to wait a few hours when you change something to see the result. <br><img src="https://habrastorage.org/webt/8l/e3/rl/8le3rlpztinlmzkuq13gsduc_ha.jpeg"><br>  Semantic tegger.  We tried conditional random fields and recurrent networks.  Networks, of course, work much better, it is not a secret to anyone.  We do not have unique architectures there, the usual bidirectional LSTM with attention, plus or minus state-of-the-art for the tagging task.  Everybody does it and we do it. <br><br>  The only thing is that we actively use the N-best hypotheses, we do not generate only the most likely hypothesis, because sometimes we need not the most likely.  For example, we often reweigh hypotheses, depending on the current state of the dialog in the dialog manager. <br><br>  If we know that at the previous step we asked a question about something, and there is a hypothesis where a tegger found something and a hypothesis where he did not, then probably, all other things being equal, the first is more likely.  Such tricks allow us to slightly improve the quality. <br><br>  And the machine-trained tegger sometimes makes mistakes, and it is not entirely accurate to find the meaning of slots in the most plausible hypothesis.  In this case, we are looking for a hypothesis in N-best, which is in better agreement with what we know about the types of slots, which also allows us to earn some quality. <br><img src="https://habrastorage.org/webt/gp/uy/aw/gpuyawwvfegdxcdfimbntyon7oc.jpeg"><br>  Even in the dialogues there is such a phenomenon Anaphora.  This is when you use a pronoun to refer to an object that was previously in the dialogue.  Say, say "the height of Everest," and then "in what country it is located."  We are able to resolve anaphores.  For this we have two systems. <br><img src="https://habrastorage.org/webt/kq/zn/yz/kqznyzjoviab3eun0ww73big46k.jpeg"><br>  One general-purpose system that can work on any replicas.  It works on top of parsing all user replicas.  If we see a pronoun in its current cue, we look for known phrases in what he said earlier, count the speed for each of them, see if we can substitute pronouns for it, and choose the best if we can. <br><br>  We also have an anaphor resolution system based on form filling, it works like this: if there was a geoobject in the previous intent, and there is a slot for the geoobject in the current intent, and it is not populated, and we also hit the current intent with the pronoun ‚Äúthere‚Äù, then probably the previous geoobject can be imported from the form and substituted here.  This is a simple heuristic, but it makes a good impression and works great.  In the part of intents, one system works, and in part both.  We look where it works, where it does not work, flexibly customize it. <br><img src="https://habrastorage.org/webt/w5/lh/o1/w5lho1vrjri_xtmcnelnweyfoq8.jpeg"><br>  There is an ellipse.  This is when in a dialogue you omit some words, because they are implied from the context.  For example, you can say ‚Äútell the weather,‚Äù and then ‚Äúand on weekends?‚Äù, Meaning ‚Äútell the weather on the weekend,‚Äù but you want to repeat these words, because this is useless. <br><br>  We also know how to work with ellipses in the following way.  Elliptical or clarifying phrases are separate intents. <br><img src="https://habrastorage.org/webt/wx/ze/zj/wxzezjv3nvkvvy7_bkbkumfwarw.jpeg"><br>  If there is an get_weather intent for which there are phrases like ‚Äútell the weather‚Äù, ‚Äúwhat is the weather today‚Äù, then he will have a pair intent get_weather_ellipsis, in which various weather updates are: ‚Äúand for tomorrow‚Äù, ‚Äúand for the weekend‚Äù, ‚Äúand what is there in Sochi "and so on.  And these elliptic intents in the classifier of intents compete on equal terms with their parents.  If you say ‚Äúbut in Moscow?‚Äù, The classifier of intents, for example, would say that with a probability of 0.5 this is a refinement in the intensity of the weather, and with a probability of 0.5 it is a refinement in the search for organizations, for example.  And then the dialogue engine is re-weighted by the scores, which were assigned by the classifier of intents, which were assigned with regard to the current dialogue, because, for example, it knows that there was a conversation about the weather before, and this was hardly a clarification about the search for organizations, rather it was about the weather. . <br><br>  This approach allows learning and defining ellipses without context.  You can simply type examples of elliptical phrases from somewhere without what was before.  This is quite convenient when you make new intents that are not in the logs of your service.  You can either fantasize or invent something, or try to collect long dialogues on a crowdsourcing platform.  And you can easily synthesize for the first iteration of such elliptical phrases, they will somehow work, and then collect logs. <br><img src="https://habrastorage.org/webt/th/wm/kh/thwmkhgamcbl4yjeg9w_f9f0whk.jpeg"><br>  Here is the jewel of our collection, we call it a chatterer.  This is the neural network that, in any incomprehensible situation, is responsible for something on behalf of Alice and allows to conduct with her often strange and often amusing dialogues. <br><img src="https://habrastorage.org/webt/dx/ps/q8/dxpsq86bak-yzd9q7w7ebjbfwg8.jpeg"><br>  Talk - actually fallback.  In Alice, it works in such a way that if the intents classifier cannot confidently determine what the user wants, the other binary classifier first tries to solve it - maybe this is a search query and we will find something useful in the search and send it there?  If the classifier says no, this is not a search query, but just chatter, then a fallback to the chat will work.  A chat is a system that receives the current context of a dialogue, and its task is to generate the most relevant answer.  Moreover, scenario dialogues can also be part of the context: if you talked about the weather, and then said something incomprehensible, the talker will work. <br><img src="https://habrastorage.org/webt/dy/mp/ve/dympvem4-tilguyqprargsruzkg.jpeg"><br>  This allows us to do these things.  You asked about the weather, and then she somehow commented on the talker.  When it works, it looks very cool. <br><img src="https://habrastorage.org/webt/qo/sa/s5/qosas5p6fssvugvpgrw6z0favsu.jpeg"><br>  Boltalka is a DSSM-like neural network, where there are two encoder towers.  One encoder encodes the current dialog context, the other is the candidate response.  You get two embedding-vectors for the answer and the context, and the network is trained so that the cosine distance between them is the greater, the more relevant the response in the context and the more inappropriate.  In the literature, this idea has long been known. <br><img src="https://habrastorage.org/webt/bn/qf/6l/bnqf6lszoydmbowqgc4nyzuu9vq.jpeg"><br>  Why does everything seem to work well with us - it seems like a bit better than in the articles? <br><br>  There is no silver bullet.  There is no technology that allows you to suddenly make a cool talking neural network.  We managed to achieve good quality, because we have won everywhere as a little.  We have long picked up the architecture of these tower-encoder so that they work best.  It is very important to choose the right scheme for sampling negative examples in training.  When you study on interactive cases, you have only positive examples that were once said by someone in such a context.  And there are no negative ones - they need to be somehow generated from this body.  There are many different techniques, and some work better than others. <br><br>  It is important how you choose the answer from the top candidates.  You can choose the most likely answer offered by the model, but this is not always the best thing to do, because when training, the model did not take into account all the characteristics of a good answer that exist from a product point of view. <br><br>  It is also very important what data sets you use, how you filter them. <br><img src="https://habrastorage.org/webt/bm/oo/ym/bmooymekgdyr74vcbzm7abuwvky.jpeg"><br>  In order to make a bit of quality out of this, you need to be able to measure everything you do.  And here our pride is that we can measure all aspects of the quality of the system on our crowdsourcing platform using a button.  When we have a new algorithm for generating results, we can generate a response from a new model on a special test package in a few clicks.  And - to measure all aspects of the quality of the resulting model in Toloka.  The main metric we use is the logical relevance of responses in context.  Do not talk nonsense, which is in no way connected with this context. <br><br>  There are a number of additional metrics that we try to optimize.  This is when Alice addresses the user to ‚Äúyou‚Äù, speaks about herself in the masculine gender and says all sorts of audacity, nastiness and nonsense. <br><br>  Highly, I told everything I wanted.  Thank. </div><p>Source: <a href="https://habr.com/ru/post/349372/">https://habr.com/ru/post/349372/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../349360/index.html">Bitcoin will not be a new digital money: about the drug business, transaction costs and gold</a></li>
<li><a href="../349362/index.html">SVG masks and wow effects: magic with simple words</a></li>
<li><a href="../349364/index.html">OOP without "O"</a></li>
<li><a href="../349366/index.html">Web components. Part 1: Custom Elements</a></li>
<li><a href="../349370/index.html">How I adjusted Telegram notifications for Mi Band 2</a></li>
<li><a href="../349374/index.html">Script Editor Age of Empires 2 can be turned into a Turing machine</a></li>
<li><a href="../349376/index.html">Game development for NES in C. Chapters 11-13. We write and debug a simple platformer</a></li>
<li><a href="../349378/index.html">Security Operations: protection against cyber threats in ServiceNow</a></li>
<li><a href="../349380/index.html">Angular 5: Unit tests</a></li>
<li><a href="../349382/index.html">DigiCert bought Symantec Website Security: implications for users of SSL / TLS certificates</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>