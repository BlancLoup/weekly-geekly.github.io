<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How we increased Tensorflow Serving performance by 70%</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tensorflow has become a standard machine learning platform (ML), popular in both industry and research. Many free libraries, tools and frameworks have...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How we increased Tensorflow Serving performance by 70%</h1><div class="post__text post__text-html js-mediator-article">  Tensorflow has become a standard machine learning platform (ML), popular in both industry and research.  Many free libraries, tools and frameworks have been created for teaching and servicing ML models.  The Tensorflow Serving project helps to serve ML models in a distributed production environment. <br><br>  Our Mux service uses Tensorflow Serving in several parts of the infrastructure; we have already discussed the use of Tensorflow Serving in video encoding by titles.  Today we will focus on methods that improve latency by optimizing both the prediction server and the client.  Model forecasts are usually ‚Äúon-line‚Äù operations (on the critical path of an application request), so the main optimization goals are to handle large volumes of requests with the lowest possible latency. <br><a name="habracut"></a><br><h1>  What is Tensorflow Serving? </h1><br>  Tensorflow Serving provides a flexible server architecture for deploying and maintaining ML models.  Once the model is trained and ready to use for prediction, Tensorflow Serving requires exporting it to a compatible (servable) format. <br><br>  <i>Servable</i> is a central abstraction that wraps Tensorflow objects.  For example, a model can be represented as one or more Servable objects.  Thus, Servable are the basic objects that the client uses to perform calculations.  The size of Servable matters: smaller models take up less space, use less memory and load faster.  for download and maintenance using the Predict API, models must be in the SavedModel format. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/20b/9f9/47e/20b9f947ea0f318dc7c2eba619b3901f.png"><br><br>  Tensorflow Serving integrates the main components for creating a gRPC / HTTP server that serves several ML models (or several versions), provides monitoring components and a custom architecture. <br><br><h1>  Tensorflow Serving with Docker </h1><br>  Let's look at the basic metrics of performance delays in forecasting with the standard settings of Tensorflow Serving (without CPU optimization). <br><br>  First, download the latest image from the TensorFlow Docker hub: <br><br><pre><code class="bash hljs">docker pull tensorflow/serving:latest</code> </pre> <br>  In this article, all containers run on a host with four cores, 15 GB, Ubuntu 16.04. <br><br><h3>  Export Tensorflow model in SavedModel format </h3><br>  When the model is trained using Tensorflow, the output can be saved as variable control points (files on disk).  The output is performed directly by restoring the control points of the model or on a frozen graph (binary file) fixed format. <br><br>  For Tensorflow Serving this frozen graph needs to be exported to the SavedModel format.  The <a href="https://www.tensorflow.org/serving/serving_basic">Tensorflow documentation</a> has examples of exporting trained models to the SavedModel format. <br><br>  Tensorflow also provides many <a href="https://github.com/tensorflow/models">official and research</a> models as a starting point for experimentation, research or production. <br><br>  As an example, we will use the <a href="https://github.com/tensorflow/models/tree/master/official/resnet">deep residual neural network (ResNet) model</a> to classify the ImageNet data set of 1000 classes.  Download the <a href="https://github.com/tensorflow/models/tree/master/official/resnet">pre</a> - <code>ResNet-50 v2</code> model, specifically the Channels_last (NHWC) <i>variant</i> in <i>SavedModel</i> : as a rule, it works better on the CPU. <br><br>  Copy the RestNet model directory to the following structure: <br><br><pre> <code class="plaintext hljs">models/ 1/ saved_model.pb variables/ variables.data-00000-of-00001 variables.index</code> </pre> <br>  Tensorflow Serving expects a numerically ordered directory structure for versioning.  In our case, the <code>1/</code> directory corresponds to the version 1 model, which contains the model architecture <code>saved_model.pb</code> with a snapshot of the model weights (variables). <br><br><h3>  Loading and processing SavedModel </h3><br>  The following command starts the Tensorflow Serving model server in the Docker container.  To load the SavedModel, you must mount the model directory in the expected container directory. <br><br><pre> <code class="plaintext hljs">docker run -d -p 9000:8500 \ -v $(pwd)/models:/models/resnet -e MODEL_NAME=resnet \ -t tensorflow/serving:latest</code> </pre> <br>  Checking container logs shows that ModelServer is running and ready to serve output requests for the <code>resnet</code> model at the gRPC and HTTP endpoints: <br><br><pre> <code class="plaintext hljs">... I tensorflow_serving/core/loader_harness.cc:86] Successfully loaded servable version {name: resnet version: 1} I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ... I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 ...</code> </pre> <br><h3>  Forecasting client </h3><br>  Tensorflow Serving defines a <a href="https://developers.google.com/protocol-buffers/">protocol</a> API in <a href="https://developers.google.com/protocol-buffers/">protocol buffers</a> (protobufs) format.  The gRPC client implementations for the prediction API are packaged as a tensor flow tensorflow_serving.apis package.  We will also need the Python package <code>tensorflow</code> for service functions. <br><br>  Install the dependencies to create a simple client: <br><br><pre> <code class="plaintext hljs">virtualenv .env &amp;&amp; source .env/bin/activate &amp;&amp; \ pip install numpy grpcio opencv-python tensorflow tensorflow-serving-api</code> </pre> <br>  The <code>ResNet-50 v2</code> model waits on input for floating point tensors in the formatted channels_last data structure (NHWC).  Therefore, the input image is read using opencv-python and loaded into the numpy array (height √ó width √ó channels) as the float32 data type.  The script below creates a prediction client stub and loads the JPEG data into the numpy array, converts it into tensor_proto, to make a gRPC prediction request: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/env python from __future__ import print_function import argparse import numpy as np import time tt = time.time() import cv2 import tensorflow as tf from grpc.beta import implementations from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2 parser = argparse.ArgumentParser(description='incetion grpc client flags.') parser.add_argument('--host', default='0.0.0.0', help='inception serving host') parser.add_argument('--port', default='9000', help='inception serving port') parser.add_argument('--image', default='', help='path to JPEG image file') FLAGS = parser.parse_args() def main(): # create prediction service client stub channel = implementations.insecure_channel(FLAGS.host, int(FLAGS.port)) stub = prediction_service_pb2.beta_create_PredictionService_stub(channel) # create request request = predict_pb2.PredictRequest() request.model_spec.name = 'resnet' request.model_spec.signature_name = 'serving_default' # read image into numpy array img = cv2.imread(FLAGS.image).astype(np.float32) # convert to tensor proto and make request # shape is in NHWC (num_samples x height x width x channels) format tensor = tf.contrib.util.make_tensor_proto(img, shape=[1]+list(img.shape)) request.inputs['input'].CopyFrom(tensor) resp = stub.Predict(request, 30.0) print('total time: {}s'.format(time.time() - tt)) if __name__ == '__main__': main()</span></span></code> </pre> <br>  Having received a JPEG at the input, a working client will produce the following result: <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 2.56152906418s</code> </pre> <br>  The resulting tensor contains the forecast in the form of an integer value and the probability of features. <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">1</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> } } outputs { key: <span class="hljs-string"><span class="hljs-string">"probabilities"</span></span> ...</code> </pre> <br>  For a single request, such a delay is unacceptable.  But nothing surprising: the default Tensorflow Serving binary is designed for the widest range of equipment for most uses.  You probably noticed the following lines in the logs of the standard Tensorflow Serving container: <br><br><pre> <code class="plaintext hljs">I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA</code> </pre> <br>  This points to the TensorFlow Serving binary, running on a CPU platform for which it has not been optimized. <br><br><h3>  Build an optimized binary </h3><br>  According to the Tensorflow <a href="">documentation</a> , it is recommended to compile Tensorflow from source with all the optimizations available for the CPU on the host where the binary will work.  When building special flags, you can activate CPU instruction sets for a specific platform: <br><br><table><tbody><tr><th>  Instruction set </th><th>  Flags </th></tr><tr><td>  Avx </td><td>  --copt = -mavx </td></tr><tr><td>  AVX2 </td><td>  --copt = -mavx2 </td></tr><tr><td>  Fma </td><td>  --copt = -mfma </td></tr><tr><td>  SSE 4.1 </td><td>  --copt = -msse4.1 </td></tr><tr><td>  SSE 4.2 </td><td>  --copt = -msse4.2 </td></tr><tr><td>  All processor supported </td><td>  --copt = -march = native </td></tr></tbody></table><br>  Clone Tensorflow Serving a specific version.  In our case, this is 1.13 (the last one at the time of publication of this article): <br><br><pre> <code class="bash hljs">USER=<span class="hljs-variable"><span class="hljs-variable">$1</span></span> TAG=<span class="hljs-variable"><span class="hljs-variable">$2</span></span> TF_SERVING_VERSION_GIT_BRANCH=<span class="hljs-string"><span class="hljs-string">"r1.13"</span></span> git <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> --branch=<span class="hljs-string"><span class="hljs-string">"</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$TF_SERVING_VERSION_GIT_BRANCH</span></span></span><span class="hljs-string">"</span></span> https://github.com/tensorflow/serving</code> </pre> <br>  In the dev-image Tensorflow Serving, the Basel tool is used to build.  We configure it for specific sets of CPU instructions: <br><br><pre> <code class="bash hljs">TF_SERVING_BUILD_OPTIONS=<span class="hljs-string"><span class="hljs-string">"--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2"</span></span></code> </pre> <br>  If there is not enough memory, limit the memory consumption during the build process to the <code>--local_resources=2048,.5,1.0</code> .  For information on flags, see the <a href="https://www.tensorflow.org/tfx/serving/docker">Tensorflow Serving and Docker</a> Help, as well as the <a href="https://docs.bazel.build/versions/master/user-manual.html">Bazel documentation</a> . <br><br>  Create a working image based on the existing one: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash USER=$1 TAG=$2 TF_SERVING_VERSION_GIT_BRANCH="r1.13" git clone --branch="${TF_SERVING_VERSION_GIT_BRANCH}" https://github.com/tensorflow/serving TF_SERVING_BUILD_OPTIONS="--copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.1 --copt=-msse4.2" cd serving &amp;&amp; \ docker build --pull -t $USER/tensorflow-serving-devel:$TAG \ --build-arg TF_SERVING_VERSION_GIT_BRANCH="${TF_SERVING_VERSION_GIT_BRANCH}" \ --build-arg TF_SERVING_BUILD_OPTIONS="${TF_SERVING_BUILD_OPTIONS}" \ -f tensorflow_serving/tools/docker/Dockerfile.devel . cd serving &amp;&amp; \ docker build -t $USER/tensorflow-serving:$TAG \ --build-arg TF_SERVING_BUILD_IMAGE=$USER/tensorflow-serving-devel:$TAG \ -f tensorflow_serving/tools/docker/Dockerfile .</span></span></code> </pre> <br>  ModelServer is configured using <a href="https://github.com/tensorflow/tensorflow/blob/26b4dfa65d360f2793ad75083c797d57f8661b93/tensorflow/core/protobuf/config.proto">TensorFlow flags</a> to support concurrency.  The following parameters configure two thread pools for parallel operation: <br><br><pre> <code class="plaintext hljs">intra_op_parallelism_threads</code> </pre> <br><ul><li>  controls the maximum number of threads for parallel execution of a single operation; <br></li><li>  used to parallelize operations that have suboperations that are independent in nature. </li></ul><br><pre> <code class="plaintext hljs">inter_op_parallelism_threads</code> </pre> <br><ul><li>  controls the maximum number of threads for concurrent independent operations; <br></li><li>  operations on Tensorflow Graph, which are independent of each other and, therefore, can be performed in different threads. </li></ul><br>  The default for both parameters is <code>0</code> .  This means that the system itself selects the corresponding number, which most often means one thread per core.  However, the parameter can be manually changed for multi-core concurrency. <br><br>  Then start the Serving container similarly to the previous one, this time with the Docker image gathered from the sources, and with the Tensorflow optimization flags for the specific processor: <br><br><pre> <code class="bash hljs">docker run -d -p 9000:8500 \ -v $(<span class="hljs-built_in"><span class="hljs-built_in">pwd</span></span>)/models:/models/resnet -e MODEL_NAME=resnet \ -t <span class="hljs-variable"><span class="hljs-variable">$USER</span></span>/tensorflow-serving:<span class="hljs-variable"><span class="hljs-variable">$TAG</span></span> \ --tensorflow_intra_op_parallelism=4 \ --tensorflow_inter_op_parallelism=4</code> </pre> <br>  Container logs should no longer show warnings about an undefined CPU.  Without changing the code on the same forecast request, the delay is reduced by about 35.8%: <br><br><pre> <code class="bash hljs">python tf_serving_client.py --image=images/pupper.jpg total time: 1.64234706879s</code> </pre> <br><h3>  Speed ‚Äã‚Äãincrease in client prediction </h3><br>  Is it possible to accelerate?  We optimized the server part for our CPU, but the delay of more than 1 second still seems too long. <br><br>  It so happened that the load on the <code>tensorflow_serving</code> and <code>tensorflow</code> libraries makes a significant contribution to the delay.  Each unnecessary <code>tf.contrib.util.make_tensor_proto</code> call also adds a split second. <br><br>  You will ask: ‚ÄúDo we not need TensorFlow Python packages to actually make prediction requests to the Tensorflow server?‚Äù In fact, there is no real <i>need</i> for <code>tensorflow_serving</code> and <code>tensorflow</code> packages. <br><br>  As noted earlier, the Tensorflow prediction APIs are defined as protobuffers.  Therefore, two external dependencies can be replaced with the corresponding <code>tensorflow</code> and <code>tensorflow_serving</code> - and then there is no need to pull out the entire (heavy) Tensorflow library on the client. <br><br>  First, get rid of the dependencies of <code>tensorflow</code> and <code>tensorflow_serving</code> and add the <code>grpcio-tools</code> package. <br><br><pre> <code class="bash hljs">pip uninstall tensorflow tensorflow-serving-api &amp;&amp; \ pip install grpcio-tools==1.0.0</code> </pre> <br>  Clone <code>tensorflow/tensorflow</code> and <code>tensorflow/serving</code> repositories and copy the following protobuf files into the client project: <br><br><pre> <code class="plaintext hljs">tensorflow/serving/ tensorflow_serving/apis/model.proto tensorflow_serving/apis/predict.proto tensorflow_serving/apis/prediction_service.proto tensorflow/tensorflow/ tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/types.proto</code> </pre> <br>  Copy these protobuf files to the <code>protos/</code> directory while maintaining the original paths: <br><br><pre> <code class="plaintext hljs">protos/ tensorflow_serving/ apis/ *.proto tensorflow/ core/ framework/ *.proto</code> </pre> <br>  For simplicity, the <a href="https://github.com/tensorflow/serving/blob/master/tensorflow_serving/apis/prediction_service.proto">prediction_service.proto</a> can be simplified to implement only Predict RPC in order not to download the nested dependencies of other RPCs specified in the service.  <a href="https://gist.github.com/masroorhasan/8e728917ca23328895499179f4575bb8">Here is</a> an example of a simplified <code>prediction_service.</code> . <code>prediction_service.</code> . <br><br>  Create gRPC Python implementations using <code>grpcio.tools.protoc</code> : <br><br><pre> <code class="plaintext hljs">PROTOC_OUT=protos/ PROTOS=$(find . | grep "\.proto$") for p in $PROTOS; do python -m grpc.tools.protoc -I . --python_out=$PROTOC_OUT --grpc_python_out=$PROTOC_OUT $p done</code> </pre> <br>  Now the entire <code>tensorflow_serving</code> module can be removed: <br><br><pre> <code class="plaintext hljs">from tensorflow_serving.apis import predict_pb2 from tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  ... and replace with generated protobuffers from <code>protos/tensorflow_serving/apis</code> : <br><br><pre> <code class="plaintext hljs">from protos.tensorflow_serving.apis import predict_pb2 from protos.tensorflow_serving.apis import prediction_service_pb2</code> </pre> <br>  The Tensorflow library is imported to use the auxiliary function <code>make_tensor_proto</code> , which is <a href="https://www.tensorflow.org/api_docs/python/tf/make_tensor_proto">needed to wrap the</a> python / numpy object as a TensorProto object. <br><br>  Thus, we can replace the following dependency and code snippet: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf ... tensor = tf.contrib.util.make_tensor_proto(features) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br>  importing protobuffers and building a TensorProto object: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensor_shape_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> protos.tensorflow.core.framework <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> types_pb2 ... <span class="hljs-comment"><span class="hljs-comment"># ensure NHWC shape and build tensor proto tensor_shape = [1]+list(img.shape) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) for dim in tensor_shape] tensor_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=tensor_shape, float_val=list(img.reshape(-1))) request.inputs['inputs'].CopyFrom(tensor)</span></span></code> </pre> <br>  The full Python script is <a href="https://gist.github.com/masroorhasan/0e73a7fc7bb2558c65933338d8194130">here</a> .  Run an updated initial client that makes a prediction request for optimized Tensorflow Serving: <br><br><pre> <code class="bash hljs">python tf_inception_grpc_client.py --image=images/pupper.jpg total time: 0.58314920859s</code> </pre> <br>  The following diagram shows the forecast execution time in an optimized version of Tensorflow Serving compared to the standard one, during 10 launches: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48d/990/b83/48d990b83dc54762fec809a580c450c8.png"><br><br>  The average delay decreased by approximately 3.38 times. <br><br><h1>  Bandwidth optimization </h1><br>  Tensorflow Serving can be configured to handle large amounts of data.  Bandwidth optimization is typically performed for ‚Äúoffline‚Äù batch processing, where tight delay boundaries are not a strict requirement. <br><br><h3>  Server side batch processing </h3><br>  As stated in the <a href="https://github.com/tensorflow/serving/tree/master/tensorflow_serving/batching">documentation</a> , server-side batch processing is natively supported in Tensorflow Serving. <br><br>  The tradeoffs between latency and throughput are determined by batch processing parameters.  They allow you to achieve the maximum bandwidth that hardware accelerators are capable of. <br><br>  To enable wrapping, set the <code>--enable_batching</code> and <code>--batching_parameters_file</code> .  Parameters are set according to <a href="https://github.com/tensorflow/serving/blob/d77c9768e33e1207ac8757cff56b9ed9a53f8765/tensorflow_serving/servables/tensorflow/session_bundle_config.proto">SessionBundleConfig</a> .  For systems on a CPU, set <code>num_batch_threads</code> to the number of available cores.  For GPUs, see the appropriate options <a href="">here</a> . <br><br>  After filling the whole package on the server side, the issuance requests are combined into one large request (tensor), and sent to the Tensorflow session by a combined request.  In such a situation, CPU / GPU parallelism is truly involved. <br><br>  Some common uses for Tensorflow batch processing include: <br><br><ul><li>  Using asynchronous client requests to populate server side packages <br></li><li>  Acceleration of batch processing due to the transfer of components of the model graph to the CPU / GPU <br></li><li>  Serial requests from several models from one server <br></li><li>  Batch processing is highly recommended for ‚Äúoffline‚Äù processing of a large number of requests. </li></ul><br><h3>  Client-side batch processing </h3><br>  Batch processing on the client side groups several incoming requests into one. <br><br>  Since the ResNet model is waiting for input in the NHWC format (the first dimension is the number of inputs), we can combine several input images into one RPC request: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">... </span></span>batch = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> jpeg <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(FLAGS.images_path): path = os.path.join(FLAGS.images_path, jpeg) img = cv2.imread(path).astype(np.float32) batch.append(img) ... batch_np = np.array(batch).astype(np.float32) dims = [tensor_shape_pb2.TensorShapeProto.Dim(size=dim) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> dim <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> batch_np.shape] t_shape = tensor_shape_pb2.TensorShapeProto(dim=dims) tensor = tensor_pb2.TensorProto( dtype=types_pb2.DT_FLOAT, tensor_shape=t_shape, float_val=list(batched_np.reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>))) request.inputs[<span class="hljs-string"><span class="hljs-string">'inputs'</span></span>].CopyFrom(tensor)</code> </pre> <br>  For a batch of N images, the output tensor in the response will contain the prediction results for the same number of inputs.  In our case, N = 2: <br><br><pre> <code class="python hljs">outputs { key: <span class="hljs-string"><span class="hljs-string">"classes"</span></span> value { dtype: DT_INT64 tensor_shape { dim { size: <span class="hljs-number"><span class="hljs-number">2</span></span> } } int64_val: <span class="hljs-number"><span class="hljs-number">238</span></span> int64_val: <span class="hljs-number"><span class="hljs-number">121</span></span> } } ...</code> </pre> <br><h1>  Hardware acceleration </h1><br>  A few words about graphics processors. <br><br>  The learning process naturally uses parallelization on the GPU, since the construction of deep neural networks requires massive computations to achieve the optimal solution. <br><br>  But for outputting results, parallelization is not so obvious.  Often, it is possible to speed up the output of a neural network to the GPU, but it is necessary to carefully select and test equipment, and to conduct in-depth technical and economic analysis.  Hardware paralleling is more valuable for batch processing of ‚Äúautonomous‚Äù outputs (massive volumes). <br><br>  Before moving to a GPU, consider business requirements with a thorough analysis of costs (cash, operational, technical) for the intended benefit (reduced latency, high throughput). </div><p>Source: <a href="https://habr.com/ru/post/445928/">https://habr.com/ru/post/445928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445918/index.html">20 years of RollerCoaster Tycoon: an interview with the creator of the game</a></li>
<li><a href="../445920/index.html">LIVE: How to curb iOS development in large teams</a></li>
<li><a href="../445922/index.html">Why watch online broadcasts if you can read Habr</a></li>
<li><a href="../445924/index.html">ARRIVALS: when smart watches get weird</a></li>
<li><a href="../445926/index.html">The US UFO secret program has also been researching wormholes and additional measurements.</a></li>
<li><a href="../445932/index.html">Client application security: practical tips for Front-end developer</a></li>
<li><a href="../445936/index.html">Electronics development. About microcontrollers on fingers</a></li>
<li><a href="../445940/index.html">AMA with Habr, v 7.0. Lemon, Donates and News</a></li>
<li><a href="../445946/index.html">MWC: instructions for use</a></li>
<li><a href="../445948/index.html">C ++ Inheritance: beginner, intermediate, advanced</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>